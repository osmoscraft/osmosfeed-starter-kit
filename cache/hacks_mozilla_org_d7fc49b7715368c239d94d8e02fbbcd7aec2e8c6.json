{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "Mozilla Hacks – the Web developer blog",
  "feed_url": "https://hacks.mozilla.org/feed/",
  "items": [
    {
      "id": "https://hacks.mozilla.org/?p=47880",
      "url": "https://hacks.mozilla.org/2022/06/fuzzing-rust-minidump-for-embarrassment-and-crashes/",
      "title": "Fuzzing rust-minidump for Embarrassment and Crashes – Part 2",
      "summary": "For the last year, we've been working on the development of rust-minidump, a pure-Rust replacement for the minidump-processing half of google-breakpad. The final part in this series takes you through fuzzing rust-minidump.\nThe post Fuzzing rust-minidump for Embarrassment and Crashes – Part 2 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">This is part 2 of a series of articles on rust-minidump. For part 1, see <a href=\"https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/\">here</a>.</span></p>\n<p><span style=\"font-weight: 400;\">So to recap, we rewrote breakpad&#8217;s minidump processor in Rust, wrote a ton of tests, and deployed to production without any issues. We killed it, perfect job.</span></p>\n<p><span style=\"font-weight: 400;\">And we </span><i><span style=\"font-weight: 400;\">still</span></i><span style=\"font-weight: 400;\"> got massively dunked on by the fuzzer. Just absolutely destroyed.</span></p>\n<p><span style=\"font-weight: 400;\">I was starting to pivot off of rust-minidump work because I needed a bit of palette cleanser before tackling round 2 (handling native debuginfo, filling in features for other groups who were interested in rust-minidump, adding extra analyses that we&#8217;d always wanted but were too much work to do in Breakpad, etc etc etc).</span></p>\n<p><span style=\"font-weight: 400;\">I was still getting some PRs from people filling in the corners they needed, but nothing that needed too much attention, a</span><span style=\"font-weight: 400;\">nd then</span><a href=\"https://github.com/5225225\"> <span style=\"font-weight: 400;\">@5225225</span></a><span style=\"font-weight: 400;\"> smashed through the windows and released a bunch of exploding fuzzy rabbits into my office. </span></p>\n<p><span style=\"font-weight: 400;\">I had no idea who they were or why they were there. When I asked they just lowered one of their seven pairs of sunglasses and said &#8220;Because I can. Now hold this bunny&#8221;. I did as I was told and held the bunny. It was a good bun. Dare I say, it was a true </span><i><span style=\"font-weight: 400;\">bnnuy</span></i><span style=\"font-weight: 400;\">: it was</span><a href=\"https://www.llvm.org/docs/LibFuzzer.html\"> <span style=\"font-weight: 400;\">libfuzzer</span></a><span style=\"font-weight: 400;\">. (Huh? You thought it was gonna be</span><a href=\"https://github.com/google/AFL\"> <span style=\"font-weight: 400;\">AFL</span></a><span style=\"font-weight: 400;\">? Weird.)</span></p>\n<p><span style=\"font-weight: 400;\">As it turns out, several folks had built out some </span><i><span style=\"font-weight: 400;\">really nice</span></i><span style=\"font-weight: 400;\"> infrastructure for quickly setting up a decent fuzzer for some Rust code:</span><a href=\"https://github.com/rust-fuzz/cargo-fuzz\"> <span style=\"font-weight: 400;\">cargo-fuzz</span></a><span style=\"font-weight: 400;\">. They even wrote</span><a href=\"https://rust-fuzz.github.io/book/cargo-fuzz.html\"> <span style=\"font-weight: 400;\">a little book that walks you through the process</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">Apparently those folks had done such a good job that 5225225 had decided it would be a really great hobby to just pick up a random rust project and implement fuzzing for it. And then to fuzz it. And file issues. And PRs that fix those issues. And then implement even more fuzzing for it.</span></p>\n<p><span style=\"font-weight: 400;\">Please help my office is drowning in rabbits and I haven&#8217;t seen my wife in weeks.</span></p>\n<p><span style=\"font-weight: 400;\">As far as I can tell, the process seems to genuinely be pretty easy! I think their</span><a href=\"https://github.com/luser/rust-minidump/pull/405\"> <span style=\"font-weight: 400;\">first fuzzer for rust-minidump</span></a><span style=\"font-weight: 400;\"> was basically just:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">checked out the project</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">run cargo fuzz init (which autogenerates a bunch of config files)</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">write a file with this:</span></li>\n</ul>\n<pre><span style=\"font-weight: 400;\">#![no_main]</span>\r\n\r\n<span style=\"font-weight: 400;\">use libfuzzer_sys::fuzz_target;</span>\r\n<span style=\"font-weight: 400;\">use minidump::*;</span>\r\n\r\n<span style=\"font-weight: 400;\">fuzz_target!(|data: &amp;[u8]| {</span>\r\n<span style=\"font-weight: 400;\">    // Parse a minidump like a normal user of the library</span>\r\n<span style=\"font-weight: 400;\">    if let Ok(dump) = minidump::Minidump::read(data) {</span>\r\n<span style=\"font-weight: 400;\">        // Ask the library to get+parse several streams like a normal user.</span>\r\n\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpAssertion&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpBreakpadInfo&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpCrashpadInfo&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpException&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpLinuxCpuInfo&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpLinuxEnviron&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpLinuxLsbRelease&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpLinuxMaps&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpLinuxProcStatus&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpMacCrashInfo&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpMemoryInfoList&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpMemoryList&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpMiscInfo&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpModuleList&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpSystemInfo&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpThreadNames&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpThreadList&gt;();</span>\r\n<span style=\"font-weight: 400;\">        let _ = dump.get_stream::&lt;MinidumpUnloadedModuleList&gt;();</span>\r\n<span style=\"font-weight: 400;\">    }</span>\r\n<span style=\"font-weight: 400;\">});</span></pre>\n<p><span style=\"font-weight: 400;\">And that&#8217;s&#8230; it? And all you have to do is type </span><span style=\"font-weight: 400;\">cargo fuzz run</span><span style=\"font-weight: 400;\"> and it downloads, builds, and spins up an instance of</span><a href=\"https://www.llvm.org/docs/LibFuzzer.html\"> <span style=\"font-weight: 400;\">libfuzzer</span></a><span style=\"font-weight: 400;\"> and finds bugs in your project overnight?</span></p>\n<p><span style=\"font-weight: 400;\">Surely that won&#8217;t find anything interesting. Oh it did? It was largely all bugs in code I wrote? </span><b>Nice.</b></p>\n<p><span style=\"font-weight: 400;\">cargo fuzz is clearly awesome but let&#8217;s not downplay the amount of bafflingly incredible work that 5225225 did here! Fuzzers, sanitizers, and other code analysis tools have a </span><i><span style=\"font-weight: 400;\">very bad</span></i><span style=\"font-weight: 400;\"> reputation for drive-by contributions. </span></p>\n<p><span style=\"font-weight: 400;\">I think we&#8217;ve all heard stories of someone running a shiny new tool on some big project they know nothing about, mass filing a bunch of issues that just say &#8220;this tool says your code has a problem, fix it&#8221; and then disappearing into the mist and claiming victory.</span></p>\n<p><span style=\"font-weight: 400;\">This is not a pleasant experience for someone trying to maintain a project. You&#8217;re dumping a lot on my plate if I don&#8217;t know the tool, have trouble running the tool, don&#8217;t know exactly how you ran it, etc. </span></p>\n<p><span style=\"font-weight: 400;\">It&#8217;s also very easy to come up with a huge pile of issues with very little sense of how significant they are. </span></p>\n<p><span style=\"font-weight: 400;\">Some things are only vaguely dubious, while others are horribly terrifying exploits. We only have so much time to work on stuff, you&#8217;ve gotta help us out!</span></p>\n<p><span style=\"font-weight: 400;\">And in this regard 5225225&#8217;s contributions were just, bloody beautiful. </span></p>\n<p><span style=\"font-weight: 400;\">Like, shockingly fantastic.</span></p>\n<p><span style=\"font-weight: 400;\">They wrote really clear and detailed issues. When I skimmed those issues and misunderstood them, they quickly clarified and got me on the same page. And then they submitted a fix for the issue before I even considered working on the fix. And quickly responded to review comments. I didn&#8217;t even bother asking them to squashing their commits because damnit they </span><i><span style=\"font-weight: 400;\">earned</span></i><span style=\"font-weight: 400;\"> those 3 commits in the tree to fix one overflow.</span></p>\n<p><span style=\"font-weight: 400;\">Then they submitted a PR to merge the fuzzer. They helped me understand how to use it and debug issues. Then they started asking questions about the project and started writing more fuzzers for other parts of it. And now there&#8217;s like 5 fuzzers and a bunch of fixed issues!</span></p>\n<p><span style=\"font-weight: 400;\">I don&#8217;t care how good cargo fuzz is, that&#8217;s a lot of friggin&#8217; really good work! Like I am going to cry!! This was so helpful??? <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f62d.png\" alt=\"😭\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></span></p>\n<p><span style=\"font-weight: 400;\">That said, I will take a </span><i><span style=\"font-weight: 400;\">little</span></i><span style=\"font-weight: 400;\"> credit for this going so smoothly: both Rust itself and rust-minidump are written in a way that&#8217;s very friendly to fuzzing. Specifically, rust-minidump is riddled with assertions for &#8220;hmm this seems messed up and shouldn&#8217;t happen but maybe?&#8221; and Rust turns integer overflows into panics (crashes) in debug builds (and index-out-of-bounds is always a panic).</span></p>\n<p><span style=\"font-weight: 400;\">Having lots of assertions everywhere makes it </span><i><span style=\"font-weight: 400;\">a lot</span></i><span style=\"font-weight: 400;\"> easier to detect situations where things go wrong. And when you </span><i><span style=\"font-weight: 400;\">do</span></i><span style=\"font-weight: 400;\"> detect that situation, the crash will often point pretty close to where things went wrong.</span></p>\n<p><span style=\"font-weight: 400;\">As someone who has worked on detecting bugs in Firefox with sanitizer and fuzzing folks, let me tell you what really sucks to try to do anything with: &#8220;Hey so on my machine this enormous complicated machine-generated input caused Firefox to crash </span><i><span style=\"font-weight: 400;\">somewhere</span></i><span style=\"font-weight: 400;\"> this </span><i><span style=\"font-weight: 400;\">one time</span></i><span style=\"font-weight: 400;\">. No, I can&#8217;t reproduce it. You won&#8217;t be able to reproduce it either. Anyway, try to fix it?&#8221;</span></p>\n<p><span style=\"font-weight: 400;\">That&#8217;s not me throwing shade on anyone here. I am all of the people in that conversation. The struggle of productively fuzzing Firefox is all too real, and I do not have a good track record of fixing those kinds of bugs. </span></p>\n<p><span style=\"font-weight: 400;\">By comparison I am absolutely </span><i><span style=\"font-weight: 400;\">thriving</span></i><span style=\"font-weight: 400;\"> under &#8220;Yeah you can deterministically trip this assertion with this tiny input you can just check in as a unit test&#8221;.</span></p>\n<p><span style=\"font-weight: 400;\">And what did we screw up? Some legit stuff! It&#8217;s Rust code, so I am fairly confident none of the issues were </span><i><span style=\"font-weight: 400;\">security</span></i><span style=\"font-weight: 400;\"> concerns, but they were definitely quality of implementation issues, and could have been used to at very least denial-of-service the minidump processor.</span></p>\n<p><span style=\"font-weight: 400;\">Now let&#8217;s dig into the issues they found!</span></p>\n<h2><b>#428: Corrupt stacks caused infinite loops until OOM on ARM64</b></h2>\n<p><a href=\"https://github.com/luser/rust-minidump/issues/428\"><span style=\"font-weight: 400;\">Issue</span></a></p>\n<p><span style=\"font-weight: 400;\">As noted in the background, stackwalking is a giant heuristic mess and you can find yourself going backwards or stuck in an infinite loop. To keep this under control, stackwalkers generally require </span><i><span style=\"font-weight: 400;\">forward progress</span></i><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">Specifically, they require the stack pointer to move down the stack. If the stack pointer ever goes backwards or stays the same, we just call it quits and end the stackwalk there.</span></p>\n<p><span style=\"font-weight: 400;\">However, you can&#8217;t be </span><i><span style=\"font-weight: 400;\">so</span></i><span style=\"font-weight: 400;\"> strict on ARM because leaf functions </span><i><span style=\"font-weight: 400;\">may not change the stack size at all</span></i><span style=\"font-weight: 400;\">. Normally this would be impossible because every function call </span><i><span style=\"font-weight: 400;\">at least</span></i><span style=\"font-weight: 400;\"> has to push the return address to the stack, but ARM has the </span><i><span style=\"font-weight: 400;\">link register</span></i><span style=\"font-weight: 400;\"> which is basically an extra buffer for the return address. </span></p>\n<p><span style=\"font-weight: 400;\">The existence of the link register in conjunction with an ABI that makes the callee responsible for saving and restoring it means leaf functions </span><i><span style=\"font-weight: 400;\">can</span></i><span style=\"font-weight: 400;\"> have 0-sized stack frames!</span></p>\n<p><span style=\"font-weight: 400;\">To handle this, an ARM stackwalker must allow for there to be no forward progress for the </span><i><span style=\"font-weight: 400;\">first</span></i><span style=\"font-weight: 400;\"> frame of a stackwalk, </span><b>and then become more strict</b><span style=\"font-weight: 400;\">. Unfortunately I hand-waved that second part and ended up allowing infinite loops with no forward progress:</span></p>\n<pre><span style=\"font-weight: 400;\">// If the new stack pointer is at a lower address than the old,</span>\r\n<span style=\"font-weight: 400;\">// then that's clearly incorrect. Treat this as end-of-stack to</span>\r\n<span style=\"font-weight: 400;\">// enforce progress and avoid infinite loops.</span>\r\n<span style=\"font-weight: 400;\">//</span>\r\n<span style=\"font-weight: 400;\">// NOTE: this check allows for equality because arm leaf functions</span>\r\n<span style=\"font-weight: 400;\">// may not actually touch the stack (thanks to the link register</span>\r\n<span style=\"font-weight: 400;\">// allowing you to \"push\" the return address to a register).</span>\r\n<span style=\"font-weight: 400;\">if frame.context.get_stack_pointer() &lt; self.get_register_always(\"sp\") as u64 {</span>\r\n<span style=\"font-weight: 400;\">    trace!(\"unwind: stack pointer went backwards, assuming unwind complete\");</span>\r\n<span style=\"font-weight: 400;\">    return None;</span>\r\n<span style=\"font-weight: 400;\">}</span></pre>\n<p><span style=\"font-weight: 400;\">So if the ARM64 stackwalker ever gets stuck in an infinite loop on one frame, it will just build up an infinite backtrace until it&#8217;s killed by an OOM. This is very nasty because it&#8217;s a potentially very slow denial-of-service that eats up all the memory on the machine!</span></p>\n<p><span style=\"font-weight: 400;\">This issue was actually originally discovered and fixed in</span><a href=\"https://github.com/luser/rust-minidump/issues/300\"> <span style=\"font-weight: 400;\">#300</span></a> <i><span style=\"font-weight: 400;\">without</span></i><span style=\"font-weight: 400;\"> a fuzzer, but when I fixed it for ARM (32-bit) I completely forgot to do the same for ARM64. Thankfully the fuzzer was evil enough to discover this infinite looping situation on its own, and the fix was just &#8220;copy-paste the logic from the 32-bit impl&#8221;.</span></p>\n<p><span style=\"font-weight: 400;\">Because this issue was actually encountered in the wild, we know this was a serious concern! Good job, fuzzer!</span></p>\n<p><span style=\"font-weight: 400;\">(This issue specifically affected minidump-processor and minidump-stackwalk)</span></p>\n<h3><b>#407: MinidumpLinuxMaps address-based queries didn&#8217;t work at all</b></h3>\n<p><a href=\"https://github.com/luser/rust-minidump/issues/407\"><span style=\"font-weight: 400;\">Issue</span></a></p>\n<p><span style=\"font-weight: 400;\">MinidumpLinuxMaps is an interface for querying the dumped contents of Linux&#8217;s /proc/self/maps file. This provides metadata on the permissions and allocation state for mapped ranges of memory in the crashing process.</span></p>\n<p><span style=\"font-weight: 400;\">There are two usecases for this: just getting a full dump of all the process state, and specifically querying the memory properties for a specific address (&#8220;hey is this address executable?&#8221;). The dump usecase is handled by just shoving everything in a Vec. The address usecase requires us to create a RangeMap over the entries.</span></p>\n<p><span style=\"font-weight: 400;\">Unfortunately, a comparison was flipped in the code that created the keys to the RangeMap, which resulted in every </span><i><span style=\"font-weight: 400;\">correct</span></i><span style=\"font-weight: 400;\"> memory range being discarded AND invalid memory ranges being accepted. The fuzzer was able to catch this because the invalid ranges tripped an assertion when they got fed into the RangeMap (hurray for redundant checks!).</span></p>\n<pre><span style=\"font-weight: 400;\">// OOPS</span>\r\n<span style=\"font-weight: 400;\">if self.base_address &lt; self.final_address { </span>\r\n<span style=\"font-weight: 400;\"> return None; </span>\r\n<span style=\"font-weight: 400;\">}</span></pre>\n<p><span style=\"font-weight: 400;\">Although tests were written for MinidumpLinuxMaps, they didn&#8217;t include any invalid ranges, and just used the dump interface, so the fact that the RangeMap was empty went unnoticed!</span></p>\n<p><span style=\"font-weight: 400;\">This </span><i><span style=\"font-weight: 400;\">probably</span></i><span style=\"font-weight: 400;\"> would have been quickly found as soon as anyone tried to actually use this API in practice, but it&#8217;s nice that we caught it beforehand! Hooray for fuzzers!</span></p>\n<p><span style=\"font-weight: 400;\">(This issue specifically affected the minidump crate which technically could affect minidump-processor and minidump-stackwalk. Although they didn&#8217;t yet actually do address queries, they may have crashed when fed invalid ranges.)</span></p>\n<h2><b>#381: OOM from reserving memory based on untrusted list length.</b></h2>\n<p><a href=\"https://github.com/luser/rust-minidump/issues/381\"><span style=\"font-weight: 400;\">Issue</span></a></p>\n<p><span style=\"font-weight: 400;\">Minidumps have lots of lists which we end up collecting up in a Vec or some other collection. It&#8217;s quite natural and more efficient to start this process with something like </span><span style=\"font-weight: 400;\">Vec::with_capacity(list_length)</span><span style=\"font-weight: 400;\">. Usually this is fine, but if the minidump is corrupt (or malicious), then this length could be impossibly large and cause us to immediately OOM.</span></p>\n<p><span style=\"font-weight: 400;\">We were broadly aware that this was a problem, and had discussed the issue in</span><a href=\"https://github.com/luser/rust-minidump/issues/326\"> <span style=\"font-weight: 400;\">#326</span></a><span style=\"font-weight: 400;\">, but then everyone left for the holidays. #381 was a nice kick in the pants to actually fix it, and gave us a free simple test case to check in.</span></p>\n<p><span style=\"font-weight: 400;\">Although the naive solution would be to fix this by just removing the reserves, we opted for a solution that guarded against obviously-incorrect array lengths. This allowed us to keep the performance win of reserving memory while also making rust-minidump fast-fail instead of vaguely trying to do something and hallucinating a mess.</span></p>\n<p><span style=\"font-weight: 400;\">Specifically, @Swatinem introduced a function for checking that the amount of memory left in the section we&#8217;re parsing is </span><i><span style=\"font-weight: 400;\">large enough</span></i><span style=\"font-weight: 400;\"> to even hold the claimed amount of items (based on their known serialized size). This should mean the minidump crate can only be induced to reserve O(n) memory, where n is the size of the minidump itself.</span></p>\n<p><span style=\"font-weight: 400;\">For some scale:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">A minidump for Firefox&#8217;s main process with about 100 threads is about 3MB.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">A minidump for a stackoverflow from infinite recursion (8MB stack, 9000 calls) is about 8MB.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">A breakpad symbol file for Firefox&#8217;s main module can be about </span><b>200MB</b><span style=\"font-weight: 400;\">.</span></li>\n</ul>\n<p><span style=\"font-weight: 400;\">If you&#8217;re symbolicating, Minidumps probably won&#8217;t be your memory bottleneck. <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f639.png\" alt=\"😹\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></span></p>\n<p><span style=\"font-weight: 400;\">(This issue specifically affected the minidump crate and therefore also minidump-processor and minidump-stackwalk.)</span></p>\n<h2><b>The Many Integer Overflows and My Greatest Defeat</b></h2>\n<p><span style=\"font-weight: 400;\">The rest of the issues found were relatively benign integer overflows. I claim they&#8217;re benign because rust-minidump should </span><i><span style=\"font-weight: 400;\">already</span></i><span style=\"font-weight: 400;\"> be working under the assumption that all the values it reads out of the minidump could be corrupt garbage. This means its code is riddled with &#8220;is this nonsense&#8221; checks and those usually very quickly catch an overflow (or at worst print a nonsense value for some pointer).</span></p>\n<p><span style=\"font-weight: 400;\">We still fixed them all, because that&#8217;s shaky as heck logic and we want to be robust. But yeah none of these were even denial-of-service issues, as far as I know.</span></p>\n<p><span style=\"font-weight: 400;\">To demonstrate this, let&#8217;s discuss the most evil and embarrassing overflow which was definitely my fault and I am </span><i><span style=\"font-weight: 400;\">still</span></i><span style=\"font-weight: 400;\"> mad about it but in a like &#8220;how the heck&#8221; kind of way!?</span></p>\n<p><span style=\"font-weight: 400;\">The overflow is back in our old friend the stackwalker. Specifically in the code that attempts to unwind using frame pointers. Even more specifically, when offsetting the supposed frame-pointer to get the location of the supposed return address:</span></p>\n<pre><span style=\"font-weight: 400;\">let caller_ip = stack_memory.get_memory_at_address(last_bp + POINTER_WIDTH)?;</span>\r\n<span style=\"font-weight: 400;\">let caller_bp = stack_memory.get_memory_at_address(last_bp)?;</span>\r\n<span style=\"font-weight: 400;\">let caller_sp = last_bp + POINTER_WIDTH * 2;</span></pre>\n<p><span style=\"font-weight: 400;\">If the frame pointer (</span><span style=\"font-weight: 400;\">last_bp</span><span style=\"font-weight: 400;\">) was ~</span><span style=\"font-weight: 400;\">u64::MAX</span><span style=\"font-weight: 400;\">, the offset on the first line would overflow and we would instead try to load ~null. All of our loads are explicitly fallible (we assume everything is corrupt garbage!), and nothing is ever mapped to the null page in normal applications, so this load would reliably fail as if we had guarded the overflow. Hooray!</span></p>\n<p><span style=\"font-weight: 400;\">&#8230;but the overflow would panic in debug builds because that&#8217;s how debug builds work in Rust!</span></p>\n<p><span style=\"font-weight: 400;\">This was actually found, reported, and fixed </span><i><span style=\"font-weight: 400;\">without</span></i><span style=\"font-weight: 400;\"> a fuzzer in</span><a href=\"https://github.com/luser/rust-minidump/issues/251\"> <span style=\"font-weight: 400;\">#251</span></a><span style=\"font-weight: 400;\">. All it took was a simple guard:</span></p>\n<p><span style=\"font-weight: 400;\">(All the casts are because this specific code is used in the x86 impl </span><i><span style=\"font-weight: 400;\">and</span></i><span style=\"font-weight: 400;\"> the x64 impl.)</span></p>\n<pre><span style=\"font-weight: 400;\">if last_bp as u64 &gt;= u64::MAX - POINTER_WIDTH as u64 * 2 {</span>\r\n<span style=\"font-weight: 400;\">    // Although this code generally works fine if the pointer math overflows,</span>\r\n<span style=\"font-weight: 400;\">    // debug builds will still panic, and this guard protects against it without</span>\r\n<span style=\"font-weight: 400;\">    // drowning the rest of the code in checked_add.</span>\r\n<span style=\"font-weight: 400;\">    return None;</span>\r\n<span style=\"font-weight: 400;\">}</span>\r\n\r\n<span style=\"font-weight: 400;\">let caller_ip = stack_memory.get_memory_at_address(last_bp as u64 + POINTER_WIDTH as u64)?;</span>\r\n<span style=\"font-weight: 400;\">let caller_bp = stack_memory.get_memory_at_address(last_bp as u64)?;</span>\r\n<span style=\"font-weight: 400;\">let caller_sp = last_bp + POINTER_WIDTH * 2;</span></pre>\n<p><span style=\"font-weight: 400;\">And then it was found, reported, and fixed </span><b>again</b> <i><span style=\"font-weight: 400;\">with a fuzzer</span></i><span style=\"font-weight: 400;\"> in</span><a href=\"https://github.com/luser/rust-minidump/issues/422\"> <span style=\"font-weight: 400;\">#422</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">Wait what?</span></p>\n<p><span style=\"font-weight: 400;\">Unlike the infinite loop bug, I </span><i><span style=\"font-weight: 400;\">did</span></i><span style=\"font-weight: 400;\"> remember to add guards to all the unwinders for this problem&#8230; but I did the overflow check in 64-bit </span><i><span style=\"font-weight: 400;\">even for the 32-bit platforms</span></i><span style=\"font-weight: 400;\">.</span></p>\n<p><b>slaps forehead</b></p>\n<p><span style=\"font-weight: 400;\">This made the bug report especially confusing at first because the overflow was like 3 lines away from </span><i><span style=\"font-weight: 400;\">a guard for that exact overflow</span></i><span style=\"font-weight: 400;\">. As it turns out, the mistake wasn&#8217;t actually as obvious as it sounds! To understand what went wrong, let&#8217;s talk a bit more about pointer width in minidumps.</span></p>\n<p><span style=\"font-weight: 400;\">A single instance of rust-minidump has to be able to handle crash reports from </span><i><span style=\"font-weight: 400;\">any</span></i><span style=\"font-weight: 400;\"> platform, even ones it isn&#8217;t natively running on. This means it needs to be able to handle both 32-bit and 64-bit platforms in one binary. To avoid the misery of copy-pasting everything or making everything generic over pointer size, rust-minidump prefers to work with 64-bit values wherever possible, even for 32-bit plaftorms.</span></p>\n<p><span style=\"font-weight: 400;\">This isn&#8217;t just us being lazy: the minidump format itself does this! Regardless of the platform, a minidump will refer to ranges of memory with a</span><a href=\"https://docs.microsoft.com/en-us/windows/win32/api/minidumpapiset/ns-minidumpapiset-minidump_memory_descriptor\"> <span style=\"font-weight: 400;\">MINIDUMP_MEMORY_DESCRIPTOR</span></a><span style=\"font-weight: 400;\"> whose base address is a 64-bit value, even on 32-bit platforms!</span></p>\n<pre><span style=\"font-weight: 400;\">typedef struct _MINIDUMP_MEMORY_DESCRIPTOR {</span>\r\n<span style=\"font-weight: 400;\">  ULONG64                      StartOfMemoryRange;</span>\r\n<span style=\"font-weight: 400;\">  MINIDUMP_LOCATION_DESCRIPTOR Memory;</span>\r\n<span style=\"font-weight: 400;\">} MINIDUMP_MEMORY_DESCRIPTOR, *PMINIDUMP_MEMORY_DESCRIPTOR;</span></pre>\n<p><span style=\"font-weight: 400;\">So quite naturally rust-minidump&#8217;s interface for querying saved regions of memory just operates on 64-bit (u64) addresses unconditionally, and 32-bit-specific code casts its u32 address to a u64 before querying memory.</span></p>\n<p><span style=\"font-weight: 400;\">That means the code with the overflow guard </span><i><span style=\"font-weight: 400;\">was</span></i><span style=\"font-weight: 400;\"> manipulating those values as u64s on x86! The </span><i><span style=\"font-weight: 400;\">problem</span></i><span style=\"font-weight: 400;\"> is that after all the memory loads we would then go back to &#8220;native&#8221; sizes and compute </span><span style=\"font-weight: 400;\">caller_sp = last_bp + POINTER_WIDTH * 2</span><span style=\"font-weight: 400;\">. This would overflow a u32 and crash in debug builds. <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f63f.png\" alt=\"😿\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></span></p>\n<p><span style=\"font-weight: 400;\">But here&#8217;s the really messed up part: </span><i><span style=\"font-weight: 400;\">getting to that point meant we were successfully loading memory up to that address</span></i><span style=\"font-weight: 400;\">. The first line where we compute caller_ip reads it! So this overflow means&#8230; we were&#8230; loading memory&#8230; from an address that was beyond u32::MAX&#8230;!?</span></p>\n<p><span style=\"font-weight: 400;\">Yes!!!!!!!!</span></p>\n<p><span style=\"font-weight: 400;\">The fuzzer had found an absolutely </span><i><span style=\"font-weight: 400;\">brilliantly evil input</span></i><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">It abused the fact that MINIDUMP_MEMORY_DESCRIPTOR </span><i><span style=\"font-weight: 400;\">technically</span></i><span style=\"font-weight: 400;\"> lets 32-bit minidumps define memory ranges beyond </span><span style=\"font-weight: 400;\">u32::MAX</span> <i><span style=\"font-weight: 400;\">even though they could never actually access that memory!</span></i><span style=\"font-weight: 400;\"> It could then have the u64-based memory accesses succeed but still have the &#8220;native&#8221; 32-bit operation overflow!</span></p>\n<p><span style=\"font-weight: 400;\">This is so messed up that I didn&#8217;t even </span><i><span style=\"font-weight: 400;\">comprehend</span></i><span style=\"font-weight: 400;\"> that it had done this until I wrote my own test and realized that it wasn&#8217;t actually failing because I </span><i><span style=\"font-weight: 400;\">foolishly</span></i><span style=\"font-weight: 400;\"> had limited the range of valid memory to the mere 4GB a normal x86 process is restricted to.</span></p>\n<p><span style=\"font-weight: 400;\">And I mean that quite literally: this is exactly the issue that creates</span><a href=\"https://youtu.be/kpk2tdsPh0A?t=638\"> <span style=\"font-weight: 400;\">Parallel Universes in Super Mario 64</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">But hey my code was probably just bad. I know google loves sanitizers and fuzzers, so I bet google breakpad found this overflow ages ago and fixed it:</span></p>\n<pre><span style=\"font-weight: 400;\">uint32_t last_esp = last_frame-&gt;context.esp;</span>\r\n<span style=\"font-weight: 400;\">uint32_t last_ebp = last_frame-&gt;context.ebp;</span>\r\n<span style=\"font-weight: 400;\">uint32_t caller_eip, caller_esp, caller_ebp;</span>\r\n\r\n<span style=\"font-weight: 400;\">if (memory_-&gt;GetMemoryAtAddress(last_ebp + 4, &amp;caller_eip) &amp;&amp;</span>\r\n<span style=\"font-weight: 400;\">    memory_-&gt;GetMemoryAtAddress(last_ebp, &amp;caller_ebp)) {</span>\r\n<span style=\"font-weight: 400;\">    caller_esp = last_ebp + 8;</span>\r\n<span style=\"font-weight: 400;\">    trust = StackFrame::FRAME_TRUST_FP;</span>\r\n<span style=\"font-weight: 400;\">} else {</span>\r\n<span style=\"font-weight: 400;\">    ...</span></pre>\n<p><span style=\"font-weight: 400;\">Ah. Hmm. They don&#8217;t guard for any kind of overflow for those uint32_t&#8217;s (or the uint64_t&#8217;s in the x64 impl).</span></p>\n<p><span style=\"font-weight: 400;\">Well ok GetMemoryAtAddress does actual bounds checks so the load from ~null will generally fail like it does in rust-minidump. But what about the Parallel Universe overflow that lets GetMemoryAtAddress succeed?</span></p>\n<p><span style=\"font-weight: 400;\">Ah well surely breakpad is more principled with integer width than I was&#8211;</span></p>\n<pre><span style=\"font-weight: 400;\">virtual bool GetMemoryAtAddress(uint64_t address, uint8_t*  value) const = 0;</span>\r\n<span style=\"font-weight: 400;\">virtual bool GetMemoryAtAddress(uint64_t address, uint16_t* value) const = 0;</span>\r\n<span style=\"font-weight: 400;\">virtual bool GetMemoryAtAddress(uint64_t address, uint32_t* value) const = 0;</span>\r\n<span style=\"font-weight: 400;\">virtual bool GetMemoryAtAddress(uint64_t address, uint64_t* value) const = 0;</span>\r\n</pre>\n<p><span style=\"font-weight: 400;\">Whelp congrats to 5225225 for finding an overflow that&#8217;s portable between two implementations in two completely different languages by exploiting the very nature of the file format itself!</span></p>\n<p><span style=\"font-weight: 400;\">In case you&#8217;re wondering what the implications of this overflow are: it&#8217;s still basically benign. Both rust-minidump and google-breakpad will successfully complete the frame pointer analysis and yield a frame with a ~null stack pointer.</span></p>\n<p><span style=\"font-weight: 400;\">Then the outer layer of the stackwalker which runs all the different passes in sequence will see something succeeded but that the frame pointer went backwards. At this point it will discard the stack frame and terminate the stackwalk normally and just calmly output whatever the backtrace was up to that point. Totally normal and reasonable operation.</span></p>\n<p><span style=\"font-weight: 400;\">I expect this is why no one would notice this in breakpad even if you run fuzzers and sanitizers on it: nothing in the code actually does anything </span><i><span style=\"font-weight: 400;\">wrong</span></i><span style=\"font-weight: 400;\">. Unsigned integers are defined to wrap, the program behaves reasonably, everything is </span><i><span style=\"font-weight: 400;\">kinda</span></i><span style=\"font-weight: 400;\"> fine. We only noticed this in rust-minidump because </span><i><span style=\"font-weight: 400;\">all</span></i><span style=\"font-weight: 400;\"> integer overflows panic in Rust debug builds.</span></p>\n<p><span style=\"font-weight: 400;\">However this &#8220;benign&#8221; behaviour </span><i><span style=\"font-weight: 400;\">is</span></i><span style=\"font-weight: 400;\"> slightly different from properly guarding the overflow. Both implementations will normally try to move on to </span><i><span style=\"font-weight: 400;\">stack scanning</span></i><span style=\"font-weight: 400;\"> when the frame pointer analysis fails, but in this case they give up immediately. It&#8217;s </span><i><span style=\"font-weight: 400;\">important</span></i><span style=\"font-weight: 400;\"> that the frame pointer analysis properly identifies failures so that this cascading can occur. Failing to do so is definitely a bug!</span></p>\n<p><span style=\"font-weight: 400;\">However in this case the stack is partially in a parallel universe, so getting any kind of useful backtrace out of it is&#8230; dubious to say the least.</span></p>\n<p><span style=\"font-weight: 400;\">So I totally stand by &#8220;this is totally benign and not actually a problem&#8221; but also &#8220;this is sketchy and we should have the bounds check so we can be confident in this code&#8217;s robustness and correctness&#8221;.</span></p>\n<p><span style=\"font-weight: 400;\">Minidumps are </span><i><span style=\"font-weight: 400;\">all</span></i><span style=\"font-weight: 400;\"> corner cases &#8212; they literally get generated </span><i><span style=\"font-weight: 400;\">when a program encounters an unexpected corner case</span></i><span style=\"font-weight: 400;\">! It&#8217;s </span><i><span style=\"font-weight: 400;\">so</span></i><span style=\"font-weight: 400;\"> tempting to constantly shrug off situations as &#8220;well no reasonable program would ever do this, so we can ignore it&#8221;&#8230; but YOU CAN&#8217;T.</span></p>\n<p><span style=\"font-weight: 400;\">You would not have a minidump at your doorstep if the program had behaved reasonably! The fact that you are trying to inspect a minidump means something messed up happened, and you need to just deal with it!</span></p>\n<p><span style=\"font-weight: 400;\">That&#8217;s why we put so much energy into testing this thing, it&#8217;s a nightmare!</span></p>\n<p><span style=\"font-weight: 400;\">I am </span><i><span style=\"font-weight: 400;\">extremely</span></i><span style=\"font-weight: 400;\"> paranoid about this stuff, but that paranoia is based on the horrors I have seen. There are always more corner cases. </span></p>\n<p><span style=\"font-weight: 400;\">There are </span><i><span style=\"font-weight: 400;\">ALWAYS</span></i><span style=\"font-weight: 400;\"> more corner cases. </span><b>ALWAYS</b><span style=\"font-weight: 400;\">.</span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/fuzzing-rust-minidump-for-embarrassment-and-crashes/\">Fuzzing rust-minidump for Embarrassment and Crashes &#8211; Part 2</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "This is part 2 of a series of articles on rust-minidump. For part 1, see here.\nSo to recap, we rewrote breakpad’s minidump processor in Rust, wrote a ton of tests, and deployed to production without any issues. We killed it, perfect job.\nAnd we still got massively dunked on by the fuzzer. Just absolutely destroyed.\nI was starting to pivot off of rust-minidump work because I needed a bit of palette cleanser before tackling round 2 (handling native debuginfo, filling in features for other groups who were interested in rust-minidump, adding extra analyses that we’d always wanted but were too much work to do in Breakpad, etc etc etc).\nI was still getting some PRs from people filling in the corners they needed, but nothing that needed too much attention, and then @5225225 smashed through the windows and released a bunch of exploding fuzzy rabbits into my office. \nI had no idea who they were or why they were there. When I asked they just lowered one of their seven pairs of sunglasses and said “Because I can. Now hold this bunny”. I did as I was told and held the bunny. It was a good bun. Dare I say, it was a true bnnuy: it was libfuzzer. (Huh? You thought it was gonna be AFL? Weird.)\nAs it turns out, several folks had built out some really nice infrastructure for quickly setting up a decent fuzzer for some Rust code: cargo-fuzz. They even wrote a little book that walks you through the process.\nApparently those folks had done such a good job that 5225225 had decided it would be a really great hobby to just pick up a random rust project and implement fuzzing for it. And then to fuzz it. And file issues. And PRs that fix those issues. And then implement even more fuzzing for it.\nPlease help my office is drowning in rabbits and I haven’t seen my wife in weeks.\nAs far as I can tell, the process seems to genuinely be pretty easy! I think their first fuzzer for rust-minidump was basically just:\n\nchecked out the project\nrun cargo fuzz init (which autogenerates a bunch of config files)\nwrite a file with this:\n\n#![no_main]\n\nuse libfuzzer_sys::fuzz_target;\nuse minidump::*;\n\nfuzz_target!(|data: &[u8]| {\n    // Parse a minidump like a normal user of the library\n    if let Ok(dump) = minidump::Minidump::read(data) {\n        // Ask the library to get+parse several streams like a normal user.\n\n        let _ = dump.get_stream::<MinidumpAssertion>();\n        let _ = dump.get_stream::<MinidumpBreakpadInfo>();\n        let _ = dump.get_stream::<MinidumpCrashpadInfo>();\n        let _ = dump.get_stream::<MinidumpException>();\n        let _ = dump.get_stream::<MinidumpLinuxCpuInfo>();\n        let _ = dump.get_stream::<MinidumpLinuxEnviron>();\n        let _ = dump.get_stream::<MinidumpLinuxLsbRelease>();\n        let _ = dump.get_stream::<MinidumpLinuxMaps>();\n        let _ = dump.get_stream::<MinidumpLinuxProcStatus>();\n        let _ = dump.get_stream::<MinidumpMacCrashInfo>();\n        let _ = dump.get_stream::<MinidumpMemoryInfoList>();\n        let _ = dump.get_stream::<MinidumpMemoryList>();\n        let _ = dump.get_stream::<MinidumpMiscInfo>();\n        let _ = dump.get_stream::<MinidumpModuleList>();\n        let _ = dump.get_stream::<MinidumpSystemInfo>();\n        let _ = dump.get_stream::<MinidumpThreadNames>();\n        let _ = dump.get_stream::<MinidumpThreadList>();\n        let _ = dump.get_stream::<MinidumpUnloadedModuleList>();\n    }\n});\nAnd that’s… it? And all you have to do is type cargo fuzz run and it downloads, builds, and spins up an instance of libfuzzer and finds bugs in your project overnight?\nSurely that won’t find anything interesting. Oh it did? It was largely all bugs in code I wrote? Nice.\ncargo fuzz is clearly awesome but let’s not downplay the amount of bafflingly incredible work that 5225225 did here! Fuzzers, sanitizers, and other code analysis tools have a very bad reputation for drive-by contributions. \nI think we’ve all heard stories of someone running a shiny new tool on some big project they know nothing about, mass filing a bunch of issues that just say “this tool says your code has a problem, fix it” and then disappearing into the mist and claiming victory.\nThis is not a pleasant experience for someone trying to maintain a project. You’re dumping a lot on my plate if I don’t know the tool, have trouble running the tool, don’t know exactly how you ran it, etc. \nIt’s also very easy to come up with a huge pile of issues with very little sense of how significant they are. \nSome things are only vaguely dubious, while others are horribly terrifying exploits. We only have so much time to work on stuff, you’ve gotta help us out!\nAnd in this regard 5225225’s contributions were just, bloody beautiful. \nLike, shockingly fantastic.\nThey wrote really clear and detailed issues. When I skimmed those issues and misunderstood them, they quickly clarified and got me on the same page. And then they submitted a fix for the issue before I even considered working on the fix. And quickly responded to review comments. I didn’t even bother asking them to squashing their commits because damnit they earned those 3 commits in the tree to fix one overflow.\nThen they submitted a PR to merge the fuzzer. They helped me understand how to use it and debug issues. Then they started asking questions about the project and started writing more fuzzers for other parts of it. And now there’s like 5 fuzzers and a bunch of fixed issues!\nI don’t care how good cargo fuzz is, that’s a lot of friggin’ really good work! Like I am going to cry!! This was so helpful??? \nThat said, I will take a little credit for this going so smoothly: both Rust itself and rust-minidump are written in a way that’s very friendly to fuzzing. Specifically, rust-minidump is riddled with assertions for “hmm this seems messed up and shouldn’t happen but maybe?” and Rust turns integer overflows into panics (crashes) in debug builds (and index-out-of-bounds is always a panic).\nHaving lots of assertions everywhere makes it a lot easier to detect situations where things go wrong. And when you do detect that situation, the crash will often point pretty close to where things went wrong.\nAs someone who has worked on detecting bugs in Firefox with sanitizer and fuzzing folks, let me tell you what really sucks to try to do anything with: “Hey so on my machine this enormous complicated machine-generated input caused Firefox to crash somewhere this one time. No, I can’t reproduce it. You won’t be able to reproduce it either. Anyway, try to fix it?”\nThat’s not me throwing shade on anyone here. I am all of the people in that conversation. The struggle of productively fuzzing Firefox is all too real, and I do not have a good track record of fixing those kinds of bugs. \nBy comparison I am absolutely thriving under “Yeah you can deterministically trip this assertion with this tiny input you can just check in as a unit test”.\nAnd what did we screw up? Some legit stuff! It’s Rust code, so I am fairly confident none of the issues were security concerns, but they were definitely quality of implementation issues, and could have been used to at very least denial-of-service the minidump processor.\nNow let’s dig into the issues they found!\n#428: Corrupt stacks caused infinite loops until OOM on ARM64\nIssue\nAs noted in the background, stackwalking is a giant heuristic mess and you can find yourself going backwards or stuck in an infinite loop. To keep this under control, stackwalkers generally require forward progress. \nSpecifically, they require the stack pointer to move down the stack. If the stack pointer ever goes backwards or stays the same, we just call it quits and end the stackwalk there.\nHowever, you can’t be so strict on ARM because leaf functions may not change the stack size at all. Normally this would be impossible because every function call at least has to push the return address to the stack, but ARM has the link register which is basically an extra buffer for the return address. \nThe existence of the link register in conjunction with an ABI that makes the callee responsible for saving and restoring it means leaf functions can have 0-sized stack frames!\nTo handle this, an ARM stackwalker must allow for there to be no forward progress for the first frame of a stackwalk, and then become more strict. Unfortunately I hand-waved that second part and ended up allowing infinite loops with no forward progress:\n// If the new stack pointer is at a lower address than the old,\n// then that's clearly incorrect. Treat this as end-of-stack to\n// enforce progress and avoid infinite loops.\n//\n// NOTE: this check allows for equality because arm leaf functions\n// may not actually touch the stack (thanks to the link register\n// allowing you to \"push\" the return address to a register).\nif frame.context.get_stack_pointer() < self.get_register_always(\"sp\") as u64 {\n    trace!(\"unwind: stack pointer went backwards, assuming unwind complete\");\n    return None;\n}\nSo if the ARM64 stackwalker ever gets stuck in an infinite loop on one frame, it will just build up an infinite backtrace until it’s killed by an OOM. This is very nasty because it’s a potentially very slow denial-of-service that eats up all the memory on the machine!\nThis issue was actually originally discovered and fixed in #300 without a fuzzer, but when I fixed it for ARM (32-bit) I completely forgot to do the same for ARM64. Thankfully the fuzzer was evil enough to discover this infinite looping situation on its own, and the fix was just “copy-paste the logic from the 32-bit impl”.\nBecause this issue was actually encountered in the wild, we know this was a serious concern! Good job, fuzzer!\n(This issue specifically affected minidump-processor and minidump-stackwalk)\n#407: MinidumpLinuxMaps address-based queries didn’t work at all\nIssue\nMinidumpLinuxMaps is an interface for querying the dumped contents of Linux’s /proc/self/maps file. This provides metadata on the permissions and allocation state for mapped ranges of memory in the crashing process.\nThere are two usecases for this: just getting a full dump of all the process state, and specifically querying the memory properties for a specific address (“hey is this address executable?”). The dump usecase is handled by just shoving everything in a Vec. The address usecase requires us to create a RangeMap over the entries.\nUnfortunately, a comparison was flipped in the code that created the keys to the RangeMap, which resulted in every correct memory range being discarded AND invalid memory ranges being accepted. The fuzzer was able to catch this because the invalid ranges tripped an assertion when they got fed into the RangeMap (hurray for redundant checks!).\n// OOPS\nif self.base_address < self.final_address { \n return None; \n}\nAlthough tests were written for MinidumpLinuxMaps, they didn’t include any invalid ranges, and just used the dump interface, so the fact that the RangeMap was empty went unnoticed!\nThis probably would have been quickly found as soon as anyone tried to actually use this API in practice, but it’s nice that we caught it beforehand! Hooray for fuzzers!\n(This issue specifically affected the minidump crate which technically could affect minidump-processor and minidump-stackwalk. Although they didn’t yet actually do address queries, they may have crashed when fed invalid ranges.)\n#381: OOM from reserving memory based on untrusted list length.\nIssue\nMinidumps have lots of lists which we end up collecting up in a Vec or some other collection. It’s quite natural and more efficient to start this process with something like Vec::with_capacity(list_length). Usually this is fine, but if the minidump is corrupt (or malicious), then this length could be impossibly large and cause us to immediately OOM.\nWe were broadly aware that this was a problem, and had discussed the issue in #326, but then everyone left for the holidays. #381 was a nice kick in the pants to actually fix it, and gave us a free simple test case to check in.\nAlthough the naive solution would be to fix this by just removing the reserves, we opted for a solution that guarded against obviously-incorrect array lengths. This allowed us to keep the performance win of reserving memory while also making rust-minidump fast-fail instead of vaguely trying to do something and hallucinating a mess.\nSpecifically, @Swatinem introduced a function for checking that the amount of memory left in the section we’re parsing is large enough to even hold the claimed amount of items (based on their known serialized size). This should mean the minidump crate can only be induced to reserve O(n) memory, where n is the size of the minidump itself.\nFor some scale:\n\nA minidump for Firefox’s main process with about 100 threads is about 3MB.\nA minidump for a stackoverflow from infinite recursion (8MB stack, 9000 calls) is about 8MB.\nA breakpad symbol file for Firefox’s main module can be about 200MB.\n\nIf you’re symbolicating, Minidumps probably won’t be your memory bottleneck. \n(This issue specifically affected the minidump crate and therefore also minidump-processor and minidump-stackwalk.)\nThe Many Integer Overflows and My Greatest Defeat\nThe rest of the issues found were relatively benign integer overflows. I claim they’re benign because rust-minidump should already be working under the assumption that all the values it reads out of the minidump could be corrupt garbage. This means its code is riddled with “is this nonsense” checks and those usually very quickly catch an overflow (or at worst print a nonsense value for some pointer).\nWe still fixed them all, because that’s shaky as heck logic and we want to be robust. But yeah none of these were even denial-of-service issues, as far as I know.\nTo demonstrate this, let’s discuss the most evil and embarrassing overflow which was definitely my fault and I am still mad about it but in a like “how the heck” kind of way!?\nThe overflow is back in our old friend the stackwalker. Specifically in the code that attempts to unwind using frame pointers. Even more specifically, when offsetting the supposed frame-pointer to get the location of the supposed return address:\nlet caller_ip = stack_memory.get_memory_at_address(last_bp + POINTER_WIDTH)?;\nlet caller_bp = stack_memory.get_memory_at_address(last_bp)?;\nlet caller_sp = last_bp + POINTER_WIDTH * 2;\nIf the frame pointer (last_bp) was ~u64::MAX, the offset on the first line would overflow and we would instead try to load ~null. All of our loads are explicitly fallible (we assume everything is corrupt garbage!), and nothing is ever mapped to the null page in normal applications, so this load would reliably fail as if we had guarded the overflow. Hooray!\n…but the overflow would panic in debug builds because that’s how debug builds work in Rust!\nThis was actually found, reported, and fixed without a fuzzer in #251. All it took was a simple guard:\n(All the casts are because this specific code is used in the x86 impl and the x64 impl.)\nif last_bp as u64 >= u64::MAX - POINTER_WIDTH as u64 * 2 {\n    // Although this code generally works fine if the pointer math overflows,\n    // debug builds will still panic, and this guard protects against it without\n    // drowning the rest of the code in checked_add.\n    return None;\n}\n\nlet caller_ip = stack_memory.get_memory_at_address(last_bp as u64 + POINTER_WIDTH as u64)?;\nlet caller_bp = stack_memory.get_memory_at_address(last_bp as u64)?;\nlet caller_sp = last_bp + POINTER_WIDTH * 2;\nAnd then it was found, reported, and fixed again with a fuzzer in #422.\nWait what?\nUnlike the infinite loop bug, I did remember to add guards to all the unwinders for this problem… but I did the overflow check in 64-bit even for the 32-bit platforms.\nslaps forehead\nThis made the bug report especially confusing at first because the overflow was like 3 lines away from a guard for that exact overflow. As it turns out, the mistake wasn’t actually as obvious as it sounds! To understand what went wrong, let’s talk a bit more about pointer width in minidumps.\nA single instance of rust-minidump has to be able to handle crash reports from any platform, even ones it isn’t natively running on. This means it needs to be able to handle both 32-bit and 64-bit platforms in one binary. To avoid the misery of copy-pasting everything or making everything generic over pointer size, rust-minidump prefers to work with 64-bit values wherever possible, even for 32-bit plaftorms.\nThis isn’t just us being lazy: the minidump format itself does this! Regardless of the platform, a minidump will refer to ranges of memory with a MINIDUMP_MEMORY_DESCRIPTOR whose base address is a 64-bit value, even on 32-bit platforms!\ntypedef struct _MINIDUMP_MEMORY_DESCRIPTOR {\n  ULONG64                      StartOfMemoryRange;\n  MINIDUMP_LOCATION_DESCRIPTOR Memory;\n} MINIDUMP_MEMORY_DESCRIPTOR, *PMINIDUMP_MEMORY_DESCRIPTOR;\nSo quite naturally rust-minidump’s interface for querying saved regions of memory just operates on 64-bit (u64) addresses unconditionally, and 32-bit-specific code casts its u32 address to a u64 before querying memory.\nThat means the code with the overflow guard was manipulating those values as u64s on x86! The problem is that after all the memory loads we would then go back to “native” sizes and compute caller_sp = last_bp + POINTER_WIDTH * 2. This would overflow a u32 and crash in debug builds. \nBut here’s the really messed up part: getting to that point meant we were successfully loading memory up to that address. The first line where we compute caller_ip reads it! So this overflow means… we were… loading memory… from an address that was beyond u32::MAX…!?\nYes!!!!!!!!\nThe fuzzer had found an absolutely brilliantly evil input.\nIt abused the fact that MINIDUMP_MEMORY_DESCRIPTOR technically lets 32-bit minidumps define memory ranges beyond u32::MAX even though they could never actually access that memory! It could then have the u64-based memory accesses succeed but still have the “native” 32-bit operation overflow!\nThis is so messed up that I didn’t even comprehend that it had done this until I wrote my own test and realized that it wasn’t actually failing because I foolishly had limited the range of valid memory to the mere 4GB a normal x86 process is restricted to.\nAnd I mean that quite literally: this is exactly the issue that creates Parallel Universes in Super Mario 64.\nBut hey my code was probably just bad. I know google loves sanitizers and fuzzers, so I bet google breakpad found this overflow ages ago and fixed it:\nuint32_t last_esp = last_frame->context.esp;\nuint32_t last_ebp = last_frame->context.ebp;\nuint32_t caller_eip, caller_esp, caller_ebp;\n\nif (memory_->GetMemoryAtAddress(last_ebp + 4, &caller_eip) &&\n    memory_->GetMemoryAtAddress(last_ebp, &caller_ebp)) {\n    caller_esp = last_ebp + 8;\n    trust = StackFrame::FRAME_TRUST_FP;\n} else {\n    ...\nAh. Hmm. They don’t guard for any kind of overflow for those uint32_t’s (or the uint64_t’s in the x64 impl).\nWell ok GetMemoryAtAddress does actual bounds checks so the load from ~null will generally fail like it does in rust-minidump. But what about the Parallel Universe overflow that lets GetMemoryAtAddress succeed?\nAh well surely breakpad is more principled with integer width than I was–\nvirtual bool GetMemoryAtAddress(uint64_t address, uint8_t*  value) const = 0;\nvirtual bool GetMemoryAtAddress(uint64_t address, uint16_t* value) const = 0;\nvirtual bool GetMemoryAtAddress(uint64_t address, uint32_t* value) const = 0;\nvirtual bool GetMemoryAtAddress(uint64_t address, uint64_t* value) const = 0;\n\nWhelp congrats to 5225225 for finding an overflow that’s portable between two implementations in two completely different languages by exploiting the very nature of the file format itself!\nIn case you’re wondering what the implications of this overflow are: it’s still basically benign. Both rust-minidump and google-breakpad will successfully complete the frame pointer analysis and yield a frame with a ~null stack pointer.\nThen the outer layer of the stackwalker which runs all the different passes in sequence will see something succeeded but that the frame pointer went backwards. At this point it will discard the stack frame and terminate the stackwalk normally and just calmly output whatever the backtrace was up to that point. Totally normal and reasonable operation.\nI expect this is why no one would notice this in breakpad even if you run fuzzers and sanitizers on it: nothing in the code actually does anything wrong. Unsigned integers are defined to wrap, the program behaves reasonably, everything is kinda fine. We only noticed this in rust-minidump because all integer overflows panic in Rust debug builds.\nHowever this “benign” behaviour is slightly different from properly guarding the overflow. Both implementations will normally try to move on to stack scanning when the frame pointer analysis fails, but in this case they give up immediately. It’s important that the frame pointer analysis properly identifies failures so that this cascading can occur. Failing to do so is definitely a bug!\nHowever in this case the stack is partially in a parallel universe, so getting any kind of useful backtrace out of it is… dubious to say the least.\nSo I totally stand by “this is totally benign and not actually a problem” but also “this is sketchy and we should have the bounds check so we can be confident in this code’s robustness and correctness”.\nMinidumps are all corner cases — they literally get generated when a program encounters an unexpected corner case! It’s so tempting to constantly shrug off situations as “well no reasonable program would ever do this, so we can ignore it”… but YOU CAN’T.\nYou would not have a minidump at your doorstep if the program had behaved reasonably! The fact that you are trying to inspect a minidump means something messed up happened, and you need to just deal with it!\nThat’s why we put so much energy into testing this thing, it’s a nightmare!\nI am extremely paranoid about this stuff, but that paranoia is based on the horrors I have seen. There are always more corner cases. \nThere are ALWAYS more corner cases. ALWAYS.\n \nThe post Fuzzing rust-minidump for Embarrassment and Crashes – Part 2 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-06-23T17:19:31.000Z",
      "date_modified": "2022-06-23T17:19:31.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47854",
      "url": "https://hacks.mozilla.org/2022/06/hacks-decoded-bikes-and-boomboxes-with-samuel-aboagye/",
      "title": "Hacks Decoded: Bikes and Boomboxes with Samuel Aboagye",
      "summary": "Samuel Aboagye is a genius. Aboagye is 17 years old. In those 17 years, he’s crafted more inventions than you have, probably. Among them: a solar-powered bike and a Bluetooth speaker, both using recycled materials. We caught up with Aboagye over video chat in hopes that he’d talk with us about his creations, and ultimately how he’s way cooler than any of us at 17.\nThe post Hacks Decoded: Bikes and Boomboxes with Samuel Aboagye appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><i>Welcome to our Hacks: Decoded Interview series!</i></p>\n<p><i>Once a month, </i><a href=\"https://foundation.mozilla.org/\" target=\"_blank\" rel=\"noopener\"><i>Mozilla Foundation</i></a><i>’s </i><a href=\"https://www.xavierharding.com/\" target=\"_blank\" rel=\"noopener\"><i>Xavier Harding</i></a><i> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </i><a href=\"https://hacks.mozilla.org/\"><i>Hacks</i></a><i> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.</i></p>\n<p><strong>Meet Samuel Aboagye!</strong></p>\n<p>Samuel Aboagye is a genius. Aboagye is 17 years old. In those 17 years, he’s crafted more inventions than you have, probably. Among them: a solar-powered bike and a Bluetooth speaker, both using recycled materials. We caught up with Ghanaian inventor Samuel Aboagye over video chat in hopes that he’d talk with us about his creations, and ultimately how he’s way cooler than any of us were at 17.</p>\n<div style=\"width: 368px;\" class=\"wp-video\"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->\n<video class=\"wp-video-shortcode\" id=\"video-47854-1\" width=\"368\" height=\"640\" poster=\"https://hacks.mozilla.org/files/2022/06/Screenshot-2022-06-16-at-09.31.34.png\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://hacks.mozilla.org/files/2022/06/Untitled.mp4?_=1\" /><a href=\"https://hacks.mozilla.org/files/2022/06/Untitled.mp4\">https://hacks.mozilla.org/files/2022/06/Untitled.mp4</a></video></div>\n<p>&nbsp;</p>\n<p><b>Samuel, you’ve put together lots of inventions like an electric bike and Bluetooth speaker and even a fan. What made you want to make them?</b><i></i></p>\n<p><span style=\"font-weight: 400;\">For the speaker, I thought of how I could minimize the rate at which yellow plastic containers pollute the environment.  I tried to make good use of it after it served its purpose. So, with the little knowledge, I acquired in my science lessons, instead of the empty container just lying down and polluting the environment, I tried to create something useful with it.  </span></p>\n<p><span style=\"font-weight: 400;\">After the Bluetooth speaker was successful, I realized there was more in me I could show to the universe. More importantly, we live in a very poor ventilated room and we couldn’t afford an electric fan so the room was unbearably hot. As such, this situation triggered and motivated me to manufacture a fan to solve this family problem.</span></p>\n<p><span style=\"font-weight: 400;\">With the bike, I thought it would be wise to make life easier for the physically challenged because I was always sad to see them go through all these challenges just to live their daily lives. Electric motors are very expensive and not common in my country, so I decided to do something to help. </span></p>\n<p><span style=\"font-weight: 400;\">Since solar energy is almost always readily available in my part of the world and able to renew itself, I thought that if I am able to make a bike with it, it would help the physically challenged to move from one destination to another without stress or thinking of how to purchase a battery or fuel.  </span></p>\n<p><b>So how did you go about making them? Did you run into any trouble?</b><i></i></p>\n<p><span style=\"font-weight: 400;\">I went around my community gathering used items and old gadgets like radio sets and other electronics and then removed parts that could help in my work. With the electrical energy training given to me by my science teacher after discovering me since JHS1, I was able to apply this and also combined with my God-given talent. </span></p>\n<p><span style=\"font-weight: 400;\">Whenever I need some sort of technical guidance, I call on my teacher Sir David. He has also been my financial help for all my projects.  Financing projects has always been my biggest struggle and most times I have to wait on him to raise funds for me to continue.</span></p>\n<p><b>The tricycle: Was it much harder to make than a bike?</b></p>\n<p><b>​​</b><span style=\"font-weight: 400;\">Yes, it was a little bit harder to make the tricycle than the bike. It’s time-consuming and also cost more than a bike. It needs extra technical and critical thinking too. </span><i></i></p>\n<p><b>You made the bike and speaker out of recycled materials. This answer is probably obvious but I’ve gotta ask: why recycled materials?  Is environment-friendly tech important to you?</b></p>\n<p><span style=\"font-weight: 400;\">I used recycled materials because they were readily available and comparable to cheap and easy to get. With all my inventions I make sure they are all environmentally friendly so as not to pose any danger now or future to the beings on Earth.  But also, I want the world to be a safe and healthy place to be. </span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/hacks-decoded-bikes-and-boomboxes-with-samuel-aboagye/\">Hacks Decoded: Bikes and Boomboxes with Samuel Aboagye</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Welcome to our Hacks: Decoded Interview series!\nOnce a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.\nMeet Samuel Aboagye!\nSamuel Aboagye is a genius. Aboagye is 17 years old. In those 17 years, he’s crafted more inventions than you have, probably. Among them: a solar-powered bike and a Bluetooth speaker, both using recycled materials. We caught up with Ghanaian inventor Samuel Aboagye over video chat in hopes that he’d talk with us about his creations, and ultimately how he’s way cooler than any of us were at 17.\n\nhttps://hacks.mozilla.org/files/2022/06/Untitled.mp4\n \nSamuel, you’ve put together lots of inventions like an electric bike and Bluetooth speaker and even a fan. What made you want to make them?\nFor the speaker, I thought of how I could minimize the rate at which yellow plastic containers pollute the environment.  I tried to make good use of it after it served its purpose. So, with the little knowledge, I acquired in my science lessons, instead of the empty container just lying down and polluting the environment, I tried to create something useful with it.  \nAfter the Bluetooth speaker was successful, I realized there was more in me I could show to the universe. More importantly, we live in a very poor ventilated room and we couldn’t afford an electric fan so the room was unbearably hot. As such, this situation triggered and motivated me to manufacture a fan to solve this family problem.\nWith the bike, I thought it would be wise to make life easier for the physically challenged because I was always sad to see them go through all these challenges just to live their daily lives. Electric motors are very expensive and not common in my country, so I decided to do something to help. \nSince solar energy is almost always readily available in my part of the world and able to renew itself, I thought that if I am able to make a bike with it, it would help the physically challenged to move from one destination to another without stress or thinking of how to purchase a battery or fuel.  \nSo how did you go about making them? Did you run into any trouble?\nI went around my community gathering used items and old gadgets like radio sets and other electronics and then removed parts that could help in my work. With the electrical energy training given to me by my science teacher after discovering me since JHS1, I was able to apply this and also combined with my God-given talent. \nWhenever I need some sort of technical guidance, I call on my teacher Sir David. He has also been my financial help for all my projects.  Financing projects has always been my biggest struggle and most times I have to wait on him to raise funds for me to continue.\nThe tricycle: Was it much harder to make than a bike?\n​​Yes, it was a little bit harder to make the tricycle than the bike. It’s time-consuming and also cost more than a bike. It needs extra technical and critical thinking too. \nYou made the bike and speaker out of recycled materials. This answer is probably obvious but I’ve gotta ask: why recycled materials?  Is environment-friendly tech important to you?\nI used recycled materials because they were readily available and comparable to cheap and easy to get. With all my inventions I make sure they are all environmentally friendly so as not to pose any danger now or future to the beings on Earth.  But also, I want the world to be a safe and healthy place to be. \n \nThe post Hacks Decoded: Bikes and Boomboxes with Samuel Aboagye appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-06-16T15:00:15.000Z",
      "date_modified": "2022-06-16T15:00:15.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47842",
      "url": "https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/",
      "title": "Everything Is Broken: Shipping rust-minidump at Mozilla – Part 1",
      "summary": "For the last year, we've been working on the development of rust-minidump, a pure-Rust replacement for the minidump-processing half of google-breakpad. The first in this two-part series explains what minidumps are, and how we made rust-minidump.\nThe post Everything Is Broken: Shipping rust-minidump at Mozilla – Part 1 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<h1><strong>Everything Is Broken: Shipping rust-minidump at Mozilla</strong></h1>\n<p>For the last year I&#8217;ve been leading the development of <a href=\"https://github.com/luser/rust-minidump/\">rust-minidump</a>, a pure-Rust replacement for the minidump-processing half of <a href=\"https://chromium.googlesource.com/breakpad/breakpad/\">google-breakpad</a>.</p>\n<p>Well actually in some sense I <em>finished</em> that work, because Mozilla already <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">deployed it</a> as <a href=\"https://crash-stats.mozilla.org/\">the crash processing backend for Firefox</a> 6 months ago, it runs in half the time, and seems to be more reliable. (And you know, <em>isn&#8217;t</em> a terrifying ball of C++ that parses and evaluates arbitrary input from the internet. We did our best to isolate Breakpad, but still… <em>yikes</em>.)</p>\n<p>This is a pretty fantastic result, but there&#8217;s always more work to do because <em>Minidumps are an inky abyss that grows deeper the further you delve…</em> wait no I&#8217;m getting ahead of myself. First the light, then the abyss. Yes. Light first.</p>\n<p>What I <em>can</em> say is that we have a very solid implementation of the core functionality of minidump parsing+analysis for the biggest platforms (x86, x64, ARM, ARM64; Windows, MacOS, Linux, Android). But if you want to read minidumps generated on a <em>PlayStation 3</em> or process a <em>Full Memory</em> dump, you won&#8217;t be served quite as well.</p>\n<p>We&#8217;ve put a lot of effort into documenting and testing this thing, so I&#8217;m pretty confident in it!</p>\n<p><strong>Unfortunately! Confidence! Is! Worth! Nothing!</strong></p>\n<p>Which is why this is the story of how we did our best to make this nightmare as robust as we could and still got 360 dunked on from space by the sudden and <em>incredible</em> fuzzing efforts of <a href=\"https://github.com/5225225\">@5225225</a>.</p>\n<p>This article is broken into two parts:</p>\n<ol>\n<li>what minidumps are, and how we made rust-minidump</li>\n<li>how we got absolutely owned by simple fuzzing</li>\n</ol>\n<p>You are reading part 1, wherein we build up our hubris.</p>\n<h1><strong>Background: What&#8217;s A Minidump, and Why Write rust-minidump?</strong></h1>\n<p>Your program crashes. You want to know why your program crashed, but it happened on a user&#8217;s machine on the other side of the world. A full coredump (all memory allocated by the program) is enormous &#8212; we can&#8217;t have users sending us 4GB files! Ok let&#8217;s just collect up the most important regions of memory like the stacks and where the program crashed. Oh and I guess if we&#8217;re taking the time, let&#8217;s stuff some metadata about the system and process in there too.</p>\n<p>Congratulations you have invented <a href=\"https://docs.microsoft.com/en-us/windows/win32/debug/minidump-files\">Minidumps</a>. Now you can turn a 100-thread coredump that would otherwise be 4GB into a nice little 2MB file that you can send over the internet and do postmortem analysis on.</p>\n<p>Or more specifically, Microsoft did. So long ago that their docs don&#8217;t even discuss platform support. MiniDumpWriteDump&#8217;s supported versions are simply &#8220;Windows&#8221;. Microsoft Research has presumably developed a time machine to guarantee this.</p>\n<p>Then Google came along (circa 2006-2007) and said &#8220;wouldn&#8217;t it be nice if we could make minidumps on <em>any</em> platform&#8221;? Thankfully Microsoft had actually built the format pretty extensibly, so it wasn&#8217;t too bad to extend the format for Linux, MacOS, BSD, Solaris, and so on. Those extensions became <a href=\"https://chromium.googlesource.com/breakpad/breakpad/\">google-breakpad</a> (or just Breakpad) which included a ton of different tools for generating, parsing, and analyzing their extended minidump format (and native Microsoft ones).</p>\n<p>Mozilla helped out with this a lot because apparently, our crash reporting infrastructure (&#8220;Talkback&#8221;) was <em>miserable</em> circa 2007, and this seemed like a nice improvement. Needless to say, we&#8217;re pretty invested in breakpad&#8217;s minidumps at this point.</p>\n<p>Fast forward to the present day and in a hilarious twist of fate, products like VSCode mean that Microsoft now supports applications that run on Linux and MacOS so it runs breakpad in production and has to handle non-Microsoft minidumps somewhere in its crash reporting infra, so someone else&#8217;s extension of their own format is somehow their problem now!</p>\n<p>Meanwhile, Google has kind-of moved on to <a href=\"https://chromium.googlesource.com/crashpad/crashpad\">Crashpad</a>. I say kind-of because there&#8217;s still a lot of Breakpad in there, but they&#8217;re more interested in building out tooling on top of it than improving Breakpad itself. Having made a few changes to Breakpad: <strong>honestly fair</strong>, I don&#8217;t want to work on it either. Still, this was a bit of a problem for us, because it meant the project became increasingly under-staffed.</p>\n<p>By the time I started working on crash reporting, Mozilla had basically given up on upstreaming fixes/improvements to Breakpad, and was just using its own patched fork. But even <em>without</em> the need for upstreaming patches, every change to Breakpad filled us with dread: many proposed improvements to our crash reporting infrastructure stalled out at &#8220;time to implement this in Breakpad&#8221;.</p>\n<p>Why is working on Breakpad so miserable, you ask?</p>\n<p>Parsing and analyzing minidumps is basically an exercise in writing a fractal parser of platform-specific formats nested in formats nested in formats. For many operating systems. For many hardware architectures. And all the inputs you&#8217;re parsing and analyzing are terrible and buggy so you <em>have</em> to write a really permissive parser and crawl forward however you can.</p>\n<p>Some specific MSVC toolchain that was part of Windows XP had a bug in its debuginfo format? <strong>Too bad, symbolicate that stack frame anyway!</strong></p>\n<p>The program crashed because it horribly corrupted its own stack? <strong>Too bad, produce a backtrace anyway!</strong></p>\n<p>The minidump writer itself completely freaked out and wrote a bunch of garbage to one stream? <strong>Too bad, produce whatever output you can anyway!</strong></p>\n<p>Hey, you know who has a lot of experience dealing with really complicated permissive parsers written in C++? Mozilla! That&#8217;s like <em>the core functionality</em> of a web browser.</p>\n<p>Do you know Mozilla&#8217;s secret solution to writing really complicated permissive parsers in C++?</p>\n<p><strong>We stopped doing it.</strong></p>\n<p>We developed Rust and ported our nastiest parsers to it.</p>\n<p>We&#8217;ve done it a lot, and <a href=\"https://hacks.mozilla.org/2017/08/inside-a-super-fast-css-engine-quantum-css-aka-stylo/\">when we do</a> we&#8217;re always like <a href=\"https://www.joshmatthews.net/rbr17/\">&#8220;wow this is so much more reliable and easy to maintain and it&#8217;s even faster now&#8221;</a>. Rust is a really good language for writing parsers. C++ really isn&#8217;t.</p>\n<p>So we Rewrote It In Rust (or as the kids call it, &#8220;Oxidized It&#8221;). Breakpad is big, so we haven&#8217;t actually covered all of its features. We&#8217;ve specifically written and deployed:</p>\n<ul>\n<li><a href=\"https://github.com/mozilla/dump_syms\">dump_syms</a> which processes native build artifacts into symbol files.</li>\n<li><a href=\"https://github.com/luser/rust-minidump/\">rust-minidump</a> which is a collection of crates that parse and analyze minidumps. Or more specifically, we deployed <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">minidump-stackwalk</a>, which is the high-level cli interface to all of rust-minidump.</li>\n</ul>\n<p>Notably missing from this picture is <em>minidump writing</em>, or what google-breakpad calls a <em>client</em> (because it runs on the client&#8217;s machine). We <em>are </em>working <a href=\"https://github.com/rust-minidump/minidump-writer\">on a rust-based minidump writer</a>, but it&#8217;s not something we can recommend using quite yet (although it has sped up a lot thanks to help from <a href=\"https://embark.dev/\">Embark Studios</a>).</p>\n<p>This is arguably the messiest and hardest work because it has a horrible job: use a bunch of native system APIs to gather up a bunch of OS-specific and Hardware-specific information about the crash AND do it for a program that just crashed, on a machine that <em>caused </em>the program to crash.</p>\n<p>We have a long road ahead but every time we get to the other side of one of these projects it&#8217;s <em>wonderful</em>.</p>\n<p>&nbsp;</p>\n<h1><strong>Background: Stackwalking and Calling Conventions</strong></h1>\n<p>One of rust-minidump&#8217;s (<a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">minidump-stackwalk&#8217;s</a>) most important jobs is to take the state for a thread (general purpose registers and stack memory) and create a backtrace for that thread (unwind/stackwalk). This is a surprisingly complicated and messy job, made only more complicated by the fact that <em>we are trying to analyze the memory of a process that got messed up enough to crash</em>.</p>\n<p>This means our stackwalkers are inherently working with dubious data, and all of our stackwalking techniques are based on heuristics that can go wrong and we can very easily find ourselves in situations where the stackwalk goes backwards or sideways or infinite and we just have to try to deal with it!</p>\n<p>It&#8217;s also pretty common to see a stackwalker start <em>hallucinating</em>, which is my term for &#8220;the stackwalker found something that looked plausible enough and went on a wacky adventure through the stack and made up a whole pile of useless garbage frames&#8221;. Hallucination is most common near the bottom of the stack where it&#8217;s also least offensive. This is because each frame you walk is another chance for something to go wrong, but also increasingly uninteresting because you&#8217;re rarely interested in confirming that a thread started in The Same Function All Threads Start In.</p>\n<p>All of these problems would basically go away if everyone agreed to properly preserve their cpu&#8217;s <a href=\"https://gankra.github.io/blah/compact-unwinding/#frame-pointer-unwinding-standard-prologues\">PERFECTLY GOOD DEDICATED FRAME POINTER REGISTER</a>. Just kidding, turning on frame pointers doesn&#8217;t really work either because Microsoft <a href=\"https://github.com/rust-lang/rust/issues/82333\">invented chaos frame pointers</a> that can&#8217;t be used for unwinding! I assume this happened because they accidentally stepped on the wrong butterfly while they were traveling back in time to invent minidumps. (I&#8217;m sure it was a decision that made more sense 20 years ago, but it has not aged well.)</p>\n<p>If you would like to learn more about the different techniques for unwinding, <a href=\"https://gankra.github.io/blah/compact-unwinding/#background-unwinding-and-debug-info\">I wrote about them over here</a> in my <a href=\"https://gankra.github.io/blah/compact-unwinding\">article on Apple&#8217;s Compact Unwind Info</a>. I&#8217;ve also attempted to <a href=\"https://docs.rs/breakpad-symbols/latest/breakpad_symbols/walker/index.html\">document breakpad&#8217;s STACK WIN and STACK CFI unwind info formats here</a>, which are more similar to the  DWARF and PE32 unwind tables (which are basically tiny programming languages).</p>\n<p>If you would like to learn more about ABIs in general, <a href=\"https://gankra.github.io/blah/rust-layouts-and-abis/#calling-conventions\">I wrote an entire article about them here</a>. The end of that article also includes an <a href=\"https://gankra.github.io/blah/rust-layouts-and-abis/#calling-conventions\">introduction to how calling conventions work</a>. Understanding calling conventions is key to implementing unwinders.</p>\n<p>&nbsp;</p>\n<p><strong>How Hard Did You Really Test Things?</strong></p>\n<p>Hopefully you now have a bit of a glimpse into why analyzing minidumps is an enormous headache. And of course you know how the story ends: that fuzzer kicks our butts! But of course to really savor our defeat, you have to see how hard we tried to do a good job! It&#8217;s time to build up our hubris and pat ourselves on the back.</p>\n<p>So how much work <em>actually</em> went into making rust-minidump robust before the fuzzer went to work on it?</p>\n<p>Quite a bit!</p>\n<p>I&#8217;ll never argue all the work we did was <em>perfect</em> but we definitely did some good work here, both for synthetic inputs and real world ones. Probably the biggest &#8220;flaw&#8221; in our methodology was the fact that we were only focused on getting Firefox&#8217;s usecase to work. Firefox runs on a lot of platforms and sees a lot of messed up stuff, but it&#8217;s still a fairly coherent product that only uses so many features of minidumps.</p>\n<p>This is one of the nice benefits of our recent work with <a href=\"https://sentry.io/\">Sentry</a>, which is basically a Crash Reporting As A Service company. They are <em>way</em> more liable to stress test all kinds of weird corners of the format that Firefox doesn&#8217;t, and they have definitely found (and fixed!) some places where something is wrong or missing! (And they recently deployed it into production too! <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f389.png\" alt=\"🎉\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" />)</p>\n<p>But hey don&#8217;t take my word for it, check out all the different testing we did:</p>\n<h2><strong>Synthetic Minidumps for Unit Tests</strong></h2>\n<p>rust-minidump includes a <a href=\"https://github.com/rust-minidump/rust-minidump/tree/553735e2624dcc6af82167f502cf92ae9a9fdc87/minidump-synth\">synthetic minidump generator</a> which lets you come up with a high-level description of the contents of a minidump, and then produces an actual minidump binary that we can feed it into the full parser:</p>\n<p>// Let&#8217;s make a synth minidump with this particular Crashpad Info&#8230;</p>\n<pre>let module = ModuleCrashpadInfo::new(42, Endian::Little)\r\n    .add_list_annotation(\"annotation\")\r\n    .add_simple_annotation(\"simple\", \"module\")\r\n    .add_annotation_object(\"string\", AnnotationValue::String(\"value\".to_owned()))\r\n    .add_annotation_object(\"invalid\", AnnotationValue::Invalid)\r\n    .add_annotation_object(\"custom\", AnnotationValue::Custom(0x8001, vec![42]));\r\n\r\nlet crashpad_info = CrashpadInfo::new(Endian::Little)\r\n    .add_module(module)\r\n    .add_simple_annotation(\"simple\", \"info\");\r\n\r\nlet dump = SynthMinidump::with_endian(Endian::Little).add_crashpad_info(crashpad_info);\r\n\r\n// convert the synth minidump to binary and read it like a normal minidump\r\nlet dump = read_synth_dump(dump).unwrap();</pre>\n<p>// Now check that the minidump reports the values we expect…</p>\n<p>minidump-synth intentionally avoids sharing layout code with the actual implementation so that incorrect changes to layouts won&#8217;t &#8220;accidentally&#8221; pass tests.</p>\n<p><em>A brief aside for some history</em>: this testing framework was started by the original lead on this project, <a href=\"https://twitter.com/TedMielczarek\">Ted Mielczarek</a>. He started rust-minidump as a side project to learn Rust when 1.0 was released and just never had the time to finish it. Back then he was working at Mozilla and also a major contributor to Breakpad, which is why rust-minidump has a lot of similar design choices and terminology.</p>\n<p>This case is no exception: our minidump-synth is a shameless copy of the <a href=\"https://chromium.googlesource.com/breakpad/breakpad/+/refs/heads/main/src/processor/synth_minidump.cc\">synth-minidump utility in breakpad&#8217;s code</a>, which was originally written by our <em>other</em> coworker <a href=\"https://www.red-bean.com/~jimb/\">Jim Blandy</a>. Jim is one of the only people in the world that I will actually admit writes really good tests and docs, so I am totally happy to blatantly copy his work here.</p>\n<p>Since this was all a learning experiment, Ted was understandably less rigorous about testing than usual. This meant a lot of minidump-synth was unimplemented when I came along, which also meant lots of minidump features were completely untested. (He built an absolutely great skeleton, just hadn&#8217;t had the time to fill it all in!)</p>\n<p>We spent <em>a lot</em> of time filling in more of minidump-synth&#8217;s implementation so we could write more tests and catch more issues, but this is <em>definitely</em> the weakest part of our tests. Some stuff was implemented before I got here, so I don&#8217;t even <em>know</em> what tests are missing!</p>\n<p>This is a good argument for some code coverage checks, but it would probably come back with &#8220;wow you should write a lot more tests&#8221; and we would all look at it and go &#8220;wow we sure should&#8221; and then we would probably never get around to it, because there are <em>many</em> things we <em>should</em> do.</p>\n<p>On the other hand, Sentry has been very useful in this regard because they already <em>have</em> a mature suite of tests full of weird corner cases they&#8217;ve built up over time, so they can easily identify things that really matter, know what the fix should roughly be, and can contribute pre-existing test cases!</p>\n<h2><strong>Integration and Snapshot Tests</strong></h2>\n<p>We tried our best to shore up coverage issues in our unit tests by adding more holistic tests. There&#8217;s a few checked in Real Minidumps that we have <a href=\"https://github.com/luser/rust-minidump/blob/40c3390f5705890f932f78b7db4fc02866e012b8/minidump-processor/tests/test_processor.rs\">some integration tests for</a> to make sure we handle Real Inputs properly.</p>\n<p>We even wrote a bunch of <a href=\"https://github.com/luser/rust-minidump/blob/40c3390f5705890f932f78b7db4fc02866e012b8/minidump-stackwalk/tests/test-minidump-stackwalk.rs\">integration tests for the CLI application that snapshot its output</a> to confirm that we never <em>accidentally</em> change the results.</p>\n<p>Part of the motivation for this is to ensure we don&#8217;t break the JSON output, which we also wrote a <a href=\"https://github.com/luser/rust-minidump/blob/40c3390f5705890f932f78b7db4fc02866e012b8/minidump-processor/json-schema.md\">very detailed schema document for</a> and are trying to keep stable so people can actually rely on it while the actual implementation details are still in flux.</p>\n<p>Yes, <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">minidump-stackwalk</a> is supposed to be stable and reasonable to use in production!</p>\n<p>For our snapshot tests we use <a href=\"https://github.com/mitsuhiko/insta\">insta</a>, which I think is fantastic and more people should use. All you need to do is assert_snapshot! any output you want to keep track of and it will magically take care of the storing, loading, and diffing.</p>\n<p>Here&#8217;s one of the snapshot tests where we invoke the CLI interface and snapshot stdout:</p>\n<pre>#[test]\r\nfn test_evil_json() {\r\n    // For a while this didn't parse right\r\n    let bin = env!(\"CARGO_BIN_EXE_minidump-stackwalk\");\r\n    let output = Command::new(bin)\r\n        .arg(\"--json\")\r\n        .arg(\"--pretty\")\r\n        .arg(\"--raw-json\")\r\n        .arg(\"../testdata/evil.json\")\r\n        .arg(\"../testdata/test.dmp\")\r\n        .arg(\"../testdata/symbols/\")\r\n        .stdout(Stdio::piped())\r\n        .stderr(Stdio::piped())\r\n        .output()\r\n        .unwrap();\r\n\r\n    let stdout = String::from_utf8(output.stdout).unwrap();\r\n    let stderr = String::from_utf8(output.stderr).unwrap();\r\n\r\n    assert!(output.status.success());\r\n    insta::assert_snapshot!(\"json-pretty-evil-symbols\", stdout);\r\n    assert_eq!(stderr, \"\");\r\n}\r\n\r\n</pre>\n<h2><b>Stackwalker Unit Testing</b></h2>\n<p>The stackwalker is easily the most complicated and subtle part of the new implementation, because every platform can have <em>slight</em> quirks and you need to implement several different unwinding strategies and carefully tune everything to work well <em>in practice</em>.</p>\n<p>The scariest part of this was the call frame information (CFI) unwinders, because they are basically little virtual machines we need to parse and execute at runtime. Thankfully breakpad had long ago smoothed over this issue by defining a simplified and unified CFI format, STACK CFI (well, nearly unified, x86 Windows was still a special case as STACK WIN). So even if DWARF CFI has a ton of complex features, we mostly need to implement a <a href=\"https://en.wikipedia.org/wiki/Reverse_Polish_notation\">Reverse Polish Notation Calculator</a> except it can read registers and load memory from addresses it computes (and for STACK WIN it has access to named variables it can declare and mutate).</p>\n<p>Unfortunately, <a href=\"https://chromium.googlesource.com/breakpad/breakpad/+/master/docs/symbol_files.md\">Breakpad&#8217;s description for this format is pretty underspecified</a> so I had to basically pick some semantics I thought made sense and go with that. This made me <em>extremely</em> paranoid about the implementation. (And yes I will be more first-person for this part, because this part was genuinely where I personally spent most of my time and did a lot of stuff from scratch. All the blame belongs to me here!)</p>\n<p>The<a href=\"https://docs.rs/breakpad-symbols/latest/breakpad_symbols/walker/index.html\"> STACK WIN / STACK CFI parser+evaluator</a> is 1700 lines. 500 of those lines are a detailed documentation and discussion of the format, and 700 of those lines are an enormous pile of ~80 test cases where I tried to come up with every corner case I could think of.</p>\n<p>I even checked in two tests I <em>knew</em> were failing just to be honest that there were a couple cases to fix! One of them is a corner case involving dividing by a negative number that almost certainly just doesn&#8217;t matter. The other is a buggy input that old x86 Microsoft toolchains actually produce and parsers need to deal with. The latter was fixed before the fuzzing started.</p>\n<p>And 5225225 <em>still</em> found an integer overflow in the STACK WIN preprocessing step! (Not actually that surprising, it&#8217;s a hacky mess that tries to cover up for how messed up x86 Windows unwinding tables were.)</p>\n<p>(The code isn&#8217;t terribly interesting here, it&#8217;s just a ton of assertions that a given input string produces a given output/error.)</p>\n<p>Of course, I wasn&#8217;t satisfied with just coming up with my own semantics and testing them: I also <a href=\"https://github.com/luser/rust-minidump/blob/master/minidump-processor/src/stackwalker/x86_unittest.rs\">ported most of breakpad&#8217;s own stackwalker tests to rust-minidump</a>! This definitely found a bunch of bugs I had, but also taught me some weird quirks in Breakpad&#8217;s stackwalkers that I&#8217;m not sure I <em>actually</em> agree with. But in this case I was flying so blind that even being bug-compatible with Breakpad was some kind of relief.</p>\n<p>Those tests also included several tests for the non-CFI paths, which were similarly wobbly and quirky. I still really hate a lot of the weird platform-specific rules they have for stack scanning, but I&#8217;m forced to work on the assumption that they might be load-bearing. (I definitely had several cases where I disabled a breakpad test because it was &#8220;obviously nonsense&#8221; and then hit it in the wild while testing. I quickly learned to accept that <strong>Nonsense Happens And Cannot Be Ignored</strong>.)</p>\n<p>One major thing I <em>didn&#8217;t</em> replicate was some of the really hairy hacks for STACK WIN. Like there are several places where they introduce extra stack-scanning to try to deal with the fact that stack frames can have mysterious extra alignment that the windows unwinding tables just don&#8217;t tell you about? I guess?</p>\n<p>There&#8217;s almost certainly some exotic situations that rust-minidump does worse on because of this, but it probably also means we do better in some random other situations too. I never got the two to perfectly agree, but at some point the divergences were all in weird enough situations, and as far as I was concerned both stackwalkers were producing equally bad results in a bad situation. Absent any reason to prefer one over the other, divergence seemed acceptable to keep the implementation cleaner.</p>\n<p>Here&#8217;s a simplified version of one of the ported breakpad tests, if you&#8217;re curious (thankfully minidump-synth is based off of the same binary data mocking framework these tests use):</p>\n<pre>#[test]\r\nfn test_x86_frame_pointer() {\r\n    let mut f = TestFixture::new();\r\n    let frame0_ebp = Label::new();\r\n    let frame1_ebp = Label::new();\r\n    let mut stack = Section::new();\r\n\r\n    // Setup the stack and registers so frame pointers will work\r\n    stack.start().set_const(0x80000000);\r\n    stack = stack\r\n        .append_repeated(12, 0) // frame 0: space\r\n        .mark(&amp;frame0_ebp)      // frame 0 %ebp points here\r\n        .D32(&amp;frame1_ebp)       // frame 0: saved %ebp\r\n        .D32(0x40008679)        // frame 0: return address\r\n        .append_repeated(8, 0)  // frame 1: space\r\n        .mark(&amp;frame1_ebp)      // frame 1 %ebp points here\r\n        .D32(0)                 // frame 1: saved %ebp (stack end)\r\n        .D32(0);                // frame 1: return address (stack end)\r\n    f.raw.eip = 0x4000c7a5;\r\n    f.raw.esp = stack.start().value().unwrap() as u32;\r\n    f.raw.ebp = frame0_ebp.value().unwrap() as u32;\r\n\r\n    // Check the stackwalker's output:\r\n    let s = f.walk_stack(stack).await;\r\n    assert_eq!(s.frames.len(), 2);\r\n    {\r\n        let f0 = &amp;s.frames[0];\r\n        assert_eq!(f0.trust, FrameTrust::Context);\r\n        assert_eq!(f0.context.valid, MinidumpContextValidity::All);\r\n        assert_eq!(f0.instruction, 0x4000c7a5);\r\n    }\r\n    {\r\n        let f1 = &amp;s.frames[1];\r\n        assert_eq!(f1.trust, FrameTrust::FramePointer);\r\n        assert_eq!(f1.instruction, 0x40008678);\r\n    }\r\n}</pre>\n<h2>A Dedicated Production Diffing, Simulating, and Debugging Tool</h2>\n<p>Because minidumps are so horribly fractal and corner-casey, I spent <em>a lot</em> of time terrified of subtle issues that would become huge disasters if we ever actually tried to deploy to production. So I also spent a bunch of time building <a href=\"https://github.com/Gankra/socc-pair/\">socc-pair</a>, which takes the id of a crash report from Mozilla&#8217;s <a href=\"https://crash-stats.mozilla.org/\">crash reporting system</a> and pulls down the minidump, the old breakpad-based implementation&#8217;s output, and extra metadata.</p>\n<p>It then runs a local rust-minidump (minidump-stackwalk) implementation on the minidump and does a domain-specific diff over the two inputs. The most substantial part of this is a fuzzy diff on the stackwalks that tries to better handle situations like when one implementation adds an extra frame but the two otherwise agree. It also uses the reported techniques each implementation used to try to identify whose output is more trustworthy when they totally diverge.</p>\n<p>I also ended up adding a bunch of mocking and benchmarking functionality to it as well, as I found more and more places where I just wanted to simulate a production environment.</p>\n<p>Oh also I added <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk#debugging-stackwalking\">really detailed trace-logging for the stackwalker</a> so that I could easily post-mortem debug why it made the decisions it made.</p>\n<p>This tool found so many issues and more importantly has helped me quickly isolate their causes. I am so happy I made it. Because of it, we know we actually <em>fixed</em> several issues that happened with the old breakpad implementation, which is great!</p>\n<p>Here&#8217;s a trimmed down version of the kind of report socc-pair would produce (yeah I abused diff syntax to get error highlighting. It&#8217;s a great hack, and I love it like a child):</p>\n<pre>comparing json...\r\n\r\n: {\r\n    crash_info: {\r\n        address: 0x7fff1760aca0\r\n        crashing_thread: 8\r\n        type: EXCEPTION_BREAKPOINT\r\n    }\r\n    crashing_thread: {\r\n        frames: [\r\n            0: {\r\n                file: wrappers.cpp:1750da2d7f9db490b9d15b3ee696e89e6aa68cb7\r\n                frame: 0\r\n                function: RustMozCrash(char const*, int, char const*)\r\n                function_offset: 0x00000010\r\n-               did not match\r\n+               line: 17\r\n-               line: 20\r\n                module: xul.dll\r\n\r\n.....\r\n\r\n    unloaded_modules: [\r\n        0: {\r\n            base_addr: 0x7fff48290000\r\n-           local val was null instead of:\r\n            code_id: 68798D2F9000\r\n            end_addr: 0x7fff48299000\r\n            filename: KBDUS.DLL\r\n        }\r\n        1: {\r\n            base_addr: 0x7fff56020000\r\n            code_id: DFD6E84B14000\r\n            end_addr: 0x7fff56034000\r\n            filename: resourcepolicyclient.dll\r\n        }\r\n    ]\r\n~   ignoring field write_combine_size: \"0\"\r\n}\r\n\r\n- Total errors: 288, warnings: 39\r\n\r\nbenchmark results (ms):\r\n    2388, 1986, 2268, 1989, 2353, \r\n    average runtime: 00m:02s:196ms (2196ms)\r\n    median runtime: 00m:02s:268ms (2268ms)\r\n    min runtime: 00m:01s:986ms (1986ms)\r\n    max runtime: 00m:02s:388ms (2388ms)\r\n\r\nmax memory (rss) results (bytes):\r\n    267755520, 261152768, 272441344, 276131840, 279134208, \r\n    average max-memory: 258MB (271323136 bytes)\r\n    median max-memory: 259MB (272441344 bytes)\r\n    min max-memory: 249MB (261152768 bytes)\r\n    max max-memory: 266MB (279134208 bytes)\r\n\r\nOutput Files: \r\n    * (download) Minidump: b4f58e9f-49be-4ba5-a203-8ef160211027.dmp\r\n    * (download) Socorro Processed Crash: b4f58e9f-49be-4ba5-a203-8ef160211027.json\r\n    * (download) Raw JSON: b4f58e9f-49be-4ba5-a203-8ef160211027.raw.json\r\n    * Local minidump-stackwalk Output: b4f58e9f-49be-4ba5-a203-8ef160211027.local.json\r\n    * Local minidump-stackwalk Logs: b4f58e9f-49be-4ba5-a203-8ef160211027.log.txt</pre>\n<h2><b>Staging and Deploying to Production</b></h2>\n<p>Once we were confident enough in the implementation, a lot of the remaining testing was taken over by Will Kahn-Greene, who&#8217;s responsible for a lot of the server-side details of our crash-reporting infrastructure.</p>\n<p>Will spent a bunch of time getting a bunch of machinery setup to manage the deployment and monitoring of rust-minidump. He also did a lot of the hard work of cleaning up all our server-side configuration scripts to handle any differences between the two implementations. (Although I spent a lot of time on compatibility, we both agreed this was a good opportunity to clean up old cruft and mistakes.)</p>\n<p>Once all of this was set up, he turned it on in staging and we got our first look at how rust-minidump actually worked in ~production:</p>\n<p><strong>Terribly!</strong></p>\n<p>Our staging servers take in about 10% of the inputs that also go to our production servers, but even at that reduced scale we very quickly found several new corner cases and we were getting <em>tons</em> of crashes, which is mildly embarrassing for<em> the thing that handles other people&#8217;s crashes</em>.</p>\n<p>Will did a great job here in monitoring and reporting the issues. Thankfully they were all fairly easy for us to fix. Eventually, everything smoothed out and things seemed to be working just as reliably as the old implementation on the production server. The only places where we were completely failing to produce any output were for horribly truncated minidumps that may as well have been empty files.</p>\n<p>We originally <em>did</em> have some grand ambitions of running socc-pair on everything the staging servers processed or something to get <em>really</em> confident in the results. But by the time we got to that point, we were completely exhausted and feeling pretty confident in the new implementation.</p>\n<p>Eventually Will just said &#8220;let&#8217;s turn it on in production&#8221; and I said &#8220;AAAAAAAAAAAAAAA&#8221;.</p>\n<p>This moment was pure terror. There had always been <em>more</em> corner cases. There&#8217;s no way we could just be <em>done</em>. This will probably set all of Mozilla on fire and delete Firefox from the internet!</p>\n<p>But Will convinced me. We wrote up some docs detailing all the subtle differences and sent them to everyone we could. Then the moment of truth finally came: Will turned it on in production, and I got to really see how well it worked in production:</p>\n<p><em>*dramatic drum roll*</em></p>\n<p>It worked fine.</p>\n<p>After all that stress and anxiety, we turned it on and it was <em>fine</em>.</p>\n<p>Heck, I&#8217;ll say it: it ran <em>well</em>.</p>\n<p>It was faster, it crashed less, and we even knew it fixed some issues.</p>\n<p>I was in a bit of a stupor for the rest of that week, because I kept waiting for the other shoe to drop. I kept waiting for someone to emerge from the mist and explain that I had somehow bricked <em>Thunderbird</em> or something. But no, it just worked.</p>\n<p>So we left for the holidays, and I kept waiting for it to break, but it was <em>still fine</em>.</p>\n<p>I am honestly still shocked about this!</p>\n<p>But hey, as it turns out we really did put a <em>lot</em> of careful work into testing the implementation. At every step we found new problems but that was <em>good</em>, because once we got to the final step there were no more problems to surprise us.</p>\n<p><strong>And the fuzzer still kicked our butts afterwards.</strong></p>\n<p>But that&#8217;s part 2! Thanks for reading!</p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/\">Everything Is Broken: Shipping rust-minidump at Mozilla &#8211; Part 1</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Everything Is Broken: Shipping rust-minidump at Mozilla\nFor the last year I’ve been leading the development of rust-minidump, a pure-Rust replacement for the minidump-processing half of google-breakpad.\nWell actually in some sense I finished that work, because Mozilla already deployed it as the crash processing backend for Firefox 6 months ago, it runs in half the time, and seems to be more reliable. (And you know, isn’t a terrifying ball of C++ that parses and evaluates arbitrary input from the internet. We did our best to isolate Breakpad, but still… yikes.)\nThis is a pretty fantastic result, but there’s always more work to do because Minidumps are an inky abyss that grows deeper the further you delve… wait no I’m getting ahead of myself. First the light, then the abyss. Yes. Light first.\nWhat I can say is that we have a very solid implementation of the core functionality of minidump parsing+analysis for the biggest platforms (x86, x64, ARM, ARM64; Windows, MacOS, Linux, Android). But if you want to read minidumps generated on a PlayStation 3 or process a Full Memory dump, you won’t be served quite as well.\nWe’ve put a lot of effort into documenting and testing this thing, so I’m pretty confident in it!\nUnfortunately! Confidence! Is! Worth! Nothing!\nWhich is why this is the story of how we did our best to make this nightmare as robust as we could and still got 360 dunked on from space by the sudden and incredible fuzzing efforts of @5225225.\nThis article is broken into two parts:\n\nwhat minidumps are, and how we made rust-minidump\nhow we got absolutely owned by simple fuzzing\n\nYou are reading part 1, wherein we build up our hubris.\nBackground: What’s A Minidump, and Why Write rust-minidump?\nYour program crashes. You want to know why your program crashed, but it happened on a user’s machine on the other side of the world. A full coredump (all memory allocated by the program) is enormous — we can’t have users sending us 4GB files! Ok let’s just collect up the most important regions of memory like the stacks and where the program crashed. Oh and I guess if we’re taking the time, let’s stuff some metadata about the system and process in there too.\nCongratulations you have invented Minidumps. Now you can turn a 100-thread coredump that would otherwise be 4GB into a nice little 2MB file that you can send over the internet and do postmortem analysis on.\nOr more specifically, Microsoft did. So long ago that their docs don’t even discuss platform support. MiniDumpWriteDump’s supported versions are simply “Windows”. Microsoft Research has presumably developed a time machine to guarantee this.\nThen Google came along (circa 2006-2007) and said “wouldn’t it be nice if we could make minidumps on any platform”? Thankfully Microsoft had actually built the format pretty extensibly, so it wasn’t too bad to extend the format for Linux, MacOS, BSD, Solaris, and so on. Those extensions became google-breakpad (or just Breakpad) which included a ton of different tools for generating, parsing, and analyzing their extended minidump format (and native Microsoft ones).\nMozilla helped out with this a lot because apparently, our crash reporting infrastructure (“Talkback”) was miserable circa 2007, and this seemed like a nice improvement. Needless to say, we’re pretty invested in breakpad’s minidumps at this point.\nFast forward to the present day and in a hilarious twist of fate, products like VSCode mean that Microsoft now supports applications that run on Linux and MacOS so it runs breakpad in production and has to handle non-Microsoft minidumps somewhere in its crash reporting infra, so someone else’s extension of their own format is somehow their problem now!\nMeanwhile, Google has kind-of moved on to Crashpad. I say kind-of because there’s still a lot of Breakpad in there, but they’re more interested in building out tooling on top of it than improving Breakpad itself. Having made a few changes to Breakpad: honestly fair, I don’t want to work on it either. Still, this was a bit of a problem for us, because it meant the project became increasingly under-staffed.\nBy the time I started working on crash reporting, Mozilla had basically given up on upstreaming fixes/improvements to Breakpad, and was just using its own patched fork. But even without the need for upstreaming patches, every change to Breakpad filled us with dread: many proposed improvements to our crash reporting infrastructure stalled out at “time to implement this in Breakpad”.\nWhy is working on Breakpad so miserable, you ask?\nParsing and analyzing minidumps is basically an exercise in writing a fractal parser of platform-specific formats nested in formats nested in formats. For many operating systems. For many hardware architectures. And all the inputs you’re parsing and analyzing are terrible and buggy so you have to write a really permissive parser and crawl forward however you can.\nSome specific MSVC toolchain that was part of Windows XP had a bug in its debuginfo format? Too bad, symbolicate that stack frame anyway!\nThe program crashed because it horribly corrupted its own stack? Too bad, produce a backtrace anyway!\nThe minidump writer itself completely freaked out and wrote a bunch of garbage to one stream? Too bad, produce whatever output you can anyway!\nHey, you know who has a lot of experience dealing with really complicated permissive parsers written in C++? Mozilla! That’s like the core functionality of a web browser.\nDo you know Mozilla’s secret solution to writing really complicated permissive parsers in C++?\nWe stopped doing it.\nWe developed Rust and ported our nastiest parsers to it.\nWe’ve done it a lot, and when we do we’re always like “wow this is so much more reliable and easy to maintain and it’s even faster now”. Rust is a really good language for writing parsers. C++ really isn’t.\nSo we Rewrote It In Rust (or as the kids call it, “Oxidized It”). Breakpad is big, so we haven’t actually covered all of its features. We’ve specifically written and deployed:\n\ndump_syms which processes native build artifacts into symbol files.\nrust-minidump which is a collection of crates that parse and analyze minidumps. Or more specifically, we deployed minidump-stackwalk, which is the high-level cli interface to all of rust-minidump.\n\nNotably missing from this picture is minidump writing, or what google-breakpad calls a client (because it runs on the client’s machine). We are working on a rust-based minidump writer, but it’s not something we can recommend using quite yet (although it has sped up a lot thanks to help from Embark Studios).\nThis is arguably the messiest and hardest work because it has a horrible job: use a bunch of native system APIs to gather up a bunch of OS-specific and Hardware-specific information about the crash AND do it for a program that just crashed, on a machine that caused the program to crash.\nWe have a long road ahead but every time we get to the other side of one of these projects it’s wonderful.\n \nBackground: Stackwalking and Calling Conventions\nOne of rust-minidump’s (minidump-stackwalk’s) most important jobs is to take the state for a thread (general purpose registers and stack memory) and create a backtrace for that thread (unwind/stackwalk). This is a surprisingly complicated and messy job, made only more complicated by the fact that we are trying to analyze the memory of a process that got messed up enough to crash.\nThis means our stackwalkers are inherently working with dubious data, and all of our stackwalking techniques are based on heuristics that can go wrong and we can very easily find ourselves in situations where the stackwalk goes backwards or sideways or infinite and we just have to try to deal with it!\nIt’s also pretty common to see a stackwalker start hallucinating, which is my term for “the stackwalker found something that looked plausible enough and went on a wacky adventure through the stack and made up a whole pile of useless garbage frames”. Hallucination is most common near the bottom of the stack where it’s also least offensive. This is because each frame you walk is another chance for something to go wrong, but also increasingly uninteresting because you’re rarely interested in confirming that a thread started in The Same Function All Threads Start In.\nAll of these problems would basically go away if everyone agreed to properly preserve their cpu’s PERFECTLY GOOD DEDICATED FRAME POINTER REGISTER. Just kidding, turning on frame pointers doesn’t really work either because Microsoft invented chaos frame pointers that can’t be used for unwinding! I assume this happened because they accidentally stepped on the wrong butterfly while they were traveling back in time to invent minidumps. (I’m sure it was a decision that made more sense 20 years ago, but it has not aged well.)\nIf you would like to learn more about the different techniques for unwinding, I wrote about them over here in my article on Apple’s Compact Unwind Info. I’ve also attempted to document breakpad’s STACK WIN and STACK CFI unwind info formats here, which are more similar to the  DWARF and PE32 unwind tables (which are basically tiny programming languages).\nIf you would like to learn more about ABIs in general, I wrote an entire article about them here. The end of that article also includes an introduction to how calling conventions work. Understanding calling conventions is key to implementing unwinders.\n \nHow Hard Did You Really Test Things?\nHopefully you now have a bit of a glimpse into why analyzing minidumps is an enormous headache. And of course you know how the story ends: that fuzzer kicks our butts! But of course to really savor our defeat, you have to see how hard we tried to do a good job! It’s time to build up our hubris and pat ourselves on the back.\nSo how much work actually went into making rust-minidump robust before the fuzzer went to work on it?\nQuite a bit!\nI’ll never argue all the work we did was perfect but we definitely did some good work here, both for synthetic inputs and real world ones. Probably the biggest “flaw” in our methodology was the fact that we were only focused on getting Firefox’s usecase to work. Firefox runs on a lot of platforms and sees a lot of messed up stuff, but it’s still a fairly coherent product that only uses so many features of minidumps.\nThis is one of the nice benefits of our recent work with Sentry, which is basically a Crash Reporting As A Service company. They are way more liable to stress test all kinds of weird corners of the format that Firefox doesn’t, and they have definitely found (and fixed!) some places where something is wrong or missing! (And they recently deployed it into production too! )\nBut hey don’t take my word for it, check out all the different testing we did:\nSynthetic Minidumps for Unit Tests\nrust-minidump includes a synthetic minidump generator which lets you come up with a high-level description of the contents of a minidump, and then produces an actual minidump binary that we can feed it into the full parser:\n// Let’s make a synth minidump with this particular Crashpad Info…\nlet module = ModuleCrashpadInfo::new(42, Endian::Little)\n    .add_list_annotation(\"annotation\")\n    .add_simple_annotation(\"simple\", \"module\")\n    .add_annotation_object(\"string\", AnnotationValue::String(\"value\".to_owned()))\n    .add_annotation_object(\"invalid\", AnnotationValue::Invalid)\n    .add_annotation_object(\"custom\", AnnotationValue::Custom(0x8001, vec![42]));\n\nlet crashpad_info = CrashpadInfo::new(Endian::Little)\n    .add_module(module)\n    .add_simple_annotation(\"simple\", \"info\");\n\nlet dump = SynthMinidump::with_endian(Endian::Little).add_crashpad_info(crashpad_info);\n\n// convert the synth minidump to binary and read it like a normal minidump\nlet dump = read_synth_dump(dump).unwrap();\n// Now check that the minidump reports the values we expect…\nminidump-synth intentionally avoids sharing layout code with the actual implementation so that incorrect changes to layouts won’t “accidentally” pass tests.\nA brief aside for some history: this testing framework was started by the original lead on this project, Ted Mielczarek. He started rust-minidump as a side project to learn Rust when 1.0 was released and just never had the time to finish it. Back then he was working at Mozilla and also a major contributor to Breakpad, which is why rust-minidump has a lot of similar design choices and terminology.\nThis case is no exception: our minidump-synth is a shameless copy of the synth-minidump utility in breakpad’s code, which was originally written by our other coworker Jim Blandy. Jim is one of the only people in the world that I will actually admit writes really good tests and docs, so I am totally happy to blatantly copy his work here.\nSince this was all a learning experiment, Ted was understandably less rigorous about testing than usual. This meant a lot of minidump-synth was unimplemented when I came along, which also meant lots of minidump features were completely untested. (He built an absolutely great skeleton, just hadn’t had the time to fill it all in!)\nWe spent a lot of time filling in more of minidump-synth’s implementation so we could write more tests and catch more issues, but this is definitely the weakest part of our tests. Some stuff was implemented before I got here, so I don’t even know what tests are missing!\nThis is a good argument for some code coverage checks, but it would probably come back with “wow you should write a lot more tests” and we would all look at it and go “wow we sure should” and then we would probably never get around to it, because there are many things we should do.\nOn the other hand, Sentry has been very useful in this regard because they already have a mature suite of tests full of weird corner cases they’ve built up over time, so they can easily identify things that really matter, know what the fix should roughly be, and can contribute pre-existing test cases!\nIntegration and Snapshot Tests\nWe tried our best to shore up coverage issues in our unit tests by adding more holistic tests. There’s a few checked in Real Minidumps that we have some integration tests for to make sure we handle Real Inputs properly.\nWe even wrote a bunch of integration tests for the CLI application that snapshot its output to confirm that we never accidentally change the results.\nPart of the motivation for this is to ensure we don’t break the JSON output, which we also wrote a very detailed schema document for and are trying to keep stable so people can actually rely on it while the actual implementation details are still in flux.\nYes, minidump-stackwalk is supposed to be stable and reasonable to use in production!\nFor our snapshot tests we use insta, which I think is fantastic and more people should use. All you need to do is assert_snapshot! any output you want to keep track of and it will magically take care of the storing, loading, and diffing.\nHere’s one of the snapshot tests where we invoke the CLI interface and snapshot stdout:\n#[test]\nfn test_evil_json() {\n    // For a while this didn't parse right\n    let bin = env!(\"CARGO_BIN_EXE_minidump-stackwalk\");\n    let output = Command::new(bin)\n        .arg(\"--json\")\n        .arg(\"--pretty\")\n        .arg(\"--raw-json\")\n        .arg(\"../testdata/evil.json\")\n        .arg(\"../testdata/test.dmp\")\n        .arg(\"../testdata/symbols/\")\n        .stdout(Stdio::piped())\n        .stderr(Stdio::piped())\n        .output()\n        .unwrap();\n\n    let stdout = String::from_utf8(output.stdout).unwrap();\n    let stderr = String::from_utf8(output.stderr).unwrap();\n\n    assert!(output.status.success());\n    insta::assert_snapshot!(\"json-pretty-evil-symbols\", stdout);\n    assert_eq!(stderr, \"\");\n}\n\n\nStackwalker Unit Testing\nThe stackwalker is easily the most complicated and subtle part of the new implementation, because every platform can have slight quirks and you need to implement several different unwinding strategies and carefully tune everything to work well in practice.\nThe scariest part of this was the call frame information (CFI) unwinders, because they are basically little virtual machines we need to parse and execute at runtime. Thankfully breakpad had long ago smoothed over this issue by defining a simplified and unified CFI format, STACK CFI (well, nearly unified, x86 Windows was still a special case as STACK WIN). So even if DWARF CFI has a ton of complex features, we mostly need to implement a Reverse Polish Notation Calculator except it can read registers and load memory from addresses it computes (and for STACK WIN it has access to named variables it can declare and mutate).\nUnfortunately, Breakpad’s description for this format is pretty underspecified so I had to basically pick some semantics I thought made sense and go with that. This made me extremely paranoid about the implementation. (And yes I will be more first-person for this part, because this part was genuinely where I personally spent most of my time and did a lot of stuff from scratch. All the blame belongs to me here!)\nThe STACK WIN / STACK CFI parser+evaluator is 1700 lines. 500 of those lines are a detailed documentation and discussion of the format, and 700 of those lines are an enormous pile of ~80 test cases where I tried to come up with every corner case I could think of.\nI even checked in two tests I knew were failing just to be honest that there were a couple cases to fix! One of them is a corner case involving dividing by a negative number that almost certainly just doesn’t matter. The other is a buggy input that old x86 Microsoft toolchains actually produce and parsers need to deal with. The latter was fixed before the fuzzing started.\nAnd 5225225 still found an integer overflow in the STACK WIN preprocessing step! (Not actually that surprising, it’s a hacky mess that tries to cover up for how messed up x86 Windows unwinding tables were.)\n(The code isn’t terribly interesting here, it’s just a ton of assertions that a given input string produces a given output/error.)\nOf course, I wasn’t satisfied with just coming up with my own semantics and testing them: I also ported most of breakpad’s own stackwalker tests to rust-minidump! This definitely found a bunch of bugs I had, but also taught me some weird quirks in Breakpad’s stackwalkers that I’m not sure I actually agree with. But in this case I was flying so blind that even being bug-compatible with Breakpad was some kind of relief.\nThose tests also included several tests for the non-CFI paths, which were similarly wobbly and quirky. I still really hate a lot of the weird platform-specific rules they have for stack scanning, but I’m forced to work on the assumption that they might be load-bearing. (I definitely had several cases where I disabled a breakpad test because it was “obviously nonsense” and then hit it in the wild while testing. I quickly learned to accept that Nonsense Happens And Cannot Be Ignored.)\nOne major thing I didn’t replicate was some of the really hairy hacks for STACK WIN. Like there are several places where they introduce extra stack-scanning to try to deal with the fact that stack frames can have mysterious extra alignment that the windows unwinding tables just don’t tell you about? I guess?\nThere’s almost certainly some exotic situations that rust-minidump does worse on because of this, but it probably also means we do better in some random other situations too. I never got the two to perfectly agree, but at some point the divergences were all in weird enough situations, and as far as I was concerned both stackwalkers were producing equally bad results in a bad situation. Absent any reason to prefer one over the other, divergence seemed acceptable to keep the implementation cleaner.\nHere’s a simplified version of one of the ported breakpad tests, if you’re curious (thankfully minidump-synth is based off of the same binary data mocking framework these tests use):\n#[test]\nfn test_x86_frame_pointer() {\n    let mut f = TestFixture::new();\n    let frame0_ebp = Label::new();\n    let frame1_ebp = Label::new();\n    let mut stack = Section::new();\n\n    // Setup the stack and registers so frame pointers will work\n    stack.start().set_const(0x80000000);\n    stack = stack\n        .append_repeated(12, 0) // frame 0: space\n        .mark(&frame0_ebp)      // frame 0 %ebp points here\n        .D32(&frame1_ebp)       // frame 0: saved %ebp\n        .D32(0x40008679)        // frame 0: return address\n        .append_repeated(8, 0)  // frame 1: space\n        .mark(&frame1_ebp)      // frame 1 %ebp points here\n        .D32(0)                 // frame 1: saved %ebp (stack end)\n        .D32(0);                // frame 1: return address (stack end)\n    f.raw.eip = 0x4000c7a5;\n    f.raw.esp = stack.start().value().unwrap() as u32;\n    f.raw.ebp = frame0_ebp.value().unwrap() as u32;\n\n    // Check the stackwalker's output:\n    let s = f.walk_stack(stack).await;\n    assert_eq!(s.frames.len(), 2);\n    {\n        let f0 = &s.frames[0];\n        assert_eq!(f0.trust, FrameTrust::Context);\n        assert_eq!(f0.context.valid, MinidumpContextValidity::All);\n        assert_eq!(f0.instruction, 0x4000c7a5);\n    }\n    {\n        let f1 = &s.frames[1];\n        assert_eq!(f1.trust, FrameTrust::FramePointer);\n        assert_eq!(f1.instruction, 0x40008678);\n    }\n}\nA Dedicated Production Diffing, Simulating, and Debugging Tool\nBecause minidumps are so horribly fractal and corner-casey, I spent a lot of time terrified of subtle issues that would become huge disasters if we ever actually tried to deploy to production. So I also spent a bunch of time building socc-pair, which takes the id of a crash report from Mozilla’s crash reporting system and pulls down the minidump, the old breakpad-based implementation’s output, and extra metadata.\nIt then runs a local rust-minidump (minidump-stackwalk) implementation on the minidump and does a domain-specific diff over the two inputs. The most substantial part of this is a fuzzy diff on the stackwalks that tries to better handle situations like when one implementation adds an extra frame but the two otherwise agree. It also uses the reported techniques each implementation used to try to identify whose output is more trustworthy when they totally diverge.\nI also ended up adding a bunch of mocking and benchmarking functionality to it as well, as I found more and more places where I just wanted to simulate a production environment.\nOh also I added really detailed trace-logging for the stackwalker so that I could easily post-mortem debug why it made the decisions it made.\nThis tool found so many issues and more importantly has helped me quickly isolate their causes. I am so happy I made it. Because of it, we know we actually fixed several issues that happened with the old breakpad implementation, which is great!\nHere’s a trimmed down version of the kind of report socc-pair would produce (yeah I abused diff syntax to get error highlighting. It’s a great hack, and I love it like a child):\ncomparing json...\n\n: {\n    crash_info: {\n        address: 0x7fff1760aca0\n        crashing_thread: 8\n        type: EXCEPTION_BREAKPOINT\n    }\n    crashing_thread: {\n        frames: [\n            0: {\n                file: wrappers.cpp:1750da2d7f9db490b9d15b3ee696e89e6aa68cb7\n                frame: 0\n                function: RustMozCrash(char const*, int, char const*)\n                function_offset: 0x00000010\n-               did not match\n+               line: 17\n-               line: 20\n                module: xul.dll\n\n.....\n\n    unloaded_modules: [\n        0: {\n            base_addr: 0x7fff48290000\n-           local val was null instead of:\n            code_id: 68798D2F9000\n            end_addr: 0x7fff48299000\n            filename: KBDUS.DLL\n        }\n        1: {\n            base_addr: 0x7fff56020000\n            code_id: DFD6E84B14000\n            end_addr: 0x7fff56034000\n            filename: resourcepolicyclient.dll\n        }\n    ]\n~   ignoring field write_combine_size: \"0\"\n}\n\n- Total errors: 288, warnings: 39\n\nbenchmark results (ms):\n    2388, 1986, 2268, 1989, 2353, \n    average runtime: 00m:02s:196ms (2196ms)\n    median runtime: 00m:02s:268ms (2268ms)\n    min runtime: 00m:01s:986ms (1986ms)\n    max runtime: 00m:02s:388ms (2388ms)\n\nmax memory (rss) results (bytes):\n    267755520, 261152768, 272441344, 276131840, 279134208, \n    average max-memory: 258MB (271323136 bytes)\n    median max-memory: 259MB (272441344 bytes)\n    min max-memory: 249MB (261152768 bytes)\n    max max-memory: 266MB (279134208 bytes)\n\nOutput Files: \n    * (download) Minidump: b4f58e9f-49be-4ba5-a203-8ef160211027.dmp\n    * (download) Socorro Processed Crash: b4f58e9f-49be-4ba5-a203-8ef160211027.json\n    * (download) Raw JSON: b4f58e9f-49be-4ba5-a203-8ef160211027.raw.json\n    * Local minidump-stackwalk Output: b4f58e9f-49be-4ba5-a203-8ef160211027.local.json\n    * Local minidump-stackwalk Logs: b4f58e9f-49be-4ba5-a203-8ef160211027.log.txt\nStaging and Deploying to Production\nOnce we were confident enough in the implementation, a lot of the remaining testing was taken over by Will Kahn-Greene, who’s responsible for a lot of the server-side details of our crash-reporting infrastructure.\nWill spent a bunch of time getting a bunch of machinery setup to manage the deployment and monitoring of rust-minidump. He also did a lot of the hard work of cleaning up all our server-side configuration scripts to handle any differences between the two implementations. (Although I spent a lot of time on compatibility, we both agreed this was a good opportunity to clean up old cruft and mistakes.)\nOnce all of this was set up, he turned it on in staging and we got our first look at how rust-minidump actually worked in ~production:\nTerribly!\nOur staging servers take in about 10% of the inputs that also go to our production servers, but even at that reduced scale we very quickly found several new corner cases and we were getting tons of crashes, which is mildly embarrassing for the thing that handles other people’s crashes.\nWill did a great job here in monitoring and reporting the issues. Thankfully they were all fairly easy for us to fix. Eventually, everything smoothed out and things seemed to be working just as reliably as the old implementation on the production server. The only places where we were completely failing to produce any output were for horribly truncated minidumps that may as well have been empty files.\nWe originally did have some grand ambitions of running socc-pair on everything the staging servers processed or something to get really confident in the results. But by the time we got to that point, we were completely exhausted and feeling pretty confident in the new implementation.\nEventually Will just said “let’s turn it on in production” and I said “AAAAAAAAAAAAAAA”.\nThis moment was pure terror. There had always been more corner cases. There’s no way we could just be done. This will probably set all of Mozilla on fire and delete Firefox from the internet!\nBut Will convinced me. We wrote up some docs detailing all the subtle differences and sent them to everyone we could. Then the moment of truth finally came: Will turned it on in production, and I got to really see how well it worked in production:\n*dramatic drum roll*\nIt worked fine.\nAfter all that stress and anxiety, we turned it on and it was fine.\nHeck, I’ll say it: it ran well.\nIt was faster, it crashed less, and we even knew it fixed some issues.\nI was in a bit of a stupor for the rest of that week, because I kept waiting for the other shoe to drop. I kept waiting for someone to emerge from the mist and explain that I had somehow bricked Thunderbird or something. But no, it just worked.\nSo we left for the holidays, and I kept waiting for it to break, but it was still fine.\nI am honestly still shocked about this!\nBut hey, as it turns out we really did put a lot of careful work into testing the implementation. At every step we found new problems but that was good, because once we got to the final step there were no more problems to surprise us.\nAnd the fuzzer still kicked our butts afterwards.\nBut that’s part 2! Thanks for reading!\n \nThe post Everything Is Broken: Shipping rust-minidump at Mozilla – Part 1 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-06-14T15:05:06.000Z",
      "date_modified": "2022-06-14T15:05:06.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47809",
      "url": "https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/",
      "title": "Training efficient neural network models for Firefox Translations",
      "summary": "The Bergamot project is a collaboration between Mozilla, University of Edinburgh, Charles University in Prague, the University of Sheffield, and University of Tartu with funding from the European Union’s Horizon 2020 research and innovation programme. It brings MT to the local environment, providing small, high-quality, CPU optimized NMT models. The Firefox Translations web extension utilizes proceedings of project Bergamot and brings local translations to Firefox. In this article, we will discuss the components used to train our efficient NMT models.\nThe post Training efficient neural network models for Firefox Translations appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Machine Translation is an important tool for expanding the accessibility of web content. Usually, people use cloud providers to translate web pages. State-of-the-art Neural Machine Translation (NMT) models are large and often require specialized hardware like GPUs to run inference in real-time.</p>\n<p>If people were able to run a compact Machine Translation (MT) model on their local machine CPU without sacrificing translation accuracy it would help to preserve privacy and reduce costs.</p>\n<p>The Bergamot <a href=\"https://browser.mt/\" target=\"_blank\" rel=\"noopener\">project</a> is a collaboration between Mozilla, the University of Edinburgh, Charles University in Prague, the University of Sheffield, and the University of Tartu with funding from the European Union’s Horizon 2020 research and innovation programme. It brings MT to the local environment, providing small, high-quality, CPU optimized NMT models. <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">The Firefox Translations web extension</a> utilizes proceedings of project Bergamot and brings local translations to Firefox.</p>\n<p>In this article, we will discuss the components used to train our efficient NMT models. The project is open-source, so you can give it a try and train your model too!</p>\n<h2><b>Architecture</b></h2>\n<p>NMT models are trained as language pairs, translating from language A to language B. The <a href=\"https://github.com/mozilla/firefox-translations-training\" target=\"_blank\" rel=\"noopener\">training pipeline</a> was designed to train translation models for a language pair end-to-end, from environment configuration to exporting the ready-to-use models. The pipeline run is completely reproducible given the same code, hardware and configuration files.</p>\n<p>The complexity of the pipeline comes from the requirement to produce an efficient model. We use Teacher-Student distillation to compress a high-quality but resource-intensive teacher model into an efficient CPU-optimized student model that still has good translation quality. We explain this further in the Compression section.</p>\n<p>The pipeline includes many steps: compiling of components, downloading and cleaning datasets, training teacher, student and backward models, decoding, quantization, evaluation etc (more details below). The pipeline can be represented as a Directly Acyclic Graph (DAG).</p>\n<p>&nbsp;</p>\n<p><img class=\"aligncenter wp-image-47810\" src=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-500x246.png\" alt=\"Firfox Translation training pipeline DAG\" width=\"591\" height=\"291\" srcset=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-500x246.png 500w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-250x123.png 250w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-768x379.png 768w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-1536x757.png 1536w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-2048x1009.png 2048w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></p>\n<p>The workflow is file-based and employs self-sufficient scripts that use data on disk as input, and write intermediate and output results back to disk.</p>\n<p>We use the Marian Neural Machine Translation engine. It is written in C++ and designed to be fast. The engine is open-sourced and used by many universities and companies, including Microsoft.</p>\n<h2><b>Training a quality model</b></h2>\n<p>The first task of the pipeline is to train a high-quality model that will be compressed later. The main challenge at this stage is to find a good parallel corpus that contains translations of the same sentences in both source and target languages and then apply appropriate cleaning procedures.</p>\n<h3><b>Datasets</b></h3>\n<p>It turned out there are many open-source parallel datasets for machine translation available on the internet. The most interesting project that aggregates such datasets is <a href=\"https://opus.nlpl.eu/\" target=\"_blank\" rel=\"noopener\">OPUS</a>. The Annual Conference on Machine Translation also collects and distributes some datasets for competitions, for example, <a href=\"https://www.statmt.org/wmt21/translation-task.html#download\" target=\"_blank\" rel=\"noopener\">WMT21 Machine Translation of News</a>. Another great source of MT corpus is the <a href=\"https://paracrawl.eu/\" target=\"_blank\" rel=\"noopener\">Paracrawl</a> project.</p>\n<p>OPUS dataset search interface:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47814\" src=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-500x403.png\" alt=\"OPUS dataset search interface\" width=\"591\" height=\"476\" srcset=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-500x403.png 500w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-250x202.png 250w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-768x619.png 768w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-1536x1238.png 1536w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM.png 1992w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></p>\n<p>It is possible to use any dataset on disk, but automating dataset downloading from Open source resources makes adding new language pairs easy, and whenever the data set is expanded we can then easily retrain the model to take advantage of the additional data. Make sure to check the licenses of the open-source datasets before usage.</p>\n<h3><b>Data cleaning</b></h3>\n<p>Most open-source datasets are somewhat noisy. Good examples are crawled websites and translation of subtitles. Texts from websites can be poor-quality automatic translations or contain unexpected HTML, and subtitles are often free-form translations that change the meaning of the text.</p>\n<p>It is well known in the world of Machine Learning (ML) that if we feed garbage into the model we get garbage as a result. Dataset cleaning is probably the most crucial step in the pipeline to achieving good quality.</p>\n<p>We employ some basic cleaning techniques that work for most datasets like removing too short or too long sentences and filtering the ones with an unrealistic source to target length ratio. We also use <a href=\"https://github.com/bitextor/bicleaner\" target=\"_blank\" rel=\"noopener\">bicleaner</a>, a pre-trained ML classifier that attempts to indicate whether the training example in a dataset is a reversible translation. We can then remove low-scoring translation pairs that may be incorrect or otherwise add unwanted noise.</p>\n<p>Automation is necessary when your training set is large. However, it is always recommended to look at your data manually in order to tune the cleaning thresholds and add dataset-specific fixes to get the best quality.</p>\n<h3><b>Data augmentation</b></h3>\n<p>There are more than 7000 languages spoken in the world and most of them are classified as low-resource for our purposes, meaning there is little parallel corpus data available for training. In these cases, we use a popular data augmentation strategy called back-translation.</p>\n<p>Back-translation is a technique to increase the amount of training data available by adding synthetic translations. We get these synthetic examples by training a translation model from the target language to the source language. Then we use it to translate monolingual data from the target language into the source language, creating synthetic examples that are added to the training data for the model we actually want, from the source language to the target language.</p>\n<h3><b>The model</b></h3>\n<p>Finally, when we have a clean parallel corpus we train a big transformer model to reach the best quality we can.</p>\n<p>Once the model converges on the augmented dataset, we fine-tune it on the original parallel corpus that doesn’t include synthetic examples from back-translation to further improve quality.</p>\n<h2><b>Compression</b></h2>\n<p>The trained model can be 800Mb or more in size depending on configuration and requires significant computing power to perform translation (decoding). At this point, it’s generally executed on GPUs and not practical to run on most consumer laptops. In the next steps we will prepare a model that works efficiently on consumer CPUs.</p>\n<h3><b>Knowledge distillation</b></h3>\n<p>The main technique we use for compression is Teacher-Student Knowledge Distillation. The idea is to decode a lot of text from the source language into the target language using the heavy model we trained (Teacher) and then train a much smaller model with fewer parameters (Student) on these synthetic translations. The student is supposed to imitate the teacher’s behavior and demonstrate similar translation quality despite being significantly faster and more compact.</p>\n<p>We also augment the parallel corpus data with monolingual data in the source language for decoding. This improves the student by providing additional training examples of the teacher&#8217;s behavior.</p>\n<h3><b>Ensemble</b></h3>\n<p>Another trick is to use not just one teacher but an ensemble of 2-4 teachers independently trained on the same parallel corpus. It can boost quality a little bit at the cost of having to train more teachers. The pipeline supports training and decoding with an ensemble of teachers.</p>\n<h3><b>Quantization</b></h3>\n<p>One more popular technique for model compression is quantization. We use 8-bit quantization which essentially means that we store weights of the neural net as int8 instead of float32. It saves space and speeds up matrix multiplication on inference.</p>\n<h3><b>Other tricks</b></h3>\n<p>Other features worth mentioning but beyond the scope of this already lengthy article are the specialized Neural Network architecture of the student model, half-precision decoding by the teacher model to speed it up, lexical shortlists, training of word alignments, and finetuning of the quantized student.</p>\n<p>Yes, it’s a lot! Now you can see why we wanted to have an end-to-end pipeline.</p>\n<h2><b>How to learn more</b></h2>\n<p>This work is based on a lot of research. If you are interested in the science behind the training pipeline, check out reference publications listed <a href=\"https://github.com/mozilla/firefox-translations-training#references\" target=\"_blank\" rel=\"noopener\">in the training pipeline repository README</a> and <a href=\"https://browser.mt/publications\" target=\"_blank\" rel=\"noopener\">across the wider Bergamot project</a>. <a href=\"https://aclanthology.org/2020.ngt-1.26/\" target=\"_blank\" rel=\"noopener\">Edinburgh&#8217;s Submissions to the 2020 Machine Translation Efficiency Task</a> is a good academic starting article. Check <a href=\"https://nbogoychev.com/efficient-machine-translation/\" target=\"_blank\" rel=\"noopener\">this tutorial</a> by Nikolay Bogoychev for a more practical and operational explanation of the steps.</p>\n<h2><b>Results</b></h2>\n<p>The final student model is 47 times smaller and 37 times faster than the original teacher model and has only a small quality decrease!</p>\n<p>Benchmarks for en-pt model and Flores dataset:</p>\n<table style=\"table-layout: fixed; width: 100%;\">\n<tbody>\n<tr>\n<td><b>Model</b></td>\n<td><b>Size</b></td>\n<td><b>Total number of parameters</b></td>\n<td><b>Dataset decoding time on 1 CPU core</b></td>\n<td><b>Quality, BLEU</b></td>\n</tr>\n<tr>\n<td>Teacher</td>\n<td>798Mb</td>\n<td>192.75M</td>\n<td>631s</td>\n<td>52.5</td>\n</tr>\n<tr>\n<td>Student quantized</td>\n<td>17Mb</td>\n<td>15.7M</td>\n<td>17.9s</td>\n<td>50.7</td>\n</tr>\n</tbody>\n</table>\n<p>We evaluate results using MT standard <a href=\"https://en.wikipedia.org/wiki/BLEU\" target=\"_blank\" rel=\"noopener\">BLEU scores</a> that essentially represent how similar translated and reference texts are. This method is not perfect but it has been shown that BLEU scores correlate well with human judgment of translation quality.</p>\n<p>We have a <a href=\"https://github.com/mozilla/firefox-translations-models\" target=\"_blank\" rel=\"noopener\">GitHub repository</a> with all the trained models and <a href=\"https://github.com/mozilla/firefox-translations-models/blob/main/evaluation/prod/results.md\" target=\"_blank\" rel=\"noopener\">evaluation results</a> where we compare the accuracy of our models to popular APIs of cloud providers. We can see that some models perform similarly, or even outperform, the cloud providers which is a great result taking into account our model’s efficiency, reproducibility and open-source nature.</p>\n<p>For example, here you can see evaluation results for the English to Portuguese model trained by Mozilla using open-source data only.</p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47818\" src=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM.png\" alt=\"Evaluation results en-pt\" width=\"591\" height=\"476\" srcset=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM.png 2066w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-250x201.png 250w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-500x403.png 500w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-768x619.png 768w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-1536x1237.png 1536w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-2048x1650.png 2048w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></p>\n<p>Anyone can train models and contribute them to our repo. Those contributions can be used in the <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">Firefox Translations web extension</a> and other places (see below).</p>\n<h2><b>Scaling</b></h2>\n<p>It is of course possible to run the whole pipeline on one machine, though it may take a while. Some steps of the pipeline are CPU bound and difficult to parallelize, while other steps can be offloaded to multiple GPUs. Most of the official models in the repository were trained on machines with 8 GPUs. A few steps, like teacher decoding during knowledge distillation, can take days even on well-resourced single machines. So to speed things up, we added cluster support to be able to spread different steps of the pipeline over multiple nodes.</p>\n<h3><b>Workflow manager</b></h3>\n<p>To manage this complexity we chose <a href=\"https://snakemake.github.io/\" target=\"_blank\" rel=\"noopener\">Snakemake</a> which is very popular in the bioinformatics community. It uses file-based workflows, allows specifying step dependencies in Python, supports containerization and integration with different cluster software. We considered alternative solutions that focus on job scheduling, but ultimately chose Snakemake because it was more ergonomic for one-run experimentation workflows.</p>\n<p>Example of a Snakemake rule (dependencies between rules are inferred implicitly):</p>\n<pre><code class=\"js\">rule train_teacher:\r\n    message: \"Training teacher on all data\"\r\n    log: f\"{log_dir}/train_teacher{{ens}}.log\"\r\n    conda: \"envs/base.yml\"\r\n    threads: gpus_num*2\r\n    resources: gpu=gpus_num\r\n    input:\r\n        rules.merge_devset.output, \r\n        train_src=f'{teacher_corpus}.{src}.gz',\r\n        train_trg=f'{teacher_corpus}.{trg}.gz',\r\n        bin=ancient(trainer), \r\n        vocab=vocab_path\r\n    output: model=f'{teacher_base_dir}{{ens}}/{best_model}'\r\n    params: \r\n        prefix_train=teacher_corpus, \r\n        prefix_test=f\"{original}/devset\", \r\n        dir=directory(f'{teacher_base_dir}{{ens}}'),\r\n        args=get_args(\"training-teacher-base\")\r\n    shell: '''bash pipeline/train/train.sh \\\r\n                teacher train {src} {trg} \"{params.prefix_train}\" \\\r\n                \"{params.prefix_test}\" \"{params.dir}\" \\\r\n                \"{input.vocab}\" {params.args} &gt;&gt; {log} 2&gt;&amp;1'''</code></pre>\n<h3><b>Cluster support</b></h3>\n<p>To parallelize workflow steps across cluster nodes we use <a href=\"https://slurm.schedmd.com/\">Slurm</a> resource manager. It is relatively simple to operate, fits well for high-performance experimentation workflows, and supports Singularity containers for easier reproducibility. Slurm is also the most popular cluster manager for High-Performance Computers (HPC) used for model training in academia, and most of the consortium partners were already using or familiar with it.</p>\n<h2><b>How to start training</b></h2>\n<p>The workflow is quite resource-intensive, so you’ll need a pretty good server machine or even a cluster. We recommend using 4-8 Nvidia 2080-equivalent or better GPUs per machine.</p>\n<p>Clone <a href=\"https://github.com/mozilla/firefox-translations-training\" target=\"_blank\" rel=\"noopener\">https://github.com/mozilla/firefox-translations-training</a> and follow the instructions in the <a href=\"https://github.com/mozilla/firefox-translations-training/blob/main/README.md\" target=\"_blank\" rel=\"noopener\">readme</a> for configuration.</p>\n<p>The most important part is to find parallel datasets and properly configure settings based on your available data and hardware. You can learn more about this in the readme.</p>\n<h2><b>How to use the existing models</b></h2>\n<p>The existing models are shipped with the <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">Firefox Translations web extension</a>, enabling users to translate web pages in Firefox. The models are downloaded to a local machine on demand. The web extension uses these models with the<a href=\"https://github.com/browsermt/bergamot-translator\" target=\"_blank\" rel=\"noopener\"> bergamot-translator</a> Marian wrapper compiled to Web Assembly.</p>\n<p>Also, there is a playground website at <a href=\"https://mozilla.github.io/translate\" target=\"_blank\" rel=\"noopener\">https://mozilla.github.io/translate</a> where you can input text and translate it right away, also locally but served as a static website instead of a browser extension.</p>\n<p>If you are interested in an efficient NMT inference on the server, you can try a prototype <a href=\"https://github.com/mozilla/translation-service\" target=\"_blank\" rel=\"noopener\">HTTP service</a> that uses bergamot-translator natively compiled, instead of compiled to WASM.</p>\n<p>Or follow the build instructions in the <a href=\"https://github.com/browsermt/bergamot-translator#build-instructions\" target=\"_blank\" rel=\"noopener\">bergamot-translator readme</a> to directly use the C++, JavaScript WASM, or Python bindings.</p>\n<h2><b>Conclusion</b></h2>\n<p>It is fascinating how far Machine Translation research has come in recent years. Local high-quality translations are the future and it’s becoming more and more practical for companies and researchers to train such models even without access to proprietary data or large-scale computing power.</p>\n<p>We hope that <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">Firefox Translations</a> will set a new standard of privacy-preserving, efficient, open-source machine translation accessible for all.</p>\n<h2><b>Acknowledgements</b></h2>\n<p>I would like to thank all the participants of <a href=\"https://browser.mt/\" target=\"_blank\" rel=\"noopener\">the Bergamot Project</a> for making this technology possible, my teammates Andre Natal and Abhishek Aggarwal for the incredible work they have done bringing Firefox Translations to life, Lonnen for managing the project and editing this blog post and of course awesome Mozilla community for helping with localization of the web-extension and testing its early builds.</p>\n<p><i>This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 825303 <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f1ea-1f1fa.png\" alt=\"🇪🇺\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></i></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/\">Training efficient neural network models for Firefox Translations</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Machine Translation is an important tool for expanding the accessibility of web content. Usually, people use cloud providers to translate web pages. State-of-the-art Neural Machine Translation (NMT) models are large and often require specialized hardware like GPUs to run inference in real-time.\nIf people were able to run a compact Machine Translation (MT) model on their local machine CPU without sacrificing translation accuracy it would help to preserve privacy and reduce costs.\nThe Bergamot project is a collaboration between Mozilla, the University of Edinburgh, Charles University in Prague, the University of Sheffield, and the University of Tartu with funding from the European Union’s Horizon 2020 research and innovation programme. It brings MT to the local environment, providing small, high-quality, CPU optimized NMT models. The Firefox Translations web extension utilizes proceedings of project Bergamot and brings local translations to Firefox.\nIn this article, we will discuss the components used to train our efficient NMT models. The project is open-source, so you can give it a try and train your model too!\nArchitecture\nNMT models are trained as language pairs, translating from language A to language B. The training pipeline was designed to train translation models for a language pair end-to-end, from environment configuration to exporting the ready-to-use models. The pipeline run is completely reproducible given the same code, hardware and configuration files.\nThe complexity of the pipeline comes from the requirement to produce an efficient model. We use Teacher-Student distillation to compress a high-quality but resource-intensive teacher model into an efficient CPU-optimized student model that still has good translation quality. We explain this further in the Compression section.\nThe pipeline includes many steps: compiling of components, downloading and cleaning datasets, training teacher, student and backward models, decoding, quantization, evaluation etc (more details below). The pipeline can be represented as a Directly Acyclic Graph (DAG).\n \n\nThe workflow is file-based and employs self-sufficient scripts that use data on disk as input, and write intermediate and output results back to disk.\nWe use the Marian Neural Machine Translation engine. It is written in C++ and designed to be fast. The engine is open-sourced and used by many universities and companies, including Microsoft.\nTraining a quality model\nThe first task of the pipeline is to train a high-quality model that will be compressed later. The main challenge at this stage is to find a good parallel corpus that contains translations of the same sentences in both source and target languages and then apply appropriate cleaning procedures.\nDatasets\nIt turned out there are many open-source parallel datasets for machine translation available on the internet. The most interesting project that aggregates such datasets is OPUS. The Annual Conference on Machine Translation also collects and distributes some datasets for competitions, for example, WMT21 Machine Translation of News. Another great source of MT corpus is the Paracrawl project.\nOPUS dataset search interface:\n\nIt is possible to use any dataset on disk, but automating dataset downloading from Open source resources makes adding new language pairs easy, and whenever the data set is expanded we can then easily retrain the model to take advantage of the additional data. Make sure to check the licenses of the open-source datasets before usage.\nData cleaning\nMost open-source datasets are somewhat noisy. Good examples are crawled websites and translation of subtitles. Texts from websites can be poor-quality automatic translations or contain unexpected HTML, and subtitles are often free-form translations that change the meaning of the text.\nIt is well known in the world of Machine Learning (ML) that if we feed garbage into the model we get garbage as a result. Dataset cleaning is probably the most crucial step in the pipeline to achieving good quality.\nWe employ some basic cleaning techniques that work for most datasets like removing too short or too long sentences and filtering the ones with an unrealistic source to target length ratio. We also use bicleaner, a pre-trained ML classifier that attempts to indicate whether the training example in a dataset is a reversible translation. We can then remove low-scoring translation pairs that may be incorrect or otherwise add unwanted noise.\nAutomation is necessary when your training set is large. However, it is always recommended to look at your data manually in order to tune the cleaning thresholds and add dataset-specific fixes to get the best quality.\nData augmentation\nThere are more than 7000 languages spoken in the world and most of them are classified as low-resource for our purposes, meaning there is little parallel corpus data available for training. In these cases, we use a popular data augmentation strategy called back-translation.\nBack-translation is a technique to increase the amount of training data available by adding synthetic translations. We get these synthetic examples by training a translation model from the target language to the source language. Then we use it to translate monolingual data from the target language into the source language, creating synthetic examples that are added to the training data for the model we actually want, from the source language to the target language.\nThe model\nFinally, when we have a clean parallel corpus we train a big transformer model to reach the best quality we can.\nOnce the model converges on the augmented dataset, we fine-tune it on the original parallel corpus that doesn’t include synthetic examples from back-translation to further improve quality.\nCompression\nThe trained model can be 800Mb or more in size depending on configuration and requires significant computing power to perform translation (decoding). At this point, it’s generally executed on GPUs and not practical to run on most consumer laptops. In the next steps we will prepare a model that works efficiently on consumer CPUs.\nKnowledge distillation\nThe main technique we use for compression is Teacher-Student Knowledge Distillation. The idea is to decode a lot of text from the source language into the target language using the heavy model we trained (Teacher) and then train a much smaller model with fewer parameters (Student) on these synthetic translations. The student is supposed to imitate the teacher’s behavior and demonstrate similar translation quality despite being significantly faster and more compact.\nWe also augment the parallel corpus data with monolingual data in the source language for decoding. This improves the student by providing additional training examples of the teacher’s behavior.\nEnsemble\nAnother trick is to use not just one teacher but an ensemble of 2-4 teachers independently trained on the same parallel corpus. It can boost quality a little bit at the cost of having to train more teachers. The pipeline supports training and decoding with an ensemble of teachers.\nQuantization\nOne more popular technique for model compression is quantization. We use 8-bit quantization which essentially means that we store weights of the neural net as int8 instead of float32. It saves space and speeds up matrix multiplication on inference.\nOther tricks\nOther features worth mentioning but beyond the scope of this already lengthy article are the specialized Neural Network architecture of the student model, half-precision decoding by the teacher model to speed it up, lexical shortlists, training of word alignments, and finetuning of the quantized student.\nYes, it’s a lot! Now you can see why we wanted to have an end-to-end pipeline.\nHow to learn more\nThis work is based on a lot of research. If you are interested in the science behind the training pipeline, check out reference publications listed in the training pipeline repository README and across the wider Bergamot project. Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task is a good academic starting article. Check this tutorial by Nikolay Bogoychev for a more practical and operational explanation of the steps.\nResults\nThe final student model is 47 times smaller and 37 times faster than the original teacher model and has only a small quality decrease!\nBenchmarks for en-pt model and Flores dataset:\n\n\n\nModel\nSize\nTotal number of parameters\nDataset decoding time on 1 CPU core\nQuality, BLEU\n\n\nTeacher\n798Mb\n192.75M\n631s\n52.5\n\n\nStudent quantized\n17Mb\n15.7M\n17.9s\n50.7\n\n\n\nWe evaluate results using MT standard BLEU scores that essentially represent how similar translated and reference texts are. This method is not perfect but it has been shown that BLEU scores correlate well with human judgment of translation quality.\nWe have a GitHub repository with all the trained models and evaluation results where we compare the accuracy of our models to popular APIs of cloud providers. We can see that some models perform similarly, or even outperform, the cloud providers which is a great result taking into account our model’s efficiency, reproducibility and open-source nature.\nFor example, here you can see evaluation results for the English to Portuguese model trained by Mozilla using open-source data only.\n\nAnyone can train models and contribute them to our repo. Those contributions can be used in the Firefox Translations web extension and other places (see below).\nScaling\nIt is of course possible to run the whole pipeline on one machine, though it may take a while. Some steps of the pipeline are CPU bound and difficult to parallelize, while other steps can be offloaded to multiple GPUs. Most of the official models in the repository were trained on machines with 8 GPUs. A few steps, like teacher decoding during knowledge distillation, can take days even on well-resourced single machines. So to speed things up, we added cluster support to be able to spread different steps of the pipeline over multiple nodes.\nWorkflow manager\nTo manage this complexity we chose Snakemake which is very popular in the bioinformatics community. It uses file-based workflows, allows specifying step dependencies in Python, supports containerization and integration with different cluster software. We considered alternative solutions that focus on job scheduling, but ultimately chose Snakemake because it was more ergonomic for one-run experimentation workflows.\nExample of a Snakemake rule (dependencies between rules are inferred implicitly):\nrule train_teacher:\n    message: \"Training teacher on all data\"\n    log: f\"{log_dir}/train_teacher{{ens}}.log\"\n    conda: \"envs/base.yml\"\n    threads: gpus_num*2\n    resources: gpu=gpus_num\n    input:\n        rules.merge_devset.output, \n        train_src=f'{teacher_corpus}.{src}.gz',\n        train_trg=f'{teacher_corpus}.{trg}.gz',\n        bin=ancient(trainer), \n        vocab=vocab_path\n    output: model=f'{teacher_base_dir}{{ens}}/{best_model}'\n    params: \n        prefix_train=teacher_corpus, \n        prefix_test=f\"{original}/devset\", \n        dir=directory(f'{teacher_base_dir}{{ens}}'),\n        args=get_args(\"training-teacher-base\")\n    shell: '''bash pipeline/train/train.sh \\\n                teacher train {src} {trg} \"{params.prefix_train}\" \\\n                \"{params.prefix_test}\" \"{params.dir}\" \\\n                \"{input.vocab}\" {params.args} >> {log} 2>&1'''\nCluster support\nTo parallelize workflow steps across cluster nodes we use Slurm resource manager. It is relatively simple to operate, fits well for high-performance experimentation workflows, and supports Singularity containers for easier reproducibility. Slurm is also the most popular cluster manager for High-Performance Computers (HPC) used for model training in academia, and most of the consortium partners were already using or familiar with it.\nHow to start training\nThe workflow is quite resource-intensive, so you’ll need a pretty good server machine or even a cluster. We recommend using 4-8 Nvidia 2080-equivalent or better GPUs per machine.\nClone https://github.com/mozilla/firefox-translations-training and follow the instructions in the readme for configuration.\nThe most important part is to find parallel datasets and properly configure settings based on your available data and hardware. You can learn more about this in the readme.\nHow to use the existing models\nThe existing models are shipped with the Firefox Translations web extension, enabling users to translate web pages in Firefox. The models are downloaded to a local machine on demand. The web extension uses these models with the bergamot-translator Marian wrapper compiled to Web Assembly.\nAlso, there is a playground website at https://mozilla.github.io/translate where you can input text and translate it right away, also locally but served as a static website instead of a browser extension.\nIf you are interested in an efficient NMT inference on the server, you can try a prototype HTTP service that uses bergamot-translator natively compiled, instead of compiled to WASM.\nOr follow the build instructions in the bergamot-translator readme to directly use the C++, JavaScript WASM, or Python bindings.\nConclusion\nIt is fascinating how far Machine Translation research has come in recent years. Local high-quality translations are the future and it’s becoming more and more practical for companies and researchers to train such models even without access to proprietary data or large-scale computing power.\nWe hope that Firefox Translations will set a new standard of privacy-preserving, efficient, open-source machine translation accessible for all.\nAcknowledgements\nI would like to thank all the participants of the Bergamot Project for making this technology possible, my teammates Andre Natal and Abhishek Aggarwal for the incredible work they have done bringing Firefox Translations to life, Lonnen for managing the project and editing this blog post and of course awesome Mozilla community for helping with localization of the web-extension and testing its early builds.\nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 825303 \nThe post Training efficient neural network models for Firefox Translations appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-06-07T15:25:47.000Z",
      "date_modified": "2022-06-07T15:25:47.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47829",
      "url": "https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/",
      "title": "Improved Process Isolation in Firefox 100",
      "summary": "Firefox uses a multi-process model for additional security and stability while browsing: Web Content (such as HTML/CSS and Javascript) is rendered in separate processes that are isolated from the rest of the operating system and managed by a privileged parent process. This way, the amount of control gained by an attacker that exploits a bug in a content process is limited. In this article, we would like to dive a bit further into the latest major milestone we have reached: Win32k Lockdown, which greatly reduces the capabilities of the content process when running on Windows.\nThe post Improved Process Isolation in Firefox 100 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<h2><strong>Introduction</strong></h2>\n<p><span style=\"font-weight: 400;\">Firefox uses a </span><a href=\"https://hacks.mozilla.org/2021/05/introducing-firefox-new-site-isolation-security-architecture/\"><span style=\"font-weight: 400;\">multi-process model</span></a><span style=\"font-weight: 400;\"> for additional security and stability while browsing: Web Content (such as HTML/CSS and Javascript) is rendered in separate processes that are isolated from the rest of the operating system and managed by a privileged parent process. This way, the amount of control gained by an attacker that exploits a bug in a content process is limited. </span></p>\n<p><span style=\"font-weight: 400;\">Ever since we deployed this model, we have been working on improving the isolation of the content processes to further limit the attack surface. This is a challenging task since content processes need access to some operating system APIs to properly function: for example, they still need to be able to talk to the parent process. </span></p>\n<p><span style=\"font-weight: 400;\">In this article, we would like to dive a bit further into the latest major milestone we have reached: </span><i><span style=\"font-weight: 400;\">Win32k Lockdown,</span></i><span style=\"font-weight: 400;\"> which greatly reduces the capabilities of the content process when running on Windows. Together with two major earlier efforts (</span><a href=\"https://hacks.mozilla.org/2021/05/introducing-firefox-new-site-isolation-security-architecture/\"><span style=\"font-weight: 400;\">Fission</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://hacks.mozilla.org/2021/12/webassembly-and-back-again-fine-grained-sandboxing-in-firefox-95/\"><span style=\"font-weight: 400;\">RLBox</span></a><span style=\"font-weight: 400;\">) that shipped before, this completes a sequence of large leaps forward that will significantly improve Firefox&#8217;s security.</span></p>\n<p><span style=\"font-weight: 400;\">Although </span><i><span style=\"font-weight: 400;\">Win32k Lockdown</span></i><span style=\"font-weight: 400;\"> is a Windows-specific technique, it became possible because of a significant re-architecting of the Firefox security boundaries that Mozilla has been working on for around four years, which allowed similar security advances to be made on other operating systems.</span></p>\n<h2><strong>The Goal: Win32k Lockdown</strong></h2>\n<p><span style=\"font-weight: 400;\">Firefox runs the processes that render web content with quite a few restrictions on what they are allowed to do when running on Windows. Unfortunately, by default they still have access to the entire Windows API, which opens up a large attack surface: the Windows API consists of many parts, for example, a core part dealing with threads, processes, and memory management, but also networking and socket libraries, printing and multimedia APIs, and so on.</span></p>\n<p><span style=\"font-weight: 400;\">Of particular interest for us is the </span><i><span style=\"font-weight: 400;\">win32k.sys API,</span></i><span style=\"font-weight: 400;\"> which includes many graphical and widget related system calls that have a history of being exploitable. Going back further in Windows&#8217; origins, this situation is likely the result of Microsoft moving many operations that were originally running in user mode into the kernel in order to improve performance around the Windows 95 and NT4 timeframe. </span></p>\n<p><span style=\"font-weight: 400;\">Having likely never been originally designed to run in this sensitive context, these APIs have been a traditional target for hackers to break out of application sandboxes and into the kernel.</span></p>\n<p><span style=\"font-weight: 400;\">In Windows 8, Microsoft introduced a new mitigation named </span><a href=\"https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-process_mitigation_system_call_disable_policy\"><span style=\"font-weight: 400;\">PROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY</span></a><span style=\"font-weight: 400;\"> that an application can use to disable access to win32k.sys system calls. That is a long name to keep repeating, so we&#8217;ll refer to it hereafter by our internal designation: &#8220;</span><i><span style=\"font-weight: 400;\">Win32k Lockdown</span></i><span style=\"font-weight: 400;\">&#8220;.</span></p>\n<h2><strong>The Work Required</strong></h2>\n<p><span style=\"font-weight: 400;\">Flipping the Win32k Lockdown flag on the Web Content processes &#8211; the processes most vulnerable to potentially hostile web pages and JavaScript &#8211; means that those processes can no longer perform any graphical, window management, input processing, etc. operations themselves. </span></p>\n<p><span style=\"font-weight: 400;\">To accomplish these tasks, such operations must be remoted to a process that has the necessary permissions, typically the process that has access to the GPU and handles compositing and drawing (hereafter called the GPU Process), or the privileged parent process. </span></p>\n<h3><span style=\"font-weight: 400;\">Drawing web pages: WebRender</span></h3>\n<p><span style=\"font-weight: 400;\">For painting the web pages&#8217; contents, Firefox historically used various methods for interacting with the Windows APIs, ranging from using modern Direct3D based textures, to falling back to GDI surfaces, and eventually dropping into pure software mode. </span></p>\n<p><span style=\"font-weight: 400;\">These different options would have taken quite some work to remote, as most of the graphics API is off limits in Win32k Lockdown. The good news is that as of Firefox 92, our rendering stack has switched to </span><a href=\"https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/\"><span style=\"font-weight: 400;\">WebRender</span></a><span style=\"font-weight: 400;\">, which moves all the actual drawing from the content processes to WebRender in the GPU Process.</span></p>\n<p><span style=\"font-weight: 400;\">Because with WebRender the content process no longer has a need to directly interact with the platform drawing APIs, this avoids any Win32k Lockdown related problems. WebRender itself has been designed partially to be </span><a href=\"https://github.com/servo/webrender/wiki/\"><span style=\"font-weight: 400;\">more similar to game engines, and thus, be less susceptible to driver bugs</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">For the remaining drivers that are just too broken to be of any use, it still has a </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1601053\"><span style=\"font-weight: 400;\">fully software-based mode</span></a><span style=\"font-weight: 400;\">, which means we have no further fallbacks to consider.</span></p>\n<h3><span style=\"font-weight: 400;\">Webpages drawing: Canvas 2D and WebGL 3D</span></h3>\n<p><span style=\"font-weight: 400;\">The </span><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API\"><span style=\"font-weight: 400;\">Canvas API</span></a><span style=\"font-weight: 400;\"> provides web pages with the ability to draw 2D graphics. In the original Firefox implementation, these JavaScript APIs were executed in the Web Content processes and the calls to the Windows drawing APIs were made directly from the same processes. </span></p>\n<p><span style=\"font-weight: 400;\">In a Win32k Lockdown scenario, this is no longer possible, so </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1464032\"><span style=\"font-weight: 400;\">all drawing commands are remoted</span></a><span style=\"font-weight: 400;\"> by recording and playing them back in the GPU process over IPC.</span></p>\n<p><span style=\"font-weight: 400;\">Although the initial implementation had good performance, there were nevertheless reports from some sites that experienced performance regressions (the web sites that became faster generally didn&#8217;t complain!). A particular pain point are applications that call </span><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/getImageData\"><span style=\"font-weight: 400;\">getImageData()</span></a><span style=\"font-weight: 400;\"> repeatedly: having the Canvas remoted means that GPU textures must now be obtained from another process and sent over IPC. </span></p>\n<p><span style=\"font-weight: 400;\">We compensated for this in the scenario where getImageData is called at the start of a frame, by detecting this and preparing the right surfaces proactively to make the copying from the GPU faster.</span></p>\n<p><span style=\"font-weight: 400;\">Besides the Canvas API to draw 2D graphics, the web platform also exposes an </span><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API\"><span style=\"font-weight: 400;\">API to do 3D drawing, called WebGL</span></a><span style=\"font-weight: 400;\">. WebGL is a state-heavy API, so properly and efficiently synchronizing child and parent (as well as parent and driver) takes </span><a href=\"https://phabricator.services.mozilla.com/D54019\"><span style=\"font-weight: 400;\">great</span></a> <a href=\"https://phabricator.services.mozilla.com/D54019\"><span style=\"font-weight: 400;\">care</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">WebGL originally handled all validation in Content, but with access to the GPU and the associated attack surface removed from there, we needed to craft a robust validating API between child and parent as well to get the full security benefit.</span></p>\n<h3><span style=\"font-weight: 400;\">(Non-)Native Theming for Forms</span></h3>\n<p><span style=\"font-weight: 400;\">HTML web pages have the ability to display form controls. While the overwhelming majority of websites provide a </span><a href=\"https://developer.mozilla.org/en-US/docs/Learn/Forms/Advanced_form_styling\"><span style=\"font-weight: 400;\">custom look and styling for those form controls</span></a><span style=\"font-weight: 400;\">, not all of them do, and if they do not you get an input GUI widget that is styled like (and originally was!) a </span><a href=\"http://stephenhorlander.com/form-controls.html\"><span style=\"font-weight: 400;\">native element of the operating system</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\"> Historically, these were drawn by calling the appropriate OS widget APIs from within the content process, but those are not available under Win32k Lockdown. </span></p>\n<p><span style=\"font-weight: 400;\">This cannot easily be fixed by remoting the calls, as the widgets themselves come in an infinite amount of sizes, shapes, and styles can be interacted with, and need to be responsive to user input and dispatch messages. We settled on having Firefox </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1381938\"><span style=\"font-weight: 400;\">draw the form controls itself</span></a><span style=\"font-weight: 400;\">, in a cross-platform style. </span></p>\n<p><span style=\"font-weight: 400;\">While changing the look of form controls has web compatibility implications, and some people prefer the more native look &#8211; on the few pages that don&#8217;t apply their own styles to controls &#8211; Firefox’s approach is consistent with that taken by other browsers, probably because of very similar considerations.</span></p>\n<p><span style=\"font-weight: 400;\">Scrollbars were a particular pain point: we didn&#8217;t want to draw the main scrollbar of the content window in a different manner as the rest of the UX, since nested scrollbars would show up with different styles which would look awkward. But, unlike the rather rare non-styled form widgets, the main scrollbar is visible on most web pages, and because it conceptually belongs to the browser UX we really wanted it to look native. </span></p>\n<p><span style=\"font-weight: 400;\">We, therefore, decided to draw all scrollbars to match the system theme, although it&#8217;s a bit of an open question though how things should look if even </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1719427#c3\"><span style=\"font-weight: 400;\">the vendor of the operating system can&#8217;t seem to decide what the &#8220;native&#8221; look is</span></a><span style=\"font-weight: 400;\">.</span></p>\n<h2><strong>Final Hurdles</strong></h2>\n<h3><span style=\"font-weight: 400;\">Line Breaking</span></h3>\n<p><span style=\"font-weight: 400;\">With the above changes, we thought we had all the usual suspects that would access graphics and widget APIs in win32k.sys wrapped up, so we started running the full Firefox test suite with win32k syscalls disabled. This caused at least one unexpected failure: Firefox was </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1713973\"><span style=\"font-weight: 400;\">crashing when trying to find line breaks</span></a><span style=\"font-weight: 400;\"> for some languages with complex scripts. </span></p>\n<p><span style=\"font-weight: 400;\">While Firefox is able to correctly determine word endings in multibyte character streams for most languages by itself, the support for Thai, Lao, Tibetan and Khmer is known to be imperfect, and </span><a href=\"https://searchfox.org/mozilla-central/rev/80f11ac5d938f6fce255c56279f46f13a49ea5c3/intl/lwbrk/LineBreaker.h#65\"><span style=\"font-weight: 400;\">in these cases, Firefox can ask the operating system to handle the line breaking</span></a><span style=\"font-weight: 400;\"> for it. But at least on Windows, the functions to do so are covered by the Win32k Lockdown switch. Oops!</span></p>\n<p><span style=\"font-weight: 400;\">There are </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1684927\"><span style=\"font-weight: 400;\">efforts underway to incorporate ICU4X</span></a><span style=\"font-weight: 400;\"> and base all i18n related functionality on that, meaning that Firefox will be able to handle all scripts perfectly without involving the OS, but this is a major effort and it was not clear if it would end up delaying the rollout of win32k lockdown. </span></p>\n<p><span style=\"font-weight: 400;\">We did some experimentation with trying to forward the line breaking over IPC. Initially, this had bad performance, but when we </span><a href=\"https://phabricator.services.mozilla.com/D129125\"><span style=\"font-weight: 400;\">added caching</span></a><span style=\"font-weight: 400;\"> performance was satisfactory or sometimes even improved, since OS calls could be avoided in many cases now.</span></p>\n<h3><span style=\"font-weight: 400;\">DLL Loading &amp; Third Party Interactions</span></h3>\n<p><span style=\"font-weight: 400;\">Another complexity of disabling win32k.sys access is that so much Windows functionality assumes it is available by default, and specific effort must be taken to ensure the relevant DLLs do not get loaded on startup. Firefox itself for example won&#8217;t load the user32 DLL containing some win32k APIs, but </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1719212\"><span style=\"font-weight: 400;\">injected third party DLLs sometimes do</span></a><span style=\"font-weight: 400;\">. This causes problems because </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1751367\"><span style=\"font-weight: 400;\">COM initialization in particular uses win32k calls to get the Window Station and Desktop</span></a><span style=\"font-weight: 400;\"> if the DLL is present. Those calls will fail with Win32k Lockdown enabled, silently breaking COM and features that depend on it such as our accessibility support. </span></p>\n<p><span style=\"font-weight: 400;\">On Windows 10 Fall Creators Update and later we have a fix that blocks these calls and forces a fallback, which keeps everything working nicely. We measured that </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1750742#c9\"><span style=\"font-weight: 400;\">not loading the DLLs causes about a 15% performance gain</span></a><span style=\"font-weight: 400;\"> when opening new tabs, adding a nice performance bonus on top of the security benefit.</span></p>\n<h3><span style=\"font-weight: 400;\">Remaining Work</span></h3>\n<p><span style=\"font-weight: 400;\">As hinted in the previous section, Win32k Lockdown will initially roll out on Windows 10 Fall Creators Update and later. On Windows 8, and unpatched Windows 10 (which unfortunately seems to be in use!), we are still testing a fix for the case where third party DLLs interfere, so support for those will come in a </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1759167#c7\"><span style=\"font-weight: 400;\">future release</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">For Canvas 2D support, we&#8217;re still </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1766402\"><span style=\"font-weight: 400;\">looking into improving the performance of applications</span></a><span style=\"font-weight: 400;\"> that regressed when the processes were switched around. Simultaneously, there is experimentation underway to see if hardware acceleration for </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1739448\"><span style=\"font-weight: 400;\">Canvas 2D can be implemented through WebGL</span></a><span style=\"font-weight: 400;\">, which would increase code sharing between the 2D and 3D implementations and take advantage of modern video drivers being better optimized for the 3D case.</span></p>\n<h2><strong>Conclusion</strong></h2>\n<p><span style=\"font-weight: 400;\">Retrofitting a significant change in the separation of responsibilities in a large application like Firefox presents a large, multi-year engineering challenge, but it is absolutely required in order to advance browser security and to continue keeping our users safe. We&#8217;re pleased to have made it through and present you with the result in Firefox 100.</span></p>\n<h3><span style=\"font-weight: 400;\">Other Platforms</span></h3>\n<p><span style=\"font-weight: 400;\">If you&#8217;re a Mac user, you might wonder if there’s anything similar to Win32k Lockdown that can be done for macOS. You&#8217;d be right, and I have good news for you: we already quietly </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1467758\"><span style=\"font-weight: 400;\">shipped the changes that block access to the WindowServer</span></a><span style=\"font-weight: 400;\"> in Firefox 95, improving security and speeding process startup by about 30-70%. This too became possible because of the Remote WebGL and Non-Native Theming work described above.</span></p>\n<p><span style=\"font-weight: 400;\">For Linux users, </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1129492\"><span style=\"font-weight: 400;\">we removed the connection from content processes to the X11 Server</span></a><span style=\"font-weight: 400;\">, which stops attackers from exploiting the unsecured X11 protocol. Although Linux distributions have been moving towards the more secure Wayland protocol as the default, we still see a lot of users that are using X11 or XWayland configurations, so this is definitely a nice-to-have, which shipped in Firefox 99.</span></p>\n<h2><strong>We&#8217;re Hiring</strong></h2>\n<p><span style=\"font-weight: 400;\">If you found the technical background story above fascinating, I&#8217;d like to point out that our OS Integration &amp; Hardening team is going to be hiring soon. We&#8217;re especially looking for experienced C++ programmers with some interest in Rust and in-depth knowledge of Windows programming. </span></p>\n<p><span style=\"font-weight: 400;\">If you fit this description and are interested in taking the next leap in Firefox security together with us, </span><a href=\"https://www.mozilla.org/en-US/careers/\"><span style=\"font-weight: 400;\">we&#8217;d encourage you to keep an eye on our careers page</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><i>Thanks to Bob Owen, Chris Martin, and Stephen Pohl for their technical input to this article, and for all the heavy lifting they did together with Kelsey Gilbert and Jed Davis to make these security improvements ship.<br />\n</i></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/\">Improved Process Isolation in Firefox 100</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Introduction\nFirefox uses a multi-process model for additional security and stability while browsing: Web Content (such as HTML/CSS and Javascript) is rendered in separate processes that are isolated from the rest of the operating system and managed by a privileged parent process. This way, the amount of control gained by an attacker that exploits a bug in a content process is limited. \nEver since we deployed this model, we have been working on improving the isolation of the content processes to further limit the attack surface. This is a challenging task since content processes need access to some operating system APIs to properly function: for example, they still need to be able to talk to the parent process. \nIn this article, we would like to dive a bit further into the latest major milestone we have reached: Win32k Lockdown, which greatly reduces the capabilities of the content process when running on Windows. Together with two major earlier efforts (Fission and RLBox) that shipped before, this completes a sequence of large leaps forward that will significantly improve Firefox’s security.\nAlthough Win32k Lockdown is a Windows-specific technique, it became possible because of a significant re-architecting of the Firefox security boundaries that Mozilla has been working on for around four years, which allowed similar security advances to be made on other operating systems.\nThe Goal: Win32k Lockdown\nFirefox runs the processes that render web content with quite a few restrictions on what they are allowed to do when running on Windows. Unfortunately, by default they still have access to the entire Windows API, which opens up a large attack surface: the Windows API consists of many parts, for example, a core part dealing with threads, processes, and memory management, but also networking and socket libraries, printing and multimedia APIs, and so on.\nOf particular interest for us is the win32k.sys API, which includes many graphical and widget related system calls that have a history of being exploitable. Going back further in Windows’ origins, this situation is likely the result of Microsoft moving many operations that were originally running in user mode into the kernel in order to improve performance around the Windows 95 and NT4 timeframe. \nHaving likely never been originally designed to run in this sensitive context, these APIs have been a traditional target for hackers to break out of application sandboxes and into the kernel.\nIn Windows 8, Microsoft introduced a new mitigation named PROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY that an application can use to disable access to win32k.sys system calls. That is a long name to keep repeating, so we’ll refer to it hereafter by our internal designation: “Win32k Lockdown“.\nThe Work Required\nFlipping the Win32k Lockdown flag on the Web Content processes – the processes most vulnerable to potentially hostile web pages and JavaScript – means that those processes can no longer perform any graphical, window management, input processing, etc. operations themselves. \nTo accomplish these tasks, such operations must be remoted to a process that has the necessary permissions, typically the process that has access to the GPU and handles compositing and drawing (hereafter called the GPU Process), or the privileged parent process. \nDrawing web pages: WebRender\nFor painting the web pages’ contents, Firefox historically used various methods for interacting with the Windows APIs, ranging from using modern Direct3D based textures, to falling back to GDI surfaces, and eventually dropping into pure software mode. \nThese different options would have taken quite some work to remote, as most of the graphics API is off limits in Win32k Lockdown. The good news is that as of Firefox 92, our rendering stack has switched to WebRender, which moves all the actual drawing from the content processes to WebRender in the GPU Process.\nBecause with WebRender the content process no longer has a need to directly interact with the platform drawing APIs, this avoids any Win32k Lockdown related problems. WebRender itself has been designed partially to be more similar to game engines, and thus, be less susceptible to driver bugs. \nFor the remaining drivers that are just too broken to be of any use, it still has a fully software-based mode, which means we have no further fallbacks to consider.\nWebpages drawing: Canvas 2D and WebGL 3D\nThe Canvas API provides web pages with the ability to draw 2D graphics. In the original Firefox implementation, these JavaScript APIs were executed in the Web Content processes and the calls to the Windows drawing APIs were made directly from the same processes. \nIn a Win32k Lockdown scenario, this is no longer possible, so all drawing commands are remoted by recording and playing them back in the GPU process over IPC.\nAlthough the initial implementation had good performance, there were nevertheless reports from some sites that experienced performance regressions (the web sites that became faster generally didn’t complain!). A particular pain point are applications that call getImageData() repeatedly: having the Canvas remoted means that GPU textures must now be obtained from another process and sent over IPC. \nWe compensated for this in the scenario where getImageData is called at the start of a frame, by detecting this and preparing the right surfaces proactively to make the copying from the GPU faster.\nBesides the Canvas API to draw 2D graphics, the web platform also exposes an API to do 3D drawing, called WebGL. WebGL is a state-heavy API, so properly and efficiently synchronizing child and parent (as well as parent and driver) takes great care. \nWebGL originally handled all validation in Content, but with access to the GPU and the associated attack surface removed from there, we needed to craft a robust validating API between child and parent as well to get the full security benefit.\n(Non-)Native Theming for Forms\nHTML web pages have the ability to display form controls. While the overwhelming majority of websites provide a custom look and styling for those form controls, not all of them do, and if they do not you get an input GUI widget that is styled like (and originally was!) a native element of the operating system.\n Historically, these were drawn by calling the appropriate OS widget APIs from within the content process, but those are not available under Win32k Lockdown. \nThis cannot easily be fixed by remoting the calls, as the widgets themselves come in an infinite amount of sizes, shapes, and styles can be interacted with, and need to be responsive to user input and dispatch messages. We settled on having Firefox draw the form controls itself, in a cross-platform style. \nWhile changing the look of form controls has web compatibility implications, and some people prefer the more native look – on the few pages that don’t apply their own styles to controls – Firefox’s approach is consistent with that taken by other browsers, probably because of very similar considerations.\nScrollbars were a particular pain point: we didn’t want to draw the main scrollbar of the content window in a different manner as the rest of the UX, since nested scrollbars would show up with different styles which would look awkward. But, unlike the rather rare non-styled form widgets, the main scrollbar is visible on most web pages, and because it conceptually belongs to the browser UX we really wanted it to look native. \nWe, therefore, decided to draw all scrollbars to match the system theme, although it’s a bit of an open question though how things should look if even the vendor of the operating system can’t seem to decide what the “native” look is.\nFinal Hurdles\nLine Breaking\nWith the above changes, we thought we had all the usual suspects that would access graphics and widget APIs in win32k.sys wrapped up, so we started running the full Firefox test suite with win32k syscalls disabled. This caused at least one unexpected failure: Firefox was crashing when trying to find line breaks for some languages with complex scripts. \nWhile Firefox is able to correctly determine word endings in multibyte character streams for most languages by itself, the support for Thai, Lao, Tibetan and Khmer is known to be imperfect, and in these cases, Firefox can ask the operating system to handle the line breaking for it. But at least on Windows, the functions to do so are covered by the Win32k Lockdown switch. Oops!\nThere are efforts underway to incorporate ICU4X and base all i18n related functionality on that, meaning that Firefox will be able to handle all scripts perfectly without involving the OS, but this is a major effort and it was not clear if it would end up delaying the rollout of win32k lockdown. \nWe did some experimentation with trying to forward the line breaking over IPC. Initially, this had bad performance, but when we added caching performance was satisfactory or sometimes even improved, since OS calls could be avoided in many cases now.\nDLL Loading & Third Party Interactions\nAnother complexity of disabling win32k.sys access is that so much Windows functionality assumes it is available by default, and specific effort must be taken to ensure the relevant DLLs do not get loaded on startup. Firefox itself for example won’t load the user32 DLL containing some win32k APIs, but injected third party DLLs sometimes do. This causes problems because COM initialization in particular uses win32k calls to get the Window Station and Desktop if the DLL is present. Those calls will fail with Win32k Lockdown enabled, silently breaking COM and features that depend on it such as our accessibility support. \nOn Windows 10 Fall Creators Update and later we have a fix that blocks these calls and forces a fallback, which keeps everything working nicely. We measured that not loading the DLLs causes about a 15% performance gain when opening new tabs, adding a nice performance bonus on top of the security benefit.\nRemaining Work\nAs hinted in the previous section, Win32k Lockdown will initially roll out on Windows 10 Fall Creators Update and later. On Windows 8, and unpatched Windows 10 (which unfortunately seems to be in use!), we are still testing a fix for the case where third party DLLs interfere, so support for those will come in a future release.\nFor Canvas 2D support, we’re still looking into improving the performance of applications that regressed when the processes were switched around. Simultaneously, there is experimentation underway to see if hardware acceleration for Canvas 2D can be implemented through WebGL, which would increase code sharing between the 2D and 3D implementations and take advantage of modern video drivers being better optimized for the 3D case.\nConclusion\nRetrofitting a significant change in the separation of responsibilities in a large application like Firefox presents a large, multi-year engineering challenge, but it is absolutely required in order to advance browser security and to continue keeping our users safe. We’re pleased to have made it through and present you with the result in Firefox 100.\nOther Platforms\nIf you’re a Mac user, you might wonder if there’s anything similar to Win32k Lockdown that can be done for macOS. You’d be right, and I have good news for you: we already quietly shipped the changes that block access to the WindowServer in Firefox 95, improving security and speeding process startup by about 30-70%. This too became possible because of the Remote WebGL and Non-Native Theming work described above.\nFor Linux users, we removed the connection from content processes to the X11 Server, which stops attackers from exploiting the unsecured X11 protocol. Although Linux distributions have been moving towards the more secure Wayland protocol as the default, we still see a lot of users that are using X11 or XWayland configurations, so this is definitely a nice-to-have, which shipped in Firefox 99.\nWe’re Hiring\nIf you found the technical background story above fascinating, I’d like to point out that our OS Integration & Hardening team is going to be hiring soon. We’re especially looking for experienced C++ programmers with some interest in Rust and in-depth knowledge of Windows programming. \nIf you fit this description and are interested in taking the next leap in Firefox security together with us, we’d encourage you to keep an eye on our careers page.\nThanks to Bob Owen, Chris Martin, and Stephen Pohl for their technical input to this article, and for all the heavy lifting they did together with Kelsey Gilbert and Jed Davis to make these security improvements ship.\n\nThe post Improved Process Isolation in Firefox 100 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-05-12T15:09:10.000Z",
      "date_modified": "2022-05-12T15:09:10.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47798",
      "url": "https://hacks.mozilla.org/2022/04/common-voice-dataset-tops-20000-hours/",
      "title": "Common Voice dataset tops 20,000 hours",
      "summary": "The latest Common Voice dataset, released today, has achieved a major milestone: More than 20,000 hours of open-source speech data that anyone, anywhere can use. The dataset has nearly doubled in the past year. Mozilla’s Common Voice seeks to change the language technology ecosystem by supporting communities to collect voice data for the creation of voice-enabled applications for their own languages. \nThe post Common Voice dataset tops 20,000 hours appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">The latest Common Voice dataset, released today, has achieved a major milestone: More than 20,000 hours of open-source speech data that anyone, anywhere can use. The dataset has nearly doubled in the past year.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47799 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/common-voice.png\" alt=\"\" width=\"512\" height=\"269\" srcset=\"https://hacks.mozilla.org/files/2022/04/common-voice.png 512w, https://hacks.mozilla.org/files/2022/04/common-voice-250x131.png 250w, https://hacks.mozilla.org/files/2022/04/common-voice-500x263.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<h2><b>Why should you care about Common Voice?</b></h2>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Do you have to change your accent to be understood by a virtual assistant? </span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Are you worried that so many voice-operated devices are collecting your voice data for proprietary Big Tech datasets?</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Are automatic subtitles unavailable for you in your language?</span></li>\n</ul>\n<p><span style=\"font-weight: 400;\">Automatic Speech Recognition plays an important role in the way we can access information, however, of the 7,000 languages spoken globally today only a handful are supported by most products.</span></p>\n<p><a href=\"https://commonvoice.mozilla.org/\"><span style=\"font-weight: 400;\">Mozilla’s Common Voice</span></a><span style=\"font-weight: 400;\"> seeks to change the language technology ecosystem by supporting communities to collect voice data for the creation of voice-enabled applications for their own languages. </span></p>\n<h2><b>Common Voice Dataset Release </b></h2>\n<p><span style=\"font-weight: 400;\">This release wouldn’t be possible without our contributors — from voice donations to initiating their language in our project, to opening new opportunities for people to build voice technology tools that can support every language spoken across the world.</span></p>\n<p><span style=\"font-weight: 400;\">Access the dataset:</span><a href=\"https://commonvoice.mozilla.org/datasets\"><span style=\"font-weight: 400;\"> https://commonvoice.mozilla.org/datasets</span></a></p>\n<p><span style=\"font-weight: 400;\">Access the metadata: </span><a href=\"https://github.com/common-voice/cv-dataset\"><span style=\"font-weight: 400;\">https://github.com/common-voice/cv-dataset</span></a><span style=\"font-weight: 400;\"> </span></p>\n<h3><b>Highlights from the latest dataset:</b><b></b></h3>\n<ul>\n<li aria-level=\"1\"><b>The new release also features six new languages:</b><span style=\"font-weight: 400;\"> Tigre, Taiwanese (Minnan), Meadow Mari, Bengali, Toki Pona and Cantonese.</span></li>\n<li aria-level=\"1\"><b>Twenty-seven languages now have at least 100 hours of speech data. </b><span style=\"font-weight: 400;\">They include Bengali, Thai, Basque, and Frisian.</span></li>\n<li aria-level=\"1\"><b>Nine languages now have at least 500 hours of speech data. </b><span style=\"font-weight: 400;\">They include Kinyarwanda (2,383 hours), Catalan (2,045 hours), and Swahili (719 hours).</span></li>\n<li aria-level=\"1\"><b>Nine languages now all have at least 45% of their gender tags as female. </b><span style=\"font-weight: 400;\">They include Marathi, Dhivehi, and Luganda.</span></li>\n<li aria-level=\"1\"><b>The Catalan community fueled major growth.</b><span style=\"font-weight: 400;\"> The Catalan community&#8217;s</span><a href=\"https://www.projecteaina.cat/\"> <span style=\"font-weight: 400;\">Project AINA</span></a> <span style=\"font-weight: 400;\">— a collaboration between Barcelona Supercomputing Center and the Catalan Government — mobilized Catalan speakers to contribute to Common Voice. </span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><b>Supporting community participation in decision making yet.</b><span style=\"font-weight: 400;\"> The </span><a href=\"https://foundation.mozilla.org/en/blog/introducing-cvlr-20212022/\"><span style=\"font-weight: 400;\">Common Voice language Rep Cohort</span></a><span style=\"font-weight: 400;\"> has contributed feedback and learnings about optimal sentence collection, the inclusion of language variants, and more. </span></li>\n</ul>\n<h2><b> Create with the Dataset </b></h2>\n<p><span style=\"font-weight: 400;\">How will you create with the Common Voice Dataset?</span></p>\n<p><span style=\"font-weight: 400;\">Take some inspiration from technologists who are creating conversational chatbots, spoken language identifiers, research papers and virtual assistants with the Common Voice Dataset by watching this talk: </span></p>\n<p><a href=\"https://mozilla.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6492f3ae-3a0d-4363-99f6-adc00111b706\"><span style=\"font-weight: 400;\">https://mozilla.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6492f3ae-3a0d-4363-99f6-adc00111b706</span></a><span style=\"font-weight: 400;\"> </span></p>\n<p><span style=\"font-weight: 400;\">Share with us how you are using the dataset on social media using #CommonVoice or sharing on </span><a href=\"https://discourse.mozilla.org/c/voice/using/661\"><span style=\"font-weight: 400;\">our Community discourse.</span></a><span style=\"font-weight: 400;\"> </span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/common-voice-dataset-tops-20000-hours/\">Common Voice dataset tops 20,000 hours</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "The latest Common Voice dataset, released today, has achieved a major milestone: More than 20,000 hours of open-source speech data that anyone, anywhere can use. The dataset has nearly doubled in the past year.\n\nWhy should you care about Common Voice?\n\nDo you have to change your accent to be understood by a virtual assistant? \nAre you worried that so many voice-operated devices are collecting your voice data for proprietary Big Tech datasets?\nAre automatic subtitles unavailable for you in your language?\n\nAutomatic Speech Recognition plays an important role in the way we can access information, however, of the 7,000 languages spoken globally today only a handful are supported by most products.\nMozilla’s Common Voice seeks to change the language technology ecosystem by supporting communities to collect voice data for the creation of voice-enabled applications for their own languages. \nCommon Voice Dataset Release \nThis release wouldn’t be possible without our contributors — from voice donations to initiating their language in our project, to opening new opportunities for people to build voice technology tools that can support every language spoken across the world.\nAccess the dataset: https://commonvoice.mozilla.org/datasets\nAccess the metadata: https://github.com/common-voice/cv-dataset \nHighlights from the latest dataset:\n\nThe new release also features six new languages: Tigre, Taiwanese (Minnan), Meadow Mari, Bengali, Toki Pona and Cantonese.\nTwenty-seven languages now have at least 100 hours of speech data. They include Bengali, Thai, Basque, and Frisian.\nNine languages now have at least 500 hours of speech data. They include Kinyarwanda (2,383 hours), Catalan (2,045 hours), and Swahili (719 hours).\nNine languages now all have at least 45% of their gender tags as female. They include Marathi, Dhivehi, and Luganda.\nThe Catalan community fueled major growth. The Catalan community’s Project AINA — a collaboration between Barcelona Supercomputing Center and the Catalan Government — mobilized Catalan speakers to contribute to Common Voice. \nSupporting community participation in decision making yet. The Common Voice language Rep Cohort has contributed feedback and learnings about optimal sentence collection, the inclusion of language variants, and more. \n\n Create with the Dataset \nHow will you create with the Common Voice Dataset?\nTake some inspiration from technologists who are creating conversational chatbots, spoken language identifiers, research papers and virtual assistants with the Common Voice Dataset by watching this talk: \nhttps://mozilla.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6492f3ae-3a0d-4363-99f6-adc00111b706 \nShare with us how you are using the dataset on social media using #CommonVoice or sharing on our Community discourse. \n \nThe post Common Voice dataset tops 20,000 hours appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-04-28T15:23:57.000Z",
      "date_modified": "2022-04-28T15:23:57.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47805",
      "url": "https://hacks.mozilla.org/2022/04/mdn-plus-now-available-in-more-markets/",
      "title": "MDN Plus now available in more regions",
      "summary": "Almost a month ago, we announced MDN Plus, a new premium service on MDN that allows users to customize their experience on the website.\nWe are very glad to announce today that it is now possible for MDN users around the globe to create an MDN Plus free account, no matter where they are.\nThe post MDN Plus now available in more regions appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>At the end of March this year, we announced MDN Plus, a new premium service on MDN that allows users to customize their experience on the website.</p>\n<p>We are very glad to announce today that it is now possible for MDN users around the globe to create an MDN Plus free account, no matter where they are.</p>\n<p>Click <a href=\"https://developer.mozilla.org/en-US/plus#subscribe\">here</a> to create an MDN Plus free account*.</p>\n<p>The premium version of the service is currently available as follows: in the United States, Canada (since March 24th, 2022), Austria, Belgium, Finland, France, United Kingdom, Germany, Ireland, Italy, Malaysia, the Netherlands, New Zealand, Puerto Rico, Sweden, Singapore, Switzerland, Spain (since April 28th, 2022), Estonia, Greece, Latvia, Lithuania, Portugal, Slovakia and Slovenia (since June 15th, 2022).</p>\n<p>We continue to work towards expanding this list even further.</p>\n<p>Click<a href=\"https://developer.mozilla.org/en-US/plus#subscribe\"> here</a> to create an MDN Plus premium account**.</p>\n<p><span style=\"font-weight: 400;\">* Now available to everyone</span></p>\n<p>** You will need to subscribe from one of the regions mentioned above to be able to have an MDN Plus premium account at this time</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/mdn-plus-now-available-in-more-markets/\">MDN Plus now available in more regions</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "At the end of March this year, we announced MDN Plus, a new premium service on MDN that allows users to customize their experience on the website.\nWe are very glad to announce today that it is now possible for MDN users around the globe to create an MDN Plus free account, no matter where they are.\nClick here to create an MDN Plus free account*.\nThe premium version of the service is currently available as follows: in the United States, Canada (since March 24th, 2022), Austria, Belgium, Finland, France, United Kingdom, Germany, Ireland, Italy, Malaysia, the Netherlands, New Zealand, Puerto Rico, Sweden, Singapore, Switzerland, Spain (since April 28th, 2022), Estonia, Greece, Latvia, Lithuania, Portugal, Slovakia and Slovenia (since June 15th, 2022).\nWe continue to work towards expanding this list even further.\nClick here to create an MDN Plus premium account**.\n* Now available to everyone\n** You will need to subscribe from one of the regions mentioned above to be able to have an MDN Plus premium account at this time\nThe post MDN Plus now available in more regions appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-04-28T10:05:35.000Z",
      "date_modified": "2022-04-28T10:05:35.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47784",
      "url": "https://hacks.mozilla.org/2022/04/adopting-users-design-feedback/",
      "title": "Adopting users’ design feedback",
      "summary": "On March 1st, 2022, MDN Web Docs released a new design and a new brand identity. Overall, the community responded to the redesign enthusiastically and we received many positive messages and kudos. We also received valuable feedback on some of the things we didn’t get quite right, like the browser compatibility table changes as well as some accessibility and readability issues.\nThe post Adopting users’ design feedback appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">On March 1st, 2022, MDN Web Docs released a new design and a new brand identity. Overall, the community responded to the </span><a href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\"><span style=\"font-weight: 400;\">redesign</span></a><span style=\"font-weight: 400;\"> enthusiastically and we received many positive messages and kudos. We also received valuable feedback on some of the things we didn’t get quite right, like the browser compatibility table changes as well as some accessibility and readability issues.</span></p>\n<p><span style=\"font-weight: 400;\">For us, MDN Web Docs has always been synonymous with the term Ubuntu, </span><em>“I am because we are.”</em><span style=\"font-weight: 400;\"> Translated in this context, “MDN Web Docs is the amazing resource it is because of our community’s support, feedback, and contributions.”</span></p>\n<p><span style=\"font-weight: 400;\"> Since the initial launch of the </span><a href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\"><span style=\"font-weight: 400;\">redesign</span></a><span style=\"font-weight: 400;\"> and of </span><a href=\"https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/\"><span style=\"font-weight: 400;\">MDN Plus</span></a><span style=\"font-weight: 400;\"> afterwards, we have been humbled and overwhelmed by the level of support we received from our community of readers. We do our best to listen to what you have to say and to act on suggestions so that together, we make MDN better. </span></p>\n<p><span style=\"font-weight: 400;\">Here is a summary of how we went about addressing the feedback we received.</span></p>\n<p><span style=\"font-weight: 400;\">Eight days after the redesign launch, we started the </span><i><span style=\"font-weight: 400;\">MDN Web Docs Readability Project</span></i><span style=\"font-weight: 400;\">. Our first task was to triage all issues submitted by the community that related to readability and accessibility on MDN Web Docs. Next up, we identified common themes and </span><a href=\"https://github.com/mdn/yari/issues/5546\"><span style=\"font-weight: 400;\">collected them in this meta issue</span></a><span style=\"font-weight: 400;\">. Over time, this grew into 27 unique issues and several related discussions and </span><span style=\"font-weight: 400;\">comments. We collected feedback on GitHub and also from our communities on</span> <a href=\"https://twitter.com/MozDevNet\"><span style=\"font-weight: 400;\">Twitter</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://matrix.to/#/%23mdn:mozilla.org\"><span style=\"font-weight: 400;\">Matrix</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">With the main pain points identified, we </span><a href=\"https://github.com/mdn/yari/discussions/5715\"><span style=\"font-weight: 400;\">opened a discussion on GitHub</span></a><span style=\"font-weight: 400;\">, inviting our readers to follow along and provide feedback on the changes as they were rolled out to a staging instance of the website. Today, roughly six weeks later, we are pleased to announce that all these changes are in production. This was not the effort of any one person but is made up of the work and contributions of people across staff and community.</span></p>\n<p><span style=\"font-weight: 400;\">Below are some of the highlights from this work.</span></p>\n<h2><strong>Dark mode</strong></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47789 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/mdn.png\" alt=\"\" width=\"512\" height=\"171\" srcset=\"https://hacks.mozilla.org/files/2022/04/mdn.png 512w, https://hacks.mozilla.org/files/2022/04/mdn-250x83.png 250w, https://hacks.mozilla.org/files/2022/04/mdn-500x167.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<p><span style=\"font-weight: 400;\">We updated the color palette used in dark mode in particular.</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reworked the initial color palette to use colors that are slightly</span><a href=\"https://github.com/mdn/yari/issues/5378\"><span style=\"font-weight: 400;\"> more subtle in dark mode</span></a><span style=\"font-weight: 400;\"> while ensuring that we still meet AA accessibility guidelines for color contrast.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reconsidered the darkness of the primary background color in dark mode and settled on a compromise that improved the experience for the majority of readers.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We </span><a href=\"https://github.com/mdn/yari/discussions/5715#discussioncomment-2457075\"><span style=\"font-weight: 400;\">cleaned up the notecards</span></a><span style=\"font-weight: 400;\"> that indicate notices such as warnings, experimental features, items not on the standards track, etc.</span></li>\n</ul>\n<h2><strong>Readability</strong></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47793 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/readibility.png\" alt=\"\" width=\"512\" height=\"171\" srcset=\"https://hacks.mozilla.org/files/2022/04/readibility.png 512w, https://hacks.mozilla.org/files/2022/04/readibility-250x83.png 250w, https://hacks.mozilla.org/files/2022/04/readibility-500x167.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<p><span style=\"font-weight: 400;\">We got a clear sense from some of our community folks that readers found it more difficult to skim content and find sections of interest after the redesign. To address these issues, we made the following improvements:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We implemented a clearly defined type-scale </span><a href=\"https://github.com/mdn/yari/issues/5546#issuecomment-1095192000\"><span style=\"font-weight: 400;\">adjusted for mobile</span></a><span style=\"font-weight: 400;\"> to optimize legibility and to effectively use space, especially on smaller screens.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We made the distinction</span><a href=\"https://github.com/mdn/yari/issues/5485\"><span style=\"font-weight: 400;\"> between the different heading levels</span></a><span style=\"font-weight: 400;\"> clearer.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We moved away from changing </span><a href=\"https://github.com/mdn/yari/issues/5755\"><span style=\"font-weight: 400;\">link colors across different areas</span></a><span style=\"font-weight: 400;\"> of MDN Web Docs. We still retain some of the intent of this design decision, but this is now more subtle.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">The font size was bumped up across all pages, </span><a href=\"https://github.com/mdn/yari/issues/5372\"><span style=\"font-weight: 400;\">including the home page</span></a><span style=\"font-weight: 400;\">. We have also optimized letter and line spacing for a more effortless reading experience. This has also improved the reading experience for our Asian readers, for whom </span><a href=\"https://github.com/mdn/yari/issues/5415\"><span style=\"font-weight: 400;\">line heights were much too tight</span></a><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Missing links are </span><a href=\"https://github.com/mdn/yari/issues/5906#issuecomment-1090400185\"><span style=\"font-weight: 400;\">clearly distinguishable</span></a><span style=\"font-weight: 400;\"> from other links and content.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We improved the layout and </span><a href=\"https://github.com/mdn/yari/issues/5632\"><span style=\"font-weight: 400;\">readability of specifications pages</span></a><span style=\"font-weight: 400;\"> across desktop and small screen devices.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We addressed a bug in how the highlighting in the table of contents worked and </span><a href=\"https://github.com/mdn/yari/pull/5852\"><span style=\"font-weight: 400;\">moved to use an IntersectionObserver</span></a><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We made styling for tables consistent across all pages.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">To ensure readers are always oriented regarding which page they are currently viewing, we have </span><a href=\"https://github.com/mdn/yari/issues/5521\"><span style=\"font-weight: 400;\">made the header (which includes the breadcrumbs) sticky</span></a><span style=\"font-weight: 400;\"> on desktop and mobile.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We fixed our </span><a href=\"https://github.com/mdn/yari/issues/5919\"><span style=\"font-weight: 400;\">accessibility skip navigation</span></a><span style=\"font-weight: 400;\"> to now offer skip to content, skip to search, and skip to language selectors.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We </span><a href=\"https://github.com/mdn/yari/pull/5977\"><span style=\"font-weight: 400;\">fixed a tricky issue</span></a><span style=\"font-weight: 400;\"> that caused some elements to flicker when interacting with the page, especially in dark mode. Many thanks to Daniel Holbert for his assistance in diagnosing the problem.</span></li>\n</ul>\n<h2><strong>Browser compatibility tables</strong></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47785 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/browser-compat.png\" alt=\"\" width=\"512\" height=\"171\" srcset=\"https://hacks.mozilla.org/files/2022/04/browser-compat.png 512w, https://hacks.mozilla.org/files/2022/04/browser-compat-250x83.png 250w, https://hacks.mozilla.org/files/2022/04/browser-compat-500x167.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<p><span style=\"font-weight: 400;\">Another area of the site for which we received feedback after the redesign launch was the browser compatibility tables. Almost its own project inside the larger readability effort, the work we invested here resulted, we believe, in a much-improved user experience. All of the changes listed below are now in production:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We restored version numbers in the overview, which are now color-coded across desktop and mobile.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">The font size has been bumped up for easier reading and skimming.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">The line height of rows has been increased for readability.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reduced the table cells to </span><a href=\"https://github.com/mdn/yari/pull/5648\"><span style=\"font-weight: 400;\">one focusable button element</span></a><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://github.com/mdn/yari/pull/5749\"><span style=\"font-weight: 400;\">Browser icons</span></a><span style=\"font-weight: 400;\"> have been restored in the overview header.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reordered </span><a href=\"https://github.com/mdn/yari/pull/5649\"><span style=\"font-weight: 400;\">support history chronologically</span></a><span style=\"font-weight: 400;\"> to make the version range that the support notes refer to visually unambiguous.</span></li>\n</ul>\n<p><span style=\"font-weight: 400;\">We also fixed the following </span><a href=\"https://github.com/mdn/yari/pulls?q=is%3Apr+is%3Aclosed+label%3Abrowser-compat\"><span style=\"font-weight: 400;\">bugs</span></a><span style=\"font-weight: 400;\">:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Color-coded pre-release versions in the overview</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Showing consistent mouseover titles with release dates</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Added the missing </span><a href=\"https://github.com/mdn/yari/pull/5557\"><span style=\"font-weight: 400;\">footnote icon</span></a><span style=\"font-weight: 400;\"> in the overview</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Showing </span><a href=\"https://github.com/mdn/yari/pull/5614\"><span style=\"font-weight: 400;\">correct</span></a><span style=\"font-weight: 400;\"> support status for </span><a href=\"https://github.com/mdn/yari/pull/5616\"><span style=\"font-weight: 400;\">edge cases</span></a><span style=\"font-weight: 400;\"> (e.g., omit prefix symbol if prefixed and unprefixed support)</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Streamlined mobile </span><a href=\"https://github.com/mdn/yari/pull/5933\"><span style=\"font-weight: 400;\">dark mode</span></a></li>\n</ul>\n<p><span style=\"font-weight: 400;\">We believe this is a big step in the right direction but we are not done. We can, and will, continue to </span><span style=\"font-weight: 400;\">improve site-wide readability and functionality of page areas,</span><span style=\"font-weight: 400;\"> such as the sidebars and general accessibility. As with the current improvements, we invite you to </span><a href=\"https://github.com/mdn/yari/issues/new/choose\"><span style=\"font-weight: 400;\">provide us with your feedback</span></a><span style=\"font-weight: 400;\"> and always welcome your pull requests to address </span><a href=\"https://github.com/mdn/yari/issues\"><span style=\"font-weight: 400;\">known issues</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">This was a collective effort, but we&#8217;d like to mention folks who went above and beyond. </span><a href=\"https://github.com/schalkneethling\"><span style=\"font-weight: 400;\">Schalk Neethling</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://github.com/caugner\"><span style=\"font-weight: 400;\">Claas Augner</span></a><span style=\"font-weight: 400;\"> from the MDN Team were responsible for most of the updates. From the community, we’d like to especially thank </span><a href=\"https://github.com/OnkarRuikar\"><span style=\"font-weight: 400;\">Onkar Ruikar</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://github.com/danielhjacobs\"><span style=\"font-weight: 400;\">Daniel Jacobs</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://github.com/iDave2\"><span style=\"font-weight: 400;\">Dave King</span></a><span style=\"font-weight: 400;\">, and </span><a href=\"https://github.com/queengooborg\"><span style=\"font-weight: 400;\">Queen Vinyl Da.i’gyu-Kazotetsu</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/adopting-users-design-feedback/\">Adopting users&#8217; design feedback</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "On March 1st, 2022, MDN Web Docs released a new design and a new brand identity. Overall, the community responded to the redesign enthusiastically and we received many positive messages and kudos. We also received valuable feedback on some of the things we didn’t get quite right, like the browser compatibility table changes as well as some accessibility and readability issues.\nFor us, MDN Web Docs has always been synonymous with the term Ubuntu, “I am because we are.” Translated in this context, “MDN Web Docs is the amazing resource it is because of our community’s support, feedback, and contributions.”\n Since the initial launch of the redesign and of MDN Plus afterwards, we have been humbled and overwhelmed by the level of support we received from our community of readers. We do our best to listen to what you have to say and to act on suggestions so that together, we make MDN better. \nHere is a summary of how we went about addressing the feedback we received.\nEight days after the redesign launch, we started the MDN Web Docs Readability Project. Our first task was to triage all issues submitted by the community that related to readability and accessibility on MDN Web Docs. Next up, we identified common themes and collected them in this meta issue. Over time, this grew into 27 unique issues and several related discussions and comments. We collected feedback on GitHub and also from our communities on Twitter and Matrix.\nWith the main pain points identified, we opened a discussion on GitHub, inviting our readers to follow along and provide feedback on the changes as they were rolled out to a staging instance of the website. Today, roughly six weeks later, we are pleased to announce that all these changes are in production. This was not the effort of any one person but is made up of the work and contributions of people across staff and community.\nBelow are some of the highlights from this work.\nDark mode\n\nWe updated the color palette used in dark mode in particular.\n\nWe reworked the initial color palette to use colors that are slightly more subtle in dark mode while ensuring that we still meet AA accessibility guidelines for color contrast.\nWe reconsidered the darkness of the primary background color in dark mode and settled on a compromise that improved the experience for the majority of readers.\nWe cleaned up the notecards that indicate notices such as warnings, experimental features, items not on the standards track, etc.\n\nReadability\n\nWe got a clear sense from some of our community folks that readers found it more difficult to skim content and find sections of interest after the redesign. To address these issues, we made the following improvements:\n\nWe implemented a clearly defined type-scale adjusted for mobile to optimize legibility and to effectively use space, especially on smaller screens.\nWe made the distinction between the different heading levels clearer.\nWe moved away from changing link colors across different areas of MDN Web Docs. We still retain some of the intent of this design decision, but this is now more subtle.\nThe font size was bumped up across all pages, including the home page. We have also optimized letter and line spacing for a more effortless reading experience. This has also improved the reading experience for our Asian readers, for whom line heights were much too tight.\nMissing links are clearly distinguishable from other links and content.\nWe improved the layout and readability of specifications pages across desktop and small screen devices.\nWe addressed a bug in how the highlighting in the table of contents worked and moved to use an IntersectionObserver.\nWe made styling for tables consistent across all pages.\nTo ensure readers are always oriented regarding which page they are currently viewing, we have made the header (which includes the breadcrumbs) sticky on desktop and mobile.\nWe fixed our accessibility skip navigation to now offer skip to content, skip to search, and skip to language selectors.\nWe fixed a tricky issue that caused some elements to flicker when interacting with the page, especially in dark mode. Many thanks to Daniel Holbert for his assistance in diagnosing the problem.\n\nBrowser compatibility tables\n\nAnother area of the site for which we received feedback after the redesign launch was the browser compatibility tables. Almost its own project inside the larger readability effort, the work we invested here resulted, we believe, in a much-improved user experience. All of the changes listed below are now in production:\n\nWe restored version numbers in the overview, which are now color-coded across desktop and mobile.\nThe font size has been bumped up for easier reading and skimming.\nThe line height of rows has been increased for readability.\nWe reduced the table cells to one focusable button element.\nBrowser icons have been restored in the overview header.\nWe reordered support history chronologically to make the version range that the support notes refer to visually unambiguous.\n\nWe also fixed the following bugs:\n\nColor-coded pre-release versions in the overview\nShowing consistent mouseover titles with release dates\nAdded the missing footnote icon in the overview\nShowing correct support status for edge cases (e.g., omit prefix symbol if prefixed and unprefixed support)\nStreamlined mobile dark mode\n\nWe believe this is a big step in the right direction but we are not done. We can, and will, continue to improve site-wide readability and functionality of page areas, such as the sidebars and general accessibility. As with the current improvements, we invite you to provide us with your feedback and always welcome your pull requests to address known issues.\nThis was a collective effort, but we’d like to mention folks who went above and beyond. Schalk Neethling and Claas Augner from the MDN Team were responsible for most of the updates. From the community, we’d like to especially thank Onkar Ruikar, Daniel Jacobs, Dave King, and Queen Vinyl Da.i’gyu-Kazotetsu.\n \nThe post Adopting users’ design feedback appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-04-21T15:04:05.000Z",
      "date_modified": "2022-04-21T15:04:05.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47775",
      "url": "https://hacks.mozilla.org/2022/04/mozilla-partners-with-the-center-for-humane-technology/",
      "title": "Mozilla partners with the Center for Humane Technology",
      "summary": "We’re pleased to announce that we have partnered with Center for Humane Tech, a nonprofit organization that radically reimagines the digital infrastructure. Its mission is to drive a comprehensive shift toward humane technology that supports the collective well-being, democracy and shared information environment.\nThe post Mozilla partners with the Center for Humane Technology appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">We’re pleased to announce that we have partnered with the </span><a href=\"https://www.humanetech.com/\"><span style=\"font-weight: 400;\">Center for Humane Technology</span></a><span style=\"font-weight: 400;\">, a nonprofit organization that radically reimagines the digital infrastructure. Its mission is to drive a comprehensive shift toward humane technology that supports the collective well-being, democracy and shared information environment. Many of you may remember the Center for Humane Tech from the Netflix documentary </span><a href=\"https://www.youtube.com/watch?v=uaaC57tcci0\"><span style=\"font-weight: 400;\">‘Social Dilemma’,</span></a><span style=\"font-weight: 400;\"> solidifying the saying “If you&#8217;re not paying for the product, then you are the product”. The Social Dilemma, is all about the dark side of technology, focusing on the individual and societal impact of algorithms. </span></p>\n<p><span style=\"font-weight: 400;\">It’s no surprise that this decision to partner was a no brainer and supports our efforts for a safe and open web that is </span>accessible and joyful for all.<span style=\"font-weight: 400;\"> Many people do not understand how AI and algorithms regularly touch our lives and feel powerless in the face of these systems. We are dedicated to making sure the public understands that we can and must have a say in when machines are used to make important decisions – and shape how those decisions are made. </span></p>\n<p><span style=\"font-weight: 400;\">Over the last few years, our work has been increasingly focused on building more trustworthy AI and safe online spaces. From challenging YouTube’s algorithms, where Mozilla </span><a href=\"https://foundation.mozilla.org/en/blog/mozilla-investigation-youtube-algorithm-recommends-videos-that-violate-the-platforms-very-own-policies/\"><span style=\"font-weight: 400;\">research</span></a><span style=\"font-weight: 400;\"> shows that the platform keeps pushing harmful videos and its algorithm is recommending videos with misinformation, violent content, hate speech and scams to its over two billion users to developing Enhanced Tracking Protection in Firefox that automatically protects your privacy while you browse, and </span><a href=\"https://www.mozilla.org/en-US/firefox/pocket/\"><span style=\"font-weight: 400;\">Pocket</span></a><span style=\"font-weight: 400;\"> which recommends high-quality, human-curated articles without collecting your browsing history or sharing your personal information with advertisers.</span></p>\n<p><span style=\"font-weight: 400;\">Let’s face it, most, if not all people, would probably prefer to use social media platforms that are safer and technologists should design products that reflect all users and without bias. As we collectively continue to think about our role in these areas &#8212; now and in the future, this course from the Center for Humane Tech is a great addition to the many tools necessary for change to take place. </span></p>\n<p><span style=\"font-weight: 400;\">The course rightly titled ‘</span><a href=\"https://www.humanetech.com/course\"><i><span style=\"font-weight: 400;\">Foundations of Humane Technology</span></i></a><i><span style=\"font-weight: 400;\">’ </span></i><span style=\"font-weight: 400;\">launched out of beta in March of this year, after rave reviews from hundreds of beta testers! </span></p>\n<p><span style=\"font-weight: 400;\">It explores the personal, societal, and practical challenges of being a humane technologist. Participants will leave the course with a strong conceptual framework, hands-on tools, and an ecosystem of support from peers and experts. Topics range from respecting human nature to minimizing harm to designing technology that deliberately avoids reinforcing inequitable dynamics of the past. </span></p>\n<p><span style=\"font-weight: 400;\">The course is completely free of charge and is centered towards building awareness and self-education through an online, at-your-own pace or binge-worthy set of eight modules. The course is marketed to professionals, with or without a technical background involved in shaping tomorrow&#8217;s technology. </span></p>\n<p><span style=\"font-weight: 400;\">It includes interactive exercises and reflections to help you internalize what you’re learning and regular optional Zoom sessions to discuss course content, connect with like-minded people, learn from experts in the field and even rewards a credential upon completion that can be shared with colleagues and prospective employers.</span></p>\n<p><span style=\"font-weight: 400;\">The problem with tech is not a new one, but this course is a stepping stone in the right direction.</span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/mozilla-partners-with-the-center-for-humane-technology/\">Mozilla partners with the Center for Humane Technology</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "We’re pleased to announce that we have partnered with the Center for Humane Technology, a nonprofit organization that radically reimagines the digital infrastructure. Its mission is to drive a comprehensive shift toward humane technology that supports the collective well-being, democracy and shared information environment. Many of you may remember the Center for Humane Tech from the Netflix documentary ‘Social Dilemma’, solidifying the saying “If you’re not paying for the product, then you are the product”. The Social Dilemma, is all about the dark side of technology, focusing on the individual and societal impact of algorithms. \nIt’s no surprise that this decision to partner was a no brainer and supports our efforts for a safe and open web that is accessible and joyful for all. Many people do not understand how AI and algorithms regularly touch our lives and feel powerless in the face of these systems. We are dedicated to making sure the public understands that we can and must have a say in when machines are used to make important decisions – and shape how those decisions are made. \nOver the last few years, our work has been increasingly focused on building more trustworthy AI and safe online spaces. From challenging YouTube’s algorithms, where Mozilla research shows that the platform keeps pushing harmful videos and its algorithm is recommending videos with misinformation, violent content, hate speech and scams to its over two billion users to developing Enhanced Tracking Protection in Firefox that automatically protects your privacy while you browse, and Pocket which recommends high-quality, human-curated articles without collecting your browsing history or sharing your personal information with advertisers.\nLet’s face it, most, if not all people, would probably prefer to use social media platforms that are safer and technologists should design products that reflect all users and without bias. As we collectively continue to think about our role in these areas — now and in the future, this course from the Center for Humane Tech is a great addition to the many tools necessary for change to take place. \nThe course rightly titled ‘Foundations of Humane Technology’ launched out of beta in March of this year, after rave reviews from hundreds of beta testers! \nIt explores the personal, societal, and practical challenges of being a humane technologist. Participants will leave the course with a strong conceptual framework, hands-on tools, and an ecosystem of support from peers and experts. Topics range from respecting human nature to minimizing harm to designing technology that deliberately avoids reinforcing inequitable dynamics of the past. \nThe course is completely free of charge and is centered towards building awareness and self-education through an online, at-your-own pace or binge-worthy set of eight modules. The course is marketed to professionals, with or without a technical background involved in shaping tomorrow’s technology. \nIt includes interactive exercises and reflections to help you internalize what you’re learning and regular optional Zoom sessions to discuss course content, connect with like-minded people, learn from experts in the field and even rewards a credential upon completion that can be shared with colleagues and prospective employers.\nThe problem with tech is not a new one, but this course is a stepping stone in the right direction.\nThe post Mozilla partners with the Center for Humane Technology appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-04-13T15:02:02.000Z",
      "date_modified": "2022-04-13T15:02:02.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47723",
      "url": "https://hacks.mozilla.org/2022/03/performance-tool-in-firefox-devtools-reloaded/",
      "title": "Performance Tool in Firefox DevTools Reloaded",
      "summary": "In Firefox 98, we’re shipping a new version of the existing Performance panel. This panel is now based on the Firefox profiler tool that can be used to capture a performance profile for a web page, inspect visualized performance data and analyze it to identify slow areas.\nThe post Performance Tool in Firefox DevTools Reloaded appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>In Firefox 98, we’re shipping a new version of the existing Performance panel. This panel is now based on the <a href=\"https://profiler.firefox.com/docs/#/\">Firefox profiler</a> tool that can be used to capture a performance profile for a web page, inspect visualized performance data and analyze it to identify slow areas.</p>\n<p>The icing on the cake of this already extremely powerful tool is that you can upload collected profile data with a single click and share the resulting link with your teammates (or anyone really). This makes it easier to collaborate on performance issues, especially in a distributed work environment.</p>\n<p>The new Performance panel is available in Firefox DevTools Toolbox by default and can be opened by <strong>Shift+F5</strong> key shortcut.</p>\n<h2>Usage</h2>\n<p>The only thing the user needs to do to start profiling is clicking on the big blue button &#8211; <strong>Start recording</strong>. Check out the screenshot below.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47724\" src=\"https://hacks.mozilla.org/files/2022/03/perf-panel.png\" alt=\"\" width=\"1037\" height=\"619\" srcset=\"https://hacks.mozilla.org/files/2022/03/perf-panel.png 1037w, https://hacks.mozilla.org/files/2022/03/perf-panel-250x149.png 250w, https://hacks.mozilla.org/files/2022/03/perf-panel-500x298.png 500w, https://hacks.mozilla.org/files/2022/03/perf-panel-768x458.png 768w\" sizes=\"(max-width: 1037px) 100vw, 1037px\" /></p>\n<p>As indicated by the onboarding message at the top of the new panel the previous profiler will be available for some time and eventually removed entirely.</p>\n<p>When profiling is started (i.e. the profiler is gathering performance data) the user can see two more buttons:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47728\" src=\"https://hacks.mozilla.org/files/2022/03/perf-panel-buttons.png\" alt=\"\" width=\"1035\" height=\"97\" srcset=\"https://hacks.mozilla.org/files/2022/03/perf-panel-buttons.png 1035w, https://hacks.mozilla.org/files/2022/03/perf-panel-buttons-250x23.png 250w, https://hacks.mozilla.org/files/2022/03/perf-panel-buttons-500x47.png 500w, https://hacks.mozilla.org/files/2022/03/perf-panel-buttons-768x72.png 768w\" sizes=\"(max-width: 1035px) 100vw, 1035px\" /></p>\n<ul>\n<li><strong>Capture recording</strong> &#8211; Stop recording, get what’s been collected so far and visualize it</li>\n<li><strong>Cancel recording</strong> &#8211; Stop recording and throw away all collected data</li>\n</ul>\n<p>When the user clicks on Capture recording all collected data are visualized in a new tab. You should see something like the following:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47740\" src=\"https://hacks.mozilla.org/files/2022/03/perf-data.png\" alt=\"\" width=\"4256\" height=\"2264\" srcset=\"https://hacks.mozilla.org/files/2022/03/perf-data.png 4256w, https://hacks.mozilla.org/files/2022/03/perf-data-250x133.png 250w, https://hacks.mozilla.org/files/2022/03/perf-data-500x266.png 500w, https://hacks.mozilla.org/files/2022/03/perf-data-768x409.png 768w, https://hacks.mozilla.org/files/2022/03/perf-data-1536x817.png 1536w, https://hacks.mozilla.org/files/2022/03/perf-data-2048x1089.png 2048w\" sizes=\"(max-width: 4256px) 100vw, 4256px\" /></p>\n<p>The inspection capabilities of the UI are powerful and let the user inspect every bit of the performance data. You might want to follow this detailed <a href=\"https://profiler.firefox.com/docs/#/./guide-ui-tour\">UI Tour</a> presentation created by the Performance team at Mozilla to learn more about all available features.</p>\n<h2>Customization</h2>\n<p>There are many options that can be used to customize how and what performance data should be collected to optimize specific use cases (see also the <strong>Edit Settings…</strong> link at the bottom of the panel).</p>\n<p>To make customization easier some presets are available and the <strong>Web Developer</strong> preset is selected by default. The profiler can be also used for profiling Firefox itself and Mozilla is extensively using it to make Firefox fast for millions of its users. The WebDeveloper preset is intended for profiling standard web pages and the rest is for profiling Firefox.</p>\n<p>The Profiler can be also used directly from the Firefox toolbar without the DevTools Toolbox being opened. The Profiler button isn’t visible in the toolbar by default, but you can enable it by loading <a href=\"https://profiler.firefox.com/\">https://profiler.firefox.com/</a> and clicking on the “Enable Firefox Profiler Menu Button” on the page.</p>\n<p>This is what the button looks like in the Firefox toolbar.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-large wp-image-47732\" src=\"https://hacks.mozilla.org/files/2022/03/toolbar-500x459.png\" alt=\"\" width=\"500\" height=\"459\" srcset=\"https://hacks.mozilla.org/files/2022/03/toolbar-500x459.png 500w, https://hacks.mozilla.org/files/2022/03/toolbar-250x230.png 250w, https://hacks.mozilla.org/files/2022/03/toolbar.png 611w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p>As you can see from the screenshot above the UI is almost exactly the same (compared to the DevTools Performance panel).</p>\n<h2>Sharing Data</h2>\n<p>Collected performance data can be shared publicly. This is one of the most powerful features of the profiler since it allows the user to upload data to the Firefox Profiler online storage. Before uploading a profile, you can select the data that you want to include, and what you don’t want to include to avoid leaking personal data. The profile link can then be shared in online chats, emails, and bug reports so other people can see and investigate a specific case.</p>\n<p>This is great for team collaboration and that’s something Firefox developers have been doing for years to work on performance. The profile can also be saved as a file on a local machine and imported later from <a href=\"https://profiler.firefox.com/\">https://profiler.firefox.com/</a></p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47736\" src=\"https://hacks.mozilla.org/files/2022/03/share-perf-data.png\" alt=\"\" width=\"930\" height=\"617\" srcset=\"https://hacks.mozilla.org/files/2022/03/share-perf-data.png 930w, https://hacks.mozilla.org/files/2022/03/share-perf-data-250x166.png 250w, https://hacks.mozilla.org/files/2022/03/share-perf-data-500x332.png 500w, https://hacks.mozilla.org/files/2022/03/share-perf-data-768x510.png 768w\" sizes=\"(max-width: 930px) 100vw, 930px\" /></p>\n<p>There are many more powerful features available and you can learn more about them in the extensive <a href=\"https://profiler.firefox.com/docs/#/\">documentation</a>. And of course, just like Firefox itself, the profiler tool is an open source project and you might want to <a href=\"https://github.com/firefox-devtools/profiler\">contribute</a> to it.</p>\n<p>There is also a great <a href=\"https://profiler.firefox.com/docs/#/./bunny\">case study</a> on using the profiler to identify performance issues.</p>\n<p>More is coming to DevTools, so stay tuned!</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/performance-tool-in-firefox-devtools-reloaded/\">Performance Tool in Firefox DevTools Reloaded</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "In Firefox 98, we’re shipping a new version of the existing Performance panel. This panel is now based on the Firefox profiler tool that can be used to capture a performance profile for a web page, inspect visualized performance data and analyze it to identify slow areas.\nThe icing on the cake of this already extremely powerful tool is that you can upload collected profile data with a single click and share the resulting link with your teammates (or anyone really). This makes it easier to collaborate on performance issues, especially in a distributed work environment.\nThe new Performance panel is available in Firefox DevTools Toolbox by default and can be opened by Shift+F5 key shortcut.\nUsage\nThe only thing the user needs to do to start profiling is clicking on the big blue button – Start recording. Check out the screenshot below.\n\nAs indicated by the onboarding message at the top of the new panel the previous profiler will be available for some time and eventually removed entirely.\nWhen profiling is started (i.e. the profiler is gathering performance data) the user can see two more buttons:\n\n\nCapture recording – Stop recording, get what’s been collected so far and visualize it\nCancel recording – Stop recording and throw away all collected data\n\nWhen the user clicks on Capture recording all collected data are visualized in a new tab. You should see something like the following:\n\nThe inspection capabilities of the UI are powerful and let the user inspect every bit of the performance data. You might want to follow this detailed UI Tour presentation created by the Performance team at Mozilla to learn more about all available features.\nCustomization\nThere are many options that can be used to customize how and what performance data should be collected to optimize specific use cases (see also the Edit Settings… link at the bottom of the panel).\nTo make customization easier some presets are available and the Web Developer preset is selected by default. The profiler can be also used for profiling Firefox itself and Mozilla is extensively using it to make Firefox fast for millions of its users. The WebDeveloper preset is intended for profiling standard web pages and the rest is for profiling Firefox.\nThe Profiler can be also used directly from the Firefox toolbar without the DevTools Toolbox being opened. The Profiler button isn’t visible in the toolbar by default, but you can enable it by loading https://profiler.firefox.com/ and clicking on the “Enable Firefox Profiler Menu Button” on the page.\nThis is what the button looks like in the Firefox toolbar.\n\nAs you can see from the screenshot above the UI is almost exactly the same (compared to the DevTools Performance panel).\nSharing Data\nCollected performance data can be shared publicly. This is one of the most powerful features of the profiler since it allows the user to upload data to the Firefox Profiler online storage. Before uploading a profile, you can select the data that you want to include, and what you don’t want to include to avoid leaking personal data. The profile link can then be shared in online chats, emails, and bug reports so other people can see and investigate a specific case.\nThis is great for team collaboration and that’s something Firefox developers have been doing for years to work on performance. The profile can also be saved as a file on a local machine and imported later from https://profiler.firefox.com/\n\nThere are many more powerful features available and you can learn more about them in the extensive documentation. And of course, just like Firefox itself, the profiler tool is an open source project and you might want to contribute to it.\nThere is also a great case study on using the profiler to identify performance issues.\nMore is coming to DevTools, so stay tuned!\nThe post Performance Tool in Firefox DevTools Reloaded appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-30T14:59:00.000Z",
      "date_modified": "2022-03-30T14:59:00.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47744",
      "url": "https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/",
      "title": "Introducing MDN Plus: Make MDN your own",
      "summary": "MDN is one of the most trusted resources for information about web standards, code samples, tools, and everything you need as a developer to create websites. Today, we are launching MDN Plus, our first step to providing a personalized and more powerful experience while continuing to invest in our always free and open webdocs.\nThe post Introducing MDN Plus: Make MDN your own appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>MDN is one of the most trusted resources for information about web standards, code samples, tools, and everything you need as a developer to create websites. In 2015, we explored how we could expand beyond documentation to provide a structured learning experience. Our first foray was the <a href=\"https://developer.mozilla.org/en-US/docs/Learn\">Learning Area</a>, with the goal of providing a useful addition to the regular MDN reference and guide material. In 2020, we added the first <a href=\"https://developer.mozilla.org/en-US/docs/Learn/Front-end_web_developer\">Front-end developer learning pathway</a>. We saw a lot of interest and engagement from users, and the learning area contributed to about 10% of MDN’s monthly web traffic. These two initiatives were the start of our exploration into how we could offer more learning resources to our community. Today, we are launching MDN Plus, our first step to providing a personalized and more powerful experience while continuing to invest in our always free and open webdocs.</p>\r\n\r\n<p><iframe loading=\"lazy\" title=\"YouTube video player\" src=\"https://www.youtube.com/embed/OBv9qnCesaQ\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe></p>\r\n<h2>Build your own MDN Experience with MDN Plus</h2>\r\n<p>In 2020 and 2021 we surveyed over 60,000 MDN users and learned that many of the respondents  wanted a customized MDN experience. They wanted to organize MDN’s vast library in a way that worked for them. For today’s premium subscription service, MDN Plus, we are releasing three new features that begin to address this need: Notifications, Collections and MDN Offline. More details about the features are listed below:</p>\r\n<ul>\r\n<li><b><i>Notifications: </i></b>Technology is ever changing, and we know how important it is to stay on top of the latest updates and developments. From tutorial pages to API references, you can now get notifications for the latest developments on MDN. When you follow a page, you’ll get notified when the documentation changes, CSS features launch, and APIs ship. Now, you can get a notification for significant events relating to the pages you want to follow. <a href=\"https://developer.mozilla.org/en-US/plus/docs/features/notifications\">Read more about it here</a>.</li>\r\n</ul>\r\n<p><img loading=\"lazy\" class=\"alignnone size-large wp-image-47750\" src=\"https://hacks.mozilla.org/files/2022/03/image-14-500x292.png\" alt=\"Screenshot of a list of notifications on mdn plus\" width=\"500\" height=\"292\" srcset=\"https://hacks.mozilla.org/files/2022/03/image-14-500x292.png 500w, https://hacks.mozilla.org/files/2022/03/image-14-250x146.png 250w, https://hacks.mozilla.org/files/2022/03/image-14-768x448.png 768w, https://hacks.mozilla.org/files/2022/03/image-14-1536x896.png 1536w, https://hacks.mozilla.org/files/2022/03/image-14-2048x1195.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\r\n<ul>\r\n<li><b><i>Collections:</i></b> Find what you need fast with our new collections feature. Not only can you pick the MDN articles you want to save, we also automatically save the pages you visit frequently. Collections help you quickly access the articles that matter the most to you and your work. <a href=\"https://developer.mozilla.org/en-US/plus/docs/features/collections\">Read more about it here</a>.</li>\r\n</ul>\r\n<p><img loading=\"lazy\" class=\"alignnone size-large wp-image-47746\" src=\"https://hacks.mozilla.org/files/2022/03/image-13-500x292.png\" alt=\"Screenshot of a collections list on mdn plus\" width=\"500\" height=\"292\" srcset=\"https://hacks.mozilla.org/files/2022/03/image-13-500x292.png 500w, https://hacks.mozilla.org/files/2022/03/image-13-250x146.png 250w, https://hacks.mozilla.org/files/2022/03/image-13-768x448.png 768w, https://hacks.mozilla.org/files/2022/03/image-13-1536x896.png 1536w, https://hacks.mozilla.org/files/2022/03/image-13-2048x1195.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\r\n<ul>\r\n<li><b><i>MDN offline</i></b>: Sometimes you need to access MDN but don’t have an internet connection. MDN offline leverages a Progressive Web Application (PWA) to give you access to MDN Web Docs even when you lack internet access so you can continue your work without any interruptions. Plus, with MDN offline you can have a faster experience while saving data. <a href=\"https://developer.mozilla.org/en-US/plus/docs/features/offline\">Read more about it here</a>.</li>\r\n</ul>\r\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47760 size-large\" src=\"https://hacks.mozilla.org/files/2022/03/image1-500x299.png\" alt=\"Screenshot of offline settings on mdn plus\" width=\"500\" height=\"299\" srcset=\"https://hacks.mozilla.org/files/2022/03/image1-500x299.png 500w, https://hacks.mozilla.org/files/2022/03/image1-250x150.png 250w, https://hacks.mozilla.org/files/2022/03/image1-768x460.png 768w, https://hacks.mozilla.org/files/2022/03/image1.png 1507w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\r\n<p id=\"countries\">Today, MDN Plus is available in the US and Canada. In the coming months, we will expand to other countries including France, Germany, Italy, Spain, Belgium, Austria, the Netherlands, Ireland, United Kingdom, Switzerland, Malaysia, New Zealand and Singapore. </p>\r\n<h2>Find the right MDN Plus plan for you</h2>\r\n<p>MDN is part of the daily life of millions of web developers. For many of us MDN helped with getting that first job or helped land a promotion. During our research we found many of these users, users who felt so much value from MDN that they wanted to contribute financially. We were both delighted and humbled by this feedback. To provide folks with a few options, we are launching MDN Plus with three plans including a supporter plan for those that want to spend a little extra. Here are the details of those plans:</p>\r\n<ul>\r\n<li aria-level=\"1\"><b><i>MDN Core</i></b>: For those who want to do a test drive before purchasing a plan, we created an option that lets you try a limited version for free.  </li>\r\n<li aria-level=\"1\"><b><i>MDN Plus 5</i></b>:  Offers unlimited access to notifications, collections, and MDN offline with new features added all the time. $5 a month or an annual subscription of $50.</li>\r\n<li aria-level=\"1\"><b><i>MDN Supporter 10</i></b>:  For MDN’s loyal supporters the supporter plan gives you everything under MDN Plus 5 plus early access to new features and a direct feedback channel to  the MDN team. It’s $10 a month or $100 for an annual subscription.  </li>\r\n</ul>\r\n<p>Additionally, we will offer a 20% discount if you subscribe to one of the annual subscription plans.</p>\r\n<p>We invite you to try the <a style=\"color:#e80840;\" href=\"https://developer.mozilla.org/en-US/plus#subscribe\">free trial version or sign up</a> today for a subscription plan that’s right for you. MDN Plus is only <a href=\"#countries\">available in selected countries</a> at this time.</p>\r\n<p>&nbsp;</p><p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/\">Introducing MDN Plus: Make MDN your own</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "MDN is one of the most trusted resources for information about web standards, code samples, tools, and everything you need as a developer to create websites. In 2015, we explored how we could expand beyond documentation to provide a structured learning experience. Our first foray was the Learning Area, with the goal of providing a useful addition to the regular MDN reference and guide material. In 2020, we added the first Front-end developer learning pathway. We saw a lot of interest and engagement from users, and the learning area contributed to about 10% of MDN’s monthly web traffic. These two initiatives were the start of our exploration into how we could offer more learning resources to our community. Today, we are launching MDN Plus, our first step to providing a personalized and more powerful experience while continuing to invest in our always free and open webdocs.\n\n\nBuild your own MDN Experience with MDN Plus\nIn 2020 and 2021 we surveyed over 60,000 MDN users and learned that many of the respondents  wanted a customized MDN experience. They wanted to organize MDN’s vast library in a way that worked for them. For today’s premium subscription service, MDN Plus, we are releasing three new features that begin to address this need: Notifications, Collections and MDN Offline. More details about the features are listed below:\n\nNotifications: Technology is ever changing, and we know how important it is to stay on top of the latest updates and developments. From tutorial pages to API references, you can now get notifications for the latest developments on MDN. When you follow a page, you’ll get notified when the documentation changes, CSS features launch, and APIs ship. Now, you can get a notification for significant events relating to the pages you want to follow. Read more about it here.\n\n\n\nCollections: Find what you need fast with our new collections feature. Not only can you pick the MDN articles you want to save, we also automatically save the pages you visit frequently. Collections help you quickly access the articles that matter the most to you and your work. Read more about it here.\n\n\n\nMDN offline: Sometimes you need to access MDN but don’t have an internet connection. MDN offline leverages a Progressive Web Application (PWA) to give you access to MDN Web Docs even when you lack internet access so you can continue your work without any interruptions. Plus, with MDN offline you can have a faster experience while saving data. Read more about it here.\n\n\nToday, MDN Plus is available in the US and Canada. In the coming months, we will expand to other countries including France, Germany, Italy, Spain, Belgium, Austria, the Netherlands, Ireland, United Kingdom, Switzerland, Malaysia, New Zealand and Singapore. \nFind the right MDN Plus plan for you\nMDN is part of the daily life of millions of web developers. For many of us MDN helped with getting that first job or helped land a promotion. During our research we found many of these users, users who felt so much value from MDN that they wanted to contribute financially. We were both delighted and humbled by this feedback. To provide folks with a few options, we are launching MDN Plus with three plans including a supporter plan for those that want to spend a little extra. Here are the details of those plans:\n\nMDN Core: For those who want to do a test drive before purchasing a plan, we created an option that lets you try a limited version for free.  \nMDN Plus 5:  Offers unlimited access to notifications, collections, and MDN offline with new features added all the time. $5 a month or an annual subscription of $50.\nMDN Supporter 10:  For MDN’s loyal supporters the supporter plan gives you everything under MDN Plus 5 plus early access to new features and a direct feedback channel to  the MDN team. It’s $10 a month or $100 for an annual subscription.  \n\nAdditionally, we will offer a 20% discount if you subscribe to one of the annual subscription plans.\nWe invite you to try the free trial version or sign up today for a subscription plan that’s right for you. MDN Plus is only available in selected countries at this time.\n The post Introducing MDN Plus: Make MDN your own appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-24T16:00:29.000Z",
      "date_modified": "2022-03-24T16:00:29.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47707",
      "url": "https://hacks.mozilla.org/2022/03/mozilla-and-open-web-docs-working-together-on-mdn/",
      "title": "Mozilla and Open Web Docs working together on MDN",
      "summary": "For both MDN and Open Web Docs (OWD), transparency is paramount to our missions. With the upcoming launch of MDN Plus, we believe it’s a good time to talk about how our two organizations work together, and if there is a financial relationship between us. Here is an overview of how our missions overlap and how they differ, and how a premium subscription service fits all this.\nThe post Mozilla and Open Web Docs working together on MDN appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">For both MDN and Open Web Docs (OWD), transparency is paramount to our missions. With the upcoming launch of MDN Plus, we believe it’s a good time to talk about how our two organizations work together and if there is a financial relationship between us. Here is an overview of how our missions overlap, how they differ, and how a premium subscription service fits all this.</span></p>\n<h2><strong>History of our collaboration</strong></h2>\n<p><span style=\"font-weight: 400;\">MDN and Open Web Docs began working together after the creation of Open Web Docs in 2021. Our organizations were born out of the same ethos, and we constantly collaborate on MDN content, contributing to different parts of MDN and even teaming up for shared projects like the conversion to Markdown. We meet on a weekly basis to discuss content strategies and maintain an open dialogue on our respective roadmaps.</span></p>\n<p><span style=\"font-weight: 400;\">MDN and Open Web Docs are different organizations; while our missions and goals frequently overlap, our work is not identical. Open Web Docs is an open collective, with a mission to contribute content to open source projects that are considered important for the future of the Web. MDN is currently the most significant project that Open Web Docs contributes to.</span></p>\n<h2><strong>Separate funding streams, division of labor</strong></h2>\n<p><span style=\"font-weight: 400;\">Mozilla and Open Web Docs collaborate closely on sustaining the Web Docs part of MDN. The Web Docs part is and will remain free and accessible to all. Each organization shoulders part of the costs of this labor, from our distinct budgets and revenue sources.</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Mozilla covers the cost of infrastructure, development and maintenance of the MDN platform including a team of engineers and its own team of dedicated writers.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Open Web Docs receives donations from companies like Google, Microsoft, Meta, Coil and others, and from private individuals. These donations pay for Technical Writing staff and help finance Open Web Docs projects. None of the donations that Open Web Docs receive go to MDN or Mozilla; rather they pay for a team of writers to contribute to MDN. </span></li>\n</ul>\n<h2><strong>Transparency and dialogue but independent decision-making</strong></h2>\n<p><span style=\"font-weight: 400;\">Mozilla and OWD have an open dialogue on content related to MDN. Mozilla sits on the Open Web Docs&#8217; Steering Committee, sharing expertise and experience but does not currently sit on the Open Web Docs’ Governing Committee. Mozilla does not provide direct financial support to Open Web Docs and does not participate in making decisions about Open Web Docs&#8217; overall direction, objectives, hiring and budgeting.</span></p>\n<h2><strong>MDN Plus: How does it fit into the big picture?</strong></h2>\n<p><span style=\"font-weight: 400;\">MDN Plus is a new premium subscription service by Mozilla that allows users to customize their MDN experience. </span></p>\n<p><span style=\"font-weight: 400;\">As with so much of our work, our organizations engaged in a transparent dialogue regarding MDN Plus. When requested, Open Web Docs has provided Mozilla with feedback, but it has not been a part of the development of MDN Plus. The resources Open Web Docs has are used only to improve the free offering of MDN. </span></p>\n<p><span style=\"font-weight: 400;\">The existence of a new subscription model will not detract from MDN&#8217;s current free Web Docs offering in any way. The current experience of accessing web documentation will not change for users who do not wish to sign up for a premium subscription. </span></p>\n<p><span style=\"font-weight: 400;\">Mozilla’s goal with MDN Plus is to help ensure that MDN&#8217;s open source content continues to be supported into the future. While Mozilla has incorporated its partners’ feedback into their vision for the product, MDN Plus has been built only with Mozilla resources. Any revenue generated by MDN Plus will stay within Mozilla. Mozilla is looking into ways to reinvest some of these additional funds into open source projects contributing to MDN but it is still in early stages.</span></p>\n<p><span style=\"font-weight: 400;\">A subscription to MDN Plus gives paying subscribers extra MDN features provided by Mozilla while a donation to Open Web Docs goes to funding writers creating content on MDN Web Docs, and potentially elsewhere. Work produced via OWD will always be publicly available and accessible to all. </span></p>\n<p><span style=\"font-weight: 400;\">Open Web Docs and Mozilla will continue to work closely together on MDN for the best possible web platform documentation for everyone!</span></p>\n<p><span style=\"font-weight: 400;\">Thanks for your continuing feedback and support.</span></p>\n<p>&nbsp;</p>\n<p><img loading=\"lazy\" class=\"wp-image-47717 alignleft\" src=\"https://hacks.mozilla.org/files/2022/03/mdn_owd-250x63.png\" alt=\"\" width=\"365\" height=\"92\" srcset=\"https://hacks.mozilla.org/files/2022/03/mdn_owd-250x63.png 250w, https://hacks.mozilla.org/files/2022/03/mdn_owd-500x125.png 500w, https://hacks.mozilla.org/files/2022/03/mdn_owd.png 600w\" sizes=\"(max-width: 365px) 100vw, 365px\" /></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/mozilla-and-open-web-docs-working-together-on-mdn/\">Mozilla and Open Web Docs working together on MDN</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "For both MDN and Open Web Docs (OWD), transparency is paramount to our missions. With the upcoming launch of MDN Plus, we believe it’s a good time to talk about how our two organizations work together and if there is a financial relationship between us. Here is an overview of how our missions overlap, how they differ, and how a premium subscription service fits all this.\nHistory of our collaboration\nMDN and Open Web Docs began working together after the creation of Open Web Docs in 2021. Our organizations were born out of the same ethos, and we constantly collaborate on MDN content, contributing to different parts of MDN and even teaming up for shared projects like the conversion to Markdown. We meet on a weekly basis to discuss content strategies and maintain an open dialogue on our respective roadmaps.\nMDN and Open Web Docs are different organizations; while our missions and goals frequently overlap, our work is not identical. Open Web Docs is an open collective, with a mission to contribute content to open source projects that are considered important for the future of the Web. MDN is currently the most significant project that Open Web Docs contributes to.\nSeparate funding streams, division of labor\nMozilla and Open Web Docs collaborate closely on sustaining the Web Docs part of MDN. The Web Docs part is and will remain free and accessible to all. Each organization shoulders part of the costs of this labor, from our distinct budgets and revenue sources.\n\nMozilla covers the cost of infrastructure, development and maintenance of the MDN platform including a team of engineers and its own team of dedicated writers.\nOpen Web Docs receives donations from companies like Google, Microsoft, Meta, Coil and others, and from private individuals. These donations pay for Technical Writing staff and help finance Open Web Docs projects. None of the donations that Open Web Docs receive go to MDN or Mozilla; rather they pay for a team of writers to contribute to MDN. \n\nTransparency and dialogue but independent decision-making\nMozilla and OWD have an open dialogue on content related to MDN. Mozilla sits on the Open Web Docs’ Steering Committee, sharing expertise and experience but does not currently sit on the Open Web Docs’ Governing Committee. Mozilla does not provide direct financial support to Open Web Docs and does not participate in making decisions about Open Web Docs’ overall direction, objectives, hiring and budgeting.\nMDN Plus: How does it fit into the big picture?\nMDN Plus is a new premium subscription service by Mozilla that allows users to customize their MDN experience. \nAs with so much of our work, our organizations engaged in a transparent dialogue regarding MDN Plus. When requested, Open Web Docs has provided Mozilla with feedback, but it has not been a part of the development of MDN Plus. The resources Open Web Docs has are used only to improve the free offering of MDN. \nThe existence of a new subscription model will not detract from MDN’s current free Web Docs offering in any way. The current experience of accessing web documentation will not change for users who do not wish to sign up for a premium subscription. \nMozilla’s goal with MDN Plus is to help ensure that MDN’s open source content continues to be supported into the future. While Mozilla has incorporated its partners’ feedback into their vision for the product, MDN Plus has been built only with Mozilla resources. Any revenue generated by MDN Plus will stay within Mozilla. Mozilla is looking into ways to reinvest some of these additional funds into open source projects contributing to MDN but it is still in early stages.\nA subscription to MDN Plus gives paying subscribers extra MDN features provided by Mozilla while a donation to Open Web Docs goes to funding writers creating content on MDN Web Docs, and potentially elsewhere. Work produced via OWD will always be publicly available and accessible to all. \nOpen Web Docs and Mozilla will continue to work closely together on MDN for the best possible web platform documentation for everyone!\nThanks for your continuing feedback and support.\n \n\n \nThe post Mozilla and Open Web Docs working together on MDN appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-17T14:07:34.000Z",
      "date_modified": "2022-03-17T14:07:34.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47622",
      "url": "https://hacks.mozilla.org/2022/03/interop-2022/",
      "title": "Announcing Interop 2022",
      "summary": "Writing high quality standards is a necessary first step to an interoperable web platform, but ensuring that browsers are consistent in their behavior requires an ongoing process. Browsers must work to ensure that they have a shared understanding of web standards, and that their implementation matches that understanding.\nInterop 2022 is a cross-browser initiative to find and address the most important interoperability pain points on the web platform. The end result is a public metric that will assess progress toward fixing these interoperability issues.\nThe post Announcing Interop 2022 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>A key benefit of the web platform is that it&#8217;s defined by standards, rather than by the code of a single implementation. This creates a shared platform that isn&#8217;t tied to specific hardware, a company, or a business model.</p>\n<p>Writing high quality standards is a necessary first step to an interoperable web platform, but ensuring that browsers are consistent in their behavior requires an ongoing process. Browsers must work to ensure that they have a shared understanding of web standards, and that their implementation matches that understanding.</p>\n<h2>Interop 2022</h2>\n<p>Interop 2022 is a cross-browser initiative to find and address the most important interoperability pain points on the web platform. The end result is a public metric that will assess progress toward fixing these interoperability issues.</p>\n<p><a href=\"https://wpt.fyi/interop-2022\"><img loading=\"lazy\" class=\"alignnone\" src=\"https://hacks.mozilla.org/files/2022/02/interop-2022-dashboard.png\" alt=\"Interop 2022 scores. Chrome/Edge 71, Firefox 74, and Safari 73.\" width=\"1200\" height=\"580\" /></a></p>\n<p>In order to identify the areas to include, we looked at two primary sources of data:</p>\n<ul>\n<li>Web developer feedback (e.g., through developer facing surveys including <a href=\"https://insights.developer.mozilla.org/\">MDN’s Web DNA Report</a>) on the most common pain points they experience.</li>\n<li>End user bug reports (e.g., via <a href=\"https://webcompat.com/\">webcompat.com</a>) that could be traced back to implementation differences between browsers.</li>\n</ul>\n<p>During the process of collecting this data, it became clear there are two principal kinds of interoperability problems which affect end users and developers:</p>\n<ul>\n<li>Problems where there&#8217;s a relatively clear and widely accepted standard, but where implementations are incomplete or buggy.</li>\n<li>Problems where the standard is missing, unclear, or doesn&#8217;t match the behavior sites depend on.</li>\n</ul>\n<p>Problems of the first kind have been termed &#8220;focus areas&#8221;. For these we use <a href=\"https://web-platform-tests.org/\">web-platform-tests</a>: a large, shared testsuite that aims to ensure web standards are implemented consistently across browsers. It accepts contributions from anyone, and browsers, including Firefox, contribute tests as part of their process for fixing bugs and shipping new features.</p>\n<p>The path to improvement for these areas is clear: identify or write tests in web-platform-tests that measure conformance to the relevant standard, and update implementations so that they pass those tests.</p>\n<p>Problems of the second kind have been termed “investigate areas”. For these it’s not possible to simply write tests as we&#8217;re not really sure what&#8217;s necessary to reach interoperability. Such unknown unknowns turn out to be extremely common sources of developer and user frustration!</p>\n<p>We’ll make progress here through investigation. And we’ll measure progress with more qualitative goals, e.g., working out what exact behavior sites depend on, and what can be implemented in practice without breaking the web.</p>\n<p>In all cases, the hope is that we can move toward a future in which we know how to make these areas interoperable, update the relevant web standards for them, and measure them with tests as we do with focus areas.</p>\n<h3>Focus areas</h3>\n<p>Interop 2022 has ten new focus areas:</p>\n<ul>\n<li>Cascade Layers</li>\n<li>Color Spaces and Functions</li>\n<li>Containment</li>\n<li>Dialog Element</li>\n<li>Forms</li>\n<li>Scrolling</li>\n<li>Subgrid</li>\n<li>Typography and Encodings</li>\n<li>Viewport Units</li>\n<li>Web Compat</li>\n</ul>\n<p>Unlike the others the Web Compat area doesn&#8217;t represent a specific technology, but is a group of specific known problems with already shipped features, where we see bugs and deviations from standards cause frequent site breakage for end users.</p>\n<p>There are also five additional areas that have been adopted from Google and Microsoft&#8217;s “Compat 2021” effort:</p>\n<ul>\n<li>Aspect Ratio</li>\n<li>Flexbox</li>\n<li>Grid</li>\n<li>Sticky Positioning</li>\n<li>Transforms</li>\n</ul>\n<p>A browser&#8217;s test pass rate in each area contributes 6% — totaling at 90% for fifteen areas — of their score of Interop 2022.</p>\n<p>We believe these are areas where the standards are in good shape for implementation, and where improving interoperability will directly improve the lives of developers and end users.</p>\n<h3>Investigate areas</h3>\n<p>Interop 2022 has three investigate areas:</p>\n<ul>\n<li>Editing, contentEditable, and execCommand</li>\n<li>Pointer and Mouse Events</li>\n<li>Viewport Measurement</li>\n</ul>\n<p>These are areas in which we often see complaints from end users, or reports of site breakage, but where the path toward solving the issues isn&#8217;t clear. Collaboration between vendors is essential to working out how to fix these problem areas, and we believe that Interop 2022 is a unique opportunity to make progress on historically neglected areas of the web platform.</p>\n<p>The overall progress in this area will contribute 10% to the overall score of Interop 2022. This score will be the same across all browsers. This reflects the fact that progress on the web platform requires browsers to collaborate on new or updated web standards and accompanying tests, to achieve the best outcomes for end users and developers.</p>\n<h2>Contributions welcome!</h2>\n<p>Whilst the focus and investigate areas for 2022 are now set, there is still much to do. For the investigate areas, the detailed targets need to be set, and the complex work of understanding the current state of the art, and assessing the options to advance it, are just starting. Additional tests for the focus areas might be needed as well to address particular edge cases.</p>\n<p>If this sounds like something you&#8217;d like to get involved with, follow the instructions on the <a href=\"https://wpt.fyi/interop-2022\">Interop 2022 Dashboard</a>.</p>\n<p>Finally, it&#8217;s also possible that Interop 2022 is missing an area you consider to be a significant pain point. It won&#8217;t be possible to add areas this year, but, if the effort is a success we may end up running further iterations. Feedback on browser differences that are making your life hard as developer or end user are always welcome and will be helpful for identifying the correct focus and investigate areas for any future edition.</p>\n<h2>Partner announcements</h2>\n<p>Bringing Interop 2022 to fruition was a collaborative effort and you might be interested in the other announcements:</p>\n<ul>\n<li>Apple&#8217;s <a href=\"https://webkit.org/blog/12288/working-together-on-interop-2022/\">Working together on Interop 2022</a></li>\n<li><a href=\"https://bocoup.com/blog/interop-2022\">Bocoup and Interop 2022</a></li>\n<li>Google&#8217;s <a href=\"https://web.dev/interop-2022\">Interop 2022: browsers working together to improve the web for developers</a></li>\n<li><a href=\"https://www.igalia.com/news/interop2022.html\">Igalia and Interop 2022</a></li>\n<li><a href=\"https://aka.ms/microsoft-interop2022\">Microsoft and Interop 2022</a></li>\n</ul>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/interop-2022/\">Announcing Interop 2022</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "A key benefit of the web platform is that it’s defined by standards, rather than by the code of a single implementation. This creates a shared platform that isn’t tied to specific hardware, a company, or a business model.\nWriting high quality standards is a necessary first step to an interoperable web platform, but ensuring that browsers are consistent in their behavior requires an ongoing process. Browsers must work to ensure that they have a shared understanding of web standards, and that their implementation matches that understanding.\nInterop 2022\nInterop 2022 is a cross-browser initiative to find and address the most important interoperability pain points on the web platform. The end result is a public metric that will assess progress toward fixing these interoperability issues.\n\nIn order to identify the areas to include, we looked at two primary sources of data:\n\nWeb developer feedback (e.g., through developer facing surveys including MDN’s Web DNA Report) on the most common pain points they experience.\nEnd user bug reports (e.g., via webcompat.com) that could be traced back to implementation differences between browsers.\n\nDuring the process of collecting this data, it became clear there are two principal kinds of interoperability problems which affect end users and developers:\n\nProblems where there’s a relatively clear and widely accepted standard, but where implementations are incomplete or buggy.\nProblems where the standard is missing, unclear, or doesn’t match the behavior sites depend on.\n\nProblems of the first kind have been termed “focus areas”. For these we use web-platform-tests: a large, shared testsuite that aims to ensure web standards are implemented consistently across browsers. It accepts contributions from anyone, and browsers, including Firefox, contribute tests as part of their process for fixing bugs and shipping new features.\nThe path to improvement for these areas is clear: identify or write tests in web-platform-tests that measure conformance to the relevant standard, and update implementations so that they pass those tests.\nProblems of the second kind have been termed “investigate areas”. For these it’s not possible to simply write tests as we’re not really sure what’s necessary to reach interoperability. Such unknown unknowns turn out to be extremely common sources of developer and user frustration!\nWe’ll make progress here through investigation. And we’ll measure progress with more qualitative goals, e.g., working out what exact behavior sites depend on, and what can be implemented in practice without breaking the web.\nIn all cases, the hope is that we can move toward a future in which we know how to make these areas interoperable, update the relevant web standards for them, and measure them with tests as we do with focus areas.\nFocus areas\nInterop 2022 has ten new focus areas:\n\nCascade Layers\nColor Spaces and Functions\nContainment\nDialog Element\nForms\nScrolling\nSubgrid\nTypography and Encodings\nViewport Units\nWeb Compat\n\nUnlike the others the Web Compat area doesn’t represent a specific technology, but is a group of specific known problems with already shipped features, where we see bugs and deviations from standards cause frequent site breakage for end users.\nThere are also five additional areas that have been adopted from Google and Microsoft’s “Compat 2021” effort:\n\nAspect Ratio\nFlexbox\nGrid\nSticky Positioning\nTransforms\n\nA browser’s test pass rate in each area contributes 6% — totaling at 90% for fifteen areas — of their score of Interop 2022.\nWe believe these are areas where the standards are in good shape for implementation, and where improving interoperability will directly improve the lives of developers and end users.\nInvestigate areas\nInterop 2022 has three investigate areas:\n\nEditing, contentEditable, and execCommand\nPointer and Mouse Events\nViewport Measurement\n\nThese are areas in which we often see complaints from end users, or reports of site breakage, but where the path toward solving the issues isn’t clear. Collaboration between vendors is essential to working out how to fix these problem areas, and we believe that Interop 2022 is a unique opportunity to make progress on historically neglected areas of the web platform.\nThe overall progress in this area will contribute 10% to the overall score of Interop 2022. This score will be the same across all browsers. This reflects the fact that progress on the web platform requires browsers to collaborate on new or updated web standards and accompanying tests, to achieve the best outcomes for end users and developers.\nContributions welcome!\nWhilst the focus and investigate areas for 2022 are now set, there is still much to do. For the investigate areas, the detailed targets need to be set, and the complex work of understanding the current state of the art, and assessing the options to advance it, are just starting. Additional tests for the focus areas might be needed as well to address particular edge cases.\nIf this sounds like something you’d like to get involved with, follow the instructions on the Interop 2022 Dashboard.\nFinally, it’s also possible that Interop 2022 is missing an area you consider to be a significant pain point. It won’t be possible to add areas this year, but, if the effort is a success we may end up running further iterations. Feedback on browser differences that are making your life hard as developer or end user are always welcome and will be helpful for identifying the correct focus and investigate areas for any future edition.\nPartner announcements\nBringing Interop 2022 to fruition was a collaborative effort and you might be interested in the other announcements:\n\nApple’s Working together on Interop 2022\nBocoup and Interop 2022\nGoogle’s Interop 2022: browsers working together to improve the web for developers\nIgalia and Interop 2022\nMicrosoft and Interop 2022\n\nThe post Announcing Interop 2022 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-03T17:00:02.000Z",
      "date_modified": "2022-03-03T17:00:02.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47643",
      "url": "https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/",
      "title": "A new year, a new MDN",
      "summary": "If you’ve accessed the MDN website today, you probably noticed that it looks quite different. We hope it’s a good different. Let us explain!\nIn mid-2021 we started to think about modernizing MDN’s design, to create a clean and inviting website that makes navigating our 44,000 articles as easy as possible. We wanted to create a more holistic experience for our users, with an emphasis on improved navigability and a universal look and feel across all our pages. \nThe post A new year, a new MDN appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">If you’ve accessed the MDN website today, you probably noticed that it looks quite different. We hope it’s a good different. Let us explain!</span></p>\n<p><span style=\"font-weight: 400;\">MDN has undergone many changes in its sixteen-year history from its early beginning as a wiki to the recent migration of a static site backed by GitHub. During that time MDN grew organically, with over 45,000 contributors and numerous developers and designers. It’s no surprise that the user experience became somewhat inconsistent throughout the website. </span></p>\n<p><span style=\"font-weight: 400;\">In mid-2021 we started to think about modernizing MDN’s design, to create a clean and inviting website that makes navigating our 44,000 articles as easy as possible. We wanted to create a more holistic experience for our users, with an emphasis on improved navigability and a universal look and feel across all our pages. </span></p>\n<h2><b>A new Homepage, focused on community</b></h2>\n<p><span style=\"font-weight: 400;\">The MDN community is the reason our content can be counted on to be both high quality and trustworthy. MDN content is scrutinized, discussed, and yes, in some cases argued about. Anyone can contribute to MDN, either by writing content, suggesting changes or fixing bugs.</span></p>\n<p><span style=\"font-weight: 400;\">We wanted to acknowledge and celebrate our awesome community and our homepage is the perfect place to do so.</span></p>\n<p><span style=\"font-weight: 400;\">The new homepage was built with a focus on the core concepts of community and simplicity. We made an improved search a central element on the page, while also showing users a selection of the newest and most-read articles. </span></p>\n<p><span style=\"font-weight: 400;\">We will also show the most recent contributions to our GitHub content repo and added a contributor spotlight where we will highlight MDN contributors.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47690 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_mdn_dark-mode.gif\" alt=\"\" width=\"1080\" height=\"675\" /></p>\n<h2><b>Redesigned article pages for improved navigation</b></h2>\n<p><span style=\"font-weight: 400;\">It’s been years—</span><a href=\"https://hacks.mozilla.org/2017/07/the-mdn-redesign-behind-the-scenes/\"><span style=\"font-weight: 400;\">five of them, in fact</span></a><span style=\"font-weight: 400;\">—since MDN’s core content presentation has received a comprehensive design review. In those years, MDN’s content has evolved and changed, with new </span><a href=\"https://hacks.mozilla.org/2018/02/mdn-browser-compatibility-data/\"><span style=\"font-weight: 400;\">ways of structuring content</span></a><span style=\"font-weight: 400;\">, new ways to </span><a href=\"https://hacks.mozilla.org/2020/12/welcome-yari-mdn-web-docs-has-a-new-platform/\"><span style=\"font-weight: 400;\">build</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://github.com/mdn/content/pull/7092\"><span style=\"font-weight: 400;\">write docs</span></a><span style=\"font-weight: 400;\">, and new </span><a href=\"https://github.com/mdn/content/pulse\"><span style=\"font-weight: 400;\">contributors</span></a><span style=\"font-weight: 400;\">. Over time, the documentation’s look and feel had become increasingly disconnected from the way it’s read and written.</span></p>\n<p><span style=\"font-weight: 400;\">While you won’t see a dizzying reinvention of what documentation is, you’ll find that most visual elements on MDN did get love and attention, creating a more coherent view of our docs. This redesign gives MDN content its due, featuring:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">More consistent colors and theming</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Better signposting of major sections, such as HTML, CSS, and JavaScript</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Improved accessibility, such as increased contrast</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Added dark mode toggle for easy switching between modes</span></li>\n</ul>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47694 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_mdn_navigation.2022-02-24-13_14_00.gif\" alt=\"\" width=\"1080\" height=\"675\" /></p>\n<p>&nbsp;</p>\n<p><span style=\"font-weight: 400;\">We’re especially proud of some subtle improvements and conveniences. For example, in-page navigation is always in view to show you where you are in the page as you scroll:</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47698 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_mdn_scrolling.gif\" alt=\"\" width=\"1080\" height=\"675\" /></p>\n<p><span style=\"font-weight: 400;\">We’re also revisiting the way browser compatibility data appears, with better at-a-glance browser support. So you don’t have to keep version numbers in your head, we’ve put more emphasis on </span><i><span style=\"font-weight: 400;\">yes</span></i><span style=\"font-weight: 400;\"> and </span><i><span style=\"font-weight: 400;\">no</span></i><span style=\"font-weight: 400;\"> iconography for browser capabilities, with the option to view the detailed information you’ve come to expect from </span><a href=\"https://github.com/mdn/browser-compat-data\"><span style=\"font-weight: 400;\">our browser compatibility data</span></a><span style=\"font-weight: 400;\">. We think you should check it out. </span></p>\n<p><span style=\"font-weight: 400;\">And we’re not stopping there. The work we’ve done is far-reaching and there are still many opportunities to polish and improve on the design we’re shipping.</span></p>\n<h2><b>A new logo, chosen by our community</b></h2>\n<p><span style=\"font-weight: 400;\">As we began working on both the redesign and expanding MDN beyond WebDocs we realized it was also time for a new logo. We wanted a modern and easily customizable logo that would represent what MDN is today while also strengthening its identity and making it consistent with Mozilla’s current brand.</span></p>\n<p><span style=\"font-weight: 400;\">We worked closely with branding specialist </span><a href=\"https://lucdoucedame.com/\"><span style=\"font-weight: 400;\">Luc Doucedame</span></a><span style=\"font-weight: 400;\">, narrowed down our options to eight potential logos and put out a call to our community of users to help us choose and invited folks to vote on their favorite. We received over 10,000 votes in just three days and are happy to share with you “the MDN people&#8217;s choice.”</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47673 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21.png\" alt=\"\" width=\"643\" height=\"277\" srcset=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21.png 643w, https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21-250x108.png 250w, https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21-500x215.png 500w\" sizes=\"(max-width: 643px) 100vw, 643px\" /></p>\n<p><span style=\"font-weight: 400;\">The winner was Option 4, an M monogram using underscore to convey the process of writing code. Many thanks to everyone who voted!</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47669 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_blog_header_MDN-Intro.png\" alt=\"\" width=\"1920\" height=\"1080\" /></p>\n<h2><b>What you can expect next with MDN</b></h2>\n<h3></h3>\n<h3><b>Bringing content to the places where you need it most</b></h3>\n<p><span style=\"font-weight: 400;\">In recent years, MDN content has grown more sophisticated for authors, such as moving from a wiki to Git and converting from HTML to Markdown. This has been a boon to contributors, who can use more powerful and familiar tools to create more structured and consistent content.</span></p>\n<p><span style=\"font-weight: 400;\">With better tools in place, we’re finally in a position to build more visible and systematic benefits to readers. For example, many of you probably navigate MDN via your favorite search engine, rather than MDN’s own site navigation. We get it. Historically, a wiki made large content architecture efforts impractical. But we’re now closer than ever to making site-wide improvements to structure and navigation.</span></p>\n<p><span style=\"font-weight: 400;\">Looking forward, we have ambitious plans to take advantage of our new tools to explore improved navigation, generated standardization and support summarizes, and embedding MDN documentation in the places where developers need it most: in their IDE, browser tools, and more.</span></p>\n<h2><b>Coming soon: MDN Plus</b></h2>\n<p><span style=\"font-weight: 400;\">MDN has built a reputation as a trusted and central resource for information about standards, codes, tools, and everything you need as a developer to create websites. In 2015, we explored ways to be more than a central resource through creating a </span><a href=\"https://developer.mozilla.org/en-US/docs/Learn\"><span style=\"font-weight: 400;\">Learning Area</span></a><span style=\"font-weight: 400;\">, with the aim of providing a useful counterpart to the regular MDN reference and guide material. </span></p>\n<p><span style=\"font-weight: 400;\">In 2020, we added the first </span><a href=\"https://developer.mozilla.org/en-US/docs/Learn/Front-end_web_developer\"><span style=\"font-weight: 400;\">Front-end developer learning pathway</span></a><span style=\"font-weight: 400;\"> to it.  We saw a lot of interest and engagement from users, the learning area currently being responsible for 10% of MDN’s monthly web traffic. This started us on a path to see what more we can do in this area for our community.</span></p>\n<p><span style=\"font-weight: 400;\">Last year we surveyed users and asked them what they wanted out of their MDN experience. The top requested features included notifications, article collections and an offline experience on MDN. The overall theme we saw was that users wanted to be able to organize MDN’s vast library in a way that worked for them. </span></p>\n<p><span style=\"font-weight: 400;\">We are always looking for ways to meet our users&#8217; needs whether it&#8217;s through MDN’s free web documentation or personalized features. In the coming months, we’ll be expanding MDN to include a premium subscription service based on the feedback we received from web developers who want to customize their MDN experience. Stay tuned for more information on MDN Plus.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47677 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.45.53.png\" alt=\"\" width=\"372\" height=\"430\" srcset=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.45.53.png 372w, https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.45.53-250x289.png 250w\" sizes=\"(max-width: 372px) 100vw, 372px\" /></p>\n<h2><b>Thank you, MDN community</b></h2>\n<p><span style=\"font-weight: 400;\">We appreciate the thousands of people who voted for the new logo as well as everyone who participated in the early beta testing phase since we started this journey. Also, many thanks to our partners from the </span><a href=\"https://openwebdocs.org\"><span style=\"font-weight: 400;\">Open Web Docs</span></a><span style=\"font-weight: 400;\">, who gave us valuable feedback on the redesign and continue to make daily contributions to MDN content. Thanks to you all we could make this a reality and we will continue to invest in improving even further the experience on MDN.</span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\">A new year, a new MDN</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "If you’ve accessed the MDN website today, you probably noticed that it looks quite different. We hope it’s a good different. Let us explain!\nMDN has undergone many changes in its sixteen-year history from its early beginning as a wiki to the recent migration of a static site backed by GitHub. During that time MDN grew organically, with over 45,000 contributors and numerous developers and designers. It’s no surprise that the user experience became somewhat inconsistent throughout the website. \nIn mid-2021 we started to think about modernizing MDN’s design, to create a clean and inviting website that makes navigating our 44,000 articles as easy as possible. We wanted to create a more holistic experience for our users, with an emphasis on improved navigability and a universal look and feel across all our pages. \nA new Homepage, focused on community\nThe MDN community is the reason our content can be counted on to be both high quality and trustworthy. MDN content is scrutinized, discussed, and yes, in some cases argued about. Anyone can contribute to MDN, either by writing content, suggesting changes or fixing bugs.\nWe wanted to acknowledge and celebrate our awesome community and our homepage is the perfect place to do so.\nThe new homepage was built with a focus on the core concepts of community and simplicity. We made an improved search a central element on the page, while also showing users a selection of the newest and most-read articles. \nWe will also show the most recent contributions to our GitHub content repo and added a contributor spotlight where we will highlight MDN contributors.\n\nRedesigned article pages for improved navigation\nIt’s been years—five of them, in fact—since MDN’s core content presentation has received a comprehensive design review. In those years, MDN’s content has evolved and changed, with new ways of structuring content, new ways to build and write docs, and new contributors. Over time, the documentation’s look and feel had become increasingly disconnected from the way it’s read and written.\nWhile you won’t see a dizzying reinvention of what documentation is, you’ll find that most visual elements on MDN did get love and attention, creating a more coherent view of our docs. This redesign gives MDN content its due, featuring:\n\nMore consistent colors and theming\nBetter signposting of major sections, such as HTML, CSS, and JavaScript\nImproved accessibility, such as increased contrast\nAdded dark mode toggle for easy switching between modes\n\n\n \nWe’re especially proud of some subtle improvements and conveniences. For example, in-page navigation is always in view to show you where you are in the page as you scroll:\n\nWe’re also revisiting the way browser compatibility data appears, with better at-a-glance browser support. So you don’t have to keep version numbers in your head, we’ve put more emphasis on yes and no iconography for browser capabilities, with the option to view the detailed information you’ve come to expect from our browser compatibility data. We think you should check it out. \nAnd we’re not stopping there. The work we’ve done is far-reaching and there are still many opportunities to polish and improve on the design we’re shipping.\nA new logo, chosen by our community\nAs we began working on both the redesign and expanding MDN beyond WebDocs we realized it was also time for a new logo. We wanted a modern and easily customizable logo that would represent what MDN is today while also strengthening its identity and making it consistent with Mozilla’s current brand.\nWe worked closely with branding specialist Luc Doucedame, narrowed down our options to eight potential logos and put out a call to our community of users to help us choose and invited folks to vote on their favorite. We received over 10,000 votes in just three days and are happy to share with you “the MDN people’s choice.”\n\nThe winner was Option 4, an M monogram using underscore to convey the process of writing code. Many thanks to everyone who voted!\n\nWhat you can expect next with MDN\n\nBringing content to the places where you need it most\nIn recent years, MDN content has grown more sophisticated for authors, such as moving from a wiki to Git and converting from HTML to Markdown. This has been a boon to contributors, who can use more powerful and familiar tools to create more structured and consistent content.\nWith better tools in place, we’re finally in a position to build more visible and systematic benefits to readers. For example, many of you probably navigate MDN via your favorite search engine, rather than MDN’s own site navigation. We get it. Historically, a wiki made large content architecture efforts impractical. But we’re now closer than ever to making site-wide improvements to structure and navigation.\nLooking forward, we have ambitious plans to take advantage of our new tools to explore improved navigation, generated standardization and support summarizes, and embedding MDN documentation in the places where developers need it most: in their IDE, browser tools, and more.\nComing soon: MDN Plus\nMDN has built a reputation as a trusted and central resource for information about standards, codes, tools, and everything you need as a developer to create websites. In 2015, we explored ways to be more than a central resource through creating a Learning Area, with the aim of providing a useful counterpart to the regular MDN reference and guide material. \nIn 2020, we added the first Front-end developer learning pathway to it.  We saw a lot of interest and engagement from users, the learning area currently being responsible for 10% of MDN’s monthly web traffic. This started us on a path to see what more we can do in this area for our community.\nLast year we surveyed users and asked them what they wanted out of their MDN experience. The top requested features included notifications, article collections and an offline experience on MDN. The overall theme we saw was that users wanted to be able to organize MDN’s vast library in a way that worked for them. \nWe are always looking for ways to meet our users’ needs whether it’s through MDN’s free web documentation or personalized features. In the coming months, we’ll be expanding MDN to include a premium subscription service based on the feedback we received from web developers who want to customize their MDN experience. Stay tuned for more information on MDN Plus.\n\nThank you, MDN community\nWe appreciate the thousands of people who voted for the new logo as well as everyone who participated in the early beta testing phase since we started this journey. Also, many thanks to our partners from the Open Web Docs, who gave us valuable feedback on the redesign and continue to make daily contributions to MDN content. Thanks to you all we could make this a reality and we will continue to invest in improving even further the experience on MDN.\nThe post A new year, a new MDN appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-01T14:00:24.000Z",
      "date_modified": "2022-03-01T14:00:24.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47607",
      "url": "https://hacks.mozilla.org/2022/02/version-100-in-chrome-and-firefox/",
      "title": "Version 100 in Chrome and Firefox",
      "summary": "Chrome and Firefox will reach version 100 in a couple of months. This has the potential to cause breakage on sites that rely on identifying the browser version to perform business logic.  This post covers the timeline of events, the strategies that Chrome and Firefox are taking to mitigate the impact, and how you can help.\nThe post Version 100 in Chrome and Firefox appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Chrome and Firefox will reach version 100 in a <a href=\"https://developer.chrome.com/blog/force-major-version-to-100/\">couple</a> of <a href=\"https://www.otsukare.info/2021/04/20/ua-three-digits-get-ready\">months</a>. This has the potential to cause breakage on sites that rely on identifying the browser version to perform business logic.  This post covers the timeline of events, the strategies that Chrome and Firefox are taking to mitigate the impact, and how you can help.</p>\n<h2>User-Agent string</h2>\n<p><a href=\"https://www.rfc-editor.org/rfc/rfc7231.html#section-5.5.3\">User-Agent (UA)</a> is a string that browsers send in HTTP headers, so servers can identify the browser.  The string is also accessible through JavaScript with <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/userAgent\">navigator.userAgent</a>. It’s usually formatted as follows:</p>\n<p><code>browserName/majorVersion.minorVersion</code></p>\n<p>For example, the latest release versions of browsers at the time of publishing this post are:</p>\n<ul>\n<li aria-level=\"1\"><code>Chrome: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36</code></li>\n<li aria-level=\"1\"><code>Firefox: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0</code></li>\n<li aria-level=\"1\"><code>Safari: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15</code></li>\n</ul>\n<h2>Major version 100—three-digit version number</h2>\n<p>Major version 100 is a big milestone for both Chrome and Firefox. It also has the potential to cause breakage on websites as we move from a two-digit to a <b>three-digit version number</b>.  Web developers use all kinds of techniques for parsing these strings, from custom code to using User-Agent parsing libraries, which can then be used to determine the corresponding processing logic. The User-Agent and any other version reporting mechanisms will soon report a three-digit version number.</p>\n<h3>Version 100 timelines</h3>\n<p>Version 100 browsers will be first released in experimental versions (Chrome Canary, Firefox Nightly), then beta versions, and then finally on the stable channel.</p>\n<table>\n<tbody>\n<tr>\n<td>Chrome (<a href=\"https://chromiumdash.appspot.com/schedule\">Release Schedule</a>)</td>\n<td>March 29, 2022</td>\n</tr>\n<tr>\n<td>Firefox (<a href=\"https://wiki.mozilla.org/Release_Management/Calendar\">Release Schedule</a>)</td>\n<td>May 3, 2022</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"three-digit\">Why can a three-digit version number be problematic?</h2>\n<p>When browsers first reached version 10 a little over 12 years ago, <a href=\"https://maqentaer.com/devopera-static-backup/http/dev.opera.com/articles/view/opera-ua-string-changes/index.html\">many issues were discovered</a> with User-Agent parsing libraries as the major version number went from one digit to two.</p>\n<p>Without a single specification to follow, <a href=\"https://developer.mozilla.org/docs/Web/HTTP/Headers/User-Agent\">different browsers have different formats</a> for the User-Agent string, and site-specific User-Agent parsing. It’s possible that some parsing libraries may have hard-coded assumptions or bugs that don’t take into account three-digit major version numbers.  Many libraries improved the parsing logic when browsers moved to two-digit version numbers, so hitting the three-digit milestone is expected to cause fewer problems. Mike Taylor, an engineer on the Chrome team, has done a survey of common UA parsing libraries which didn&#8217;t uncover any issues. Running Chrome experiments in the field has surfaced some issues, which are being worked on.</p>\n<h2>What are browsers doing about it?</h2>\n<p>Both Firefox and Chrome have been running experiments where current versions of the browser report being at major version 100 in order to detect possible website breakage. This has led to a few <a href=\"https://github.com/webcompat/web-bugs/labels/version100\">reported</a> <a href=\"https://bugs.chromium.org/p/chromium/issues/detail?id=1273958\">issues</a>, some of which have already been fixed. These experiments will continue to run until the release of version 100.</p>\n<p>There are also backup mitigation strategies in place, in case version 100 release to stable channels causes more damage to websites than anticipated.</p>\n<h2 id=\"firefox-mitigation\">Firefox mitigation</h2>\n<p>In Firefox, the strategy will depend on how important the breakage is. Firefox has a <a href=\"https://wiki.mozilla.org/Compatibility/Interventions_Releases\">site interventions mechanism</a>. Mozilla webcompat team can hot fix broken websites in Firefox using this mechanism. If you type <code>about:compat</code> in the Firefox URL bar, you can see what is currently being fixed. If a site breaks with the major version being 100 on a specific domain, it is possible to fix it by sending version 99 instead.</p>\n<p>If the breakage is widespread and individual site interventions become unmanageable, Mozilla can temporarily freeze Firefox&#8217;s major version at 99 and then test other options.</p>\n<h2 id=\"chrome-mitigation\">Chrome mitigation</h2>\n<p>In Chrome, the backup plan is to use a flag to freeze the major version at 99 and report the real major version number in the minor version part of the User-Agent string (the code has already <a href=\"https://chromium-review.googlesource.com/c/chromium/src/+/3341658\">landed</a>).</p>\n<p>The Chrome version as reported in the User-Agent string follows the pattern &lt;major_version&gt;.&lt;minor_version&gt;.&lt;build_number&gt;.&lt;patch_number&gt;.</p>\n<p>If the backup plan is employed, then the User-Agent string would look like this:</p>\n<p><code>99.101.4988.0</code></p>\n<p>Chrome is also running experiments to ensure that reporting a three-digit value in the minor version part of the string does not result in breakage, since the minor version in the Chrome User-Agent string has reported 0 for a very long time. The Chrome team will decide on whether to resort to the backup option based on the number and severity of the issues reported.</p>\n<h2>What can you do to help?</h2>\n<p>Every strategy that adds complexity to the User-Agent string has a strong impact on the ecosystem. Let’s work together to avoid yet another quirky behavior. In Chrome and Firefox Nightly, you can configure the browser to report the version as 100 right now and report any issues you come across.</p>\n<h3 id=\"firefox-config\">Configure Firefox Nightly to report the major version as 100</h3>\n<ol>\n<li aria-level=\"1\">Open Firefox Nightly’s Settings menu.</li>\n<li aria-level=\"1\">Search for “Firefox 100” and then check the “Firefox 100 User-Agent String” option.</li>\n</ol>\n<h3 id=\"chrome-config\">Configure Chrome to report the major version as 100</h3>\n<ol>\n<li aria-level=\"1\">Go to chrome://flags/#force-major-version-to-100</li>\n<li aria-level=\"1\">Set the option to `Enabled`.</li>\n</ol>\n<h3 id=\"test-report\">Test and file reports</h3>\n<ul>\n<li aria-level=\"1\"><b>If you are a website maintainer</b>, test your website with Chrome and Firefox 100. Review your User-Agent parsing code and libraries, and ensure they are able to handle three-digit version numbers. We have compiled some of the <a href=\"https://www.otsukare.info/2022/01/14/broken-ua-detection\">patterns that are currently breaking</a>.</li>\n<li aria-level=\"1\"><b>If you develop a User-Agent parsing library</b>, add tests to parse versions greater than and equal to 100. Our early tests show that recent versions of libraries can handle it correctly. But the Web is a legacy machine, so if you have old versions of parsing libraries, it’s probably time to check and eventually upgrade.</li>\n<li aria-level=\"1\"><b>If you are browsing the web</b> and notice any issues with the major version 100, <a href=\"https://webcompat.com/issues/new?label=version100\">file a report on webcompat.com</a>.</li>\n</ul>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/version-100-in-chrome-and-firefox/\">Version 100 in Chrome and Firefox</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Chrome and Firefox will reach version 100 in a couple of months. This has the potential to cause breakage on sites that rely on identifying the browser version to perform business logic.  This post covers the timeline of events, the strategies that Chrome and Firefox are taking to mitigate the impact, and how you can help.\nUser-Agent string\nUser-Agent (UA) is a string that browsers send in HTTP headers, so servers can identify the browser.  The string is also accessible through JavaScript with navigator.userAgent. It’s usually formatted as follows:\nbrowserName/majorVersion.minorVersion\nFor example, the latest release versions of browsers at the time of publishing this post are:\n\nChrome: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36\nFirefox: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0\nSafari: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\n\nMajor version 100—three-digit version number\nMajor version 100 is a big milestone for both Chrome and Firefox. It also has the potential to cause breakage on websites as we move from a two-digit to a three-digit version number.  Web developers use all kinds of techniques for parsing these strings, from custom code to using User-Agent parsing libraries, which can then be used to determine the corresponding processing logic. The User-Agent and any other version reporting mechanisms will soon report a three-digit version number.\nVersion 100 timelines\nVersion 100 browsers will be first released in experimental versions (Chrome Canary, Firefox Nightly), then beta versions, and then finally on the stable channel.\n\n\n\nChrome (Release Schedule)\nMarch 29, 2022\n\n\nFirefox (Release Schedule)\nMay 3, 2022\n\n\n\nWhy can a three-digit version number be problematic?\nWhen browsers first reached version 10 a little over 12 years ago, many issues were discovered with User-Agent parsing libraries as the major version number went from one digit to two.\nWithout a single specification to follow, different browsers have different formats for the User-Agent string, and site-specific User-Agent parsing. It’s possible that some parsing libraries may have hard-coded assumptions or bugs that don’t take into account three-digit major version numbers.  Many libraries improved the parsing logic when browsers moved to two-digit version numbers, so hitting the three-digit milestone is expected to cause fewer problems. Mike Taylor, an engineer on the Chrome team, has done a survey of common UA parsing libraries which didn’t uncover any issues. Running Chrome experiments in the field has surfaced some issues, which are being worked on.\nWhat are browsers doing about it?\nBoth Firefox and Chrome have been running experiments where current versions of the browser report being at major version 100 in order to detect possible website breakage. This has led to a few reported issues, some of which have already been fixed. These experiments will continue to run until the release of version 100.\nThere are also backup mitigation strategies in place, in case version 100 release to stable channels causes more damage to websites than anticipated.\nFirefox mitigation\nIn Firefox, the strategy will depend on how important the breakage is. Firefox has a site interventions mechanism. Mozilla webcompat team can hot fix broken websites in Firefox using this mechanism. If you type about:compat in the Firefox URL bar, you can see what is currently being fixed. If a site breaks with the major version being 100 on a specific domain, it is possible to fix it by sending version 99 instead.\nIf the breakage is widespread and individual site interventions become unmanageable, Mozilla can temporarily freeze Firefox’s major version at 99 and then test other options.\nChrome mitigation\nIn Chrome, the backup plan is to use a flag to freeze the major version at 99 and report the real major version number in the minor version part of the User-Agent string (the code has already landed).\nThe Chrome version as reported in the User-Agent string follows the pattern <major_version>.<minor_version>.<build_number>.<patch_number>.\nIf the backup plan is employed, then the User-Agent string would look like this:\n99.101.4988.0\nChrome is also running experiments to ensure that reporting a three-digit value in the minor version part of the string does not result in breakage, since the minor version in the Chrome User-Agent string has reported 0 for a very long time. The Chrome team will decide on whether to resort to the backup option based on the number and severity of the issues reported.\nWhat can you do to help?\nEvery strategy that adds complexity to the User-Agent string has a strong impact on the ecosystem. Let’s work together to avoid yet another quirky behavior. In Chrome and Firefox Nightly, you can configure the browser to report the version as 100 right now and report any issues you come across.\nConfigure Firefox Nightly to report the major version as 100\n\nOpen Firefox Nightly’s Settings menu.\nSearch for “Firefox 100” and then check the “Firefox 100 User-Agent String” option.\n\nConfigure Chrome to report the major version as 100\n\nGo to chrome://flags/#force-major-version-to-100\nSet the option to `Enabled`.\n\nTest and file reports\n\nIf you are a website maintainer, test your website with Chrome and Firefox 100. Review your User-Agent parsing code and libraries, and ensure they are able to handle three-digit version numbers. We have compiled some of the patterns that are currently breaking.\nIf you develop a User-Agent parsing library, add tests to parse versions greater than and equal to 100. Our early tests show that recent versions of libraries can handle it correctly. But the Web is a legacy machine, so if you have old versions of parsing libraries, it’s probably time to check and eventually upgrade.\nIf you are browsing the web and notice any issues with the major version 100, file a report on webcompat.com.\n\nThe post Version 100 in Chrome and Firefox appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-02-15T18:05:20.000Z",
      "date_modified": "2022-02-15T18:05:20.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47599",
      "url": "https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/",
      "title": "Improving the Storage Access API in Firefox",
      "summary": "Before we roll out State Partitioning for all Firefox users, we intend to make a few privacy and ergonomic improvements to the Storage Access API. In this blog post, we’ll detail a few of the new changes we made. \nThe post Improving the Storage Access API in Firefox appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Before we roll out </span><a class=\"editor-rtfLink\" href=\"https://hacks.mozilla.org/2021/02/introducing-state-partitioning/\" target=\"_blank\" rel=\"noopener\"><span data-preserver-spaces=\"true\">State Partitioning</span></a><span data-preserver-spaces=\"true\"> for all Firefox users, we intend to make a few privacy and ergonomic improvements to the </span><a class=\"editor-rtfLink\" href=\"https://privacycg.github.io/storage-access/\" target=\"_blank\" rel=\"noopener\"><span data-preserver-spaces=\"true\">Storage Access API</span></a><span data-preserver-spaces=\"true\">. In this blog post, we’ll detail a few of the new changes we made.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">With State Partitioning, third parties can’t access the same cookie jar when they’re embedded in different sites. Instead, they get a fresh cookie jar for each site they’re embedded in. This isn&#8217;t just limited to cookies either—all storage is partitioned in this way.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">In an ideal world, this would stop trackers from keeping tabs on you wherever they’re embedded because they can&#8217;t keep a unique identifier for you across all of these sites. Unfortunately, the world isn&#8217;t so simple—trackers aren&#8217;t the only third parties that use storage. If you&#8217;ve ever used an authentication provider that requires an embedded resource, you know how important third-party storage can be.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Enter the Storage Access API. This API lets third parties request storage access as if they were a first party. This is called “unpartitioning” and it gives browsers and users control over which third parties can maintain state across first-party origins as well as determine which origins they can access that state from. This is the preferred way for third parties to keep sharing storage across sites.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">The Storage Access API leaves a lot of room for the browser to decide when to allow a third party unrestricted storage access. This is a feature that gives the browser freedom to make decisions it feels are best for the user and decide when to present choices about storage permissions to users directly. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">On the other hand, this means the Storage Access API can vary from browser to browser and version to version. As a result, the developer experience will suffer unless we do two things: 1) Design with the developer experience in mind; and 2) communicate what we’re doing. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">So let’s dive in! Here are four changes we’re making to the Storage Access API that will improve user privacy and maintain a strong developer experience…</span></p>\n<h2 style=\"text-align: left;\"><span data-preserver-spaces=\"true\">Requiring User Consent for Third-Parties the User Never Interacted With</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">With Storage API, the browser determines whether to involve the user in the decision to grant storage access to a third party. Previously, Firefox didn’t involve users until a third party already had access to its storage on five different sites. At that point, the third party&#8217;s storage access requests were presented to users to make a decision. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We’re allowing third parties some leeway to unpartition their storage on a few sites because we’re worried about overwhelming users with popup permission requests. We feel that allowing only a few permission grants per third party would keep the permission frequency down while still preventing any one party from tracking the user on many sites.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We also wanted to improve user privacy in our Storage Access API implementation by reducing the number of times third parties can automatically unpartition themselves without overwhelming the user with storage access requests. The improvement we settled on was requiring the user to have interacted with the third party recently to give them storage access without explicitly asking the user whether or not to allow it. We believe that removing automatic storage access grants for sites the user has never seen before captures the spirit of State Partitioning without having to bother the user too much more.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Careful readers may now be concerned that any embed-only pages, like some authentication services, will be heavily impacted by this. To tip the scales even further toward low user touch, we expanded the definition of “interacting with a site” to support embed-only contexts. Now, whenever a user grants storage access via permission popups or interacts with an iframe with storage access, these both count as user interactions. This change is the result of a lot of careful balancing between preserving legitimate use cases, protecting user privacy, and not annoying users with endless permission prompts. We think we found the sweet spot.</span></p>\n<h2><span data-preserver-spaces=\"true\">Changing the Scope of First-Party Storage Access to Site</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">While rolling out State Partitioning, we’ve seen the emergence of a fair number of use cases for the Storage Access API. One common use is to enable authentication using a third party.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We found on occasion the login portal that gave first-party storage access to the authentication service was a subdomain, like </span><strong><span data-preserver-spaces=\"true\">https://login.example.com</span></strong><span data-preserver-spaces=\"true\">. This caused problems when the user navigated to </span><strong><span data-preserver-spaces=\"true\">https://example.com</span></strong><span data-preserver-spaces=\"true\"> after logging in… they were no longer logged in! This is because the storage access permission was only granted to the login subdomain and not the rest of the site. The authentication provider had access to its cookies on </span><strong><span data-preserver-spaces=\"true\">https://login.example.com</span></strong><span data-preserver-spaces=\"true\">, but not on </span><strong><span data-preserver-spaces=\"true\">https://example.com</span></strong><span data-preserver-spaces=\"true\">. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We fixed this by moving the storage access permission to the Site-scope. This means that when a third party gets storage access on a page, it has access to unpartitioned storage on all pages on that same Site. So in the example above, the authenticating third party would have access to the user&#8217;s login cookie on </span><strong><span data-preserver-spaces=\"true\">https://login.example.com</span></strong><span data-preserver-spaces=\"true\">, </span><strong><span data-preserver-spaces=\"true\">https://example.com</span></strong><span data-preserver-spaces=\"true\">, and </span><strong><span data-preserver-spaces=\"true\">https://any.different.subdomain.example.com</span></strong><span data-preserver-spaces=\"true\">! Yet they still wouldn&#8217;t have access to that login cookie on </span><strong><span data-preserver-spaces=\"true\">http://example.com</span></strong><span data-preserver-spaces=\"true\"> or </span><strong><span data-preserver-spaces=\"true\">https://different-example.com</span></strong><span data-preserver-spaces=\"true\">.</span></p>\n<h2><span data-preserver-spaces=\"true\">Cleaning Up User Interaction Requirements</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Requiring user interaction when requesting storage access was one rough edge of the Storage Access API definition. Let’s talk about that requirement.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">If a third party calls requestStorageAccess as soon as a page loads, it should not get that storage access. It needs to wait until the user interacts with their iframe. Scrolling or clicking are good ways to get this user interaction and it will expire a few seconds after it’s granted. Unfortunately, there were some corner cases in this requirement that we needed to clean up. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">One corner case concerns what to do with the user’s interaction state when they click Accept or Deny on a permission prompt. We decided that when a user clicks Deny on a storage access permission prompt, the third party should lose their user interaction. This prevents the third party from immediately requesting storage access again, bothering the user until they accept. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Conversely, we decided to reset the timer for user interaction if the user clicks Accept to reflect that the user did interact with the third party. This will allow the third party to use APIs that require both storage access and user interaction with only one user interaction in their iframe.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Another corner case concerned how strict to be when requiring user interaction for storage access requests. As we’ve iterated on the Storage Access API, minor changes have been introduced. One of the changes has to do with the case of giving a third party storage access on a page, but then the page is reloaded. Does the third party have to get a user interaction before requesting storage access again? Initially, the answer was no, but now it is yes. We updated our implementation to reflect that change and align with other browsers.</span><span data-preserver-spaces=\"true\"> </span></p>\n<h2><span data-preserver-spaces=\"true\">Integrating User Cookie Preferences</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">In the settings for Firefox </span><a class=\"editor-rtfLink\" href=\"https://support.mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop\" target=\"_blank\" rel=\"noopener\"><span data-preserver-spaces=\"true\">Enhanced Tracking Protection</span></a><span data-preserver-spaces=\"true\">, users can specify how they want the browser to handle cookies. By default, Firefox blocks cookies from known trackers. But we have a few other possible selections, such as allowing all cookies or blocking all third-party cookies. Users can alter this preference to their liking.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We have always respected this user choice when implementing the Storage Access API. However, this wasn&#8217;t clear to developers. For example, users that set Firefox to block all third-party cookies will be relieved to know the Storage Access API in no way weakens their protection; even a storage access permission doesn&#8217;t give a third party any access to storage. But this wasn’t clear to the third party&#8217;s developers. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">The returned promise from requestStorageAccess would resolve, indicating that the third party had access to its unpartitioned storage. We endeavored to fix this. In Firefox 98, when the user has disabled third-party cookies via the preferences, the function requestStorageAccess will always return a rejecting promise and hasStorageAccess will always return false.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\"> </span></p>\n<p><span data-preserver-spaces=\"true\"> </span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/\">Improving the Storage Access API in Firefox</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Before we roll out State Partitioning for all Firefox users, we intend to make a few privacy and ergonomic improvements to the Storage Access API. In this blog post, we’ll detail a few of the new changes we made.\nWith State Partitioning, third parties can’t access the same cookie jar when they’re embedded in different sites. Instead, they get a fresh cookie jar for each site they’re embedded in. This isn’t just limited to cookies either—all storage is partitioned in this way.\nIn an ideal world, this would stop trackers from keeping tabs on you wherever they’re embedded because they can’t keep a unique identifier for you across all of these sites. Unfortunately, the world isn’t so simple—trackers aren’t the only third parties that use storage. If you’ve ever used an authentication provider that requires an embedded resource, you know how important third-party storage can be.\nEnter the Storage Access API. This API lets third parties request storage access as if they were a first party. This is called “unpartitioning” and it gives browsers and users control over which third parties can maintain state across first-party origins as well as determine which origins they can access that state from. This is the preferred way for third parties to keep sharing storage across sites.\nThe Storage Access API leaves a lot of room for the browser to decide when to allow a third party unrestricted storage access. This is a feature that gives the browser freedom to make decisions it feels are best for the user and decide when to present choices about storage permissions to users directly. \nOn the other hand, this means the Storage Access API can vary from browser to browser and version to version. As a result, the developer experience will suffer unless we do two things: 1) Design with the developer experience in mind; and 2) communicate what we’re doing. \nSo let’s dive in! Here are four changes we’re making to the Storage Access API that will improve user privacy and maintain a strong developer experience…\nRequiring User Consent for Third-Parties the User Never Interacted With\nWith Storage API, the browser determines whether to involve the user in the decision to grant storage access to a third party. Previously, Firefox didn’t involve users until a third party already had access to its storage on five different sites. At that point, the third party’s storage access requests were presented to users to make a decision. \nWe’re allowing third parties some leeway to unpartition their storage on a few sites because we’re worried about overwhelming users with popup permission requests. We feel that allowing only a few permission grants per third party would keep the permission frequency down while still preventing any one party from tracking the user on many sites.\nWe also wanted to improve user privacy in our Storage Access API implementation by reducing the number of times third parties can automatically unpartition themselves without overwhelming the user with storage access requests. The improvement we settled on was requiring the user to have interacted with the third party recently to give them storage access without explicitly asking the user whether or not to allow it. We believe that removing automatic storage access grants for sites the user has never seen before captures the spirit of State Partitioning without having to bother the user too much more.\nCareful readers may now be concerned that any embed-only pages, like some authentication services, will be heavily impacted by this. To tip the scales even further toward low user touch, we expanded the definition of “interacting with a site” to support embed-only contexts. Now, whenever a user grants storage access via permission popups or interacts with an iframe with storage access, these both count as user interactions. This change is the result of a lot of careful balancing between preserving legitimate use cases, protecting user privacy, and not annoying users with endless permission prompts. We think we found the sweet spot.\nChanging the Scope of First-Party Storage Access to Site\nWhile rolling out State Partitioning, we’ve seen the emergence of a fair number of use cases for the Storage Access API. One common use is to enable authentication using a third party.\nWe found on occasion the login portal that gave first-party storage access to the authentication service was a subdomain, like https://login.example.com. This caused problems when the user navigated to https://example.com after logging in… they were no longer logged in! This is because the storage access permission was only granted to the login subdomain and not the rest of the site. The authentication provider had access to its cookies on https://login.example.com, but not on https://example.com. \nWe fixed this by moving the storage access permission to the Site-scope. This means that when a third party gets storage access on a page, it has access to unpartitioned storage on all pages on that same Site. So in the example above, the authenticating third party would have access to the user’s login cookie on https://login.example.com, https://example.com, and https://any.different.subdomain.example.com! Yet they still wouldn’t have access to that login cookie on http://example.com or https://different-example.com.\nCleaning Up User Interaction Requirements\nRequiring user interaction when requesting storage access was one rough edge of the Storage Access API definition. Let’s talk about that requirement.\nIf a third party calls requestStorageAccess as soon as a page loads, it should not get that storage access. It needs to wait until the user interacts with their iframe. Scrolling or clicking are good ways to get this user interaction and it will expire a few seconds after it’s granted. Unfortunately, there were some corner cases in this requirement that we needed to clean up. \nOne corner case concerns what to do with the user’s interaction state when they click Accept or Deny on a permission prompt. We decided that when a user clicks Deny on a storage access permission prompt, the third party should lose their user interaction. This prevents the third party from immediately requesting storage access again, bothering the user until they accept. \nConversely, we decided to reset the timer for user interaction if the user clicks Accept to reflect that the user did interact with the third party. This will allow the third party to use APIs that require both storage access and user interaction with only one user interaction in their iframe.\nAnother corner case concerned how strict to be when requiring user interaction for storage access requests. As we’ve iterated on the Storage Access API, minor changes have been introduced. One of the changes has to do with the case of giving a third party storage access on a page, but then the page is reloaded. Does the third party have to get a user interaction before requesting storage access again? Initially, the answer was no, but now it is yes. We updated our implementation to reflect that change and align with other browsers. \nIntegrating User Cookie Preferences\nIn the settings for Firefox Enhanced Tracking Protection, users can specify how they want the browser to handle cookies. By default, Firefox blocks cookies from known trackers. But we have a few other possible selections, such as allowing all cookies or blocking all third-party cookies. Users can alter this preference to their liking.\nWe have always respected this user choice when implementing the Storage Access API. However, this wasn’t clear to developers. For example, users that set Firefox to block all third-party cookies will be relieved to know the Storage Access API in no way weakens their protection; even a storage access permission doesn’t give a third party any access to storage. But this wasn’t clear to the third party’s developers. \nThe returned promise from requestStorageAccess would resolve, indicating that the third party had access to its unpartitioned storage. We endeavored to fix this. In Firefox 98, when the user has disabled third-party cookies via the preferences, the function requestStorageAccess will always return a rejecting promise and hasStorageAccess will always return false.\n \n \nThe post Improving the Storage Access API in Firefox appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-02-08T16:59:21.000Z",
      "date_modified": "2022-02-08T16:59:21.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47574",
      "url": "https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/",
      "title": "Retrospective and Technical Details on the recent Firefox Outage",
      "summary": "On January 13th 2022, Firefox became unusable for close to two hours for users worldwide. This incident interrupted many people’s workflow. This post highlights the complex series of events and circumstances that, together, triggered a bug deep in the networking code of Firefox. What Happened? Firefox has a number of servers and related infrastructure that […]\nThe post Retrospective and Technical Details on the recent Firefox Outage appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>On January 13th 2022, Firefox became unusable for close to two hours for users worldwide. This incident interrupted many people’s workflow. This post highlights the complex series of events and circumstances that, together, triggered a bug deep in the networking code of Firefox.<span id=\"more-47574\"></span></p>\n<h2>What Happened?</h2>\n<p align=\"justify\">Firefox has a number of servers and related infrastructure that handle several internal services. These include updates, telemetry, certificate management, crash reporting and other similar functionality. This infrastructure is hosted by different cloud service providers that use load balancers to distribute the load evenly across servers. For those services hosted on Google Cloud Platform (GCP) these load balancers have settings related to the HTTP protocol they should advertise and one of these settings is HTTP/3 support with three states: “Enabled”, “Disabled” or “Automatic (default)”. Our load balancers were set to the “Automatic (default)” setting and on January 13, 2022 at 07:28 UTC, GCP deployed an unannounced change to make HTTP/3 the default. As Firefox uses HTTP/3 when supported, from that point forward, some connections that Firefox makes to the services infrastructure would use HTTP/3 instead of the previously used HTTP/2 protocol.<a id=\"footnote1\"></a>¹</p>\n<p align=\"justify\">Shortly after, we noticed a spike in crashes being reported through our crash reporter and also received several reports from inside and outside of Mozilla describing a hang of the browser.</p>\n<p><div id=\"attachment_47575\" style=\"width: 510px\" class=\"wp-caption aligncenter\"><a href=\"https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2.png\"><img aria-describedby=\"caption-attachment-47575\" loading=\"lazy\" class=\"wp-image-47575 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-500x231.png\" alt=\"A graph showing the curve of unprocessed crash reports quickly growing.\" width=\"500\" height=\"231\" srcset=\"https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-500x231.png 500w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-250x115.png 250w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-768x355.png 768w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-1536x709.png 1536w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-2048x946.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a><p id=\"caption-attachment-47575\" class=\"wp-caption-text\">Backlog of pending crash reports building up and reaching close to 300K unprocessed reports.</p></div></p>\n<p align=\"justify\">As part of the incident response process, we quickly discovered that the client was hanging inside a network request to one of the Firefox internal services. However, at this point we neither had an explanation for why this would trigger just now, nor what the scope of the problem was. We continued to look for the “trigger” — some change that must have occurred to start the problem. We found that we had not shipped updates or configuration changes that could have caused this problem. At the same time, we were keeping in mind that HTTP/3 had been enabled since Firefox 88 and was actively used by some popular websites.</p>\n<p align=\"justify\">Although we couldn’t see it, we suspected that there had been some kind of “invisible” change rolled out by one of our cloud providers that somehow modified load balancer behavior. On closer inspection, none of our settings were changed. We then discovered through logs that for some reason, the load balancers for our Telemetry service were serving HTTP/3 connections while they hadn’t done that before. We disabled HTTP/3 explicitly on GCP at 09:12 UTC. This unblocked our users, but we were not yet certain about the root cause and without knowing that, it was impossible for us to tell if this would affect additional HTTP/3 connections.</p>\n<p align=\"justify\"><small><a href=\"#footnote1\">¹</a> <i>Some highly critical services such as updates use a special <code>beConservative</code> flag that prevents the use of any experimental technology for their connections (e.g. HTTP/3).</i></small></p>\n<h2>A Special Mix of Ingredients</h2>\n<p align=\"justify\">It quickly became clear to us that there must be some combination of special circumstances for the hang to occur. We performed a number of tests with various tools and remote services and were not able to reproduce the problem, not even with a regular connection to the Telemetry staging server (a server only used for testing deployments, which we had left in its original configuration for testing purposes). With Firefox itself, however, we were able to reproduce the issue with the staging server.</p>\n<p align=\"justify\">After further debugging, we found the “special ingredient” required for this bug to happen. All HTTP/3 connections go through Necko, our networking stack. However, Rust components that need direct network access are not using Necko directly, but are calling into it through an intermediate library called <a href=\"https://github.com/mozilla/application-services/tree/main/components/viaduct\"><i><code>viaduct</code></i></a>.</p>\n<p align=\"justify\">In order to understand why this mattered, we first need to understand some things about the internals of Necko, in particular about HTTP/3 upload requests. For such requests, the higher-level Necko APIs<a id=\"footnote2\"></a>² check if the <code>Content-Length</code> header is present and if it isn&#8217;t, it will automatically be added. The lower-level HTTP/3 code later relies on this header to determine the request size. This works fine for web content and other requests in our code.</p>\n<p align=\"justify\">When requests pass through <code>viaduct</code> first, however, <code>viaduct</code> will lower-case each header and pass it on to Necko. And here is the problem: the API checks in Necko are case-<b>insensitive</b> while the lower-level HTTP/3 code is case-<b>sensitive</b>. So if any code was to add a <code>Content-Length</code> header and pass the request through <code>viaduct</code>, it would pass the Necko API checks but the HTTP/3 code would not find the header.</p>\n<p align=\"justify\">It just so happens that Telemetry is currently the only Rust-based component in Firefox Desktop that uses the network stack and adds a <code>Content-Length</code> header. This is why users who disabled Telemetry would see this problem resolved even though the problem is not related to Telemetry functionality itself and could have been triggered otherwise.</p>\n<p><div id=\"attachment_47579\" style=\"width: 510px\" class=\"wp-caption aligncenter\"><a href=\"https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4.png\"><img aria-describedby=\"caption-attachment-47579\" loading=\"lazy\" class=\"wp-image-47579 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-500x266.png\" alt=\"A diagram showing the different network components in Firefox.\" width=\"500\" height=\"266\" srcset=\"https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-500x266.png 500w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-250x133.png 250w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-768x409.png 768w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-1536x818.png 1536w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4.png 1826w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a><p id=\"caption-attachment-47579\" class=\"wp-caption-text\">A specific code path was required to trigger the problem in the HTTP/3 protocol implementation.</p></div></p>\n<p align=\"justify\"><small><a href=\"#footnote2\">²</a> <i>These are internal APIs, not accessible to web content.</i></small></p>\n<h2>The Infinite Loop</h2>\n<p align=\"justify\">With the load balancer change in place, and a special code path in a new Rust service now active, the necessary final ingredient to trigger the problem for users was deep in Necko HTTP/3 code.</p>\n<p align=\"justify\">When handling a request, the code <a href=\"https://searchfox.org/mozilla-central/rev/435a77f1a1aaf1a78d30a2aaa81c6158a2f83dba/netwerk/protocol/http/Http3Stream.cpp#71,79-83\">looked up the field in a case-sensitive way</a> and failed to find the header as it had been lower-cased by <code>viaduct</code>. Without the header, the request was determined by the Necko code to be complete, leaving the real request body unsent. However, this code would only terminate when there was no additional content to send. This <a href=\"https://searchfox.org/mozilla-central/rev/435a77f1a1aaf1a78d30a2aaa81c6158a2f83dba/netwerk/protocol/http/Http3Stream.cpp#223,228,272-274\">unexpected state caused the code to loop indefinitely rather than returning an error</a>. Because all network requests go through one <i>socket thread</i>, this loop blocked any further network communication and made Firefox unresponsive, unable to load web content.</p>\n<h2>Lessons Learned</h2>\n<p align=\"justify\">As so often is the case, the issue was a lot more complex than it appeared at first glance and there were many contributing factors working together. Some of the key factors we have identified include:</p>\n<ul>\n<li aria-level=\"1\">\n<p align=\"justify\">GCP’s deployment of HTTP/3 as default was unannounced. We are actively working with them to improve the situation. We realize that an announcement (as is usually sent) might not have entirely mitigated the risk of an incident, but it would likely have triggered more controlled experiments (e.g. in a staging environment) and deployment.</p>\n</li>\n<li aria-level=\"1\">\n<p align=\"justify\">Our setting of “Automatic (default)” on the load balancers instead of a more explicit choice allowed the deployment to take place automatically. We are reviewing all service configurations to avoid similar mistakes in the future.</p>\n</li>\n<li aria-level=\"1\">\n<p align=\"justify\">The particular combination of HTTP/3 and <code>viaduct</code> on Firefox Desktop was not covered in our continuous integration system. While we cannot test every possible combination of configurations and components, the choice of HTTP version is a fairly major change that should have been tested, as well as the use of an additional networking layer like <code>viaduct</code>. Current HTTP/3 tests cover the low-level protocol behavior and the Necko layer as it is used by web content. We should run more system tests with different HTTP versions and doing so could have revealed this problem.</p>\n</li>\n</ul>\n<p align=\"justify\">We are also investigating action points both to make the browser more resilient towards such problems and to make incident response even faster. Learning as much as possible from this incident will help us improve the quality of our products. We’re grateful to all the users who have sent crash reports, worked with us in Bugzilla or helped others to work around the problem.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/\">Retrospective and Technical Details on the recent Firefox Outage</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "On January 13th 2022, Firefox became unusable for close to two hours for users worldwide. This incident interrupted many people’s workflow. This post highlights the complex series of events and circumstances that, together, triggered a bug deep in the networking code of Firefox.\nWhat Happened?\nFirefox has a number of servers and related infrastructure that handle several internal services. These include updates, telemetry, certificate management, crash reporting and other similar functionality. This infrastructure is hosted by different cloud service providers that use load balancers to distribute the load evenly across servers. For those services hosted on Google Cloud Platform (GCP) these load balancers have settings related to the HTTP protocol they should advertise and one of these settings is HTTP/3 support with three states: “Enabled”, “Disabled” or “Automatic (default)”. Our load balancers were set to the “Automatic (default)” setting and on January 13, 2022 at 07:28 UTC, GCP deployed an unannounced change to make HTTP/3 the default. As Firefox uses HTTP/3 when supported, from that point forward, some connections that Firefox makes to the services infrastructure would use HTTP/3 instead of the previously used HTTP/2 protocol.¹\nShortly after, we noticed a spike in crashes being reported through our crash reporter and also received several reports from inside and outside of Mozilla describing a hang of the browser.\nBacklog of pending crash reports building up and reaching close to 300K unprocessed reports.\nAs part of the incident response process, we quickly discovered that the client was hanging inside a network request to one of the Firefox internal services. However, at this point we neither had an explanation for why this would trigger just now, nor what the scope of the problem was. We continued to look for the “trigger” — some change that must have occurred to start the problem. We found that we had not shipped updates or configuration changes that could have caused this problem. At the same time, we were keeping in mind that HTTP/3 had been enabled since Firefox 88 and was actively used by some popular websites.\nAlthough we couldn’t see it, we suspected that there had been some kind of “invisible” change rolled out by one of our cloud providers that somehow modified load balancer behavior. On closer inspection, none of our settings were changed. We then discovered through logs that for some reason, the load balancers for our Telemetry service were serving HTTP/3 connections while they hadn’t done that before. We disabled HTTP/3 explicitly on GCP at 09:12 UTC. This unblocked our users, but we were not yet certain about the root cause and without knowing that, it was impossible for us to tell if this would affect additional HTTP/3 connections.\n¹ Some highly critical services such as updates use a special beConservative flag that prevents the use of any experimental technology for their connections (e.g. HTTP/3).\nA Special Mix of Ingredients\nIt quickly became clear to us that there must be some combination of special circumstances for the hang to occur. We performed a number of tests with various tools and remote services and were not able to reproduce the problem, not even with a regular connection to the Telemetry staging server (a server only used for testing deployments, which we had left in its original configuration for testing purposes). With Firefox itself, however, we were able to reproduce the issue with the staging server.\nAfter further debugging, we found the “special ingredient” required for this bug to happen. All HTTP/3 connections go through Necko, our networking stack. However, Rust components that need direct network access are not using Necko directly, but are calling into it through an intermediate library called viaduct.\nIn order to understand why this mattered, we first need to understand some things about the internals of Necko, in particular about HTTP/3 upload requests. For such requests, the higher-level Necko APIs² check if the Content-Length header is present and if it isn’t, it will automatically be added. The lower-level HTTP/3 code later relies on this header to determine the request size. This works fine for web content and other requests in our code.\nWhen requests pass through viaduct first, however, viaduct will lower-case each header and pass it on to Necko. And here is the problem: the API checks in Necko are case-insensitive while the lower-level HTTP/3 code is case-sensitive. So if any code was to add a Content-Length header and pass the request through viaduct, it would pass the Necko API checks but the HTTP/3 code would not find the header.\nIt just so happens that Telemetry is currently the only Rust-based component in Firefox Desktop that uses the network stack and adds a Content-Length header. This is why users who disabled Telemetry would see this problem resolved even though the problem is not related to Telemetry functionality itself and could have been triggered otherwise.\nA specific code path was required to trigger the problem in the HTTP/3 protocol implementation.\n² These are internal APIs, not accessible to web content.\nThe Infinite Loop\nWith the load balancer change in place, and a special code path in a new Rust service now active, the necessary final ingredient to trigger the problem for users was deep in Necko HTTP/3 code.\nWhen handling a request, the code looked up the field in a case-sensitive way and failed to find the header as it had been lower-cased by viaduct. Without the header, the request was determined by the Necko code to be complete, leaving the real request body unsent. However, this code would only terminate when there was no additional content to send. This unexpected state caused the code to loop indefinitely rather than returning an error. Because all network requests go through one socket thread, this loop blocked any further network communication and made Firefox unresponsive, unable to load web content.\nLessons Learned\nAs so often is the case, the issue was a lot more complex than it appeared at first glance and there were many contributing factors working together. Some of the key factors we have identified include:\n\n\nGCP’s deployment of HTTP/3 as default was unannounced. We are actively working with them to improve the situation. We realize that an announcement (as is usually sent) might not have entirely mitigated the risk of an incident, but it would likely have triggered more controlled experiments (e.g. in a staging environment) and deployment.\n\n\nOur setting of “Automatic (default)” on the load balancers instead of a more explicit choice allowed the deployment to take place automatically. We are reviewing all service configurations to avoid similar mistakes in the future.\n\n\nThe particular combination of HTTP/3 and viaduct on Firefox Desktop was not covered in our continuous integration system. While we cannot test every possible combination of configurations and components, the choice of HTTP version is a fairly major change that should have been tested, as well as the use of an additional networking layer like viaduct. Current HTTP/3 tests cover the low-level protocol behavior and the Necko layer as it is used by web content. We should run more system tests with different HTTP versions and doing so could have revealed this problem.\n\n\nWe are also investigating action points both to make the browser more resilient towards such problems and to make incident response even faster. Learning as much as possible from this incident will help us improve the quality of our products. We’re grateful to all the users who have sent crash reports, worked with us in Bugzilla or helped others to work around the problem.\nThe post Retrospective and Technical Details on the recent Firefox Outage appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-02-02T09:00:50.000Z",
      "date_modified": "2022-02-02T09:00:50.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47585",
      "url": "https://hacks.mozilla.org/2022/01/hacks-decoded-adewale-adetona/",
      "title": "Hacks Decoded: Adewale Adetona",
      "summary": "Adetona Adewale Akeem, more popularly known as iSlimfit, is a Nigeria-born revered digital technologist and marketing expert. He is the co-founder of Menopays, a fintech startup offering another Buy Now Pay Later (BNPL) option across Africa. We chatted with him about founding Menopays and the impact of tech solutions developed in Nigeria. \nThe post Hacks Decoded: Adewale Adetona appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><i>Welcome to our Hacks: Decoded Interview series!</i></p>\n<p><i>Once a month, </i><a href=\"https://foundation.mozilla.org/\" target=\"_blank\" rel=\"noopener\"><i>Mozilla Foundation</i></a><i>’s </i><a href=\"https://www.xavierharding.com/\" target=\"_blank\" rel=\"noopener\"><i>Xavier Harding</i></a><i> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </i><a href=\"https://hacks.mozilla.org/\"><i>Hacks</i></a><i> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.</i></p>\n<p><strong>Meet Adetona Adewale Akeem!</strong></p>\n<p><strong><img loading=\"lazy\" class=\"aligncenter\" src=\"https://cdn.vanguardngr.com/wp-content/uploads/2021/07/WA-683x1024.jpeg\" alt=\"Adewale Adetona\" width=\"395\" height=\"593\" /></strong></p>\n<p><span style=\"font-weight: 400;\">Adetona Adewale Akeem, more popularly known as iSlimfit, is a Nigeria-born revered digital technologist and marketing expert. He is the co-founder of <a href=\"https://menopays.com/\">Menopays,</a> a fintech startup offering another Buy Now Pay Later (BNPL) option across Africa. </span></p>\n<p><b>So, I’ve got to ask — where does the name iSlimfit come from?</b></p>\n<p><span style=\"font-weight: 400;\">“Slimfit” is a nickname from my University days. But when I wanted to join social media, Twitter, in particular, I figured out the username Slimfit was already taken. All efforts to reach and plead with the user — who even up until now has never posted anything on the account — to release the username for me proved abortive. Then I came up with another username by adding “i” (which signifies referring to myself) to the front of Slimfit. </span><span style=\"font-weight: 400;\"><br />\n</span><span style=\"font-weight: 400;\"><br />\n</span><b>How did you get started in the tech industry, iSlimfit?</b><b></b></p>\n<p><span style=\"font-weight: 400;\">My journey into tech started as far back as 2014, when I made the switch from working at a Media &amp; Advertising Agency in Lagos Nigeria to working as a Digital Marketing Executive in a Fintech Company called SystemSpecs in Nigeria. Being someone that loved combining data with tech, I have always had a knack for growth marketing. So the opportunity to work in a fintech company in that capacity wasn’t something I could let slide.</span></p>\n<p><b>Where are you based currently? And where are you from originally? How does where you&#8217;re from affect how you move through the tech industry?</b></p>\n<p><span style=\"font-weight: 400;\">I am currently based in Leeds, United Kingdom after recently getting a <a href=\"https://technation.io/visa/\">Tech Nation Global Talent</a> endorsement by the UK government. I am from Ogun State, Nigeria. </span></p>\n<p><span style=\"font-weight: 400;\">There is actually no negative impact from my background or where I am from as regards my work in tech. The Nigerian tech space is huge and the opportunities are enormous. Strategic positioning and working with a goal in mind has helped me in navigating my career in tech so far.</span></p>\n<p><b>What brought about the idea of your new vlog Tech Chat with iSlimfit?</b></p>\n<p><b></b><b></b><span style=\"font-weight: 400;\">My desire to make an impact and contribute to the growth of upcoming tech professionals birthed </span><a href=\"https://www.youtube.com/c/AdetonaAdewaleSlimfit\"><span style=\"font-weight: 400;\">the vlog</span></a><span style=\"font-weight: 400;\">. Also, I wanted to replicate what I do offline with Lagos Digital Summit, in an online manner. The vlog is basically a series of YouTube chat series where I bring various people in tech — growth marketers, UI/UX designers, product managers, startup founders, mobile app developers, etc. — to share their career journey, background, transitioning, their career journey, learnings, and general questions about their day-to-day job so that Tech enthusiasts can learn from their expertise.</span></p>\n<p><b>I have to bring up the fact that in 2021, you were endorsed by Tech Nation as an Exceptional agent in Digital Tech. What’s it feel like to achieve something like that?</b><b><br />\n</b><b><br />\n</b><span style=\"font-weight: 400;\">The Tech Nation endorsement by the UK government is one of my biggest achievements. It made me realize how important my impact on the Nigerian tech industry over the years has been. The endorsement was granted based on my significant contribution to the Nigerian Digital Tech sector, my mentorship &amp; leadership capabilities, and also the potential contribution my talent &amp; expertise would add to the UK digital economy. I am particularly grateful for the opportunity to positively make an impact to the digital economy of the United Kingdom.</span></p>\n<p><b>What&#8217;s something folks may not immediately realize about the tech sector in Nigeria if they’re not from there?</b><b></b></p>\n<p><span style=\"font-weight: 400;\">Easy: the fact that the tech sector in Nigeria is the biggest in Africa, and the impact of tech solutions developed in Nigeria is felt all over Africa. Also, as we can see from a recent </span><a href=\"https://www.linkedin.com/posts/islimfit_startups-activity-6871734350942085120-Jh0V\"><b>report</b></a><span style=\"font-weight: 400;\">, Nigerian startups lead the list of African Startups that received funding in 2021.</span></p>\n<p><b>What digital policy or policies do you think Nigeria (your home country) should pursue in order to accelerate digital development in the country?</b></p>\n<p><span style=\"font-weight: 400;\">The Nigerian government need to come to terms with the fact that digital technology is the bedrock for the development of the Nation. They need to develop policies that will shape the Nation’s digital economy and design a roadmap for grassroots digital Tech empowerment of Nigeria’s agile population. </span></p>\n<p><span style=\"font-weight: 400;\">We also need more people to champion and improve on our quest for digital entrepreneurship development through various platforms.</span></p>\n<p><b>You helped co-found a company called Menopays. What were some of the hurdles when it comes to getting a tech company off the ground over there? What about the opposite? What are the ways those in tech benefit from founding and working in Nigeria?</b></p>\n<p><span style=\"font-weight: 400;\">Some hurdles in starting a tech company is putting together the right team for the job. This cuts across legal, product, marketing, and the tech itself. The idea could be great but without the right team, execution is challenging. </span></p>\n<p><span style=\"font-weight: 400;\">A great benefit is that the continent of Africa is gaining in popularity and the world is watching, so a genuine team founding a business will get the benefits of foreign investments which is great in terms of dollar value.</span></p>\n<p><b>Some take issue with </b><a href=\"https://www.nerdwallet.com/article/loans/personal-loans/buy-now-pay-later-apps\"><b>Buy Now Pay Later apps</b></a><b> and services like Menopays in how they may profit off of buyers who may have less. How is Menopays different? How does the company make money? What measures are in place to make sure you aren’t taking advantage of people?</b></p>\n<p><span style=\"font-weight: 400;\">Menopays is different because our focus goes beyond the profitability of the industry. We tailored a minimum spendable amount with a decent repayment period for the minimum wage in Nigeria. Our vision stands in the middle of every decision we make both business-wise and/or product development-wise. </span></p>\n<p><span style=\"font-weight: 400;\">The measure in place is that decisions are guided by why we started Menopays, which is “to fight poverty”. We don’t charge customers exorbitant interest as it goes against what we are preaching as a brand. So our Vision is imprinted in the heart of all the team members working towards making Menopays a family brand.</span></p>\n<p><b>You’ve </b><a href=\"https://technext.ng/2021/12/03/lagos-digital-summit-has-inspired-the-birth-of-similar-digital-gatherings-across-nigeria-adewale-adetona/\"><b>mentioned</b></a><b> Menopays is fighting poverty in Nigeria and eventually all of Africa, how so?</b><b><br />\n</b><b><br />\n</b><span style=\"font-weight: 400;\">Thinking about one of the incidents that happened to one of our co-founders, Reuben Olawale Odumosu, about eight years back. He lost his best friend because of a substandard malaria medication. His best friend in high school died because his parents couldn’t afford NGN2,500 malaria medication at the time and point of need which led to them going for a cheaper drug that eventually led to his death. Menopays exists to prevent such situations by making basic needs like healthcare, groceries and clothing available to our customers even when they don’t have the money to pay at that moment.</span></p>\n<p><span style=\"font-weight: 400;\">So in light of this, at Menopays, we believe that if some particular things are taken care of, individuals stand a lot more chances of survival. Take for instance, someone earns NGN18,000, spends NGN5,000 on transport, NGN7,000 on food and rent and some other miscellaneous of NGN6,000; with Menopays, we take out the cost of transportation and food (by providing you access to our merchants) and we give them more time to pay over the next three months. Which means each month the customer is positive cash flow of NGN6,000. We turn a negative cash flow into a positive cash flow and savings, thereby fighting poverty.</span></p>\n<p><b>If you didn’t help found Menopays, what would you be doing now instead?</b></p>\n<p><span style=\"font-weight: 400;\">I would probably be working on founding another tech startup doing something for the greater good of the world and helping brands achieve their desired marketing objectives.</span></p>\n<p><b>How can the African tech diaspora help startups similar to Menopays?</b></p>\n<p><span style=\"font-weight: 400;\">One way African tech diaspora can help startups similar to Menopays is by promoting their services, sharing with potential users, and also by investing in it.</span></p>\n<p><b>How did you come up with the idea for Lagos Digital Summit?</b><b></b></p>\n<p><span style=\"font-weight: 400;\">Lagos Digital Summit started in 2017 with just an idea in my small shared apartment back then in Lagos with my friend who is now in Canada. The goal back then was simply to facilitate a platform for the convergence of 50 to 60 digital marketing professionals and business thought leaders for the advancement of SMEs and Digital Media enthusiasts within our network.</span><span style=\"font-weight: 400;\"><br />\n</span><span style=\"font-weight: 400;\"><br />\n</span><span style=\"font-weight: 400;\">Five years down the line, despite being faced with plenty of challenges, it&#8217;s been a big success story. We have had the privilege of empowering over 5,000 businesses and individuals with diverse digital marketing skills. </span></p>\n<p><b>What’s it been like arranging that sort of summit in the midst of a pandemic?</b></p>\n<p><span style=\"font-weight: 400;\">Lagos Digital Summit 2020 has been the only edition that we’ve had to do full virtual because it was in the peak of the COVID-19 pandemic. Every other edition before then had been physical with fully packed attendees of an average of 1,000. For the 2021 edition, it was hybrid because Covid-19 restrictions were relaxed, where we had just 300 people attend physically and every other people watched online.</span></p>\n<p><b>What&#8217;s something you see everywhere in tech that you wish more people would talk about?</b></p>\n<p><span style=\"font-weight: 400;\">I wish more people would talk about the struggle, the disappointments, the challenges and the numerous sacrifices that comes with building a tech startup. A lot of times, the media only portray the success stories, especially when a startup raises funds; the headlines are always very inspiring and rosy. </span></p>\n<p><b>What’s been the most impactful thing you’ve done since working in tech? What’s been the most memorable?</b><b><br />\n</b><b><br />\n</b><span style=\"font-weight: 400;\">That should be founding Lagos Digital Summit; the kind of sponsors, corporate organisations, high-profiled speakers, volunteers and attendees that the Summit has been able to attract has been a memorable and proud feeling.</span></p>\n<p><b>What sort of lasting impact do you want to have on the industry and the world? What keeps you going?</b></p>\n<p><span style=\"font-weight: 400;\">Waking up every day, knowing that a lot of people would have a smile on their faces because I have chosen to impact lives and make the world a better place through relevant tech solutions and platforms is the best feeling for me. The fact that I can read through reports and data and see the number of people using Menopays as a Buy Now Pay Later (BNPL) payment option to ease their lifestyle is a big motivation for me. </span></p>\n<p><b>What’s some advice you’d give to others hoping to enter the tech world or hoping to start up a company?</b></p>\n<p><span style=\"font-weight: 400;\">Venturing into Tech or building a Startup takes a whole lot of concerted effort and determination. Getting the right set of partner(s) would however make the journey easier for you. Just have partners or cofounders with similar vision and complementing skills.</span></p>\n<p>—</p>\n<p><em>You can keep up with Adewale’s work by following him <a href=\"https://twitter.com/iSlimfit\" target=\"_blank\" rel=\"noopener\">here.</a> Stay tuned for more Hacks Decoded Q&amp;A’s!</em></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/01/hacks-decoded-adewale-adetona/\">Hacks Decoded: Adewale Adetona</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Welcome to our Hacks: Decoded Interview series!\nOnce a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.\nMeet Adetona Adewale Akeem!\n\nAdetona Adewale Akeem, more popularly known as iSlimfit, is a Nigeria-born revered digital technologist and marketing expert. He is the co-founder of Menopays, a fintech startup offering another Buy Now Pay Later (BNPL) option across Africa. \nSo, I’ve got to ask — where does the name iSlimfit come from?\n“Slimfit” is a nickname from my University days. But when I wanted to join social media, Twitter, in particular, I figured out the username Slimfit was already taken. All efforts to reach and plead with the user — who even up until now has never posted anything on the account — to release the username for me proved abortive. Then I came up with another username by adding “i” (which signifies referring to myself) to the front of Slimfit. \n\nHow did you get started in the tech industry, iSlimfit?\nMy journey into tech started as far back as 2014, when I made the switch from working at a Media & Advertising Agency in Lagos Nigeria to working as a Digital Marketing Executive in a Fintech Company called SystemSpecs in Nigeria. Being someone that loved combining data with tech, I have always had a knack for growth marketing. So the opportunity to work in a fintech company in that capacity wasn’t something I could let slide.\nWhere are you based currently? And where are you from originally? How does where you’re from affect how you move through the tech industry?\nI am currently based in Leeds, United Kingdom after recently getting a Tech Nation Global Talent endorsement by the UK government. I am from Ogun State, Nigeria. \nThere is actually no negative impact from my background or where I am from as regards my work in tech. The Nigerian tech space is huge and the opportunities are enormous. Strategic positioning and working with a goal in mind has helped me in navigating my career in tech so far.\nWhat brought about the idea of your new vlog Tech Chat with iSlimfit?\nMy desire to make an impact and contribute to the growth of upcoming tech professionals birthed the vlog. Also, I wanted to replicate what I do offline with Lagos Digital Summit, in an online manner. The vlog is basically a series of YouTube chat series where I bring various people in tech — growth marketers, UI/UX designers, product managers, startup founders, mobile app developers, etc. — to share their career journey, background, transitioning, their career journey, learnings, and general questions about their day-to-day job so that Tech enthusiasts can learn from their expertise.\nI have to bring up the fact that in 2021, you were endorsed by Tech Nation as an Exceptional agent in Digital Tech. What’s it feel like to achieve something like that?\n\nThe Tech Nation endorsement by the UK government is one of my biggest achievements. It made me realize how important my impact on the Nigerian tech industry over the years has been. The endorsement was granted based on my significant contribution to the Nigerian Digital Tech sector, my mentorship & leadership capabilities, and also the potential contribution my talent & expertise would add to the UK digital economy. I am particularly grateful for the opportunity to positively make an impact to the digital economy of the United Kingdom.\nWhat’s something folks may not immediately realize about the tech sector in Nigeria if they’re not from there?\nEasy: the fact that the tech sector in Nigeria is the biggest in Africa, and the impact of tech solutions developed in Nigeria is felt all over Africa. Also, as we can see from a recent report, Nigerian startups lead the list of African Startups that received funding in 2021.\nWhat digital policy or policies do you think Nigeria (your home country) should pursue in order to accelerate digital development in the country?\nThe Nigerian government need to come to terms with the fact that digital technology is the bedrock for the development of the Nation. They need to develop policies that will shape the Nation’s digital economy and design a roadmap for grassroots digital Tech empowerment of Nigeria’s agile population. \nWe also need more people to champion and improve on our quest for digital entrepreneurship development through various platforms.\nYou helped co-found a company called Menopays. What were some of the hurdles when it comes to getting a tech company off the ground over there? What about the opposite? What are the ways those in tech benefit from founding and working in Nigeria?\nSome hurdles in starting a tech company is putting together the right team for the job. This cuts across legal, product, marketing, and the tech itself. The idea could be great but without the right team, execution is challenging. \nA great benefit is that the continent of Africa is gaining in popularity and the world is watching, so a genuine team founding a business will get the benefits of foreign investments which is great in terms of dollar value.\nSome take issue with Buy Now Pay Later apps and services like Menopays in how they may profit off of buyers who may have less. How is Menopays different? How does the company make money? What measures are in place to make sure you aren’t taking advantage of people?\nMenopays is different because our focus goes beyond the profitability of the industry. We tailored a minimum spendable amount with a decent repayment period for the minimum wage in Nigeria. Our vision stands in the middle of every decision we make both business-wise and/or product development-wise. \nThe measure in place is that decisions are guided by why we started Menopays, which is “to fight poverty”. We don’t charge customers exorbitant interest as it goes against what we are preaching as a brand. So our Vision is imprinted in the heart of all the team members working towards making Menopays a family brand.\nYou’ve mentioned Menopays is fighting poverty in Nigeria and eventually all of Africa, how so?\n\nThinking about one of the incidents that happened to one of our co-founders, Reuben Olawale Odumosu, about eight years back. He lost his best friend because of a substandard malaria medication. His best friend in high school died because his parents couldn’t afford NGN2,500 malaria medication at the time and point of need which led to them going for a cheaper drug that eventually led to his death. Menopays exists to prevent such situations by making basic needs like healthcare, groceries and clothing available to our customers even when they don’t have the money to pay at that moment.\nSo in light of this, at Menopays, we believe that if some particular things are taken care of, individuals stand a lot more chances of survival. Take for instance, someone earns NGN18,000, spends NGN5,000 on transport, NGN7,000 on food and rent and some other miscellaneous of NGN6,000; with Menopays, we take out the cost of transportation and food (by providing you access to our merchants) and we give them more time to pay over the next three months. Which means each month the customer is positive cash flow of NGN6,000. We turn a negative cash flow into a positive cash flow and savings, thereby fighting poverty.\nIf you didn’t help found Menopays, what would you be doing now instead?\nI would probably be working on founding another tech startup doing something for the greater good of the world and helping brands achieve their desired marketing objectives.\nHow can the African tech diaspora help startups similar to Menopays?\nOne way African tech diaspora can help startups similar to Menopays is by promoting their services, sharing with potential users, and also by investing in it.\nHow did you come up with the idea for Lagos Digital Summit?\nLagos Digital Summit started in 2017 with just an idea in my small shared apartment back then in Lagos with my friend who is now in Canada. The goal back then was simply to facilitate a platform for the convergence of 50 to 60 digital marketing professionals and business thought leaders for the advancement of SMEs and Digital Media enthusiasts within our network.\n\nFive years down the line, despite being faced with plenty of challenges, it’s been a big success story. We have had the privilege of empowering over 5,000 businesses and individuals with diverse digital marketing skills. \nWhat’s it been like arranging that sort of summit in the midst of a pandemic?\nLagos Digital Summit 2020 has been the only edition that we’ve had to do full virtual because it was in the peak of the COVID-19 pandemic. Every other edition before then had been physical with fully packed attendees of an average of 1,000. For the 2021 edition, it was hybrid because Covid-19 restrictions were relaxed, where we had just 300 people attend physically and every other people watched online.\nWhat’s something you see everywhere in tech that you wish more people would talk about?\nI wish more people would talk about the struggle, the disappointments, the challenges and the numerous sacrifices that comes with building a tech startup. A lot of times, the media only portray the success stories, especially when a startup raises funds; the headlines are always very inspiring and rosy. \nWhat’s been the most impactful thing you’ve done since working in tech? What’s been the most memorable?\n\nThat should be founding Lagos Digital Summit; the kind of sponsors, corporate organisations, high-profiled speakers, volunteers and attendees that the Summit has been able to attract has been a memorable and proud feeling.\nWhat sort of lasting impact do you want to have on the industry and the world? What keeps you going?\nWaking up every day, knowing that a lot of people would have a smile on their faces because I have chosen to impact lives and make the world a better place through relevant tech solutions and platforms is the best feeling for me. The fact that I can read through reports and data and see the number of people using Menopays as a Buy Now Pay Later (BNPL) payment option to ease their lifestyle is a big motivation for me. \nWhat’s some advice you’d give to others hoping to enter the tech world or hoping to start up a company?\nVenturing into Tech or building a Startup takes a whole lot of concerted effort and determination. Getting the right set of partner(s) would however make the journey easier for you. Just have partners or cofounders with similar vision and complementing skills.\n—\nYou can keep up with Adewale’s work by following him here. Stay tuned for more Hacks Decoded Q&A’s!\nThe post Hacks Decoded: Adewale Adetona appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-01-31T17:44:10.000Z",
      "date_modified": "2022-01-31T17:44:10.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47555",
      "url": "https://hacks.mozilla.org/2022/01/contributing-to-mdn-meet-the-contributors/",
      "title": "Contributing to MDN: Meet the Contributors",
      "summary": "If you’ve ever built anything with web technologies, you’re probably familiar with MDN Web Docs. With about 13,000 pages documenting how to use programming languages such as HTML, CSS and JavaScript, the site has about 8,000 people using it at any given moment. MDN relies on contributors to help maintain its ever-expanding and up to date documentation. We reached out to 4 long-time community contributors to talk about how and why they started contributing, why they kept going, and ask what advice they have for new contributors.\nThe post Contributing to MDN: Meet the Contributors appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">If you’ve ever built anything with web technologies, you’re probably familiar with MDN Web Docs. With about 13,000 pages documenting how to use programming languages such as HTML, CSS and JavaScript, the site has about 8,000 people using it at any given moment.</span></p>\n<p><span style=\"font-weight: 400;\">MDN relies on contributors to help maintain its ever-expanding and up to date documentation. Supported by companies such as Open Web Docs, Google, w3c, Microsoft, Samsung and Igalia (to name a few), contributions also come from community members. These contributions take many different forms, from fixing issues to contributing code to helping newcomers and localizing content.</span></p>\n<p><span style=\"font-weight: 400;\">We reached out to 4 long-time community contributors to talk about how and why they started contributing, why they kept going, and ask what advice they have for new contributors.</span></p>\n<h2><b>Meet the contributors</b></h2>\n<p><span style=\"font-weight: 400;\">MDN contributors come from all over the world, have different backgrounds, and contribute in different ways. </span></p>\n<p><span style=\"font-weight: 400;\">Irvin and Julien&#8217;s main area of contribution is localizations. They are part of a diverse team of volunteers that ensure that MDN is translated in seven different languages (Discover </span><a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute/Localize\"><span style=\"font-weight: 400;\">here how translations of MDN content happens</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">Since the end of 2020, the translation of MDN articles happen on the new GitHub based platform.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47557 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-500x334.jpg\" alt=\"Irvin\" width=\"500\" height=\"334\" srcset=\"https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-500x334.jpg 500w, https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-250x167.jpg 250w, https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-768x513.jpg 768w, https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c.jpg 799w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p style=\"text-align: center;\"><b><i>Irvin, @irvinfly, volunteer from Mozilla Taiwan Community</i></b></p>\n<p><i><span style=\"font-weight: 400;\">I had been a front-end engineer for more than a decade. I had been a leisure contributor on MDN for a long time. I check MDN all the time when writing websites, but only made some simple contributions, like fixing typos.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">In early 2020, the MDN team asked us if zh (Chinese) locale would like to join the early stage of the localization system on </span></i><a href=\"https://github.com/mdn/yari)\"><i><span style=\"font-weight: 400;\">Yari</span></i></a><i><span style=\"font-weight: 400;\">, the new Github-based platform. We accepted the invitation and formed the </span></i><a href=\"https://github.com/mdn/translated-content/blob/main/PEERS_GUIDELINES.md#review-teams\"><i><span style=\"font-weight: 400;\">zh-review-tea</span></i></a><i><span style=\"font-weight: 400;\">m. Since then, I have begun to contribute to MDN every week.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">My primary work is collaboration with other zh reviewers to check and review the open </span></i><a href=\"https://github.com/mdn/translated-content/pulls?q=is%3Apr+label%3Al10n-zh+)\"><i><span style=\"font-weight: 400;\">pull requests</span></i></a><i><span style=\"font-weight: 400;\"> on both Traditional Chinese and Simplified Chinese locales. Our goal is to ensure that all the changes to the zh docs are well done, both regarding the file format and translations. </span></i></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47569 size-full\" src=\"https://hacks.mozilla.org/files/2022/01/ensemble.png\" alt=\"Julien\" width=\"354\" height=\"286\" srcset=\"https://hacks.mozilla.org/files/2022/01/ensemble.png 354w, https://hacks.mozilla.org/files/2022/01/ensemble-250x202.png 250w\" sizes=\"(max-width: 354px) 100vw, 354px\" /></p>\n<p style=\"text-align: center;\"><b><i>Sphinx  (Julien) (he / him), @</i></b><a href=\"https://twitter.com/Sphinx_Twitt\"><b><i>Sphinx_Twitt</i></b></a><b><i> </i></b></p>\n<p><i><span style=\"font-weight: 400;\">Most of my contributions revolve around localizing MDN content in French (translating new articles and also maintaining existing pages). Since MDN moved to GitHub, contributing also encompasses reviewing other&#8217;s contributions. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">I started to contribute when, having time as a student, I joined a collaborative translation project led by Framasoft. After a few discussions, I joined a mailing list and IRC. One of the first contribution proposals I saw was about improving the translation of the MDN Glossary in French to help newcomers. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">I started helping and was welcomed by the team and community at that time. One thing led to another, and I started helping to translate other areas of MDN in French.</span></i></p>\n<p><span style=\"font-weight: 400;\">Tanner and Kenrick are also longtime volunteers. Their main areas of activity are contributing code, solving issues in MDN repositories, as well as reviewing and assisting the submissions of other contributors.</span></p>\n<p><span style=\"font-weight: 400;\">In MDN, all users can </span><a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute\"><span style=\"font-weight: 400;\">add issues to the issue tracker, as well as contributing fixes, and reviewing other people fixes</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47561 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/tanner-casual-med-500x666.jpg\" alt=\"Tanner\" width=\"500\" height=\"666\" srcset=\"https://hacks.mozilla.org/files/2022/01/tanner-casual-med-500x666.jpg 500w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-250x333.jpg 250w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-768x1022.jpg 768w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-1154x1536.jpg 1154w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-1539x2048.jpg 1539w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-scaled.jpg 1923w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p style=\"text-align: center;\"><b><i>Tanner Dolby, @tannerdolby </i></b></p>\n<p><i><span style=\"font-weight: 400;\"> I contribute to MDN by being active in the issue tracker of MDN repositories. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">I tend to look through the issues and search for one I understand, then I read the conversation in the issue thread for context. If I have any questions or notice that the conversation wasn’t resolved, I comment in the thread to get clarification before moving forward. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">From there, I test my proposed changes locally and then submit a pull request to fix the issue on GitHub. The changes I submit are then reviewed by project maintainers. After the review, I implement recommended changes. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Outside of this, I contribute to MDN by spotting bugs and creating new issues, fixing existing issues, making feature requests for things I’d like to see on the site, assisting in the completion of a feature request, participating in code review and interacting with other contributors on existing issues.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">I started contributing to MDN by creating an issue in the mdn/yari repository. I was referencing documentation and wanted to clarify a bit of information that could be a typo. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">The MDN Web Docs team was welcoming of me resolving the issue, so I opened and reviewed/merged a PR I submitted, which fixed things. The Yari project maintainers explained things in detail, helping me to understand that the content for MDN Web Docs lived in mdn/content and not directly in mdn/yari source. The issue I originally opened was transferred to mdn/content and the corresponding fix was merged. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">My first OSS experience with MDN was really fun. It helped me to branch out and explore other issues/pull requests in MDN repositories to better understand how MDN Web Docs worked, so I could contribute again in the future.</span></i></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47565 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040-500x500.jpeg\" alt=\"Kenrick\" width=\"500\" height=\"500\" srcset=\"https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040-500x500.jpeg 500w, https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040-250x250.jpeg 250w, https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040.jpeg 512w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p style=\"text-align: center;\"><b><i>Kenrick, @kenrick95</i></b></p>\n<p><i><span style=\"font-weight: 400;\">I’ve edited content and contributed codes to MDN repositories: browser-compat-data, interactive-examples, and yari.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">My first contribution to content was a long time ago, when we could directly edit on MDN. I can no longer recall what it was, probably fixing a typo. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">My first code contribution was to the “interactive-examples” repo. I noticed that the editor had some bugs, and I found the GitHub issue. After I read the codes, it seemed to me that the bug could be easily fixed, so I went ahead and sent a pull request</span></i></p>\n<h2><b>Why contribute?</b></h2>\n<p><span style=\"font-weight: 400;\">Contributions are essential to the MDN project. When talking about why they deem contribution to MDN a critical task, contributors underlined different facets, stressing its importance as an open, reliable and easily accessible resource to programmers, web developers and learners. </span></p>\n<p><span style=\"font-weight: 400;\">Contributions to MDN documentation and infrastructure help insure the constant improvement of this resource. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">Contributions to MDN are important because it helps to provide a reliable and accessible </span></i><i><span style=\"font-weight: 400;\">source of information on the Web for developers. MDN Web Docs being open source allows for bugs to quickly be spotted by contributors and for feature requests to be readily prototyped. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Building in the open creates an environment that allows for contributors from all over the world to help make MDN a better resource for everyone and that is incredible. </span></i><span style=\"font-weight: 400;\">(Tanner)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">Contributions to the platform and tools that powers MDN are important to enhance users experience (</span></i><span style=\"font-weight: 400;\">Kenrick)</span></p></blockquote>\n<p><span style=\"font-weight: 400;\">Small and big contributions are all significant and have a real impact. A common misconception about contributing to MDN is that you can only contribute code, but that is not the case! </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">MDN is the primary place for people to check any references on web-dev tech. As small as fixing one typo, any contribution to MDN can always help thousands of programmers and learners. (Irvin)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">Contribution to localization allows learners and developers to access this resource in languages other than English, making it more accessible. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">Especially for those who are struggling with reading English docs, localization can enable them to access the latest and solid knowledge (Irvin)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">Contributing to localization help beginners on the Web finding quality documentation and explanations so that they can build sites, apps and so on without having to know English. MDN is a technical reference, but also a fantastic learning ground to educate newcomers. From basic concepts to complex techniques, language should not be a barrier to build something on the Web. (Julien)</span></i></p></blockquote>\n<h2></h2>\n<h2><b>Contributing is a rewarding experience</b></h2>\n<p><span style=\"font-weight: 400;\">We asked contributors why they find contributing to MDN a rewarding experience. </span><span style=\"font-weight: 400;\">They told us that contribution is a way to help others, but also to learn new things. They spoke about the relationship that volunteers build with other people while contributing, and the possibility to learn from and help others. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">The part of contributing that I enjoy most is providing a fix for something that positively </span></i><i><span style=\"font-weight: 400;\">impacts the experience for users browsing MDN Web Docs. This could be an update to </span></i><i><span style=\"font-weight: 400;\">documentation to help provide developers with accurate docs, or helping to land a new feature on the site that will provide users new or improved functionality. Before I started contributing to MDN, I referenced MDN Web Docs very often and really appreciated the hard work that was put into the site. To this day, I’m motivated to continue help making MDN Web Docs the best resource it can be through open source contributions. (</span></i><span style=\"font-weight: 400;\">Tanner)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">I enjoy finding different points of view on how to achieve the same things. This is natural, since the people I interact comes from different part of the world and we all are influenced by our local cultures (Kenrick)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">The part of contributing I most enjoy is definitely the part when I&#8217;m learning and discovering from what I&#8217;m translating (&#8230;). </span></i><i><span style=\"font-weight: 400;\">My best memory to contribute to MDN is that </span></i><i><span style=\"font-weight: 400;\">I had the great privilege of spending an evening watching a sunset of lava and sea with people related to MDN for whom I have the deepest esteem. (Julien)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">The journey of contribution itself is important. The support of MDN maintainers and the exchange of ideas is essential. Contribution does not happen in a silo but is a collaborative effort between volunteers and the MDN team.</span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">My best memory of contributing to MDN would have to be the journey of creating the </span></i><i><span style=\"font-weight: 400;\">copy-to-clipboard functionality for code snippets on MDN Web Docs. I remember prototyping the feature in mdn/yari locally and then beginning to see it come to life really quickly, which was wonderful to see. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">The code review process for this feature was such a joy and incredibly motivating. Each step of the feature was tested thoroughly and every win was celebrated. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Each morning, I would wake up and eagerly check my email and see if any “Re: [mdn/yari]” labelled emails were there because it meant I could get back to collaborating with the MDN Web Docs team. This contribution really opened my eyes to how incredibly fun and rewarding open source software can be. (Tanner)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">My best memory of contributing to MDN was working on </span></i><a href=\"https://github.com/mdn/yari/pull/172\"><i><span style=\"font-weight: 400;\">https://github.com/mdn/yari/pull/172</span></i></a><i><span style=\"font-weight: 400;\">. The change in itself wasn’t big, but the solution changed several times after lengthy discussion. I’m amazed on how open the maintainers are in accepting different point of views for achieving the end goal (Kenrick</span></i><i><span style=\"font-weight: 400;\">)</span></i></p></blockquote>\n<h2><b>Contributions to be proud of</b></h2>\n<p><span style=\"font-weight: 400;\">All contributions are important, but some hold a special place with each volunteer.</span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">The contribution that I’m most proud of is adding copy-to-clipboard functionality to all code snippets for documentation pages on MDN Web Docs. I use this utility very often while browsing pages on MDN Web Docs and seeing a feature I helped build live on the site for other people to use is an amazing feeling. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">This contribution was something I wanted to see on the site and after discussing the feature with the Yari team, I began prototyping and participating in code review until the feature was merged into the live site. This utility was one of the first “large” feature requests that I contributed to mdn/yari and is something I’m very proud of.</span></i><span style=\"font-weight: 400;\"> (Tanner)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">The contribution I am most proud of is having the HTML, CSS, and JavaScript section complete and up-to-date in French in 2017 after being told this would be impossible :) . More recently, helping rebuilding tools for localizers on the new MDN platform with a tracking dashboard</span></i> <span style=\"font-weight: 400;\">(Julien)</span></p></blockquote>\n<p><span style=\"font-weight: 400;\">Kenrick was most proud of adding a feature that marks the page you are looking at in the sidebar. This change makes a significant difference for visual learners. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">It was a simple change, but I felt that this UX improvement is important because it serves as a guide to the reader to check what are the documents related to the one they are reading. </span></i></p></blockquote>\n<h2></h2>\n<h2><b>Getting started </b></h2>\n<p><span style=\"font-weight: 400;\">There are many ways to contribute to MDN! Our seasoned contributors suggest starting with reporting issues and trying to fix them, follow the issue trackers and getting familiarized with GitHub. Don’t be afraid to ask questions, and to make mistakes, there are people that will help you and review your work.</span></p>\n<blockquote><p><span style=\"font-weight: 400;\"> G</span><i><span style=\"font-weight: 400;\">o at your own pace, don&#8217;t hesitate to ask questions. If you can, try to hack things to fix the issues you encounter on a project. If you are eager to learn things about the Web, check MDN as a way to contribute to open source</span></i><span style=\"font-weight: 400;\"> (Julien)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">Suppose you become aware of a bug in any MDN doc (such as a typo), you are welcome to fix them directly by clicking the &#8220;Edit on Github&#8221; button. The review team will ensure it&#8217;s good, so you don&#8217;t need to worry about making any mistakes. (Irvin)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">From taking the first steps, contributors can then progress to more difficult issues and contributions. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">Don’t be afraid of reading code. Pick up any issue from GitHub, and you can easily start contributing code! (Kenrick)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">My advice for new contributors or those getting started with open source is to get familiarized with the project that they wish to contribute in and then begin staying up-to-date with the issue tracker. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Start being active in the project by looking through issues and reading through the comments, this is a sure-fire way to learn about the project. If there is something that you aren’t ready to contribute but want to have a conversation about, drop a comment in the issue thread or create a discussion in the repository for a great way to inspire conversation about a topic. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Lastly, understanding a version control software like Git is recommended for those that are considering starting to contribute to open source software. Be open to help in any way you can when first getting started in open source, I started small with documentation fixes on MDN Web Docs and then gradually worked my way into more complex contributions as I became more familiar with the project. (Tanner)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">If you want to start contributing, please check out these resources:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute\"><span style=\"font-weight: 400;\">Contributing to MDN</span></a></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://community.mozilla.org/en/activities/contribute-to-mdn-web-docs/\"><span style=\"font-weight: 400;\">MDN activity</span></a></li>\n</ul>\n<p><span style=\"font-weight: 400;\">If you have any questions, join the</span><a href=\"https://chat.mozilla.org/#/room/#mdn:mozilla.org\"> <span style=\"font-weight: 400;\">matrix chat room</span></a><span style=\"font-weight: 400;\"> for MDN.</span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/01/contributing-to-mdn-meet-the-contributors/\">Contributing to MDN: Meet the Contributors</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "If you’ve ever built anything with web technologies, you’re probably familiar with MDN Web Docs. With about 13,000 pages documenting how to use programming languages such as HTML, CSS and JavaScript, the site has about 8,000 people using it at any given moment.\nMDN relies on contributors to help maintain its ever-expanding and up to date documentation. Supported by companies such as Open Web Docs, Google, w3c, Microsoft, Samsung and Igalia (to name a few), contributions also come from community members. These contributions take many different forms, from fixing issues to contributing code to helping newcomers and localizing content.\nWe reached out to 4 long-time community contributors to talk about how and why they started contributing, why they kept going, and ask what advice they have for new contributors.\nMeet the contributors\nMDN contributors come from all over the world, have different backgrounds, and contribute in different ways. \nIrvin and Julien’s main area of contribution is localizations. They are part of a diverse team of volunteers that ensure that MDN is translated in seven different languages (Discover here how translations of MDN content happens. \nSince the end of 2020, the translation of MDN articles happen on the new GitHub based platform.\n\nIrvin, @irvinfly, volunteer from Mozilla Taiwan Community\nI had been a front-end engineer for more than a decade. I had been a leisure contributor on MDN for a long time. I check MDN all the time when writing websites, but only made some simple contributions, like fixing typos.\nIn early 2020, the MDN team asked us if zh (Chinese) locale would like to join the early stage of the localization system on Yari, the new Github-based platform. We accepted the invitation and formed the zh-review-team. Since then, I have begun to contribute to MDN every week.\nMy primary work is collaboration with other zh reviewers to check and review the open pull requests on both Traditional Chinese and Simplified Chinese locales. Our goal is to ensure that all the changes to the zh docs are well done, both regarding the file format and translations. \n\nSphinx  (Julien) (he / him), @Sphinx_Twitt \nMost of my contributions revolve around localizing MDN content in French (translating new articles and also maintaining existing pages). Since MDN moved to GitHub, contributing also encompasses reviewing other’s contributions. \nI started to contribute when, having time as a student, I joined a collaborative translation project led by Framasoft. After a few discussions, I joined a mailing list and IRC. One of the first contribution proposals I saw was about improving the translation of the MDN Glossary in French to help newcomers. \nI started helping and was welcomed by the team and community at that time. One thing led to another, and I started helping to translate other areas of MDN in French.\nTanner and Kenrick are also longtime volunteers. Their main areas of activity are contributing code, solving issues in MDN repositories, as well as reviewing and assisting the submissions of other contributors.\nIn MDN, all users can add issues to the issue tracker, as well as contributing fixes, and reviewing other people fixes. \n\nTanner Dolby, @tannerdolby \n I contribute to MDN by being active in the issue tracker of MDN repositories. \nI tend to look through the issues and search for one I understand, then I read the conversation in the issue thread for context. If I have any questions or notice that the conversation wasn’t resolved, I comment in the thread to get clarification before moving forward. \nFrom there, I test my proposed changes locally and then submit a pull request to fix the issue on GitHub. The changes I submit are then reviewed by project maintainers. After the review, I implement recommended changes. \nOutside of this, I contribute to MDN by spotting bugs and creating new issues, fixing existing issues, making feature requests for things I’d like to see on the site, assisting in the completion of a feature request, participating in code review and interacting with other contributors on existing issues.\nI started contributing to MDN by creating an issue in the mdn/yari repository. I was referencing documentation and wanted to clarify a bit of information that could be a typo. \nThe MDN Web Docs team was welcoming of me resolving the issue, so I opened and reviewed/merged a PR I submitted, which fixed things. The Yari project maintainers explained things in detail, helping me to understand that the content for MDN Web Docs lived in mdn/content and not directly in mdn/yari source. The issue I originally opened was transferred to mdn/content and the corresponding fix was merged. \nMy first OSS experience with MDN was really fun. It helped me to branch out and explore other issues/pull requests in MDN repositories to better understand how MDN Web Docs worked, so I could contribute again in the future.\n\nKenrick, @kenrick95\nI’ve edited content and contributed codes to MDN repositories: browser-compat-data, interactive-examples, and yari.\nMy first contribution to content was a long time ago, when we could directly edit on MDN. I can no longer recall what it was, probably fixing a typo. \nMy first code contribution was to the “interactive-examples” repo. I noticed that the editor had some bugs, and I found the GitHub issue. After I read the codes, it seemed to me that the bug could be easily fixed, so I went ahead and sent a pull request\nWhy contribute?\nContributions are essential to the MDN project. When talking about why they deem contribution to MDN a critical task, contributors underlined different facets, stressing its importance as an open, reliable and easily accessible resource to programmers, web developers and learners. \nContributions to MDN documentation and infrastructure help insure the constant improvement of this resource. \nContributions to MDN are important because it helps to provide a reliable and accessible source of information on the Web for developers. MDN Web Docs being open source allows for bugs to quickly be spotted by contributors and for feature requests to be readily prototyped. \nBuilding in the open creates an environment that allows for contributors from all over the world to help make MDN a better resource for everyone and that is incredible. (Tanner)\nContributions to the platform and tools that powers MDN are important to enhance users experience (Kenrick)\nSmall and big contributions are all significant and have a real impact. A common misconception about contributing to MDN is that you can only contribute code, but that is not the case! \nMDN is the primary place for people to check any references on web-dev tech. As small as fixing one typo, any contribution to MDN can always help thousands of programmers and learners. (Irvin)\nContribution to localization allows learners and developers to access this resource in languages other than English, making it more accessible. \nEspecially for those who are struggling with reading English docs, localization can enable them to access the latest and solid knowledge (Irvin)\nContributing to localization help beginners on the Web finding quality documentation and explanations so that they can build sites, apps and so on without having to know English. MDN is a technical reference, but also a fantastic learning ground to educate newcomers. From basic concepts to complex techniques, language should not be a barrier to build something on the Web. (Julien)\n\nContributing is a rewarding experience\nWe asked contributors why they find contributing to MDN a rewarding experience. They told us that contribution is a way to help others, but also to learn new things. They spoke about the relationship that volunteers build with other people while contributing, and the possibility to learn from and help others. \nThe part of contributing that I enjoy most is providing a fix for something that positively impacts the experience for users browsing MDN Web Docs. This could be an update to documentation to help provide developers with accurate docs, or helping to land a new feature on the site that will provide users new or improved functionality. Before I started contributing to MDN, I referenced MDN Web Docs very often and really appreciated the hard work that was put into the site. To this day, I’m motivated to continue help making MDN Web Docs the best resource it can be through open source contributions. (Tanner)\nI enjoy finding different points of view on how to achieve the same things. This is natural, since the people I interact comes from different part of the world and we all are influenced by our local cultures (Kenrick)\nThe part of contributing I most enjoy is definitely the part when I’m learning and discovering from what I’m translating (…). My best memory to contribute to MDN is that I had the great privilege of spending an evening watching a sunset of lava and sea with people related to MDN for whom I have the deepest esteem. (Julien)\nThe journey of contribution itself is important. The support of MDN maintainers and the exchange of ideas is essential. Contribution does not happen in a silo but is a collaborative effort between volunteers and the MDN team.\nMy best memory of contributing to MDN would have to be the journey of creating the copy-to-clipboard functionality for code snippets on MDN Web Docs. I remember prototyping the feature in mdn/yari locally and then beginning to see it come to life really quickly, which was wonderful to see. \nThe code review process for this feature was such a joy and incredibly motivating. Each step of the feature was tested thoroughly and every win was celebrated. \nEach morning, I would wake up and eagerly check my email and see if any “Re: [mdn/yari]” labelled emails were there because it meant I could get back to collaborating with the MDN Web Docs team. This contribution really opened my eyes to how incredibly fun and rewarding open source software can be. (Tanner)\nMy best memory of contributing to MDN was working on https://github.com/mdn/yari/pull/172. The change in itself wasn’t big, but the solution changed several times after lengthy discussion. I’m amazed on how open the maintainers are in accepting different point of views for achieving the end goal (Kenrick)\nContributions to be proud of\nAll contributions are important, but some hold a special place with each volunteer.\nThe contribution that I’m most proud of is adding copy-to-clipboard functionality to all code snippets for documentation pages on MDN Web Docs. I use this utility very often while browsing pages on MDN Web Docs and seeing a feature I helped build live on the site for other people to use is an amazing feeling. \nThis contribution was something I wanted to see on the site and after discussing the feature with the Yari team, I began prototyping and participating in code review until the feature was merged into the live site. This utility was one of the first “large” feature requests that I contributed to mdn/yari and is something I’m very proud of. (Tanner)\nThe contribution I am most proud of is having the HTML, CSS, and JavaScript section complete and up-to-date in French in 2017 after being told this would be impossible :) . More recently, helping rebuilding tools for localizers on the new MDN platform with a tracking dashboard (Julien)\nKenrick was most proud of adding a feature that marks the page you are looking at in the sidebar. This change makes a significant difference for visual learners. \nIt was a simple change, but I felt that this UX improvement is important because it serves as a guide to the reader to check what are the documents related to the one they are reading. \n\nGetting started \nThere are many ways to contribute to MDN! Our seasoned contributors suggest starting with reporting issues and trying to fix them, follow the issue trackers and getting familiarized with GitHub. Don’t be afraid to ask questions, and to make mistakes, there are people that will help you and review your work.\n Go at your own pace, don’t hesitate to ask questions. If you can, try to hack things to fix the issues you encounter on a project. If you are eager to learn things about the Web, check MDN as a way to contribute to open source (Julien)\nSuppose you become aware of a bug in any MDN doc (such as a typo), you are welcome to fix them directly by clicking the “Edit on Github” button. The review team will ensure it’s good, so you don’t need to worry about making any mistakes. (Irvin)\nFrom taking the first steps, contributors can then progress to more difficult issues and contributions. \nDon’t be afraid of reading code. Pick up any issue from GitHub, and you can easily start contributing code! (Kenrick)\nMy advice for new contributors or those getting started with open source is to get familiarized with the project that they wish to contribute in and then begin staying up-to-date with the issue tracker. \nStart being active in the project by looking through issues and reading through the comments, this is a sure-fire way to learn about the project. If there is something that you aren’t ready to contribute but want to have a conversation about, drop a comment in the issue thread or create a discussion in the repository for a great way to inspire conversation about a topic. \nLastly, understanding a version control software like Git is recommended for those that are considering starting to contribute to open source software. Be open to help in any way you can when first getting started in open source, I started small with documentation fixes on MDN Web Docs and then gradually worked my way into more complex contributions as I became more familiar with the project. (Tanner)\nIf you want to start contributing, please check out these resources:\n\nContributing to MDN\nMDN activity\n\nIf you have any questions, join the matrix chat room for MDN.\nThe post Contributing to MDN: Meet the Contributors appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-01-18T16:07:59.000Z",
      "date_modified": "2022-01-18T16:07:59.000Z"
    },
    {
      "id": "https://hacks.mozilla.org/?p=47506",
      "url": "https://hacks.mozilla.org/2021/12/hacks-decoded-sara-soueidan-award-winning-ui-design-engineer-and-author/",
      "title": "Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author",
      "summary": "Sara Soueidan is an independent Web UI and design engineer, author, speaker, and trainer from Lebanon. Currently, she’s working on a new course, \"Practical Accessibility,\" meant to teach devs and designers ways to make their products accessible. We chatted with Sara about front-end web development, the importance of design and her appreciation of birds.\nThe post Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><i>Welcome to our Hacks: Decoded Interview series! </i></p>\n<p><i>Once a month, </i><a href=\"https://foundation.mozilla.org/\"><i>Mozilla Foundation</i></a><i>’s </i><a href=\"https://www.xavierharding.com/\"><i>Xavier Harding</i></a><i> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </i><a href=\"https://hacks.mozilla.org/\"><i>Hacks</i></a><i> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.</i></p>\n<p>&nbsp;</p>\n<p><strong>Meet Sara Soueidan!</strong></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47511 size-large\" src=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-Mozilla-Hacks-Decoded-interview-QA-500x466.jpg\" alt=\"\" width=\"500\" height=\"466\" /></p>\n<div class=\"block docBlock Block_container__39pfw Block_readOnly__aAcdv rootBlock rect-definition-node\" data-block-id=\"5c056e39-ea06-4755-a174-53f031ca6d3e\" data-full-hit=\"false\" data-text=\"false\" data-frame=\"false\" data-positioned=\"false\" data-editing=\"false\" data-indent=\"0\">\n<div class=\"ContextMenuHandler_wrapper__2D0Q8\">\n<div class=\"BlockType_block__Szmra\">\n<p>Sara Soueidan is an independent Web UI and design engineer, author, speaker, and trainer from Lebanon.</p>\n<p>Sara has worked with companies around the world, building web user interfaces, designing systems, and creating digital products that focus on responsive design and accessibility. She’s worked with companies like SuperFriendly, Herman Miller, Khan Academy, and has given workshops within companies like Netflix and Telus that focus on building scalable, resilient design.</p>\n<p>When Sara isn’t offering keynote speeches at conferences (she’s done so a dozen times) she’s writing books like “Codrops CSS Reference” and “Smashing Book 5.” Currently, she’s working on a new course, &#8220;Practical Accessibility,&#8221; meant to teach devs and designers ways to make their products accessible.</p>\n<p>In 2015, Sara was voted Developer of the Year in the net awards, and shortlisted for the Outstanding Contribution of the Year award. She also won an O’Reilly Web Platform Award for “exceptional leadership, creativity, and collaboration in the development of JavaScript, HTML, CSS, and the supporting Web ecosystem.”</p>\n<p>We chatted with Sara about front-end web development, the importance of design and her appreciation of birds.</p>\n</div>\n</div>\n</div>\n<p><b>Where did you get your start? How did you end up working in tech?</b></p>\n<p>I took my first HTML class in eighth grade. I instantly fell in love with it. It just made sense; and it felt like a second language that I found myself speaking fluently. But back then, it was just another class. As I continued my journey through high school, I considered architecture as a major. I never thought I&#8217;d major in anything even remotely related to tech. I always thought I’d choose a career that had nothing to do with computers. In fact, before choosing computer science as a major, I was preparing to study architecture in the Faculty of Arts.</p>\n<p>Then, life happened. A series of events had me choosing CS as a major. And even after I did, I didn’t really think I’d make a career in tech. I spent 18 months after college pondering what I could do for a living with a CS major in Lebanon, but I didn’t find my calling anywhere.</p>\n<p>My love for the web was rekindled when someone suggested I learn web development and try making websites for a living. The appeal of that was two-fold: I&#8217;d get to work remotely from the comfort of my home, and I&#8217;d get to be my own boss, and have full control over my time and the work that I choose.</p>\n<p>After a few weeks of learning modern HTML and CSS, and dipping my feet into JavaScript, I was hooked. I found myself spending more time learning and practicing. <a href=\"https://codepen.io/\">Codepen</a> was new back then, and it was a great place to do quick code exercises and experiments. I also created a one-page Web site — because if you&#8217;re going to work freelance and accept work requests, you gotta have that!</p>\n<p>As I continued learning and experimenting for a few months, I started sharing what I learned as articles on a blog that I started in 2013. A few weeks after I published my first article, I got my first client request to create the UI for a Facebook-like Web application. And over the course of the first year, I got one small client project after another.</p>\n<p>My career really kicked off though in 2014. By then, I was writing more, getting more client work, and writing a CSS reference for Codrops. Conference speaking invitations started flooding in after I delivered my first talk at CSSConf in 2014. I gave my first workshop in LA in 2015. And I have been doing what I do now since.</p>\n<p>I am grateful things didn’t work out the way I wanted them to after high school.</p>\n<p><b>You’ve been programming for a while now, you’ve co-authored a book about the craft, you’ve created guides like the Codrops CSS Reference — what drives you?</b></p>\n<p>A thirst for knowledge and a craving for variety in work. I don’t think I’d be inspired enough to do <i>any</i> kind of work that doesn’t satisfy both. I also need to feel like I’m doing something meaningful, like helping others. And I&#8217;ve been able to fulfill all of these needs in this field. That&#8217;s why I fell in love with it.</p>\n<p>Being independent, I have full control over my time and the type of work I spend it on. While building websites is my main work and source of income, I do spend a large portion of my time switching between writing, editing, giving talks, running workshops (in-house and at events), making courses (this one’s new!) and working on personal projects.</p>\n<p>Everything I do complements one another: I learn, to write, to teach; I code, to write, to speak; I code, to learn, to share. It’s a wonderful circle of creative work! This variety helps keep the spark alive, and helps me rekindle my passion for the web even after frequent burnouts.</p>\n<p>I like that I must keep learning for a living! And that I get to also teach (another passion and — dare I say — talent of mine) as part of my job. I teach through writing, through speaking, through running workshops, and even through direct collaboration with designers and engineers on client projects.</p>\n<p>I always think that even if I end up changing careers, I would still make some time to fiddle with code and make web projects on the side of whatever else I&#8217;d be doing for a living.</p>\n<p><b>When it comes to front-end versus back-end versus full stack, you seem to be #TeamFrontEnd. What is it about front-end web and app development that calls your name (more so than back-end)?</b></p>\n<p>I love working at the intersection of design and engineering! This is the area of the front end typically referred to as “the front of the front end.” It is the perfect sweet spot between design and engineering. It stimulates both parts of my brain, and keeps me inspired and challenged — a combination my brain needs to stay creative.</p>\n<p>I find building interfaces fascinating. I love the fact that the interfaces I build are the bridge between people and the information they access online.</p>\n<p>That comes with great responsibility, of course. Building for people is not easy because people are so diverse and so are the ways they access the Web. And it&#8217;s the interfaces they use that determine whether they can!</p>\n<p>It is <i>our</i> responsibility as front-end developers and designers to ensure that what we create is inclusive of as many people as possible.</p>\n<p>While this may sound intimidating and maybe even scary, I find it inspiring. It is what gives more meaning to what I do, and what pushes me to keep learning and trying to do better. The front of the front end is where I found my sweet spot: a place where I can be challenged and inspired.</p>\n<p>A couple of years ago, I was feeling this so much that <a href=\"https://twitter.com/DNABeast/status/1150326370007842816\">I shared that moment on Twitter</a>. Among the many replies I got, this quote by Douglas Adams stuck with me:</p>\n<p>“<i>We all like to congregate, at boundary conditions. Where land meets water. Where earth meets air. Where body meets mind. Where space meets time.”</i></p>\n<p><b>What do you love about coding? What’s your least favorite part?</b></p>\n<p>My favorite part is the satisfaction of seeing my code “come to life”. The idea that I can write a few lines of code that computers understand, and that so many people can consume and interact with it using various technologies — present and in the future.</p>\n<p>I also appreciate the short feedback loop in modern code environments: you write code or make changes to existing one, and see the results immediately in the browser. It is almost magical. And who doesn’t like a little bit of magic in their lives?</p>\n<p>My least favorite part, however, is that it requires so little movement. There is life in movement! One of my favorite yoga teachers once said: &#8220;Once you stop moving, you start dying.&#8221; And I felt that. Spending so much time in front of a screen is very taxing.</p>\n<p>Regular exercise is crucial for my ability to continue doing what I do. But I still sometimes feel like I need more movement <i>during</i> my work sessions. So I got a standing desk a couple of years ago.</p>\n<p>Switching between standing and sitting gives my body short &#8220;breathers&#8221; throughout the day and allows for better blood flow. A balanced lifestyle is crucial to maintaining a good health when you spend as much time in front of a screen. Try to move, drink lots of water, and go outside more.</p>\n<p><b>You’re based out of Lebanon. What’s something many folks may not realize about the tech scene there?</b></p>\n<p>I know this isn&#8217;t the answer you&#8217;re expecting, but I think what many people don&#8217;t realize about the tech scene here is how challenging it is! In Lebanon, we live in a country that has a massive, serious, and ongoing power crisis.</p>\n<p>This crisis, as you can imagine, affects almost every facet of our lives, including the digital. You need power to do work. And you need an internet connection to do work. We’ve always had problems with internet speed. And with the fuel shortage, full power outages, and reception problems, having a <i>reliable</i> connection is less likely than before.</p>\n<p>But there are some incredibly talented designers and developers still making it work through this all. Living in Lebanon brings daily challenges, but being challenged in life is inevitable.</p>\n<p>I try to look on the bright side of everything. Working on a slow connection has its upsides, you know. You learn to appreciate performance more and strive to make better, faster Web sites. You appreciate tech like Service Worker more, and learn to use it to <a href=\"https://www.sarasoueidan.com/blog/going-offline/\">make content available offline</a>. If anything, living here has made many of us more resilient to change, and more creative with our solutions in the face of crisis.</p>\n<p><b>How do you find (tech) supporting communities in Lebanon, if not where does your community live?</b></p>\n<p>I don’t. But that’s mainly because I live in an area with no active tech community. And I live far from where any tech meetups happen. I also don’t know any front-end focused developers in Lebanon. I’m sure they exist; it’s just that, being the introvert that I am, I don’t happen to know any. So my community is mainly online — on Twitter, and in a couple of not-very-busy Slack channels.</p>\n<p><b>Ok, random question. We’ve gotta know about the birds. You’ve raised at least a dozen. What’s the story there?</b></p>\n<p>It all started back in 2009, I think. A close friend had, for whatever reason, decided that I might enjoy taking care of baby birds. So, he got me a baby <a href=\"https://www.flickr.com/photos/raed_shorrosh/30669614184/\">White-spectacled Bulbul</a> (my favorite bird species currently), with all the bird food I needed to start. He taught me what I needed to know to take care of it. And he told me that, when it grows up, it won’t need to live in a cage because <i>I</i> would be its home. I had no idea back then how much I’d fall in love with that bird.</p>\n<p>I&#8217;ve raised 10+ birds since. Not a single one of them was kept in a cage. I would raise them and train them so that, when they grew up, they would fly out in the morning — making friends, living like they were meant to, and return home before the end of the day.</p>\n<p>They would drink from my tea cup, share my sandwiches, eat out of my plate (mainly rice) and spend most of the day either sitting on my shoulder and head, or napping on my arm. Friends have always told me that I was like a Disney princess with my birds. I&#8217;m not sure about that, but it did sometimes <i>feel</i> that way. x)</p>\n<p>Here’s a photo of my last two baby birds from a couple of years ago. I took them out in a car drive to &#8220;explore the outside world&#8221; for the first time.</p>\n<p>They just sat there chilling on my arm, as they watched the world (cars, mainly) pass by.</p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47515 size-large\" src=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397-500x754.jpg\" alt=\"\" width=\"500\" height=\"754\" srcset=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397-500x754.jpg 500w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397-250x377.jpg 250w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397.jpg 679w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p>Years after my friend got me my first bird, I asked him why he did, and whether he knew about the connection that was going to happen. His answer was short. He said: &#8220;<i>You have the heart of a bird. I knew you&#8217;d love creatures that are like you.</i>&#8221;</p>\n<p><b>Another random question: In an interview, you mentioned mainly working in the morning (6am-10am), and slowing down after lunch. You’re like me! How important is a flexible work day to your workflow? (And how do we convince more people that 9-to-5 work isn’t realistic for everyone? How do we normalize hard work in the morning, meetings and calls in the afternoon?) </b></p>\n<p>I can’t imagine myself working on a 9-to-5 schedule! That’s actually one of the few reasons I never took a full-time job. As I mentioned earlier, flexibility was a key factor in choosing a freelance career.</p>\n<p>I am an early bird. On <a href=\"https://www.sarasoueidan.com/desk/typical-day/\">a typical day</a>, I wake up no later than 5:30 in the morning. So my day starts very early. My brain’s information retention powers are at their highest early in the morning. So I get my best work done during that time. With my brain firing on all cylinders, I make quite a bit of headway with the day’s tasks. What makes this time even more productive is the fact that there are no expectations, nor interruptions: no emails, no client communication, not even any IRL interruptions.</p>\n<p>The earlier you start in the day, and knowing that most people are only really productive for about 4.5 hours a day, I believe it makes a lot of sense to slow down after lunch.</p>\n<p>I realize this is easier said than done, though. Being freelance gives me this flexibility but I realize others may not have that working full time. But with more companies going fully or partially remote now, I think more people will hopefully get to choose when they work during the day.</p>\n<p><b>You’re working on an accessibility course, can you talk a bit about why you decided to develop this course and the importance of creating more accessible web interfaces?</b></p>\n<p>Before COVID-19 hit, I traveled to run workshops at conferences and in-house at companies. The lockdown had us all, well, locked down, so that was put on temporary hold.</p>\n<p>Over the years, I collected some amazing feedback to my accessibility workshop from former attendees. I knew I had useful content that many others would find helpful.</p>\n<p>As many events went online, running the workshop online was the sensible plan B. But the fact that my Internet was unreliable made that a little risky — I wouldn’t want my internet connection to fail in the middle of an online workshop! So that plan was put on hold too.</p>\n<p>On the other hand, working with designers and engineers on client projects made me realize that there was a big accessibility knowledge gap in most companies I’ve worked with. I love to teach teams I work with about accessibility at every chance I get, but there’s only so much you can share in Zoom meetings and Slack channels. In-house workshops were not always an option, and online training was not feasible at the time.</p>\n<p>And last but not least, I noticed that there is quite a bit of misinformation and bad advice circulating the web community around accessibility. You can cover a good amount of information in articles, but I already had a good bunch of content I could start with from the accessibility workshop that I can use as a foundation for a more comprehensive series of teaching materials — sort of like a mini curriculum.</p>\n<p>By developing this course I am scratching my own itch. All the reasons mentioned above had me wishing I had created a course that I could share around, especially with client teams, and then with members of the community. So with the time I have in between client projects and speaking, I started working on it!</p>\n<p>The course is called <a href=\"https://practical-accessibility.today\">Practical Accessibility</a>, and is <b>still under active development</b>, coming in 2022. The content of the course is going to be much more comprehensive than that of the workshop, and it will cover much more ground, and hopefully be a great foundation for anyone wanting to learn how to create more accessible websites.</p>\n<p><b>Of everything you worked on, what’s your favorite?</b></p>\n<p>Out of all the projects I’ve worked on, probably the one that stood out for me is a project for <a href=\"http://hermanmiller.com\">Herman Miller</a> that I collaborated with <a href=\"https://superfriendlydesign.systems\">SuperFriendly</a> on. The project was under NDA, and was discontinued a few weeks after COVID-19 hit and the world realized it was going to change moving forward; so I, unfortunately, don’t have any details to share about the project itself.</p>\n<p>But what made this opportunity so special is that this was the first and only project that I was involved in from the very start— from early kick-off meetings and ideation, through research and user testing, UX and UI design, and development. I learned so much working with an amazing group of SuperFriends. The trip to the Herman Miller showroom in Atlanta, where we ran a workshop with the team at Herman Miller, was the last trip most of us took before the big lockdown.</p>\n<p>Herman Miller is a furniture company. And what many people don’t know about me is how much I <i>love</i> interior design. I even took an interior design course last year! So, on this project, I got to (1) work with an amazing team (who I get to call my friends now <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f495.png\" alt=\"💕\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" />), (2) on a creative project, (3) for a company specializing in making modern furniture, (4) in the field of interior design! How could I not love that?!</p>\n<p>The cherry on top of the cake was that I got a generous discount which I used to upgrade my office chair and desk to an ergonomic Herman Miller chair and standing desk. So even my body and health were thankful for this opportunity!</p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47519 size-large\" src=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-500x667.jpg\" alt=\"Sara Soueidan desk - Sara' favorite project was working with SuperFriendly and Herman Miller. &quot;The cherry on top of the cake was that I got a generous discount which I used to upgrade my office chair and desk to an ergonomic Herman Miller chair and standing desk. So even my body and health were thankful for this opportunity!&quot;\" width=\"500\" height=\"667\" srcset=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-500x667.jpg 500w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-250x333.jpg 250w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-768x1024.jpg 768w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-1152x1536.jpg 1152w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-1536x2048.jpg 1536w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-scaled.jpg 1920w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p><strong>Final question:</strong> <b>W</b><b>hat would you tell folks learning a programming language or aspiring to be a front end developer, or any sort of developer. What advice would you give them?</b></p>\n<p>Learn the fundamentals — HTML, accessibility, CSS, and just enough vanilla JavaScript to get started. Build upon those skills with tools and frameworks as your work needs.</p>\n<p>Don‘t get intimidated or overwhelmed by what everybody else is doing. Learn what you need when you need it. And practice as much as you can. Practice won’t make you perfect because there is no Perfect in this field, but it will make you better!</p>\n<p>This probably should have been the first piece of advice though: <b>Put the user first.</b> User experience should trump developer convenience. Once you let that guide your work, you’re already halfway through to being a better developer than many others.</p>\n<p>Oh and last but certainly not least: Create a personal website! Own your content. And share your work with the world!</p>\n<p>&#8212;</p>\n<p><em>You can keep up with Sara&#8217;s work by following her blog on her personal site <a href=\"https://www.sarasoueidan.com/blog/\">here.</a> Stay tuned for more Hacks Decoded Q&amp;A&#8217;s!</em></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/12/hacks-decoded-sara-soueidan-award-winning-ui-design-engineer-and-author/\">Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Welcome to our Hacks: Decoded Interview series! \nOnce a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.\n \nMeet Sara Soueidan!\n\n\n\n\nSara Soueidan is an independent Web UI and design engineer, author, speaker, and trainer from Lebanon.\nSara has worked with companies around the world, building web user interfaces, designing systems, and creating digital products that focus on responsive design and accessibility. She’s worked with companies like SuperFriendly, Herman Miller, Khan Academy, and has given workshops within companies like Netflix and Telus that focus on building scalable, resilient design.\nWhen Sara isn’t offering keynote speeches at conferences (she’s done so a dozen times) she’s writing books like “Codrops CSS Reference” and “Smashing Book 5.” Currently, she’s working on a new course, “Practical Accessibility,” meant to teach devs and designers ways to make their products accessible.\nIn 2015, Sara was voted Developer of the Year in the net awards, and shortlisted for the Outstanding Contribution of the Year award. She also won an O’Reilly Web Platform Award for “exceptional leadership, creativity, and collaboration in the development of JavaScript, HTML, CSS, and the supporting Web ecosystem.”\nWe chatted with Sara about front-end web development, the importance of design and her appreciation of birds.\n\n\n\nWhere did you get your start? How did you end up working in tech?\nI took my first HTML class in eighth grade. I instantly fell in love with it. It just made sense; and it felt like a second language that I found myself speaking fluently. But back then, it was just another class. As I continued my journey through high school, I considered architecture as a major. I never thought I’d major in anything even remotely related to tech. I always thought I’d choose a career that had nothing to do with computers. In fact, before choosing computer science as a major, I was preparing to study architecture in the Faculty of Arts.\nThen, life happened. A series of events had me choosing CS as a major. And even after I did, I didn’t really think I’d make a career in tech. I spent 18 months after college pondering what I could do for a living with a CS major in Lebanon, but I didn’t find my calling anywhere.\nMy love for the web was rekindled when someone suggested I learn web development and try making websites for a living. The appeal of that was two-fold: I’d get to work remotely from the comfort of my home, and I’d get to be my own boss, and have full control over my time and the work that I choose.\nAfter a few weeks of learning modern HTML and CSS, and dipping my feet into JavaScript, I was hooked. I found myself spending more time learning and practicing. Codepen was new back then, and it was a great place to do quick code exercises and experiments. I also created a one-page Web site — because if you’re going to work freelance and accept work requests, you gotta have that!\nAs I continued learning and experimenting for a few months, I started sharing what I learned as articles on a blog that I started in 2013. A few weeks after I published my first article, I got my first client request to create the UI for a Facebook-like Web application. And over the course of the first year, I got one small client project after another.\nMy career really kicked off though in 2014. By then, I was writing more, getting more client work, and writing a CSS reference for Codrops. Conference speaking invitations started flooding in after I delivered my first talk at CSSConf in 2014. I gave my first workshop in LA in 2015. And I have been doing what I do now since.\nI am grateful things didn’t work out the way I wanted them to after high school.\nYou’ve been programming for a while now, you’ve co-authored a book about the craft, you’ve created guides like the Codrops CSS Reference — what drives you?\nA thirst for knowledge and a craving for variety in work. I don’t think I’d be inspired enough to do any kind of work that doesn’t satisfy both. I also need to feel like I’m doing something meaningful, like helping others. And I’ve been able to fulfill all of these needs in this field. That’s why I fell in love with it.\nBeing independent, I have full control over my time and the type of work I spend it on. While building websites is my main work and source of income, I do spend a large portion of my time switching between writing, editing, giving talks, running workshops (in-house and at events), making courses (this one’s new!) and working on personal projects.\nEverything I do complements one another: I learn, to write, to teach; I code, to write, to speak; I code, to learn, to share. It’s a wonderful circle of creative work! This variety helps keep the spark alive, and helps me rekindle my passion for the web even after frequent burnouts.\nI like that I must keep learning for a living! And that I get to also teach (another passion and — dare I say — talent of mine) as part of my job. I teach through writing, through speaking, through running workshops, and even through direct collaboration with designers and engineers on client projects.\nI always think that even if I end up changing careers, I would still make some time to fiddle with code and make web projects on the side of whatever else I’d be doing for a living.\nWhen it comes to front-end versus back-end versus full stack, you seem to be #TeamFrontEnd. What is it about front-end web and app development that calls your name (more so than back-end)?\nI love working at the intersection of design and engineering! This is the area of the front end typically referred to as “the front of the front end.” It is the perfect sweet spot between design and engineering. It stimulates both parts of my brain, and keeps me inspired and challenged — a combination my brain needs to stay creative.\nI find building interfaces fascinating. I love the fact that the interfaces I build are the bridge between people and the information they access online.\nThat comes with great responsibility, of course. Building for people is not easy because people are so diverse and so are the ways they access the Web. And it’s the interfaces they use that determine whether they can!\nIt is our responsibility as front-end developers and designers to ensure that what we create is inclusive of as many people as possible.\nWhile this may sound intimidating and maybe even scary, I find it inspiring. It is what gives more meaning to what I do, and what pushes me to keep learning and trying to do better. The front of the front end is where I found my sweet spot: a place where I can be challenged and inspired.\nA couple of years ago, I was feeling this so much that I shared that moment on Twitter. Among the many replies I got, this quote by Douglas Adams stuck with me:\n“We all like to congregate, at boundary conditions. Where land meets water. Where earth meets air. Where body meets mind. Where space meets time.”\nWhat do you love about coding? What’s your least favorite part?\nMy favorite part is the satisfaction of seeing my code “come to life”. The idea that I can write a few lines of code that computers understand, and that so many people can consume and interact with it using various technologies — present and in the future.\nI also appreciate the short feedback loop in modern code environments: you write code or make changes to existing one, and see the results immediately in the browser. It is almost magical. And who doesn’t like a little bit of magic in their lives?\nMy least favorite part, however, is that it requires so little movement. There is life in movement! One of my favorite yoga teachers once said: “Once you stop moving, you start dying.” And I felt that. Spending so much time in front of a screen is very taxing.\nRegular exercise is crucial for my ability to continue doing what I do. But I still sometimes feel like I need more movement during my work sessions. So I got a standing desk a couple of years ago.\nSwitching between standing and sitting gives my body short “breathers” throughout the day and allows for better blood flow. A balanced lifestyle is crucial to maintaining a good health when you spend as much time in front of a screen. Try to move, drink lots of water, and go outside more.\nYou’re based out of Lebanon. What’s something many folks may not realize about the tech scene there?\nI know this isn’t the answer you’re expecting, but I think what many people don’t realize about the tech scene here is how challenging it is! In Lebanon, we live in a country that has a massive, serious, and ongoing power crisis.\nThis crisis, as you can imagine, affects almost every facet of our lives, including the digital. You need power to do work. And you need an internet connection to do work. We’ve always had problems with internet speed. And with the fuel shortage, full power outages, and reception problems, having a reliable connection is less likely than before.\nBut there are some incredibly talented designers and developers still making it work through this all. Living in Lebanon brings daily challenges, but being challenged in life is inevitable.\nI try to look on the bright side of everything. Working on a slow connection has its upsides, you know. You learn to appreciate performance more and strive to make better, faster Web sites. You appreciate tech like Service Worker more, and learn to use it to make content available offline. If anything, living here has made many of us more resilient to change, and more creative with our solutions in the face of crisis.\nHow do you find (tech) supporting communities in Lebanon, if not where does your community live?\nI don’t. But that’s mainly because I live in an area with no active tech community. And I live far from where any tech meetups happen. I also don’t know any front-end focused developers in Lebanon. I’m sure they exist; it’s just that, being the introvert that I am, I don’t happen to know any. So my community is mainly online — on Twitter, and in a couple of not-very-busy Slack channels.\nOk, random question. We’ve gotta know about the birds. You’ve raised at least a dozen. What’s the story there?\nIt all started back in 2009, I think. A close friend had, for whatever reason, decided that I might enjoy taking care of baby birds. So, he got me a baby White-spectacled Bulbul (my favorite bird species currently), with all the bird food I needed to start. He taught me what I needed to know to take care of it. And he told me that, when it grows up, it won’t need to live in a cage because I would be its home. I had no idea back then how much I’d fall in love with that bird.\nI’ve raised 10+ birds since. Not a single one of them was kept in a cage. I would raise them and train them so that, when they grew up, they would fly out in the morning — making friends, living like they were meant to, and return home before the end of the day.\nThey would drink from my tea cup, share my sandwiches, eat out of my plate (mainly rice) and spend most of the day either sitting on my shoulder and head, or napping on my arm. Friends have always told me that I was like a Disney princess with my birds. I’m not sure about that, but it did sometimes feel that way. x)\nHere’s a photo of my last two baby birds from a couple of years ago. I took them out in a car drive to “explore the outside world” for the first time.\nThey just sat there chilling on my arm, as they watched the world (cars, mainly) pass by.\n\nYears after my friend got me my first bird, I asked him why he did, and whether he knew about the connection that was going to happen. His answer was short. He said: “You have the heart of a bird. I knew you’d love creatures that are like you.”\nAnother random question: In an interview, you mentioned mainly working in the morning (6am-10am), and slowing down after lunch. You’re like me! How important is a flexible work day to your workflow? (And how do we convince more people that 9-to-5 work isn’t realistic for everyone? How do we normalize hard work in the morning, meetings and calls in the afternoon?) \nI can’t imagine myself working on a 9-to-5 schedule! That’s actually one of the few reasons I never took a full-time job. As I mentioned earlier, flexibility was a key factor in choosing a freelance career.\nI am an early bird. On a typical day, I wake up no later than 5:30 in the morning. So my day starts very early. My brain’s information retention powers are at their highest early in the morning. So I get my best work done during that time. With my brain firing on all cylinders, I make quite a bit of headway with the day’s tasks. What makes this time even more productive is the fact that there are no expectations, nor interruptions: no emails, no client communication, not even any IRL interruptions.\nThe earlier you start in the day, and knowing that most people are only really productive for about 4.5 hours a day, I believe it makes a lot of sense to slow down after lunch.\nI realize this is easier said than done, though. Being freelance gives me this flexibility but I realize others may not have that working full time. But with more companies going fully or partially remote now, I think more people will hopefully get to choose when they work during the day.\nYou’re working on an accessibility course, can you talk a bit about why you decided to develop this course and the importance of creating more accessible web interfaces?\nBefore COVID-19 hit, I traveled to run workshops at conferences and in-house at companies. The lockdown had us all, well, locked down, so that was put on temporary hold.\nOver the years, I collected some amazing feedback to my accessibility workshop from former attendees. I knew I had useful content that many others would find helpful.\nAs many events went online, running the workshop online was the sensible plan B. But the fact that my Internet was unreliable made that a little risky — I wouldn’t want my internet connection to fail in the middle of an online workshop! So that plan was put on hold too.\nOn the other hand, working with designers and engineers on client projects made me realize that there was a big accessibility knowledge gap in most companies I’ve worked with. I love to teach teams I work with about accessibility at every chance I get, but there’s only so much you can share in Zoom meetings and Slack channels. In-house workshops were not always an option, and online training was not feasible at the time.\nAnd last but not least, I noticed that there is quite a bit of misinformation and bad advice circulating the web community around accessibility. You can cover a good amount of information in articles, but I already had a good bunch of content I could start with from the accessibility workshop that I can use as a foundation for a more comprehensive series of teaching materials — sort of like a mini curriculum.\nBy developing this course I am scratching my own itch. All the reasons mentioned above had me wishing I had created a course that I could share around, especially with client teams, and then with members of the community. So with the time I have in between client projects and speaking, I started working on it!\nThe course is called Practical Accessibility, and is still under active development, coming in 2022. The content of the course is going to be much more comprehensive than that of the workshop, and it will cover much more ground, and hopefully be a great foundation for anyone wanting to learn how to create more accessible websites.\nOf everything you worked on, what’s your favorite?\nOut of all the projects I’ve worked on, probably the one that stood out for me is a project for Herman Miller that I collaborated with SuperFriendly on. The project was under NDA, and was discontinued a few weeks after COVID-19 hit and the world realized it was going to change moving forward; so I, unfortunately, don’t have any details to share about the project itself.\nBut what made this opportunity so special is that this was the first and only project that I was involved in from the very start— from early kick-off meetings and ideation, through research and user testing, UX and UI design, and development. I learned so much working with an amazing group of SuperFriends. The trip to the Herman Miller showroom in Atlanta, where we ran a workshop with the team at Herman Miller, was the last trip most of us took before the big lockdown.\nHerman Miller is a furniture company. And what many people don’t know about me is how much I love interior design. I even took an interior design course last year! So, on this project, I got to (1) work with an amazing team (who I get to call my friends now ), (2) on a creative project, (3) for a company specializing in making modern furniture, (4) in the field of interior design! How could I not love that?!\nThe cherry on top of the cake was that I got a generous discount which I used to upgrade my office chair and desk to an ergonomic Herman Miller chair and standing desk. So even my body and health were thankful for this opportunity!\n\nFinal question: What would you tell folks learning a programming language or aspiring to be a front end developer, or any sort of developer. What advice would you give them?\nLearn the fundamentals — HTML, accessibility, CSS, and just enough vanilla JavaScript to get started. Build upon those skills with tools and frameworks as your work needs.\nDon‘t get intimidated or overwhelmed by what everybody else is doing. Learn what you need when you need it. And practice as much as you can. Practice won’t make you perfect because there is no Perfect in this field, but it will make you better!\nThis probably should have been the first piece of advice though: Put the user first. User experience should trump developer convenience. Once you let that guide your work, you’re already halfway through to being a better developer than many others.\nOh and last but certainly not least: Create a personal website! Own your content. And share your work with the world!\n—\nYou can keep up with Sara’s work by following her blog on her personal site here. Stay tuned for more Hacks Decoded Q&A’s!\nThe post Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-12-30T15:13:51.000Z",
      "date_modified": "2021-12-30T15:13:51.000Z"
    }
  ],
  "description": "hacks.mozilla.org",
  "home_page_url": "https://hacks.mozilla.org"
}