{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "Mozilla Hacks – the Web developer blog",
  "feed_url": "https://hacks.mozilla.org/feed/",
  "items": [
    {
      "id": "https://hacks.mozilla.org/?p=47842",
      "url": "https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/",
      "title": "Everything Is Broken: Shipping rust-minidump at Mozilla – Part 1",
      "summary": "For the last year, we've been working on the development of rust-minidump, a pure-Rust replacement for the minidump-processing half of google-breakpad. The first in this two-part series explains what minidumps are, and how we made rust-minidump.\nThe post Everything Is Broken: Shipping rust-minidump at Mozilla – Part 1 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<h1><strong>Everything Is Broken: Shipping rust-minidump at Mozilla</strong></h1>\n<p>For the last year I&#8217;ve been leading the development of <a href=\"https://github.com/luser/rust-minidump/\">rust-minidump</a>, a pure-Rust replacement for the minidump-processing half of <a href=\"https://chromium.googlesource.com/breakpad/breakpad/\">google-breakpad</a>.</p>\n<p>Well actually in some sense I <em>finished</em> that work, because Mozilla already <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">deployed it</a> as <a href=\"https://crash-stats.mozilla.org/\">the crash processing backend for Firefox</a> 6 months ago, it runs in half the time, and seems to be more reliable. (And you know, <em>isn&#8217;t</em> a terrifying ball of C++ that parses and evaluates arbitrary input from the internet. We did our best to isolate Breakpad, but still… <em>yikes</em>.)</p>\n<p>This is a pretty fantastic result, but there&#8217;s always more work to do because <em>Minidumps are an inky abyss that grows deeper the further you delve…</em> wait no I&#8217;m getting ahead of myself. First the light, then the abyss. Yes. Light first.</p>\n<p>What I <em>can</em> say is that we have a very solid implementation of the core functionality of minidump parsing+analysis for the biggest platforms (x86, x64, ARM, ARM64; Windows, MacOS, Linux, Android). But if you want to read minidumps generated on a <em>PlayStation 3</em> or process a <em>Full Memory</em> dump, you won&#8217;t be served quite as well.</p>\n<p>We&#8217;ve put a lot of effort into documenting and testing this thing, so I&#8217;m pretty confident in it!</p>\n<p><strong>Unfortunately! Confidence! Is! Worth! Nothing!</strong></p>\n<p>Which is why this is the story of how we did our best to make this nightmare as robust as we could and still got 360 dunked on from space by the sudden and <em>incredible</em> fuzzing efforts of <a href=\"https://github.com/5225225\">@5225225</a>.</p>\n<p>This article is broken into two parts:</p>\n<ol>\n<li>what minidumps are, and how we made rust-minidump</li>\n<li>how we got absolutely owned by simple fuzzing</li>\n</ol>\n<p>You are reading part 1, wherein we build up our hubris.</p>\n<h1><strong>Background: What&#8217;s A Minidump, and Why Write rust-minidump?</strong></h1>\n<p>Your program crashes. You want to know why your program crashed, but it happened on a user&#8217;s machine on the other side of the world. A full coredump (all memory allocated by the program) is enormous &#8212; we can&#8217;t have users sending us 4GB files! Ok let&#8217;s just collect up the most important regions of memory like the stacks and where the program crashed. Oh and I guess if we&#8217;re taking the time, let&#8217;s stuff some metadata about the system and process in there too.</p>\n<p>Congratulations you have invented <a href=\"https://docs.microsoft.com/en-us/windows/win32/debug/minidump-files\">Minidumps</a>. Now you can turn a 100-thread coredump that would otherwise be 4GB into a nice little 2MB file that you can send over the internet and do postmortem analysis on.</p>\n<p>Or more specifically, Microsoft did. So long ago that their docs don&#8217;t even discuss platform support. MiniDumpWriteDump&#8217;s supported versions are simply &#8220;Windows&#8221;. Microsoft Research has presumably developed a time machine to guarantee this.</p>\n<p>Then Google came along (circa 2006-2007) and said &#8220;wouldn&#8217;t it be nice if we could make minidumps on <em>any</em> platform&#8221;? Thankfully Microsoft had actually built the format pretty extensibly, so it wasn&#8217;t too bad to extend the format for Linux, MacOS, BSD, Solaris, and so on. Those extensions became <a href=\"https://chromium.googlesource.com/breakpad/breakpad/\">google-breakpad</a> (or just Breakpad) which included a ton of different tools for generating, parsing, and analyzing their extended minidump format (and native Microsoft ones).</p>\n<p>Mozilla helped out with this a lot because apparently, our crash reporting infrastructure (&#8220;Talkback&#8221;) was <em>miserable</em> circa 2007, and this seemed like a nice improvement. Needless to say, we&#8217;re pretty invested in breakpad&#8217;s minidumps at this point.</p>\n<p>Fast forward to the present day and in a hilarious twist of fate, products like VSCode mean that Microsoft now supports applications that run on Linux and MacOS so it runs breakpad in production and has to handle non-Microsoft minidumps somewhere in its crash reporting infra, so someone else&#8217;s extension of their own format is somehow their problem now!</p>\n<p>Meanwhile, Google has kind-of moved on to <a href=\"https://chromium.googlesource.com/crashpad/crashpad\">Crashpad</a>. I say kind-of because there&#8217;s still a lot of Breakpad in there, but they&#8217;re more interested in building out tooling on top of it than improving Breakpad itself. Having made a few changes to Breakpad: <strong>honestly fair</strong>, I don&#8217;t want to work on it either. Still, this was a bit of a problem for us, because it meant the project became increasingly under-staffed.</p>\n<p>By the time I started working on crash reporting, Mozilla had basically given up on upstreaming fixes/improvements to Breakpad, and was just using its own patched fork. But even <em>without</em> the need for upstreaming patches, every change to Breakpad filled us with dread: many proposed improvements to our crash reporting infrastructure stalled out at &#8220;time to implement this in Breakpad&#8221;.</p>\n<p>Why is working on Breakpad so miserable, you ask?</p>\n<p>Parsing and analyzing minidumps is basically an exercise in writing a fractal parser of platform-specific formats nested in formats nested in formats. For many operating systems. For many hardware architectures. And all the inputs you&#8217;re parsing and analyzing are terrible and buggy so you <em>have</em> to write a really permissive parser and crawl forward however you can.</p>\n<p>Some specific MSVC toolchain that was part of Windows XP had a bug in its debuginfo format? <strong>Too bad, symbolicate that stack frame anyway!</strong></p>\n<p>The program crashed because it horribly corrupted its own stack? <strong>Too bad, produce a backtrace anyway!</strong></p>\n<p>The minidump writer itself completely freaked out and wrote a bunch of garbage to one stream? <strong>Too bad, produce whatever output you can anyway!</strong></p>\n<p>Hey, you know who has a lot of experience dealing with really complicated permissive parsers written in C++? Mozilla! That&#8217;s like <em>the core functionality</em> of a web browser.</p>\n<p>Do you know Mozilla&#8217;s secret solution to writing really complicated permissive parsers in C++?</p>\n<p><strong>We stopped doing it.</strong></p>\n<p>We developed Rust and ported our nastiest parsers to it.</p>\n<p>We&#8217;ve done it a lot, and <a href=\"https://hacks.mozilla.org/2017/08/inside-a-super-fast-css-engine-quantum-css-aka-stylo/\">when we do</a> we&#8217;re always like <a href=\"https://www.joshmatthews.net/rbr17/\">&#8220;wow this is so much more reliable and easy to maintain and it&#8217;s even faster now&#8221;</a>. Rust is a really good language for writing parsers. C++ really isn&#8217;t.</p>\n<p>So we Rewrote It In Rust (or as the kids call it, &#8220;Oxidized It&#8221;). Breakpad is big, so we haven&#8217;t actually covered all of its features. We&#8217;ve specifically written and deployed:</p>\n<ul>\n<li><a href=\"https://github.com/mozilla/dump_syms\">dump_syms</a> which processes native build artifacts into symbol files.</li>\n<li><a href=\"https://github.com/luser/rust-minidump/\">rust-minidump</a> which is a collection of crates that parse and analyze minidumps. Or more specifically, we deployed <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">minidump-stackwalk</a>, which is the high-level cli interface to all of rust-minidump.</li>\n</ul>\n<p>Notably missing from this picture is <em>minidump writing</em>, or what google-breakpad calls a <em>client</em> (because it runs on the client&#8217;s machine). We <em>are </em>working <a href=\"https://github.com/rust-minidump/minidump-writer\">on a rust-based minidump writer</a>, but it&#8217;s not something we can recommend using quite yet (although it has sped up a lot thanks to help from <a href=\"https://embark.dev/\">Embark Studios</a>).</p>\n<p>This is arguably the messiest and hardest work because it has a horrible job: use a bunch of native system APIs to gather up a bunch of OS-specific and Hardware-specific information about the crash AND do it for a program that just crashed, on a machine that <em>caused </em>the program to crash.</p>\n<p>We have a long road ahead but every time we get to the other side of one of these projects it&#8217;s <em>wonderful</em>.</p>\n<p>&nbsp;</p>\n<h1><strong>Background: Stackwalking and Calling Conventions</strong></h1>\n<p>One of rust-minidump&#8217;s (<a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">minidump-stackwalk&#8217;s</a>) most important jobs is to take the state for a thread (general purpose registers and stack memory) and create a backtrace for that thread (unwind/stackwalk). This is a surprisingly complicated and messy job, made only more complicated by the fact that <em>we are trying to analyze the memory of a process that got messed up enough to crash</em>.</p>\n<p>This means our stackwalkers are inherently working with dubious data, and all of our stackwalking techniques are based on heuristics that can go wrong and we can very easily find ourselves in situations where the stackwalk goes backwards or sideways or infinite and we just have to try to deal with it!</p>\n<p>It&#8217;s also pretty common to see a stackwalker start <em>hallucinating</em>, which is my term for &#8220;the stackwalker found something that looked plausible enough and went on a wacky adventure through the stack and made up a whole pile of useless garbage frames&#8221;. Hallucination is most common near the bottom of the stack where it&#8217;s also least offensive. This is because each frame you walk is another chance for something to go wrong, but also increasingly uninteresting because you&#8217;re rarely interested in confirming that a thread started in The Same Function All Threads Start In.</p>\n<p>All of these problems would basically go away if everyone agreed to properly preserve their cpu&#8217;s <a href=\"https://gankra.github.io/blah/compact-unwinding/#frame-pointer-unwinding-standard-prologues\">PERFECTLY GOOD DEDICATED FRAME POINTER REGISTER</a>. Just kidding, turning on frame pointers doesn&#8217;t really work either because Microsoft <a href=\"https://github.com/rust-lang/rust/issues/82333\">invented chaos frame pointers</a> that can&#8217;t be used for unwinding! I assume this happened because they accidentally stepped on the wrong butterfly while they were traveling back in time to invent minidumps. (I&#8217;m sure it was a decision that made more sense 20 years ago, but it has not aged well.)</p>\n<p>If you would like to learn more about the different techniques for unwinding, <a href=\"https://gankra.github.io/blah/compact-unwinding/#background-unwinding-and-debug-info\">I wrote about them over here</a> in my <a href=\"https://gankra.github.io/blah/compact-unwinding\">article on Apple&#8217;s Compact Unwind Info</a>. I&#8217;ve also attempted to <a href=\"https://docs.rs/breakpad-symbols/latest/breakpad_symbols/walker/index.html\">document breakpad&#8217;s STACK WIN and STACK CFI unwind info formats here</a>, which are more similar to the  DWARF and PE32 unwind tables (which are basically tiny programming languages).</p>\n<p>If you would like to learn more about ABIs in general, <a href=\"https://gankra.github.io/blah/rust-layouts-and-abis/#calling-conventions\">I wrote an entire article about them here</a>. The end of that article also includes an <a href=\"https://gankra.github.io/blah/rust-layouts-and-abis/#calling-conventions\">introduction to how calling conventions work</a>. Understanding calling conventions is key to implementing unwinders.</p>\n<p>&nbsp;</p>\n<p><strong>How Hard Did You Really Test Things?</strong></p>\n<p>Hopefully you now have a bit of a glimpse into why analyzing minidumps is an enormous headache. And of course you know how the story ends: that fuzzer kicks our butts! But of course to really savor our defeat, you have to see how hard we tried to do a good job! It&#8217;s time to build up our hubris and pat ourselves on the back.</p>\n<p>So how much work <em>actually</em> went into making rust-minidump robust before the fuzzer went to work on it?</p>\n<p>Quite a bit!</p>\n<p>I&#8217;ll never argue all the work we did was <em>perfect</em> but we definitely did some good work here, both for synthetic inputs and real world ones. Probably the biggest &#8220;flaw&#8221; in our methodology was the fact that we were only focused on getting Firefox&#8217;s usecase to work. Firefox runs on a lot of platforms and sees a lot of messed up stuff, but it&#8217;s still a fairly coherent product that only uses so many features of minidumps.</p>\n<p>This is one of the nice benefits of our recent work with <a href=\"https://sentry.io/\">Sentry</a>, which is basically a Crash Reporting As A Service company. They are <em>way</em> more liable to stress test all kinds of weird corners of the format that Firefox doesn&#8217;t, and they have definitely found (and fixed!) some places where something is wrong or missing! (And they recently deployed it into production too! <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f389.png\" alt=\"🎉\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" />)</p>\n<p>But hey don&#8217;t take my word for it, check out all the different testing we did:</p>\n<h2><strong>Synthetic Minidumps for Unit Tests</strong></h2>\n<p>rust-minidump includes a <a href=\"https://github.com/rust-minidump/rust-minidump/tree/553735e2624dcc6af82167f502cf92ae9a9fdc87/minidump-synth\">synthetic minidump generator</a> which lets you come up with a high-level description of the contents of a minidump, and then produces an actual minidump binary that we can feed it into the full parser:</p>\n<p>// Let&#8217;s make a synth minidump with this particular Crashpad Info&#8230;</p>\n<pre>let module = ModuleCrashpadInfo::new(42, Endian::Little)\r\n    .add_list_annotation(\"annotation\")\r\n    .add_simple_annotation(\"simple\", \"module\")\r\n    .add_annotation_object(\"string\", AnnotationValue::String(\"value\".to_owned()))\r\n    .add_annotation_object(\"invalid\", AnnotationValue::Invalid)\r\n    .add_annotation_object(\"custom\", AnnotationValue::Custom(0x8001, vec![42]));\r\n\r\nlet crashpad_info = CrashpadInfo::new(Endian::Little)\r\n    .add_module(module)\r\n    .add_simple_annotation(\"simple\", \"info\");\r\n\r\nlet dump = SynthMinidump::with_endian(Endian::Little).add_crashpad_info(crashpad_info);\r\n\r\n// convert the synth minidump to binary and read it like a normal minidump\r\nlet dump = read_synth_dump(dump).unwrap();</pre>\n<p>// Now check that the minidump reports the values we expect…</p>\n<p>minidump-synth intentionally avoids sharing layout code with the actual implementation so that incorrect changes to layouts won&#8217;t &#8220;accidentally&#8221; pass tests.</p>\n<p><em>A brief aside for some history</em>: this testing framework was started by the original lead on this project, <a href=\"https://twitter.com/TedMielczarek\">Ted Mielczarek</a>. He started rust-minidump as a side project to learn Rust when 1.0 was released and just never had the time to finish it. Back then he was working at Mozilla and also a major contributor to Breakpad, which is why rust-minidump has a lot of similar design choices and terminology.</p>\n<p>This case is no exception: our minidump-synth is a shameless copy of the <a href=\"https://chromium.googlesource.com/breakpad/breakpad/+/refs/heads/main/src/processor/synth_minidump.cc\">synth-minidump utility in breakpad&#8217;s code</a>, which was originally written by our <em>other</em> coworker <a href=\"https://www.red-bean.com/~jimb/\">Jim Blandy</a>. Jim is one of the only people in the world that I will actually admit writes really good tests and docs, so I am totally happy to blatantly copy his work here.</p>\n<p>Since this was all a learning experiment, Ted was understandably less rigorous about testing than usual. This meant a lot of minidump-synth was unimplemented when I came along, which also meant lots of minidump features were completely untested. (He built an absolutely great skeleton, just hadn&#8217;t had the time to fill it all in!)</p>\n<p>We spent <em>a lot</em> of time filling in more of minidump-synth&#8217;s implementation so we could write more tests and catch more issues, but this is <em>definitely</em> the weakest part of our tests. Some stuff was implemented before I got here, so I don&#8217;t even <em>know</em> what tests are missing!</p>\n<p>This is a good argument for some code coverage checks, but it would probably come back with &#8220;wow you should write a lot more tests&#8221; and we would all look at it and go &#8220;wow we sure should&#8221; and then we would probably never get around to it, because there are <em>many</em> things we <em>should</em> do.</p>\n<p>On the other hand, Sentry has been very useful in this regard because they already <em>have</em> a mature suite of tests full of weird corner cases they&#8217;ve built up over time, so they can easily identify things that really matter, know what the fix should roughly be, and can contribute pre-existing test cases!</p>\n<h2><strong>Integration and Snapshot Tests</strong></h2>\n<p>We tried our best to shore up coverage issues in our unit tests by adding more holistic tests. There&#8217;s a few checked in Real Minidumps that we have <a href=\"https://github.com/luser/rust-minidump/blob/40c3390f5705890f932f78b7db4fc02866e012b8/minidump-processor/tests/test_processor.rs\">some integration tests for</a> to make sure we handle Real Inputs properly.</p>\n<p>We even wrote a bunch of <a href=\"https://github.com/luser/rust-minidump/blob/40c3390f5705890f932f78b7db4fc02866e012b8/minidump-stackwalk/tests/test-minidump-stackwalk.rs\">integration tests for the CLI application that snapshot its output</a> to confirm that we never <em>accidentally</em> change the results.</p>\n<p>Part of the motivation for this is to ensure we don&#8217;t break the JSON output, which we also wrote a <a href=\"https://github.com/luser/rust-minidump/blob/40c3390f5705890f932f78b7db4fc02866e012b8/minidump-processor/json-schema.md\">very detailed schema document for</a> and are trying to keep stable so people can actually rely on it while the actual implementation details are still in flux.</p>\n<p>Yes, <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">minidump-stackwalk</a> is supposed to be stable and reasonable to use in production!</p>\n<p>For our snapshot tests we use <a href=\"https://github.com/mitsuhiko/insta\">insta</a>, which I think is fantastic and more people should use. All you need to do is assert_snapshot! any output you want to keep track of and it will magically take care of the storing, loading, and diffing.</p>\n<p>Here&#8217;s one of the snapshot tests where we invoke the CLI interface and snapshot stdout:</p>\n<pre>#[test]\r\nfn test_evil_json() {\r\n    // For a while this didn't parse right\r\n    let bin = env!(\"CARGO_BIN_EXE_minidump-stackwalk\");\r\n    let output = Command::new(bin)\r\n        .arg(\"--json\")\r\n        .arg(\"--pretty\")\r\n        .arg(\"--raw-json\")\r\n        .arg(\"../testdata/evil.json\")\r\n        .arg(\"../testdata/test.dmp\")\r\n        .arg(\"../testdata/symbols/\")\r\n        .stdout(Stdio::piped())\r\n        .stderr(Stdio::piped())\r\n        .output()\r\n        .unwrap();\r\n\r\n    let stdout = String::from_utf8(output.stdout).unwrap();\r\n    let stderr = String::from_utf8(output.stderr).unwrap();\r\n\r\n    assert!(output.status.success());\r\n    insta::assert_snapshot!(\"json-pretty-evil-symbols\", stdout);\r\n    assert_eq!(stderr, \"\");\r\n}\r\n\r\n</pre>\n<h2><b>Stackwalker Unit Testing</b></h2>\n<p>The stackwalker is easily the most complicated and subtle part of the new implementation, because every platform can have <em>slight</em> quirks and you need to implement several different unwinding strategies and carefully tune everything to work well <em>in practice</em>.</p>\n<p>The scariest part of this was the call frame information (CFI) unwinders, because they are basically little virtual machines we need to parse and execute at runtime. Thankfully breakpad had long ago smoothed over this issue by defining a simplified and unified CFI format, STACK CFI (well, nearly unified, x86 Windows was still a special case as STACK WIN). So even if DWARF CFI has a ton of complex features, we mostly need to implement a <a href=\"https://en.wikipedia.org/wiki/Reverse_Polish_notation\">Reverse Polish Notation Calculator</a> except it can read registers and load memory from addresses it computes (and for STACK WIN it has access to named variables it can declare and mutate).</p>\n<p>Unfortunately, <a href=\"https://chromium.googlesource.com/breakpad/breakpad/+/master/docs/symbol_files.md\">Breakpad&#8217;s description for this format is pretty underspecified</a> so I had to basically pick some semantics I thought made sense and go with that. This made me <em>extremely</em> paranoid about the implementation. (And yes I will be more first-person for this part, because this part was genuinely where I personally spent most of my time and did a lot of stuff from scratch. All the blame belongs to me here!)</p>\n<p>The<a href=\"https://docs.rs/breakpad-symbols/latest/breakpad_symbols/walker/index.html\"> STACK WIN / STACK CFI parser+evaluator</a> is 1700 lines. 500 of those lines are a detailed documentation and discussion of the format, and 700 of those lines are an enormous pile of ~80 test cases where I tried to come up with every corner case I could think of.</p>\n<p>I even checked in two tests I <em>knew</em> were failing just to be honest that there were a couple cases to fix! One of them is a corner case involving dividing by a negative number that almost certainly just doesn&#8217;t matter. The other is a buggy input that old x86 Microsoft toolchains actually produce and parsers need to deal with. The latter was fixed before the fuzzing started.</p>\n<p>And 5225225 <em>still</em> found an integer overflow in the STACK WIN preprocessing step! (Not actually that surprising, it&#8217;s a hacky mess that tries to cover up for how messed up x86 Windows unwinding tables were.)</p>\n<p>(The code isn&#8217;t terribly interesting here, it&#8217;s just a ton of assertions that a given input string produces a given output/error.)</p>\n<p>Of course, I wasn&#8217;t satisfied with just coming up with my own semantics and testing them: I also <a href=\"https://github.com/luser/rust-minidump/blob/master/minidump-processor/src/stackwalker/x86_unittest.rs\">ported most of breakpad&#8217;s own stackwalker tests to rust-minidump</a>! This definitely found a bunch of bugs I had, but also taught me some weird quirks in Breakpad&#8217;s stackwalkers that I&#8217;m not sure I <em>actually</em> agree with. But in this case I was flying so blind that even being bug-compatible with Breakpad was some kind of relief.</p>\n<p>Those tests also included several tests for the non-CFI paths, which were similarly wobbly and quirky. I still really hate a lot of the weird platform-specific rules they have for stack scanning, but I&#8217;m forced to work on the assumption that they might be load-bearing. (I definitely had several cases where I disabled a breakpad test because it was &#8220;obviously nonsense&#8221; and then hit it in the wild while testing. I quickly learned to accept that <strong>Nonsense Happens And Cannot Be Ignored</strong>.)</p>\n<p>One major thing I <em>didn&#8217;t</em> replicate was some of the really hairy hacks for STACK WIN. Like there are several places where they introduce extra stack-scanning to try to deal with the fact that stack frames can have mysterious extra alignment that the windows unwinding tables just don&#8217;t tell you about? I guess?</p>\n<p>There&#8217;s almost certainly some exotic situations that rust-minidump does worse on because of this, but it probably also means we do better in some random other situations too. I never got the two to perfectly agree, but at some point the divergences were all in weird enough situations, and as far as I was concerned both stackwalkers were producing equally bad results in a bad situation. Absent any reason to prefer one over the other, divergence seemed acceptable to keep the implementation cleaner.</p>\n<p>Here&#8217;s a simplified version of one of the ported breakpad tests, if you&#8217;re curious (thankfully minidump-synth is based off of the same binary data mocking framework these tests use):</p>\n<pre>#[test]\r\nfn test_x86_frame_pointer() {\r\n    let mut f = TestFixture::new();\r\n    let frame0_ebp = Label::new();\r\n    let frame1_ebp = Label::new();\r\n    let mut stack = Section::new();\r\n\r\n    // Setup the stack and registers so frame pointers will work\r\n    stack.start().set_const(0x80000000);\r\n    stack = stack\r\n        .append_repeated(12, 0) // frame 0: space\r\n        .mark(&amp;frame0_ebp)      // frame 0 %ebp points here\r\n        .D32(&amp;frame1_ebp)       // frame 0: saved %ebp\r\n        .D32(0x40008679)        // frame 0: return address\r\n        .append_repeated(8, 0)  // frame 1: space\r\n        .mark(&amp;frame1_ebp)      // frame 1 %ebp points here\r\n        .D32(0)                 // frame 1: saved %ebp (stack end)\r\n        .D32(0);                // frame 1: return address (stack end)\r\n    f.raw.eip = 0x4000c7a5;\r\n    f.raw.esp = stack.start().value().unwrap() as u32;\r\n    f.raw.ebp = frame0_ebp.value().unwrap() as u32;\r\n\r\n    // Check the stackwalker's output:\r\n    let s = f.walk_stack(stack).await;\r\n    assert_eq!(s.frames.len(), 2);\r\n    {\r\n        let f0 = &amp;s.frames[0];\r\n        assert_eq!(f0.trust, FrameTrust::Context);\r\n        assert_eq!(f0.context.valid, MinidumpContextValidity::All);\r\n        assert_eq!(f0.instruction, 0x4000c7a5);\r\n    }\r\n    {\r\n        let f1 = &amp;s.frames[1];\r\n        assert_eq!(f1.trust, FrameTrust::FramePointer);\r\n        assert_eq!(f1.instruction, 0x40008678);\r\n    }\r\n}</pre>\n<h2>A Dedicated Production Diffing, Simulating, and Debugging Tool</h2>\n<p>Because minidumps are so horribly fractal and corner-casey, I spent <em>a lot</em> of time terrified of subtle issues that would become huge disasters if we ever actually tried to deploy to production. So I also spent a bunch of time building <a href=\"https://github.com/Gankra/socc-pair/\">socc-pair</a>, which takes the id of a crash report from Mozilla&#8217;s <a href=\"https://crash-stats.mozilla.org/\">crash reporting system</a> and pulls down the minidump, the old breakpad-based implementation&#8217;s output, and extra metadata.</p>\n<p>It then runs a local rust-minidump (minidump-stackwalk) implementation on the minidump and does a domain-specific diff over the two inputs. The most substantial part of this is a fuzzy diff on the stackwalks that tries to better handle situations like when one implementation adds an extra frame but the two otherwise agree. It also uses the reported techniques each implementation used to try to identify whose output is more trustworthy when they totally diverge.</p>\n<p>I also ended up adding a bunch of mocking and benchmarking functionality to it as well, as I found more and more places where I just wanted to simulate a production environment.</p>\n<p>Oh also I added <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk#debugging-stackwalking\">really detailed trace-logging for the stackwalker</a> so that I could easily post-mortem debug why it made the decisions it made.</p>\n<p>This tool found so many issues and more importantly has helped me quickly isolate their causes. I am so happy I made it. Because of it, we know we actually <em>fixed</em> several issues that happened with the old breakpad implementation, which is great!</p>\n<p>Here&#8217;s a trimmed down version of the kind of report socc-pair would produce (yeah I abused diff syntax to get error highlighting. It&#8217;s a great hack, and I love it like a child):</p>\n<pre>comparing json...\r\n\r\n: {\r\n    crash_info: {\r\n        address: 0x7fff1760aca0\r\n        crashing_thread: 8\r\n        type: EXCEPTION_BREAKPOINT\r\n    }\r\n    crashing_thread: {\r\n        frames: [\r\n            0: {\r\n                file: wrappers.cpp:1750da2d7f9db490b9d15b3ee696e89e6aa68cb7\r\n                frame: 0\r\n                function: RustMozCrash(char const*, int, char const*)\r\n                function_offset: 0x00000010\r\n-               did not match\r\n+               line: 17\r\n-               line: 20\r\n                module: xul.dll\r\n\r\n.....\r\n\r\n    unloaded_modules: [\r\n        0: {\r\n            base_addr: 0x7fff48290000\r\n-           local val was null instead of:\r\n            code_id: 68798D2F9000\r\n            end_addr: 0x7fff48299000\r\n            filename: KBDUS.DLL\r\n        }\r\n        1: {\r\n            base_addr: 0x7fff56020000\r\n            code_id: DFD6E84B14000\r\n            end_addr: 0x7fff56034000\r\n            filename: resourcepolicyclient.dll\r\n        }\r\n    ]\r\n~   ignoring field write_combine_size: \"0\"\r\n}\r\n\r\n- Total errors: 288, warnings: 39\r\n\r\nbenchmark results (ms):\r\n    2388, 1986, 2268, 1989, 2353, \r\n    average runtime: 00m:02s:196ms (2196ms)\r\n    median runtime: 00m:02s:268ms (2268ms)\r\n    min runtime: 00m:01s:986ms (1986ms)\r\n    max runtime: 00m:02s:388ms (2388ms)\r\n\r\nmax memory (rss) results (bytes):\r\n    267755520, 261152768, 272441344, 276131840, 279134208, \r\n    average max-memory: 258MB (271323136 bytes)\r\n    median max-memory: 259MB (272441344 bytes)\r\n    min max-memory: 249MB (261152768 bytes)\r\n    max max-memory: 266MB (279134208 bytes)\r\n\r\nOutput Files: \r\n    * (download) Minidump: b4f58e9f-49be-4ba5-a203-8ef160211027.dmp\r\n    * (download) Socorro Processed Crash: b4f58e9f-49be-4ba5-a203-8ef160211027.json\r\n    * (download) Raw JSON: b4f58e9f-49be-4ba5-a203-8ef160211027.raw.json\r\n    * Local minidump-stackwalk Output: b4f58e9f-49be-4ba5-a203-8ef160211027.local.json\r\n    * Local minidump-stackwalk Logs: b4f58e9f-49be-4ba5-a203-8ef160211027.log.txt</pre>\n<h2><b>Staging and Deploying to Production</b></h2>\n<p>Once we were confident enough in the implementation, a lot of the remaining testing was taken over by Will Kahn-Greene, who&#8217;s responsible for a lot of the server-side details of our crash-reporting infrastructure.</p>\n<p>Will spent a bunch of time getting a bunch of machinery setup to manage the deployment and monitoring of rust-minidump. He also did a lot of the hard work of cleaning up all our server-side configuration scripts to handle any differences between the two implementations. (Although I spent a lot of time on compatibility, we both agreed this was a good opportunity to clean up old cruft and mistakes.)</p>\n<p>Once all of this was set up, he turned it on in staging and we got our first look at how rust-minidump actually worked in ~production:</p>\n<p><strong>Terribly!</strong></p>\n<p>Our staging servers take in about 10% of the inputs that also go to our production servers, but even at that reduced scale we very quickly found several new corner cases and we were getting <em>tons</em> of crashes, which is mildly embarrassing for<em> the thing that handles other people&#8217;s crashes</em>.</p>\n<p>Will did a great job here in monitoring and reporting the issues. Thankfully they were all fairly easy for us to fix. Eventually, everything smoothed out and things seemed to be working just as reliably as the old implementation on the production server. The only places where we were completely failing to produce any output were for horribly truncated minidumps that may as well have been empty files.</p>\n<p>We originally <em>did</em> have some grand ambitions of running socc-pair on everything the staging servers processed or something to get <em>really</em> confident in the results. But by the time we got to that point, we were completely exhausted and feeling pretty confident in the new implementation.</p>\n<p>Eventually Will just said &#8220;let&#8217;s turn it on in production&#8221; and I said &#8220;AAAAAAAAAAAAAAA&#8221;.</p>\n<p>This moment was pure terror. There had always been <em>more</em> corner cases. There&#8217;s no way we could just be <em>done</em>. This will probably set all of Mozilla on fire and delete Firefox from the internet!</p>\n<p>But Will convinced me. We wrote up some docs detailing all the subtle differences and sent them to everyone we could. Then the moment of truth finally came: Will turned it on in production, and I got to really see how well it worked in production:</p>\n<p><em>*dramatic drum roll*</em></p>\n<p>It worked fine.</p>\n<p>After all that stress and anxiety, we turned it on and it was <em>fine</em>.</p>\n<p>Heck, I&#8217;ll say it: it ran <em>well</em>.</p>\n<p>It was faster, it crashed less, and we even knew it fixed some issues.</p>\n<p>I was in a bit of a stupor for the rest of that week, because I kept waiting for the other shoe to drop. I kept waiting for someone to emerge from the mist and explain that I had somehow bricked <em>Thunderbird</em> or something. But no, it just worked.</p>\n<p>So we left for the holidays, and I kept waiting for it to break, but it was <em>still fine</em>.</p>\n<p>I am honestly still shocked about this!</p>\n<p>But hey, as it turns out we really did put a <em>lot</em> of careful work into testing the implementation. At every step we found new problems but that was <em>good</em>, because once we got to the final step there were no more problems to surprise us.</p>\n<p><strong>And the fuzzer still kicked our butts afterwards.</strong></p>\n<p>But that&#8217;s part 2! Thanks for reading!</p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/\">Everything Is Broken: Shipping rust-minidump at Mozilla &#8211; Part 1</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Everything Is Broken: Shipping rust-minidump at Mozilla\nFor the last year I’ve been leading the development of rust-minidump, a pure-Rust replacement for the minidump-processing half of google-breakpad.\nWell actually in some sense I finished that work, because Mozilla already deployed it as the crash processing backend for Firefox 6 months ago, it runs in half the time, and seems to be more reliable. (And you know, isn’t a terrifying ball of C++ that parses and evaluates arbitrary input from the internet. We did our best to isolate Breakpad, but still… yikes.)\nThis is a pretty fantastic result, but there’s always more work to do because Minidumps are an inky abyss that grows deeper the further you delve… wait no I’m getting ahead of myself. First the light, then the abyss. Yes. Light first.\nWhat I can say is that we have a very solid implementation of the core functionality of minidump parsing+analysis for the biggest platforms (x86, x64, ARM, ARM64; Windows, MacOS, Linux, Android). But if you want to read minidumps generated on a PlayStation 3 or process a Full Memory dump, you won’t be served quite as well.\nWe’ve put a lot of effort into documenting and testing this thing, so I’m pretty confident in it!\nUnfortunately! Confidence! Is! Worth! Nothing!\nWhich is why this is the story of how we did our best to make this nightmare as robust as we could and still got 360 dunked on from space by the sudden and incredible fuzzing efforts of @5225225.\nThis article is broken into two parts:\n\nwhat minidumps are, and how we made rust-minidump\nhow we got absolutely owned by simple fuzzing\n\nYou are reading part 1, wherein we build up our hubris.\nBackground: What’s A Minidump, and Why Write rust-minidump?\nYour program crashes. You want to know why your program crashed, but it happened on a user’s machine on the other side of the world. A full coredump (all memory allocated by the program) is enormous — we can’t have users sending us 4GB files! Ok let’s just collect up the most important regions of memory like the stacks and where the program crashed. Oh and I guess if we’re taking the time, let’s stuff some metadata about the system and process in there too.\nCongratulations you have invented Minidumps. Now you can turn a 100-thread coredump that would otherwise be 4GB into a nice little 2MB file that you can send over the internet and do postmortem analysis on.\nOr more specifically, Microsoft did. So long ago that their docs don’t even discuss platform support. MiniDumpWriteDump’s supported versions are simply “Windows”. Microsoft Research has presumably developed a time machine to guarantee this.\nThen Google came along (circa 2006-2007) and said “wouldn’t it be nice if we could make minidumps on any platform”? Thankfully Microsoft had actually built the format pretty extensibly, so it wasn’t too bad to extend the format for Linux, MacOS, BSD, Solaris, and so on. Those extensions became google-breakpad (or just Breakpad) which included a ton of different tools for generating, parsing, and analyzing their extended minidump format (and native Microsoft ones).\nMozilla helped out with this a lot because apparently, our crash reporting infrastructure (“Talkback”) was miserable circa 2007, and this seemed like a nice improvement. Needless to say, we’re pretty invested in breakpad’s minidumps at this point.\nFast forward to the present day and in a hilarious twist of fate, products like VSCode mean that Microsoft now supports applications that run on Linux and MacOS so it runs breakpad in production and has to handle non-Microsoft minidumps somewhere in its crash reporting infra, so someone else’s extension of their own format is somehow their problem now!\nMeanwhile, Google has kind-of moved on to Crashpad. I say kind-of because there’s still a lot of Breakpad in there, but they’re more interested in building out tooling on top of it than improving Breakpad itself. Having made a few changes to Breakpad: honestly fair, I don’t want to work on it either. Still, this was a bit of a problem for us, because it meant the project became increasingly under-staffed.\nBy the time I started working on crash reporting, Mozilla had basically given up on upstreaming fixes/improvements to Breakpad, and was just using its own patched fork. But even without the need for upstreaming patches, every change to Breakpad filled us with dread: many proposed improvements to our crash reporting infrastructure stalled out at “time to implement this in Breakpad”.\nWhy is working on Breakpad so miserable, you ask?\nParsing and analyzing minidumps is basically an exercise in writing a fractal parser of platform-specific formats nested in formats nested in formats. For many operating systems. For many hardware architectures. And all the inputs you’re parsing and analyzing are terrible and buggy so you have to write a really permissive parser and crawl forward however you can.\nSome specific MSVC toolchain that was part of Windows XP had a bug in its debuginfo format? Too bad, symbolicate that stack frame anyway!\nThe program crashed because it horribly corrupted its own stack? Too bad, produce a backtrace anyway!\nThe minidump writer itself completely freaked out and wrote a bunch of garbage to one stream? Too bad, produce whatever output you can anyway!\nHey, you know who has a lot of experience dealing with really complicated permissive parsers written in C++? Mozilla! That’s like the core functionality of a web browser.\nDo you know Mozilla’s secret solution to writing really complicated permissive parsers in C++?\nWe stopped doing it.\nWe developed Rust and ported our nastiest parsers to it.\nWe’ve done it a lot, and when we do we’re always like “wow this is so much more reliable and easy to maintain and it’s even faster now”. Rust is a really good language for writing parsers. C++ really isn’t.\nSo we Rewrote It In Rust (or as the kids call it, “Oxidized It”). Breakpad is big, so we haven’t actually covered all of its features. We’ve specifically written and deployed:\n\ndump_syms which processes native build artifacts into symbol files.\nrust-minidump which is a collection of crates that parse and analyze minidumps. Or more specifically, we deployed minidump-stackwalk, which is the high-level cli interface to all of rust-minidump.\n\nNotably missing from this picture is minidump writing, or what google-breakpad calls a client (because it runs on the client’s machine). We are working on a rust-based minidump writer, but it’s not something we can recommend using quite yet (although it has sped up a lot thanks to help from Embark Studios).\nThis is arguably the messiest and hardest work because it has a horrible job: use a bunch of native system APIs to gather up a bunch of OS-specific and Hardware-specific information about the crash AND do it for a program that just crashed, on a machine that caused the program to crash.\nWe have a long road ahead but every time we get to the other side of one of these projects it’s wonderful.\n \nBackground: Stackwalking and Calling Conventions\nOne of rust-minidump’s (minidump-stackwalk’s) most important jobs is to take the state for a thread (general purpose registers and stack memory) and create a backtrace for that thread (unwind/stackwalk). This is a surprisingly complicated and messy job, made only more complicated by the fact that we are trying to analyze the memory of a process that got messed up enough to crash.\nThis means our stackwalkers are inherently working with dubious data, and all of our stackwalking techniques are based on heuristics that can go wrong and we can very easily find ourselves in situations where the stackwalk goes backwards or sideways or infinite and we just have to try to deal with it!\nIt’s also pretty common to see a stackwalker start hallucinating, which is my term for “the stackwalker found something that looked plausible enough and went on a wacky adventure through the stack and made up a whole pile of useless garbage frames”. Hallucination is most common near the bottom of the stack where it’s also least offensive. This is because each frame you walk is another chance for something to go wrong, but also increasingly uninteresting because you’re rarely interested in confirming that a thread started in The Same Function All Threads Start In.\nAll of these problems would basically go away if everyone agreed to properly preserve their cpu’s PERFECTLY GOOD DEDICATED FRAME POINTER REGISTER. Just kidding, turning on frame pointers doesn’t really work either because Microsoft invented chaos frame pointers that can’t be used for unwinding! I assume this happened because they accidentally stepped on the wrong butterfly while they were traveling back in time to invent minidumps. (I’m sure it was a decision that made more sense 20 years ago, but it has not aged well.)\nIf you would like to learn more about the different techniques for unwinding, I wrote about them over here in my article on Apple’s Compact Unwind Info. I’ve also attempted to document breakpad’s STACK WIN and STACK CFI unwind info formats here, which are more similar to the  DWARF and PE32 unwind tables (which are basically tiny programming languages).\nIf you would like to learn more about ABIs in general, I wrote an entire article about them here. The end of that article also includes an introduction to how calling conventions work. Understanding calling conventions is key to implementing unwinders.\n \nHow Hard Did You Really Test Things?\nHopefully you now have a bit of a glimpse into why analyzing minidumps is an enormous headache. And of course you know how the story ends: that fuzzer kicks our butts! But of course to really savor our defeat, you have to see how hard we tried to do a good job! It’s time to build up our hubris and pat ourselves on the back.\nSo how much work actually went into making rust-minidump robust before the fuzzer went to work on it?\nQuite a bit!\nI’ll never argue all the work we did was perfect but we definitely did some good work here, both for synthetic inputs and real world ones. Probably the biggest “flaw” in our methodology was the fact that we were only focused on getting Firefox’s usecase to work. Firefox runs on a lot of platforms and sees a lot of messed up stuff, but it’s still a fairly coherent product that only uses so many features of minidumps.\nThis is one of the nice benefits of our recent work with Sentry, which is basically a Crash Reporting As A Service company. They are way more liable to stress test all kinds of weird corners of the format that Firefox doesn’t, and they have definitely found (and fixed!) some places where something is wrong or missing! (And they recently deployed it into production too! )\nBut hey don’t take my word for it, check out all the different testing we did:\nSynthetic Minidumps for Unit Tests\nrust-minidump includes a synthetic minidump generator which lets you come up with a high-level description of the contents of a minidump, and then produces an actual minidump binary that we can feed it into the full parser:\n// Let’s make a synth minidump with this particular Crashpad Info…\nlet module = ModuleCrashpadInfo::new(42, Endian::Little)\n    .add_list_annotation(\"annotation\")\n    .add_simple_annotation(\"simple\", \"module\")\n    .add_annotation_object(\"string\", AnnotationValue::String(\"value\".to_owned()))\n    .add_annotation_object(\"invalid\", AnnotationValue::Invalid)\n    .add_annotation_object(\"custom\", AnnotationValue::Custom(0x8001, vec![42]));\n\nlet crashpad_info = CrashpadInfo::new(Endian::Little)\n    .add_module(module)\n    .add_simple_annotation(\"simple\", \"info\");\n\nlet dump = SynthMinidump::with_endian(Endian::Little).add_crashpad_info(crashpad_info);\n\n// convert the synth minidump to binary and read it like a normal minidump\nlet dump = read_synth_dump(dump).unwrap();\n// Now check that the minidump reports the values we expect…\nminidump-synth intentionally avoids sharing layout code with the actual implementation so that incorrect changes to layouts won’t “accidentally” pass tests.\nA brief aside for some history: this testing framework was started by the original lead on this project, Ted Mielczarek. He started rust-minidump as a side project to learn Rust when 1.0 was released and just never had the time to finish it. Back then he was working at Mozilla and also a major contributor to Breakpad, which is why rust-minidump has a lot of similar design choices and terminology.\nThis case is no exception: our minidump-synth is a shameless copy of the synth-minidump utility in breakpad’s code, which was originally written by our other coworker Jim Blandy. Jim is one of the only people in the world that I will actually admit writes really good tests and docs, so I am totally happy to blatantly copy his work here.\nSince this was all a learning experiment, Ted was understandably less rigorous about testing than usual. This meant a lot of minidump-synth was unimplemented when I came along, which also meant lots of minidump features were completely untested. (He built an absolutely great skeleton, just hadn’t had the time to fill it all in!)\nWe spent a lot of time filling in more of minidump-synth’s implementation so we could write more tests and catch more issues, but this is definitely the weakest part of our tests. Some stuff was implemented before I got here, so I don’t even know what tests are missing!\nThis is a good argument for some code coverage checks, but it would probably come back with “wow you should write a lot more tests” and we would all look at it and go “wow we sure should” and then we would probably never get around to it, because there are many things we should do.\nOn the other hand, Sentry has been very useful in this regard because they already have a mature suite of tests full of weird corner cases they’ve built up over time, so they can easily identify things that really matter, know what the fix should roughly be, and can contribute pre-existing test cases!\nIntegration and Snapshot Tests\nWe tried our best to shore up coverage issues in our unit tests by adding more holistic tests. There’s a few checked in Real Minidumps that we have some integration tests for to make sure we handle Real Inputs properly.\nWe even wrote a bunch of integration tests for the CLI application that snapshot its output to confirm that we never accidentally change the results.\nPart of the motivation for this is to ensure we don’t break the JSON output, which we also wrote a very detailed schema document for and are trying to keep stable so people can actually rely on it while the actual implementation details are still in flux.\nYes, minidump-stackwalk is supposed to be stable and reasonable to use in production!\nFor our snapshot tests we use insta, which I think is fantastic and more people should use. All you need to do is assert_snapshot! any output you want to keep track of and it will magically take care of the storing, loading, and diffing.\nHere’s one of the snapshot tests where we invoke the CLI interface and snapshot stdout:\n#[test]\nfn test_evil_json() {\n    // For a while this didn't parse right\n    let bin = env!(\"CARGO_BIN_EXE_minidump-stackwalk\");\n    let output = Command::new(bin)\n        .arg(\"--json\")\n        .arg(\"--pretty\")\n        .arg(\"--raw-json\")\n        .arg(\"../testdata/evil.json\")\n        .arg(\"../testdata/test.dmp\")\n        .arg(\"../testdata/symbols/\")\n        .stdout(Stdio::piped())\n        .stderr(Stdio::piped())\n        .output()\n        .unwrap();\n\n    let stdout = String::from_utf8(output.stdout).unwrap();\n    let stderr = String::from_utf8(output.stderr).unwrap();\n\n    assert!(output.status.success());\n    insta::assert_snapshot!(\"json-pretty-evil-symbols\", stdout);\n    assert_eq!(stderr, \"\");\n}\n\n\nStackwalker Unit Testing\nThe stackwalker is easily the most complicated and subtle part of the new implementation, because every platform can have slight quirks and you need to implement several different unwinding strategies and carefully tune everything to work well in practice.\nThe scariest part of this was the call frame information (CFI) unwinders, because they are basically little virtual machines we need to parse and execute at runtime. Thankfully breakpad had long ago smoothed over this issue by defining a simplified and unified CFI format, STACK CFI (well, nearly unified, x86 Windows was still a special case as STACK WIN). So even if DWARF CFI has a ton of complex features, we mostly need to implement a Reverse Polish Notation Calculator except it can read registers and load memory from addresses it computes (and for STACK WIN it has access to named variables it can declare and mutate).\nUnfortunately, Breakpad’s description for this format is pretty underspecified so I had to basically pick some semantics I thought made sense and go with that. This made me extremely paranoid about the implementation. (And yes I will be more first-person for this part, because this part was genuinely where I personally spent most of my time and did a lot of stuff from scratch. All the blame belongs to me here!)\nThe STACK WIN / STACK CFI parser+evaluator is 1700 lines. 500 of those lines are a detailed documentation and discussion of the format, and 700 of those lines are an enormous pile of ~80 test cases where I tried to come up with every corner case I could think of.\nI even checked in two tests I knew were failing just to be honest that there were a couple cases to fix! One of them is a corner case involving dividing by a negative number that almost certainly just doesn’t matter. The other is a buggy input that old x86 Microsoft toolchains actually produce and parsers need to deal with. The latter was fixed before the fuzzing started.\nAnd 5225225 still found an integer overflow in the STACK WIN preprocessing step! (Not actually that surprising, it’s a hacky mess that tries to cover up for how messed up x86 Windows unwinding tables were.)\n(The code isn’t terribly interesting here, it’s just a ton of assertions that a given input string produces a given output/error.)\nOf course, I wasn’t satisfied with just coming up with my own semantics and testing them: I also ported most of breakpad’s own stackwalker tests to rust-minidump! This definitely found a bunch of bugs I had, but also taught me some weird quirks in Breakpad’s stackwalkers that I’m not sure I actually agree with. But in this case I was flying so blind that even being bug-compatible with Breakpad was some kind of relief.\nThose tests also included several tests for the non-CFI paths, which were similarly wobbly and quirky. I still really hate a lot of the weird platform-specific rules they have for stack scanning, but I’m forced to work on the assumption that they might be load-bearing. (I definitely had several cases where I disabled a breakpad test because it was “obviously nonsense” and then hit it in the wild while testing. I quickly learned to accept that Nonsense Happens And Cannot Be Ignored.)\nOne major thing I didn’t replicate was some of the really hairy hacks for STACK WIN. Like there are several places where they introduce extra stack-scanning to try to deal with the fact that stack frames can have mysterious extra alignment that the windows unwinding tables just don’t tell you about? I guess?\nThere’s almost certainly some exotic situations that rust-minidump does worse on because of this, but it probably also means we do better in some random other situations too. I never got the two to perfectly agree, but at some point the divergences were all in weird enough situations, and as far as I was concerned both stackwalkers were producing equally bad results in a bad situation. Absent any reason to prefer one over the other, divergence seemed acceptable to keep the implementation cleaner.\nHere’s a simplified version of one of the ported breakpad tests, if you’re curious (thankfully minidump-synth is based off of the same binary data mocking framework these tests use):\n#[test]\nfn test_x86_frame_pointer() {\n    let mut f = TestFixture::new();\n    let frame0_ebp = Label::new();\n    let frame1_ebp = Label::new();\n    let mut stack = Section::new();\n\n    // Setup the stack and registers so frame pointers will work\n    stack.start().set_const(0x80000000);\n    stack = stack\n        .append_repeated(12, 0) // frame 0: space\n        .mark(&frame0_ebp)      // frame 0 %ebp points here\n        .D32(&frame1_ebp)       // frame 0: saved %ebp\n        .D32(0x40008679)        // frame 0: return address\n        .append_repeated(8, 0)  // frame 1: space\n        .mark(&frame1_ebp)      // frame 1 %ebp points here\n        .D32(0)                 // frame 1: saved %ebp (stack end)\n        .D32(0);                // frame 1: return address (stack end)\n    f.raw.eip = 0x4000c7a5;\n    f.raw.esp = stack.start().value().unwrap() as u32;\n    f.raw.ebp = frame0_ebp.value().unwrap() as u32;\n\n    // Check the stackwalker's output:\n    let s = f.walk_stack(stack).await;\n    assert_eq!(s.frames.len(), 2);\n    {\n        let f0 = &s.frames[0];\n        assert_eq!(f0.trust, FrameTrust::Context);\n        assert_eq!(f0.context.valid, MinidumpContextValidity::All);\n        assert_eq!(f0.instruction, 0x4000c7a5);\n    }\n    {\n        let f1 = &s.frames[1];\n        assert_eq!(f1.trust, FrameTrust::FramePointer);\n        assert_eq!(f1.instruction, 0x40008678);\n    }\n}\nA Dedicated Production Diffing, Simulating, and Debugging Tool\nBecause minidumps are so horribly fractal and corner-casey, I spent a lot of time terrified of subtle issues that would become huge disasters if we ever actually tried to deploy to production. So I also spent a bunch of time building socc-pair, which takes the id of a crash report from Mozilla’s crash reporting system and pulls down the minidump, the old breakpad-based implementation’s output, and extra metadata.\nIt then runs a local rust-minidump (minidump-stackwalk) implementation on the minidump and does a domain-specific diff over the two inputs. The most substantial part of this is a fuzzy diff on the stackwalks that tries to better handle situations like when one implementation adds an extra frame but the two otherwise agree. It also uses the reported techniques each implementation used to try to identify whose output is more trustworthy when they totally diverge.\nI also ended up adding a bunch of mocking and benchmarking functionality to it as well, as I found more and more places where I just wanted to simulate a production environment.\nOh also I added really detailed trace-logging for the stackwalker so that I could easily post-mortem debug why it made the decisions it made.\nThis tool found so many issues and more importantly has helped me quickly isolate their causes. I am so happy I made it. Because of it, we know we actually fixed several issues that happened with the old breakpad implementation, which is great!\nHere’s a trimmed down version of the kind of report socc-pair would produce (yeah I abused diff syntax to get error highlighting. It’s a great hack, and I love it like a child):\ncomparing json...\n\n: {\n    crash_info: {\n        address: 0x7fff1760aca0\n        crashing_thread: 8\n        type: EXCEPTION_BREAKPOINT\n    }\n    crashing_thread: {\n        frames: [\n            0: {\n                file: wrappers.cpp:1750da2d7f9db490b9d15b3ee696e89e6aa68cb7\n                frame: 0\n                function: RustMozCrash(char const*, int, char const*)\n                function_offset: 0x00000010\n-               did not match\n+               line: 17\n-               line: 20\n                module: xul.dll\n\n.....\n\n    unloaded_modules: [\n        0: {\n            base_addr: 0x7fff48290000\n-           local val was null instead of:\n            code_id: 68798D2F9000\n            end_addr: 0x7fff48299000\n            filename: KBDUS.DLL\n        }\n        1: {\n            base_addr: 0x7fff56020000\n            code_id: DFD6E84B14000\n            end_addr: 0x7fff56034000\n            filename: resourcepolicyclient.dll\n        }\n    ]\n~   ignoring field write_combine_size: \"0\"\n}\n\n- Total errors: 288, warnings: 39\n\nbenchmark results (ms):\n    2388, 1986, 2268, 1989, 2353, \n    average runtime: 00m:02s:196ms (2196ms)\n    median runtime: 00m:02s:268ms (2268ms)\n    min runtime: 00m:01s:986ms (1986ms)\n    max runtime: 00m:02s:388ms (2388ms)\n\nmax memory (rss) results (bytes):\n    267755520, 261152768, 272441344, 276131840, 279134208, \n    average max-memory: 258MB (271323136 bytes)\n    median max-memory: 259MB (272441344 bytes)\n    min max-memory: 249MB (261152768 bytes)\n    max max-memory: 266MB (279134208 bytes)\n\nOutput Files: \n    * (download) Minidump: b4f58e9f-49be-4ba5-a203-8ef160211027.dmp\n    * (download) Socorro Processed Crash: b4f58e9f-49be-4ba5-a203-8ef160211027.json\n    * (download) Raw JSON: b4f58e9f-49be-4ba5-a203-8ef160211027.raw.json\n    * Local minidump-stackwalk Output: b4f58e9f-49be-4ba5-a203-8ef160211027.local.json\n    * Local minidump-stackwalk Logs: b4f58e9f-49be-4ba5-a203-8ef160211027.log.txt\nStaging and Deploying to Production\nOnce we were confident enough in the implementation, a lot of the remaining testing was taken over by Will Kahn-Greene, who’s responsible for a lot of the server-side details of our crash-reporting infrastructure.\nWill spent a bunch of time getting a bunch of machinery setup to manage the deployment and monitoring of rust-minidump. He also did a lot of the hard work of cleaning up all our server-side configuration scripts to handle any differences between the two implementations. (Although I spent a lot of time on compatibility, we both agreed this was a good opportunity to clean up old cruft and mistakes.)\nOnce all of this was set up, he turned it on in staging and we got our first look at how rust-minidump actually worked in ~production:\nTerribly!\nOur staging servers take in about 10% of the inputs that also go to our production servers, but even at that reduced scale we very quickly found several new corner cases and we were getting tons of crashes, which is mildly embarrassing for the thing that handles other people’s crashes.\nWill did a great job here in monitoring and reporting the issues. Thankfully they were all fairly easy for us to fix. Eventually, everything smoothed out and things seemed to be working just as reliably as the old implementation on the production server. The only places where we were completely failing to produce any output were for horribly truncated minidumps that may as well have been empty files.\nWe originally did have some grand ambitions of running socc-pair on everything the staging servers processed or something to get really confident in the results. But by the time we got to that point, we were completely exhausted and feeling pretty confident in the new implementation.\nEventually Will just said “let’s turn it on in production” and I said “AAAAAAAAAAAAAAA”.\nThis moment was pure terror. There had always been more corner cases. There’s no way we could just be done. This will probably set all of Mozilla on fire and delete Firefox from the internet!\nBut Will convinced me. We wrote up some docs detailing all the subtle differences and sent them to everyone we could. Then the moment of truth finally came: Will turned it on in production, and I got to really see how well it worked in production:\n*dramatic drum roll*\nIt worked fine.\nAfter all that stress and anxiety, we turned it on and it was fine.\nHeck, I’ll say it: it ran well.\nIt was faster, it crashed less, and we even knew it fixed some issues.\nI was in a bit of a stupor for the rest of that week, because I kept waiting for the other shoe to drop. I kept waiting for someone to emerge from the mist and explain that I had somehow bricked Thunderbird or something. But no, it just worked.\nSo we left for the holidays, and I kept waiting for it to break, but it was still fine.\nI am honestly still shocked about this!\nBut hey, as it turns out we really did put a lot of careful work into testing the implementation. At every step we found new problems but that was good, because once we got to the final step there were no more problems to surprise us.\nAnd the fuzzer still kicked our butts afterwards.\nBut that’s part 2! Thanks for reading!\n \nThe post Everything Is Broken: Shipping rust-minidump at Mozilla – Part 1 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-06-14T15:05:06.000Z",
      "date_modified": "2022-06-14T15:05:06.000Z",
      "_plugin": {
        "pageFilename": "4eb77a2fe8c57ebd5ab06ee0c83d52c0eca2d40927ae21c892c7dbe7bddda2f3.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47809",
      "url": "https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/",
      "title": "Training efficient neural network models for Firefox Translations",
      "summary": "The Bergamot project is a collaboration between Mozilla, University of Edinburgh, Charles University in Prague, the University of Sheffield, and University of Tartu with funding from the European Union’s Horizon 2020 research and innovation programme. It brings MT to the local environment, providing small, high-quality, CPU optimized NMT models. The Firefox Translations web extension utilizes proceedings of project Bergamot and brings local translations to Firefox. In this article, we will discuss the components used to train our efficient NMT models.\nThe post Training efficient neural network models for Firefox Translations appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Machine Translation is an important tool for expanding the accessibility of web content. Usually, people use cloud providers to translate web pages. State-of-the-art Neural Machine Translation (NMT) models are large and often require specialized hardware like GPUs to run inference in real-time.</p>\n<p>If people were able to run a compact Machine Translation (MT) model on their local machine CPU without sacrificing translation accuracy it would help to preserve privacy and reduce costs.</p>\n<p>The Bergamot <a href=\"https://browser.mt/\" target=\"_blank\" rel=\"noopener\">project</a> is a collaboration between Mozilla, the University of Edinburgh, Charles University in Prague, the University of Sheffield, and the University of Tartu with funding from the European Union’s Horizon 2020 research and innovation programme. It brings MT to the local environment, providing small, high-quality, CPU optimized NMT models. <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">The Firefox Translations web extension</a> utilizes proceedings of project Bergamot and brings local translations to Firefox.</p>\n<p>In this article, we will discuss the components used to train our efficient NMT models. The project is open-source, so you can give it a try and train your model too!</p>\n<h2><b>Architecture</b></h2>\n<p>NMT models are trained as language pairs, translating from language A to language B. The <a href=\"https://github.com/mozilla/firefox-translations-training\" target=\"_blank\" rel=\"noopener\">training pipeline</a> was designed to train translation models for a language pair end-to-end, from environment configuration to exporting the ready-to-use models. The pipeline run is completely reproducible given the same code, hardware and configuration files.</p>\n<p>The complexity of the pipeline comes from the requirement to produce an efficient model. We use Teacher-Student distillation to compress a high-quality but resource-intensive teacher model into an efficient CPU-optimized student model that still has good translation quality. We explain this further in the Compression section.</p>\n<p>The pipeline includes many steps: compiling of components, downloading and cleaning datasets, training teacher, student and backward models, decoding, quantization, evaluation etc (more details below). The pipeline can be represented as a Directly Acyclic Graph (DAG).</p>\n<p>&nbsp;</p>\n<p><img class=\"aligncenter wp-image-47810\" src=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-500x246.png\" alt=\"Firfox Translation training pipeline DAG\" width=\"591\" height=\"291\" srcset=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-500x246.png 500w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-250x123.png 250w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-768x379.png 768w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-1536x757.png 1536w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-2048x1009.png 2048w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></p>\n<p>The workflow is file-based and employs self-sufficient scripts that use data on disk as input, and write intermediate and output results back to disk.</p>\n<p>We use the Marian Neural Machine Translation engine. It is written in C++ and designed to be fast. The engine is open-sourced and used by many universities and companies, including Microsoft.</p>\n<h2><b>Training a quality model</b></h2>\n<p>The first task of the pipeline is to train a high-quality model that will be compressed later. The main challenge at this stage is to find a good parallel corpus that contains translations of the same sentences in both source and target languages and then apply appropriate cleaning procedures.</p>\n<h3><b>Datasets</b></h3>\n<p>It turned out there are many open-source parallel datasets for machine translation available on the internet. The most interesting project that aggregates such datasets is <a href=\"https://opus.nlpl.eu/\" target=\"_blank\" rel=\"noopener\">OPUS</a>. The Annual Conference on Machine Translation also collects and distributes some datasets for competitions, for example, <a href=\"https://www.statmt.org/wmt21/translation-task.html#download\" target=\"_blank\" rel=\"noopener\">WMT21 Machine Translation of News</a>. Another great source of MT corpus is the <a href=\"https://paracrawl.eu/\" target=\"_blank\" rel=\"noopener\">Paracrawl</a> project.</p>\n<p>OPUS dataset search interface:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47814\" src=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-500x403.png\" alt=\"OPUS dataset search interface\" width=\"591\" height=\"476\" srcset=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-500x403.png 500w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-250x202.png 250w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-768x619.png 768w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-1536x1238.png 1536w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM.png 1992w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></p>\n<p>It is possible to use any dataset on disk, but automating dataset downloading from Open source resources makes adding new language pairs easy, and whenever the data set is expanded we can then easily retrain the model to take advantage of the additional data. Make sure to check the licenses of the open-source datasets before usage.</p>\n<h3><b>Data cleaning</b></h3>\n<p>Most open-source datasets are somewhat noisy. Good examples are crawled websites and translation of subtitles. Texts from websites can be poor-quality automatic translations or contain unexpected HTML, and subtitles are often free-form translations that change the meaning of the text.</p>\n<p>It is well known in the world of Machine Learning (ML) that if we feed garbage into the model we get garbage as a result. Dataset cleaning is probably the most crucial step in the pipeline to achieving good quality.</p>\n<p>We employ some basic cleaning techniques that work for most datasets like removing too short or too long sentences and filtering the ones with an unrealistic source to target length ratio. We also use <a href=\"https://github.com/bitextor/bicleaner\" target=\"_blank\" rel=\"noopener\">bicleaner</a>, a pre-trained ML classifier that attempts to indicate whether the training example in a dataset is a reversible translation. We can then remove low-scoring translation pairs that may be incorrect or otherwise add unwanted noise.</p>\n<p>Automation is necessary when your training set is large. However, it is always recommended to look at your data manually in order to tune the cleaning thresholds and add dataset-specific fixes to get the best quality.</p>\n<h3><b>Data augmentation</b></h3>\n<p>There are more than 7000 languages spoken in the world and most of them are classified as low-resource for our purposes, meaning there is little parallel corpus data available for training. In these cases, we use a popular data augmentation strategy called back-translation.</p>\n<p>Back-translation is a technique to increase the amount of training data available by adding synthetic translations. We get these synthetic examples by training a translation model from the target language to the source language. Then we use it to translate monolingual data from the target language into the source language, creating synthetic examples that are added to the training data for the model we actually want, from the source language to the target language.</p>\n<h3><b>The model</b></h3>\n<p>Finally, when we have a clean parallel corpus we train a big transformer model to reach the best quality we can.</p>\n<p>Once the model converges on the augmented dataset, we fine-tune it on the original parallel corpus that doesn’t include synthetic examples from back-translation to further improve quality.</p>\n<h2><b>Compression</b></h2>\n<p>The trained model can be 800Mb or more in size depending on configuration and requires significant computing power to perform translation (decoding). At this point, it’s generally executed on GPUs and not practical to run on most consumer laptops. In the next steps we will prepare a model that works efficiently on consumer CPUs.</p>\n<h3><b>Knowledge distillation</b></h3>\n<p>The main technique we use for compression is Teacher-Student Knowledge Distillation. The idea is to decode a lot of text from the source language into the target language using the heavy model we trained (Teacher) and then train a much smaller model with fewer parameters (Student) on these synthetic translations. The student is supposed to imitate the teacher’s behavior and demonstrate similar translation quality despite being significantly faster and more compact.</p>\n<p>We also augment the parallel corpus data with monolingual data in the source language for decoding. This improves the student by providing additional training examples of the teacher&#8217;s behavior.</p>\n<h3><b>Ensemble</b></h3>\n<p>Another trick is to use not just one teacher but an ensemble of 2-4 teachers independently trained on the same parallel corpus. It can boost quality a little bit at the cost of having to train more teachers. The pipeline supports training and decoding with an ensemble of teachers.</p>\n<h3><b>Quantization</b></h3>\n<p>One more popular technique for model compression is quantization. We use 8-bit quantization which essentially means that we store weights of the neural net as int8 instead of float32. It saves space and speeds up matrix multiplication on inference.</p>\n<h3><b>Other tricks</b></h3>\n<p>Other features worth mentioning but beyond the scope of this already lengthy article are the specialized Neural Network architecture of the student model, half-precision decoding by the teacher model to speed it up, lexical shortlists, training of word alignments, and finetuning of the quantized student.</p>\n<p>Yes, it’s a lot! Now you can see why we wanted to have an end-to-end pipeline.</p>\n<h2><b>How to learn more</b></h2>\n<p>This work is based on a lot of research. If you are interested in the science behind the training pipeline, check out reference publications listed <a href=\"https://github.com/mozilla/firefox-translations-training#references\" target=\"_blank\" rel=\"noopener\">in the training pipeline repository README</a> and <a href=\"https://browser.mt/publications\" target=\"_blank\" rel=\"noopener\">across the wider Bergamot project</a>. <a href=\"https://aclanthology.org/2020.ngt-1.26/\" target=\"_blank\" rel=\"noopener\">Edinburgh&#8217;s Submissions to the 2020 Machine Translation Efficiency Task</a> is a good academic starting article. Check <a href=\"https://nbogoychev.com/efficient-machine-translation/\" target=\"_blank\" rel=\"noopener\">this tutorial</a> by Nikolay Bogoychev for a more practical and operational explanation of the steps.</p>\n<h2><b>Results</b></h2>\n<p>The final student model is 47 times smaller and 37 times faster than the original teacher model and has only a small quality decrease!</p>\n<p>Benchmarks for en-pt model and Flores dataset:</p>\n<table style=\"table-layout: fixed; width: 100%;\">\n<tbody>\n<tr>\n<td><b>Model</b></td>\n<td><b>Size</b></td>\n<td><b>Total number of parameters</b></td>\n<td><b>Dataset decoding time on 1 CPU core</b></td>\n<td><b>Quality, BLEU</b></td>\n</tr>\n<tr>\n<td>Teacher</td>\n<td>798Mb</td>\n<td>192.75M</td>\n<td>631s</td>\n<td>52.5</td>\n</tr>\n<tr>\n<td>Student quantized</td>\n<td>17Mb</td>\n<td>15.7M</td>\n<td>17.9s</td>\n<td>50.7</td>\n</tr>\n</tbody>\n</table>\n<p>We evaluate results using MT standard <a href=\"https://en.wikipedia.org/wiki/BLEU\" target=\"_blank\" rel=\"noopener\">BLEU scores</a> that essentially represent how similar translated and reference texts are. This method is not perfect but it has been shown that BLEU scores correlate well with human judgment of translation quality.</p>\n<p>We have a <a href=\"https://github.com/mozilla/firefox-translations-models\" target=\"_blank\" rel=\"noopener\">GitHub repository</a> with all the trained models and <a href=\"https://github.com/mozilla/firefox-translations-models/blob/main/evaluation/prod/results.md\" target=\"_blank\" rel=\"noopener\">evaluation results</a> where we compare the accuracy of our models to popular APIs of cloud providers. We can see that some models perform similarly, or even outperform, the cloud providers which is a great result taking into account our model’s efficiency, reproducibility and open-source nature.</p>\n<p>For example, here you can see evaluation results for the English to Portuguese model trained by Mozilla using open-source data only.</p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47818\" src=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM.png\" alt=\"Evaluation results en-pt\" width=\"591\" height=\"476\" srcset=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM.png 2066w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-250x201.png 250w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-500x403.png 500w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-768x619.png 768w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-1536x1237.png 1536w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-2048x1650.png 2048w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></p>\n<p>Anyone can train models and contribute them to our repo. Those contributions can be used in the <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">Firefox Translations web extension</a> and other places (see below).</p>\n<h2><b>Scaling</b></h2>\n<p>It is of course possible to run the whole pipeline on one machine, though it may take a while. Some steps of the pipeline are CPU bound and difficult to parallelize, while other steps can be offloaded to multiple GPUs. Most of the official models in the repository were trained on machines with 8 GPUs. A few steps, like teacher decoding during knowledge distillation, can take days even on well-resourced single machines. So to speed things up, we added cluster support to be able to spread different steps of the pipeline over multiple nodes.</p>\n<h3><b>Workflow manager</b></h3>\n<p>To manage this complexity we chose <a href=\"https://snakemake.github.io/\" target=\"_blank\" rel=\"noopener\">Snakemake</a> which is very popular in the bioinformatics community. It uses file-based workflows, allows specifying step dependencies in Python, supports containerization and integration with different cluster software. We considered alternative solutions that focus on job scheduling, but ultimately chose Snakemake because it was more ergonomic for one-run experimentation workflows.</p>\n<p>Example of a Snakemake rule (dependencies between rules are inferred implicitly):</p>\n<pre><code class=\"js\">rule train_teacher:\r\n    message: \"Training teacher on all data\"\r\n    log: f\"{log_dir}/train_teacher{{ens}}.log\"\r\n    conda: \"envs/base.yml\"\r\n    threads: gpus_num*2\r\n    resources: gpu=gpus_num\r\n    input:\r\n        rules.merge_devset.output, \r\n        train_src=f'{teacher_corpus}.{src}.gz',\r\n        train_trg=f'{teacher_corpus}.{trg}.gz',\r\n        bin=ancient(trainer), \r\n        vocab=vocab_path\r\n    output: model=f'{teacher_base_dir}{{ens}}/{best_model}'\r\n    params: \r\n        prefix_train=teacher_corpus, \r\n        prefix_test=f\"{original}/devset\", \r\n        dir=directory(f'{teacher_base_dir}{{ens}}'),\r\n        args=get_args(\"training-teacher-base\")\r\n    shell: '''bash pipeline/train/train.sh \\\r\n                teacher train {src} {trg} \"{params.prefix_train}\" \\\r\n                \"{params.prefix_test}\" \"{params.dir}\" \\\r\n                \"{input.vocab}\" {params.args} &gt;&gt; {log} 2&gt;&amp;1'''</code></pre>\n<h3><b>Cluster support</b></h3>\n<p>To parallelize workflow steps across cluster nodes we use <a href=\"https://slurm.schedmd.com/\">Slurm</a> resource manager. It is relatively simple to operate, fits well for high-performance experimentation workflows, and supports Singularity containers for easier reproducibility. Slurm is also the most popular cluster manager for High-Performance Computers (HPC) used for model training in academia, and most of the consortium partners were already using or familiar with it.</p>\n<h2><b>How to start training</b></h2>\n<p>The workflow is quite resource-intensive, so you’ll need a pretty good server machine or even a cluster. We recommend using 4-8 Nvidia 2080-equivalent or better GPUs per machine.</p>\n<p>Clone <a href=\"https://github.com/mozilla/firefox-translations-training\" target=\"_blank\" rel=\"noopener\">https://github.com/mozilla/firefox-translations-training</a> and follow the instructions in the <a href=\"https://github.com/mozilla/firefox-translations-training/blob/main/README.md\" target=\"_blank\" rel=\"noopener\">readme</a> for configuration.</p>\n<p>The most important part is to find parallel datasets and properly configure settings based on your available data and hardware. You can learn more about this in the readme.</p>\n<h2><b>How to use the existing models</b></h2>\n<p>The existing models are shipped with the <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">Firefox Translations web extension</a>, enabling users to translate web pages in Firefox. The models are downloaded to a local machine on demand. The web extension uses these models with the<a href=\"https://github.com/browsermt/bergamot-translator\" target=\"_blank\" rel=\"noopener\"> bergamot-translator</a> Marian wrapper compiled to Web Assembly.</p>\n<p>Also, there is a playground website at <a href=\"https://mozilla.github.io/translate\" target=\"_blank\" rel=\"noopener\">https://mozilla.github.io/translate</a> where you can input text and translate it right away, also locally but served as a static website instead of a browser extension.</p>\n<p>If you are interested in an efficient NMT inference on the server, you can try a prototype <a href=\"https://github.com/mozilla/translation-service\" target=\"_blank\" rel=\"noopener\">HTTP service</a> that uses bergamot-translator natively compiled, instead of compiled to WASM.</p>\n<p>Or follow the build instructions in the <a href=\"https://github.com/browsermt/bergamot-translator#build-instructions\" target=\"_blank\" rel=\"noopener\">bergamot-translator readme</a> to directly use the C++, JavaScript WASM, or Python bindings.</p>\n<h2><b>Conclusion</b></h2>\n<p>It is fascinating how far Machine Translation research has come in recent years. Local high-quality translations are the future and it’s becoming more and more practical for companies and researchers to train such models even without access to proprietary data or large-scale computing power.</p>\n<p>We hope that <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">Firefox Translations</a> will set a new standard of privacy-preserving, efficient, open-source machine translation accessible for all.</p>\n<h2><b>Acknowledgements</b></h2>\n<p>I would like to thank all the participants of <a href=\"https://browser.mt/\" target=\"_blank\" rel=\"noopener\">the Bergamot Project</a> for making this technology possible, my teammates Andre Natal and Abhishek Aggarwal for the incredible work they have done bringing Firefox Translations to life, Lonnen for managing the project and editing this blog post and of course awesome Mozilla community for helping with localization of the web-extension and testing its early builds.</p>\n<p><i>This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 825303 <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f1ea-1f1fa.png\" alt=\"🇪🇺\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></i></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/\">Training efficient neural network models for Firefox Translations</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Machine Translation is an important tool for expanding the accessibility of web content. Usually, people use cloud providers to translate web pages. State-of-the-art Neural Machine Translation (NMT) models are large and often require specialized hardware like GPUs to run inference in real-time.\nIf people were able to run a compact Machine Translation (MT) model on their local machine CPU without sacrificing translation accuracy it would help to preserve privacy and reduce costs.\nThe Bergamot project is a collaboration between Mozilla, the University of Edinburgh, Charles University in Prague, the University of Sheffield, and the University of Tartu with funding from the European Union’s Horizon 2020 research and innovation programme. It brings MT to the local environment, providing small, high-quality, CPU optimized NMT models. The Firefox Translations web extension utilizes proceedings of project Bergamot and brings local translations to Firefox.\nIn this article, we will discuss the components used to train our efficient NMT models. The project is open-source, so you can give it a try and train your model too!\nArchitecture\nNMT models are trained as language pairs, translating from language A to language B. The training pipeline was designed to train translation models for a language pair end-to-end, from environment configuration to exporting the ready-to-use models. The pipeline run is completely reproducible given the same code, hardware and configuration files.\nThe complexity of the pipeline comes from the requirement to produce an efficient model. We use Teacher-Student distillation to compress a high-quality but resource-intensive teacher model into an efficient CPU-optimized student model that still has good translation quality. We explain this further in the Compression section.\nThe pipeline includes many steps: compiling of components, downloading and cleaning datasets, training teacher, student and backward models, decoding, quantization, evaluation etc (more details below). The pipeline can be represented as a Directly Acyclic Graph (DAG).\n \n\nThe workflow is file-based and employs self-sufficient scripts that use data on disk as input, and write intermediate and output results back to disk.\nWe use the Marian Neural Machine Translation engine. It is written in C++ and designed to be fast. The engine is open-sourced and used by many universities and companies, including Microsoft.\nTraining a quality model\nThe first task of the pipeline is to train a high-quality model that will be compressed later. The main challenge at this stage is to find a good parallel corpus that contains translations of the same sentences in both source and target languages and then apply appropriate cleaning procedures.\nDatasets\nIt turned out there are many open-source parallel datasets for machine translation available on the internet. The most interesting project that aggregates such datasets is OPUS. The Annual Conference on Machine Translation also collects and distributes some datasets for competitions, for example, WMT21 Machine Translation of News. Another great source of MT corpus is the Paracrawl project.\nOPUS dataset search interface:\n\nIt is possible to use any dataset on disk, but automating dataset downloading from Open source resources makes adding new language pairs easy, and whenever the data set is expanded we can then easily retrain the model to take advantage of the additional data. Make sure to check the licenses of the open-source datasets before usage.\nData cleaning\nMost open-source datasets are somewhat noisy. Good examples are crawled websites and translation of subtitles. Texts from websites can be poor-quality automatic translations or contain unexpected HTML, and subtitles are often free-form translations that change the meaning of the text.\nIt is well known in the world of Machine Learning (ML) that if we feed garbage into the model we get garbage as a result. Dataset cleaning is probably the most crucial step in the pipeline to achieving good quality.\nWe employ some basic cleaning techniques that work for most datasets like removing too short or too long sentences and filtering the ones with an unrealistic source to target length ratio. We also use bicleaner, a pre-trained ML classifier that attempts to indicate whether the training example in a dataset is a reversible translation. We can then remove low-scoring translation pairs that may be incorrect or otherwise add unwanted noise.\nAutomation is necessary when your training set is large. However, it is always recommended to look at your data manually in order to tune the cleaning thresholds and add dataset-specific fixes to get the best quality.\nData augmentation\nThere are more than 7000 languages spoken in the world and most of them are classified as low-resource for our purposes, meaning there is little parallel corpus data available for training. In these cases, we use a popular data augmentation strategy called back-translation.\nBack-translation is a technique to increase the amount of training data available by adding synthetic translations. We get these synthetic examples by training a translation model from the target language to the source language. Then we use it to translate monolingual data from the target language into the source language, creating synthetic examples that are added to the training data for the model we actually want, from the source language to the target language.\nThe model\nFinally, when we have a clean parallel corpus we train a big transformer model to reach the best quality we can.\nOnce the model converges on the augmented dataset, we fine-tune it on the original parallel corpus that doesn’t include synthetic examples from back-translation to further improve quality.\nCompression\nThe trained model can be 800Mb or more in size depending on configuration and requires significant computing power to perform translation (decoding). At this point, it’s generally executed on GPUs and not practical to run on most consumer laptops. In the next steps we will prepare a model that works efficiently on consumer CPUs.\nKnowledge distillation\nThe main technique we use for compression is Teacher-Student Knowledge Distillation. The idea is to decode a lot of text from the source language into the target language using the heavy model we trained (Teacher) and then train a much smaller model with fewer parameters (Student) on these synthetic translations. The student is supposed to imitate the teacher’s behavior and demonstrate similar translation quality despite being significantly faster and more compact.\nWe also augment the parallel corpus data with monolingual data in the source language for decoding. This improves the student by providing additional training examples of the teacher’s behavior.\nEnsemble\nAnother trick is to use not just one teacher but an ensemble of 2-4 teachers independently trained on the same parallel corpus. It can boost quality a little bit at the cost of having to train more teachers. The pipeline supports training and decoding with an ensemble of teachers.\nQuantization\nOne more popular technique for model compression is quantization. We use 8-bit quantization which essentially means that we store weights of the neural net as int8 instead of float32. It saves space and speeds up matrix multiplication on inference.\nOther tricks\nOther features worth mentioning but beyond the scope of this already lengthy article are the specialized Neural Network architecture of the student model, half-precision decoding by the teacher model to speed it up, lexical shortlists, training of word alignments, and finetuning of the quantized student.\nYes, it’s a lot! Now you can see why we wanted to have an end-to-end pipeline.\nHow to learn more\nThis work is based on a lot of research. If you are interested in the science behind the training pipeline, check out reference publications listed in the training pipeline repository README and across the wider Bergamot project. Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task is a good academic starting article. Check this tutorial by Nikolay Bogoychev for a more practical and operational explanation of the steps.\nResults\nThe final student model is 47 times smaller and 37 times faster than the original teacher model and has only a small quality decrease!\nBenchmarks for en-pt model and Flores dataset:\n\n\n\nModel\nSize\nTotal number of parameters\nDataset decoding time on 1 CPU core\nQuality, BLEU\n\n\nTeacher\n798Mb\n192.75M\n631s\n52.5\n\n\nStudent quantized\n17Mb\n15.7M\n17.9s\n50.7\n\n\n\nWe evaluate results using MT standard BLEU scores that essentially represent how similar translated and reference texts are. This method is not perfect but it has been shown that BLEU scores correlate well with human judgment of translation quality.\nWe have a GitHub repository with all the trained models and evaluation results where we compare the accuracy of our models to popular APIs of cloud providers. We can see that some models perform similarly, or even outperform, the cloud providers which is a great result taking into account our model’s efficiency, reproducibility and open-source nature.\nFor example, here you can see evaluation results for the English to Portuguese model trained by Mozilla using open-source data only.\n\nAnyone can train models and contribute them to our repo. Those contributions can be used in the Firefox Translations web extension and other places (see below).\nScaling\nIt is of course possible to run the whole pipeline on one machine, though it may take a while. Some steps of the pipeline are CPU bound and difficult to parallelize, while other steps can be offloaded to multiple GPUs. Most of the official models in the repository were trained on machines with 8 GPUs. A few steps, like teacher decoding during knowledge distillation, can take days even on well-resourced single machines. So to speed things up, we added cluster support to be able to spread different steps of the pipeline over multiple nodes.\nWorkflow manager\nTo manage this complexity we chose Snakemake which is very popular in the bioinformatics community. It uses file-based workflows, allows specifying step dependencies in Python, supports containerization and integration with different cluster software. We considered alternative solutions that focus on job scheduling, but ultimately chose Snakemake because it was more ergonomic for one-run experimentation workflows.\nExample of a Snakemake rule (dependencies between rules are inferred implicitly):\nrule train_teacher:\n    message: \"Training teacher on all data\"\n    log: f\"{log_dir}/train_teacher{{ens}}.log\"\n    conda: \"envs/base.yml\"\n    threads: gpus_num*2\n    resources: gpu=gpus_num\n    input:\n        rules.merge_devset.output, \n        train_src=f'{teacher_corpus}.{src}.gz',\n        train_trg=f'{teacher_corpus}.{trg}.gz',\n        bin=ancient(trainer), \n        vocab=vocab_path\n    output: model=f'{teacher_base_dir}{{ens}}/{best_model}'\n    params: \n        prefix_train=teacher_corpus, \n        prefix_test=f\"{original}/devset\", \n        dir=directory(f'{teacher_base_dir}{{ens}}'),\n        args=get_args(\"training-teacher-base\")\n    shell: '''bash pipeline/train/train.sh \\\n                teacher train {src} {trg} \"{params.prefix_train}\" \\\n                \"{params.prefix_test}\" \"{params.dir}\" \\\n                \"{input.vocab}\" {params.args} >> {log} 2>&1'''\nCluster support\nTo parallelize workflow steps across cluster nodes we use Slurm resource manager. It is relatively simple to operate, fits well for high-performance experimentation workflows, and supports Singularity containers for easier reproducibility. Slurm is also the most popular cluster manager for High-Performance Computers (HPC) used for model training in academia, and most of the consortium partners were already using or familiar with it.\nHow to start training\nThe workflow is quite resource-intensive, so you’ll need a pretty good server machine or even a cluster. We recommend using 4-8 Nvidia 2080-equivalent or better GPUs per machine.\nClone https://github.com/mozilla/firefox-translations-training and follow the instructions in the readme for configuration.\nThe most important part is to find parallel datasets and properly configure settings based on your available data and hardware. You can learn more about this in the readme.\nHow to use the existing models\nThe existing models are shipped with the Firefox Translations web extension, enabling users to translate web pages in Firefox. The models are downloaded to a local machine on demand. The web extension uses these models with the bergamot-translator Marian wrapper compiled to Web Assembly.\nAlso, there is a playground website at https://mozilla.github.io/translate where you can input text and translate it right away, also locally but served as a static website instead of a browser extension.\nIf you are interested in an efficient NMT inference on the server, you can try a prototype HTTP service that uses bergamot-translator natively compiled, instead of compiled to WASM.\nOr follow the build instructions in the bergamot-translator readme to directly use the C++, JavaScript WASM, or Python bindings.\nConclusion\nIt is fascinating how far Machine Translation research has come in recent years. Local high-quality translations are the future and it’s becoming more and more practical for companies and researchers to train such models even without access to proprietary data or large-scale computing power.\nWe hope that Firefox Translations will set a new standard of privacy-preserving, efficient, open-source machine translation accessible for all.\nAcknowledgements\nI would like to thank all the participants of the Bergamot Project for making this technology possible, my teammates Andre Natal and Abhishek Aggarwal for the incredible work they have done bringing Firefox Translations to life, Lonnen for managing the project and editing this blog post and of course awesome Mozilla community for helping with localization of the web-extension and testing its early builds.\nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 825303 \nThe post Training efficient neural network models for Firefox Translations appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-06-07T15:25:47.000Z",
      "date_modified": "2022-06-07T15:25:47.000Z",
      "_plugin": {
        "pageFilename": "8d12279c1a4fc6eb4c8b3d44d0b131d7536165f22bc1bccbe6f8952084e7f4e3.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47829",
      "url": "https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/",
      "title": "Improved Process Isolation in Firefox 100",
      "summary": "Firefox uses a multi-process model for additional security and stability while browsing: Web Content (such as HTML/CSS and Javascript) is rendered in separate processes that are isolated from the rest of the operating system and managed by a privileged parent process. This way, the amount of control gained by an attacker that exploits a bug in a content process is limited. In this article, we would like to dive a bit further into the latest major milestone we have reached: Win32k Lockdown, which greatly reduces the capabilities of the content process when running on Windows.\nThe post Improved Process Isolation in Firefox 100 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<h2><strong>Introduction</strong></h2>\n<p><span style=\"font-weight: 400;\">Firefox uses a </span><a href=\"https://hacks.mozilla.org/2021/05/introducing-firefox-new-site-isolation-security-architecture/\"><span style=\"font-weight: 400;\">multi-process model</span></a><span style=\"font-weight: 400;\"> for additional security and stability while browsing: Web Content (such as HTML/CSS and Javascript) is rendered in separate processes that are isolated from the rest of the operating system and managed by a privileged parent process. This way, the amount of control gained by an attacker that exploits a bug in a content process is limited. </span></p>\n<p><span style=\"font-weight: 400;\">Ever since we deployed this model, we have been working on improving the isolation of the content processes to further limit the attack surface. This is a challenging task since content processes need access to some operating system APIs to properly function: for example, they still need to be able to talk to the parent process. </span></p>\n<p><span style=\"font-weight: 400;\">In this article, we would like to dive a bit further into the latest major milestone we have reached: </span><i><span style=\"font-weight: 400;\">Win32k Lockdown,</span></i><span style=\"font-weight: 400;\"> which greatly reduces the capabilities of the content process when running on Windows. Together with two major earlier efforts (</span><a href=\"https://hacks.mozilla.org/2021/05/introducing-firefox-new-site-isolation-security-architecture/\"><span style=\"font-weight: 400;\">Fission</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://hacks.mozilla.org/2021/12/webassembly-and-back-again-fine-grained-sandboxing-in-firefox-95/\"><span style=\"font-weight: 400;\">RLBox</span></a><span style=\"font-weight: 400;\">) that shipped before, this completes a sequence of large leaps forward that will significantly improve Firefox&#8217;s security.</span></p>\n<p><span style=\"font-weight: 400;\">Although </span><i><span style=\"font-weight: 400;\">Win32k Lockdown</span></i><span style=\"font-weight: 400;\"> is a Windows-specific technique, it became possible because of a significant re-architecting of the Firefox security boundaries that Mozilla has been working on for around four years, which allowed similar security advances to be made on other operating systems.</span></p>\n<h2><strong>The Goal: Win32k Lockdown</strong></h2>\n<p><span style=\"font-weight: 400;\">Firefox runs the processes that render web content with quite a few restrictions on what they are allowed to do when running on Windows. Unfortunately, by default they still have access to the entire Windows API, which opens up a large attack surface: the Windows API consists of many parts, for example, a core part dealing with threads, processes, and memory management, but also networking and socket libraries, printing and multimedia APIs, and so on.</span></p>\n<p><span style=\"font-weight: 400;\">Of particular interest for us is the </span><i><span style=\"font-weight: 400;\">win32k.sys API,</span></i><span style=\"font-weight: 400;\"> which includes many graphical and widget related system calls that have a history of being exploitable. Going back further in Windows&#8217; origins, this situation is likely the result of Microsoft moving many operations that were originally running in user mode into the kernel in order to improve performance around the Windows 95 and NT4 timeframe. </span></p>\n<p><span style=\"font-weight: 400;\">Having likely never been originally designed to run in this sensitive context, these APIs have been a traditional target for hackers to break out of application sandboxes and into the kernel.</span></p>\n<p><span style=\"font-weight: 400;\">In Windows 8, Microsoft introduced a new mitigation named </span><a href=\"https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-process_mitigation_system_call_disable_policy\"><span style=\"font-weight: 400;\">PROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY</span></a><span style=\"font-weight: 400;\"> that an application can use to disable access to win32k.sys system calls. That is a long name to keep repeating, so we&#8217;ll refer to it hereafter by our internal designation: &#8220;</span><i><span style=\"font-weight: 400;\">Win32k Lockdown</span></i><span style=\"font-weight: 400;\">&#8220;.</span></p>\n<h2><strong>The Work Required</strong></h2>\n<p><span style=\"font-weight: 400;\">Flipping the Win32k Lockdown flag on the Web Content processes &#8211; the processes most vulnerable to potentially hostile web pages and JavaScript &#8211; means that those processes can no longer perform any graphical, window management, input processing, etc. operations themselves. </span></p>\n<p><span style=\"font-weight: 400;\">To accomplish these tasks, such operations must be remoted to a process that has the necessary permissions, typically the process that has access to the GPU and handles compositing and drawing (hereafter called the GPU Process), or the privileged parent process. </span></p>\n<h3><span style=\"font-weight: 400;\">Drawing web pages: WebRender</span></h3>\n<p><span style=\"font-weight: 400;\">For painting the web pages&#8217; contents, Firefox historically used various methods for interacting with the Windows APIs, ranging from using modern Direct3D based textures, to falling back to GDI surfaces, and eventually dropping into pure software mode. </span></p>\n<p><span style=\"font-weight: 400;\">These different options would have taken quite some work to remote, as most of the graphics API is off limits in Win32k Lockdown. The good news is that as of Firefox 92, our rendering stack has switched to </span><a href=\"https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/\"><span style=\"font-weight: 400;\">WebRender</span></a><span style=\"font-weight: 400;\">, which moves all the actual drawing from the content processes to WebRender in the GPU Process.</span></p>\n<p><span style=\"font-weight: 400;\">Because with WebRender the content process no longer has a need to directly interact with the platform drawing APIs, this avoids any Win32k Lockdown related problems. WebRender itself has been designed partially to be </span><a href=\"https://github.com/servo/webrender/wiki/\"><span style=\"font-weight: 400;\">more similar to game engines, and thus, be less susceptible to driver bugs</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">For the remaining drivers that are just too broken to be of any use, it still has a </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1601053\"><span style=\"font-weight: 400;\">fully software-based mode</span></a><span style=\"font-weight: 400;\">, which means we have no further fallbacks to consider.</span></p>\n<h3><span style=\"font-weight: 400;\">Webpages drawing: Canvas 2D and WebGL 3D</span></h3>\n<p><span style=\"font-weight: 400;\">The </span><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API\"><span style=\"font-weight: 400;\">Canvas API</span></a><span style=\"font-weight: 400;\"> provides web pages with the ability to draw 2D graphics. In the original Firefox implementation, these JavaScript APIs were executed in the Web Content processes and the calls to the Windows drawing APIs were made directly from the same processes. </span></p>\n<p><span style=\"font-weight: 400;\">In a Win32k Lockdown scenario, this is no longer possible, so </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1464032\"><span style=\"font-weight: 400;\">all drawing commands are remoted</span></a><span style=\"font-weight: 400;\"> by recording and playing them back in the GPU process over IPC.</span></p>\n<p><span style=\"font-weight: 400;\">Although the initial implementation had good performance, there were nevertheless reports from some sites that experienced performance regressions (the web sites that became faster generally didn&#8217;t complain!). A particular pain point are applications that call </span><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/getImageData\"><span style=\"font-weight: 400;\">getImageData()</span></a><span style=\"font-weight: 400;\"> repeatedly: having the Canvas remoted means that GPU textures must now be obtained from another process and sent over IPC. </span></p>\n<p><span style=\"font-weight: 400;\">We compensated for this in the scenario where getImageData is called at the start of a frame, by detecting this and preparing the right surfaces proactively to make the copying from the GPU faster.</span></p>\n<p><span style=\"font-weight: 400;\">Besides the Canvas API to draw 2D graphics, the web platform also exposes an </span><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API\"><span style=\"font-weight: 400;\">API to do 3D drawing, called WebGL</span></a><span style=\"font-weight: 400;\">. WebGL is a state-heavy API, so properly and efficiently synchronizing child and parent (as well as parent and driver) takes </span><a href=\"https://phabricator.services.mozilla.com/D54019\"><span style=\"font-weight: 400;\">great</span></a> <a href=\"https://phabricator.services.mozilla.com/D54019\"><span style=\"font-weight: 400;\">care</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">WebGL originally handled all validation in Content, but with access to the GPU and the associated attack surface removed from there, we needed to craft a robust validating API between child and parent as well to get the full security benefit.</span></p>\n<h3><span style=\"font-weight: 400;\">(Non-)Native Theming for Forms</span></h3>\n<p><span style=\"font-weight: 400;\">HTML web pages have the ability to display form controls. While the overwhelming majority of websites provide a </span><a href=\"https://developer.mozilla.org/en-US/docs/Learn/Forms/Advanced_form_styling\"><span style=\"font-weight: 400;\">custom look and styling for those form controls</span></a><span style=\"font-weight: 400;\">, not all of them do, and if they do not you get an input GUI widget that is styled like (and originally was!) a </span><a href=\"http://stephenhorlander.com/form-controls.html\"><span style=\"font-weight: 400;\">native element of the operating system</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\"> Historically, these were drawn by calling the appropriate OS widget APIs from within the content process, but those are not available under Win32k Lockdown. </span></p>\n<p><span style=\"font-weight: 400;\">This cannot easily be fixed by remoting the calls, as the widgets themselves come in an infinite amount of sizes, shapes, and styles can be interacted with, and need to be responsive to user input and dispatch messages. We settled on having Firefox </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1381938\"><span style=\"font-weight: 400;\">draw the form controls itself</span></a><span style=\"font-weight: 400;\">, in a cross-platform style. </span></p>\n<p><span style=\"font-weight: 400;\">While changing the look of form controls has web compatibility implications, and some people prefer the more native look &#8211; on the few pages that don&#8217;t apply their own styles to controls &#8211; Firefox’s approach is consistent with that taken by other browsers, probably because of very similar considerations.</span></p>\n<p><span style=\"font-weight: 400;\">Scrollbars were a particular pain point: we didn&#8217;t want to draw the main scrollbar of the content window in a different manner as the rest of the UX, since nested scrollbars would show up with different styles which would look awkward. But, unlike the rather rare non-styled form widgets, the main scrollbar is visible on most web pages, and because it conceptually belongs to the browser UX we really wanted it to look native. </span></p>\n<p><span style=\"font-weight: 400;\">We, therefore, decided to draw all scrollbars to match the system theme, although it&#8217;s a bit of an open question though how things should look if even </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1719427#c3\"><span style=\"font-weight: 400;\">the vendor of the operating system can&#8217;t seem to decide what the &#8220;native&#8221; look is</span></a><span style=\"font-weight: 400;\">.</span></p>\n<h2><strong>Final Hurdles</strong></h2>\n<h3><span style=\"font-weight: 400;\">Line Breaking</span></h3>\n<p><span style=\"font-weight: 400;\">With the above changes, we thought we had all the usual suspects that would access graphics and widget APIs in win32k.sys wrapped up, so we started running the full Firefox test suite with win32k syscalls disabled. This caused at least one unexpected failure: Firefox was </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1713973\"><span style=\"font-weight: 400;\">crashing when trying to find line breaks</span></a><span style=\"font-weight: 400;\"> for some languages with complex scripts. </span></p>\n<p><span style=\"font-weight: 400;\">While Firefox is able to correctly determine word endings in multibyte character streams for most languages by itself, the support for Thai, Lao, Tibetan and Khmer is known to be imperfect, and </span><a href=\"https://searchfox.org/mozilla-central/rev/80f11ac5d938f6fce255c56279f46f13a49ea5c3/intl/lwbrk/LineBreaker.h#65\"><span style=\"font-weight: 400;\">in these cases, Firefox can ask the operating system to handle the line breaking</span></a><span style=\"font-weight: 400;\"> for it. But at least on Windows, the functions to do so are covered by the Win32k Lockdown switch. Oops!</span></p>\n<p><span style=\"font-weight: 400;\">There are </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1684927\"><span style=\"font-weight: 400;\">efforts underway to incorporate ICU4X</span></a><span style=\"font-weight: 400;\"> and base all i18n related functionality on that, meaning that Firefox will be able to handle all scripts perfectly without involving the OS, but this is a major effort and it was not clear if it would end up delaying the rollout of win32k lockdown. </span></p>\n<p><span style=\"font-weight: 400;\">We did some experimentation with trying to forward the line breaking over IPC. Initially, this had bad performance, but when we </span><a href=\"https://phabricator.services.mozilla.com/D129125\"><span style=\"font-weight: 400;\">added caching</span></a><span style=\"font-weight: 400;\"> performance was satisfactory or sometimes even improved, since OS calls could be avoided in many cases now.</span></p>\n<h3><span style=\"font-weight: 400;\">DLL Loading &amp; Third Party Interactions</span></h3>\n<p><span style=\"font-weight: 400;\">Another complexity of disabling win32k.sys access is that so much Windows functionality assumes it is available by default, and specific effort must be taken to ensure the relevant DLLs do not get loaded on startup. Firefox itself for example won&#8217;t load the user32 DLL containing some win32k APIs, but </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1719212\"><span style=\"font-weight: 400;\">injected third party DLLs sometimes do</span></a><span style=\"font-weight: 400;\">. This causes problems because </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1751367\"><span style=\"font-weight: 400;\">COM initialization in particular uses win32k calls to get the Window Station and Desktop</span></a><span style=\"font-weight: 400;\"> if the DLL is present. Those calls will fail with Win32k Lockdown enabled, silently breaking COM and features that depend on it such as our accessibility support. </span></p>\n<p><span style=\"font-weight: 400;\">On Windows 10 Fall Creators Update and later we have a fix that blocks these calls and forces a fallback, which keeps everything working nicely. We measured that </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1750742#c9\"><span style=\"font-weight: 400;\">not loading the DLLs causes about a 15% performance gain</span></a><span style=\"font-weight: 400;\"> when opening new tabs, adding a nice performance bonus on top of the security benefit.</span></p>\n<h3><span style=\"font-weight: 400;\">Remaining Work</span></h3>\n<p><span style=\"font-weight: 400;\">As hinted in the previous section, Win32k Lockdown will initially roll out on Windows 10 Fall Creators Update and later. On Windows 8, and unpatched Windows 10 (which unfortunately seems to be in use!), we are still testing a fix for the case where third party DLLs interfere, so support for those will come in a </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1759167#c7\"><span style=\"font-weight: 400;\">future release</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">For Canvas 2D support, we&#8217;re still </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1766402\"><span style=\"font-weight: 400;\">looking into improving the performance of applications</span></a><span style=\"font-weight: 400;\"> that regressed when the processes were switched around. Simultaneously, there is experimentation underway to see if hardware acceleration for </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1739448\"><span style=\"font-weight: 400;\">Canvas 2D can be implemented through WebGL</span></a><span style=\"font-weight: 400;\">, which would increase code sharing between the 2D and 3D implementations and take advantage of modern video drivers being better optimized for the 3D case.</span></p>\n<h2><strong>Conclusion</strong></h2>\n<p><span style=\"font-weight: 400;\">Retrofitting a significant change in the separation of responsibilities in a large application like Firefox presents a large, multi-year engineering challenge, but it is absolutely required in order to advance browser security and to continue keeping our users safe. We&#8217;re pleased to have made it through and present you with the result in Firefox 100.</span></p>\n<h3><span style=\"font-weight: 400;\">Other Platforms</span></h3>\n<p><span style=\"font-weight: 400;\">If you&#8217;re a Mac user, you might wonder if there’s anything similar to Win32k Lockdown that can be done for macOS. You&#8217;d be right, and I have good news for you: we already quietly </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1467758\"><span style=\"font-weight: 400;\">shipped the changes that block access to the WindowServer</span></a><span style=\"font-weight: 400;\"> in Firefox 95, improving security and speeding process startup by about 30-70%. This too became possible because of the Remote WebGL and Non-Native Theming work described above.</span></p>\n<p><span style=\"font-weight: 400;\">For Linux users, </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1129492\"><span style=\"font-weight: 400;\">we removed the connection from content processes to the X11 Server</span></a><span style=\"font-weight: 400;\">, which stops attackers from exploiting the unsecured X11 protocol. Although Linux distributions have been moving towards the more secure Wayland protocol as the default, we still see a lot of users that are using X11 or XWayland configurations, so this is definitely a nice-to-have, which shipped in Firefox 99.</span></p>\n<h2><strong>We&#8217;re Hiring</strong></h2>\n<p><span style=\"font-weight: 400;\">If you found the technical background story above fascinating, I&#8217;d like to point out that our OS Integration &amp; Hardening team is going to be hiring soon. We&#8217;re especially looking for experienced C++ programmers with some interest in Rust and in-depth knowledge of Windows programming. </span></p>\n<p><span style=\"font-weight: 400;\">If you fit this description and are interested in taking the next leap in Firefox security together with us, </span><a href=\"https://www.mozilla.org/en-US/careers/\"><span style=\"font-weight: 400;\">we&#8217;d encourage you to keep an eye on our careers page</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><i>Thanks to Bob Owen, Chris Martin, and Stephen Pohl for their technical input to this article, and for all the heavy lifting they did together with Kelsey Gilbert and Jed Davis to make these security improvements ship.<br />\n</i></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/\">Improved Process Isolation in Firefox 100</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Introduction\nFirefox uses a multi-process model for additional security and stability while browsing: Web Content (such as HTML/CSS and Javascript) is rendered in separate processes that are isolated from the rest of the operating system and managed by a privileged parent process. This way, the amount of control gained by an attacker that exploits a bug in a content process is limited. \nEver since we deployed this model, we have been working on improving the isolation of the content processes to further limit the attack surface. This is a challenging task since content processes need access to some operating system APIs to properly function: for example, they still need to be able to talk to the parent process. \nIn this article, we would like to dive a bit further into the latest major milestone we have reached: Win32k Lockdown, which greatly reduces the capabilities of the content process when running on Windows. Together with two major earlier efforts (Fission and RLBox) that shipped before, this completes a sequence of large leaps forward that will significantly improve Firefox’s security.\nAlthough Win32k Lockdown is a Windows-specific technique, it became possible because of a significant re-architecting of the Firefox security boundaries that Mozilla has been working on for around four years, which allowed similar security advances to be made on other operating systems.\nThe Goal: Win32k Lockdown\nFirefox runs the processes that render web content with quite a few restrictions on what they are allowed to do when running on Windows. Unfortunately, by default they still have access to the entire Windows API, which opens up a large attack surface: the Windows API consists of many parts, for example, a core part dealing with threads, processes, and memory management, but also networking and socket libraries, printing and multimedia APIs, and so on.\nOf particular interest for us is the win32k.sys API, which includes many graphical and widget related system calls that have a history of being exploitable. Going back further in Windows’ origins, this situation is likely the result of Microsoft moving many operations that were originally running in user mode into the kernel in order to improve performance around the Windows 95 and NT4 timeframe. \nHaving likely never been originally designed to run in this sensitive context, these APIs have been a traditional target for hackers to break out of application sandboxes and into the kernel.\nIn Windows 8, Microsoft introduced a new mitigation named PROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY that an application can use to disable access to win32k.sys system calls. That is a long name to keep repeating, so we’ll refer to it hereafter by our internal designation: “Win32k Lockdown“.\nThe Work Required\nFlipping the Win32k Lockdown flag on the Web Content processes – the processes most vulnerable to potentially hostile web pages and JavaScript – means that those processes can no longer perform any graphical, window management, input processing, etc. operations themselves. \nTo accomplish these tasks, such operations must be remoted to a process that has the necessary permissions, typically the process that has access to the GPU and handles compositing and drawing (hereafter called the GPU Process), or the privileged parent process. \nDrawing web pages: WebRender\nFor painting the web pages’ contents, Firefox historically used various methods for interacting with the Windows APIs, ranging from using modern Direct3D based textures, to falling back to GDI surfaces, and eventually dropping into pure software mode. \nThese different options would have taken quite some work to remote, as most of the graphics API is off limits in Win32k Lockdown. The good news is that as of Firefox 92, our rendering stack has switched to WebRender, which moves all the actual drawing from the content processes to WebRender in the GPU Process.\nBecause with WebRender the content process no longer has a need to directly interact with the platform drawing APIs, this avoids any Win32k Lockdown related problems. WebRender itself has been designed partially to be more similar to game engines, and thus, be less susceptible to driver bugs. \nFor the remaining drivers that are just too broken to be of any use, it still has a fully software-based mode, which means we have no further fallbacks to consider.\nWebpages drawing: Canvas 2D and WebGL 3D\nThe Canvas API provides web pages with the ability to draw 2D graphics. In the original Firefox implementation, these JavaScript APIs were executed in the Web Content processes and the calls to the Windows drawing APIs were made directly from the same processes. \nIn a Win32k Lockdown scenario, this is no longer possible, so all drawing commands are remoted by recording and playing them back in the GPU process over IPC.\nAlthough the initial implementation had good performance, there were nevertheless reports from some sites that experienced performance regressions (the web sites that became faster generally didn’t complain!). A particular pain point are applications that call getImageData() repeatedly: having the Canvas remoted means that GPU textures must now be obtained from another process and sent over IPC. \nWe compensated for this in the scenario where getImageData is called at the start of a frame, by detecting this and preparing the right surfaces proactively to make the copying from the GPU faster.\nBesides the Canvas API to draw 2D graphics, the web platform also exposes an API to do 3D drawing, called WebGL. WebGL is a state-heavy API, so properly and efficiently synchronizing child and parent (as well as parent and driver) takes great care. \nWebGL originally handled all validation in Content, but with access to the GPU and the associated attack surface removed from there, we needed to craft a robust validating API between child and parent as well to get the full security benefit.\n(Non-)Native Theming for Forms\nHTML web pages have the ability to display form controls. While the overwhelming majority of websites provide a custom look and styling for those form controls, not all of them do, and if they do not you get an input GUI widget that is styled like (and originally was!) a native element of the operating system.\n Historically, these were drawn by calling the appropriate OS widget APIs from within the content process, but those are not available under Win32k Lockdown. \nThis cannot easily be fixed by remoting the calls, as the widgets themselves come in an infinite amount of sizes, shapes, and styles can be interacted with, and need to be responsive to user input and dispatch messages. We settled on having Firefox draw the form controls itself, in a cross-platform style. \nWhile changing the look of form controls has web compatibility implications, and some people prefer the more native look – on the few pages that don’t apply their own styles to controls – Firefox’s approach is consistent with that taken by other browsers, probably because of very similar considerations.\nScrollbars were a particular pain point: we didn’t want to draw the main scrollbar of the content window in a different manner as the rest of the UX, since nested scrollbars would show up with different styles which would look awkward. But, unlike the rather rare non-styled form widgets, the main scrollbar is visible on most web pages, and because it conceptually belongs to the browser UX we really wanted it to look native. \nWe, therefore, decided to draw all scrollbars to match the system theme, although it’s a bit of an open question though how things should look if even the vendor of the operating system can’t seem to decide what the “native” look is.\nFinal Hurdles\nLine Breaking\nWith the above changes, we thought we had all the usual suspects that would access graphics and widget APIs in win32k.sys wrapped up, so we started running the full Firefox test suite with win32k syscalls disabled. This caused at least one unexpected failure: Firefox was crashing when trying to find line breaks for some languages with complex scripts. \nWhile Firefox is able to correctly determine word endings in multibyte character streams for most languages by itself, the support for Thai, Lao, Tibetan and Khmer is known to be imperfect, and in these cases, Firefox can ask the operating system to handle the line breaking for it. But at least on Windows, the functions to do so are covered by the Win32k Lockdown switch. Oops!\nThere are efforts underway to incorporate ICU4X and base all i18n related functionality on that, meaning that Firefox will be able to handle all scripts perfectly without involving the OS, but this is a major effort and it was not clear if it would end up delaying the rollout of win32k lockdown. \nWe did some experimentation with trying to forward the line breaking over IPC. Initially, this had bad performance, but when we added caching performance was satisfactory or sometimes even improved, since OS calls could be avoided in many cases now.\nDLL Loading & Third Party Interactions\nAnother complexity of disabling win32k.sys access is that so much Windows functionality assumes it is available by default, and specific effort must be taken to ensure the relevant DLLs do not get loaded on startup. Firefox itself for example won’t load the user32 DLL containing some win32k APIs, but injected third party DLLs sometimes do. This causes problems because COM initialization in particular uses win32k calls to get the Window Station and Desktop if the DLL is present. Those calls will fail with Win32k Lockdown enabled, silently breaking COM and features that depend on it such as our accessibility support. \nOn Windows 10 Fall Creators Update and later we have a fix that blocks these calls and forces a fallback, which keeps everything working nicely. We measured that not loading the DLLs causes about a 15% performance gain when opening new tabs, adding a nice performance bonus on top of the security benefit.\nRemaining Work\nAs hinted in the previous section, Win32k Lockdown will initially roll out on Windows 10 Fall Creators Update and later. On Windows 8, and unpatched Windows 10 (which unfortunately seems to be in use!), we are still testing a fix for the case where third party DLLs interfere, so support for those will come in a future release.\nFor Canvas 2D support, we’re still looking into improving the performance of applications that regressed when the processes were switched around. Simultaneously, there is experimentation underway to see if hardware acceleration for Canvas 2D can be implemented through WebGL, which would increase code sharing between the 2D and 3D implementations and take advantage of modern video drivers being better optimized for the 3D case.\nConclusion\nRetrofitting a significant change in the separation of responsibilities in a large application like Firefox presents a large, multi-year engineering challenge, but it is absolutely required in order to advance browser security and to continue keeping our users safe. We’re pleased to have made it through and present you with the result in Firefox 100.\nOther Platforms\nIf you’re a Mac user, you might wonder if there’s anything similar to Win32k Lockdown that can be done for macOS. You’d be right, and I have good news for you: we already quietly shipped the changes that block access to the WindowServer in Firefox 95, improving security and speeding process startup by about 30-70%. This too became possible because of the Remote WebGL and Non-Native Theming work described above.\nFor Linux users, we removed the connection from content processes to the X11 Server, which stops attackers from exploiting the unsecured X11 protocol. Although Linux distributions have been moving towards the more secure Wayland protocol as the default, we still see a lot of users that are using X11 or XWayland configurations, so this is definitely a nice-to-have, which shipped in Firefox 99.\nWe’re Hiring\nIf you found the technical background story above fascinating, I’d like to point out that our OS Integration & Hardening team is going to be hiring soon. We’re especially looking for experienced C++ programmers with some interest in Rust and in-depth knowledge of Windows programming. \nIf you fit this description and are interested in taking the next leap in Firefox security together with us, we’d encourage you to keep an eye on our careers page.\nThanks to Bob Owen, Chris Martin, and Stephen Pohl for their technical input to this article, and for all the heavy lifting they did together with Kelsey Gilbert and Jed Davis to make these security improvements ship.\n\nThe post Improved Process Isolation in Firefox 100 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-05-12T15:09:10.000Z",
      "date_modified": "2022-05-12T15:09:10.000Z",
      "_plugin": {
        "pageFilename": "64bc2bbad8d083c3a5b290ad920b41ccc5758f292f8372e36adecd7010efd70c.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47798",
      "url": "https://hacks.mozilla.org/2022/04/common-voice-dataset-tops-20000-hours/",
      "title": "Common Voice dataset tops 20,000 hours",
      "summary": "The latest Common Voice dataset, released today, has achieved a major milestone: More than 20,000 hours of open-source speech data that anyone, anywhere can use. The dataset has nearly doubled in the past year. Mozilla’s Common Voice seeks to change the language technology ecosystem by supporting communities to collect voice data for the creation of voice-enabled applications for their own languages. \nThe post Common Voice dataset tops 20,000 hours appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">The latest Common Voice dataset, released today, has achieved a major milestone: More than 20,000 hours of open-source speech data that anyone, anywhere can use. The dataset has nearly doubled in the past year.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47799 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/common-voice.png\" alt=\"\" width=\"512\" height=\"269\" srcset=\"https://hacks.mozilla.org/files/2022/04/common-voice.png 512w, https://hacks.mozilla.org/files/2022/04/common-voice-250x131.png 250w, https://hacks.mozilla.org/files/2022/04/common-voice-500x263.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<h2><b>Why should you care about Common Voice?</b></h2>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Do you have to change your accent to be understood by a virtual assistant? </span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Are you worried that so many voice-operated devices are collecting your voice data for proprietary Big Tech datasets?</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Are automatic subtitles unavailable for you in your language?</span></li>\n</ul>\n<p><span style=\"font-weight: 400;\">Automatic Speech Recognition plays an important role in the way we can access information, however, of the 7,000 languages spoken globally today only a handful are supported by most products.</span></p>\n<p><a href=\"https://commonvoice.mozilla.org/\"><span style=\"font-weight: 400;\">Mozilla’s Common Voice</span></a><span style=\"font-weight: 400;\"> seeks to change the language technology ecosystem by supporting communities to collect voice data for the creation of voice-enabled applications for their own languages. </span></p>\n<h2><b>Common Voice Dataset Release </b></h2>\n<p><span style=\"font-weight: 400;\">This release wouldn’t be possible without our contributors — from voice donations to initiating their language in our project, to opening new opportunities for people to build voice technology tools that can support every language spoken across the world.</span></p>\n<p><span style=\"font-weight: 400;\">Access the dataset:</span><a href=\"https://commonvoice.mozilla.org/datasets\"><span style=\"font-weight: 400;\"> https://commonvoice.mozilla.org/datasets</span></a></p>\n<p><span style=\"font-weight: 400;\">Access the metadata: </span><a href=\"https://github.com/common-voice/cv-dataset\"><span style=\"font-weight: 400;\">https://github.com/common-voice/cv-dataset</span></a><span style=\"font-weight: 400;\"> </span></p>\n<h3><b>Highlights from the latest dataset:</b><b></b></h3>\n<ul>\n<li aria-level=\"1\"><b>The new release also features six new languages:</b><span style=\"font-weight: 400;\"> Tigre, Taiwanese (Minnan), Meadow Mari, Bengali, Toki Pona and Cantonese.</span></li>\n<li aria-level=\"1\"><b>Twenty-seven languages now have at least 100 hours of speech data. </b><span style=\"font-weight: 400;\">They include Bengali, Thai, Basque, and Frisian.</span></li>\n<li aria-level=\"1\"><b>Nine languages now have at least 500 hours of speech data. </b><span style=\"font-weight: 400;\">They include Kinyarwanda (2,383 hours), Catalan (2,045 hours), and Swahili (719 hours).</span></li>\n<li aria-level=\"1\"><b>Nine languages now all have at least 45% of their gender tags as female. </b><span style=\"font-weight: 400;\">They include Marathi, Dhivehi, and Luganda.</span></li>\n<li aria-level=\"1\"><b>The Catalan community fueled major growth.</b><span style=\"font-weight: 400;\"> The Catalan community&#8217;s</span><a href=\"https://www.projecteaina.cat/\"> <span style=\"font-weight: 400;\">Project AINA</span></a> <span style=\"font-weight: 400;\">— a collaboration between Barcelona Supercomputing Center and the Catalan Government — mobilized Catalan speakers to contribute to Common Voice. </span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><b>Supporting community participation in decision making yet.</b><span style=\"font-weight: 400;\"> The </span><a href=\"https://foundation.mozilla.org/en/blog/introducing-cvlr-20212022/\"><span style=\"font-weight: 400;\">Common Voice language Rep Cohort</span></a><span style=\"font-weight: 400;\"> has contributed feedback and learnings about optimal sentence collection, the inclusion of language variants, and more. </span></li>\n</ul>\n<h2><b> Create with the Dataset </b></h2>\n<p><span style=\"font-weight: 400;\">How will you create with the Common Voice Dataset?</span></p>\n<p><span style=\"font-weight: 400;\">Take some inspiration from technologists who are creating conversational chatbots, spoken language identifiers, research papers and virtual assistants with the Common Voice Dataset by watching this talk: </span></p>\n<p><a href=\"https://mozilla.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6492f3ae-3a0d-4363-99f6-adc00111b706\"><span style=\"font-weight: 400;\">https://mozilla.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6492f3ae-3a0d-4363-99f6-adc00111b706</span></a><span style=\"font-weight: 400;\"> </span></p>\n<p><span style=\"font-weight: 400;\">Share with us how you are using the dataset on social media using #CommonVoice or sharing on </span><a href=\"https://discourse.mozilla.org/c/voice/using/661\"><span style=\"font-weight: 400;\">our Community discourse.</span></a><span style=\"font-weight: 400;\"> </span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/common-voice-dataset-tops-20000-hours/\">Common Voice dataset tops 20,000 hours</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "The latest Common Voice dataset, released today, has achieved a major milestone: More than 20,000 hours of open-source speech data that anyone, anywhere can use. The dataset has nearly doubled in the past year.\n\nWhy should you care about Common Voice?\n\nDo you have to change your accent to be understood by a virtual assistant? \nAre you worried that so many voice-operated devices are collecting your voice data for proprietary Big Tech datasets?\nAre automatic subtitles unavailable for you in your language?\n\nAutomatic Speech Recognition plays an important role in the way we can access information, however, of the 7,000 languages spoken globally today only a handful are supported by most products.\nMozilla’s Common Voice seeks to change the language technology ecosystem by supporting communities to collect voice data for the creation of voice-enabled applications for their own languages. \nCommon Voice Dataset Release \nThis release wouldn’t be possible without our contributors — from voice donations to initiating their language in our project, to opening new opportunities for people to build voice technology tools that can support every language spoken across the world.\nAccess the dataset: https://commonvoice.mozilla.org/datasets\nAccess the metadata: https://github.com/common-voice/cv-dataset \nHighlights from the latest dataset:\n\nThe new release also features six new languages: Tigre, Taiwanese (Minnan), Meadow Mari, Bengali, Toki Pona and Cantonese.\nTwenty-seven languages now have at least 100 hours of speech data. They include Bengali, Thai, Basque, and Frisian.\nNine languages now have at least 500 hours of speech data. They include Kinyarwanda (2,383 hours), Catalan (2,045 hours), and Swahili (719 hours).\nNine languages now all have at least 45% of their gender tags as female. They include Marathi, Dhivehi, and Luganda.\nThe Catalan community fueled major growth. The Catalan community’s Project AINA — a collaboration between Barcelona Supercomputing Center and the Catalan Government — mobilized Catalan speakers to contribute to Common Voice. \nSupporting community participation in decision making yet. The Common Voice language Rep Cohort has contributed feedback and learnings about optimal sentence collection, the inclusion of language variants, and more. \n\n Create with the Dataset \nHow will you create with the Common Voice Dataset?\nTake some inspiration from technologists who are creating conversational chatbots, spoken language identifiers, research papers and virtual assistants with the Common Voice Dataset by watching this talk: \nhttps://mozilla.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6492f3ae-3a0d-4363-99f6-adc00111b706 \nShare with us how you are using the dataset on social media using #CommonVoice or sharing on our Community discourse. \n \nThe post Common Voice dataset tops 20,000 hours appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-04-28T15:23:57.000Z",
      "date_modified": "2022-04-28T15:23:57.000Z",
      "_plugin": {
        "pageFilename": "a162175b480bc35638a3b7f7266812ac45694b30c508bd43b0956c780514ae63.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47805",
      "url": "https://hacks.mozilla.org/2022/04/mdn-plus-now-available-in-more-markets/",
      "title": "MDN Plus now available in more countries",
      "summary": "Almost a month ago, we announced MDN Plus, a new premium service on MDN that allows users to customize their experience on the website.\nWe are very glad to announce today that it is now possible for MDN users around the globe to create an MDN Plus free account, no matter where they are.\nThe post MDN Plus now available in more countries appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Almost a month ago, we announced MDN Plus, a new premium service on MDN that allows users to customize their experience on the website.</p>\n<p>We are very glad to announce today that it is now possible for MDN users around the globe to create an MDN Plus free account, no matter where they are.</p>\n<p>Click <a href=\"https://developer.mozilla.org/en-US/plus#subscribe\">here</a> to create an MDN Plus free account*.</p>\n<p>Also starting today, the premium version of the service will be available in 16 more countries: Austria, Belgium, Finland, France, United Kingdom, Germany, Ireland, Italy, Malaysia, the Netherlands, New Zealand, Puerto Rico, Sweden, Singapore, Switzerland, and Spain. We continue to work towards expanding this list even further.</p>\n<p>Click<a href=\"https://developer.mozilla.org/en-US/plus#subscribe\"> here</a> to create an MDN Plus premium account**.</p>\n<p><span style=\"font-weight: 400;\">* Now available to everyone</span></p>\n<p>** You will need to subscribe from one of the countries mentioned above to be able to have an MDN Plus premium account at this time</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/mdn-plus-now-available-in-more-markets/\">MDN Plus now available in more countries</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Almost a month ago, we announced MDN Plus, a new premium service on MDN that allows users to customize their experience on the website.\nWe are very glad to announce today that it is now possible for MDN users around the globe to create an MDN Plus free account, no matter where they are.\nClick here to create an MDN Plus free account*.\nAlso starting today, the premium version of the service will be available in 16 more countries: Austria, Belgium, Finland, France, United Kingdom, Germany, Ireland, Italy, Malaysia, the Netherlands, New Zealand, Puerto Rico, Sweden, Singapore, Switzerland, and Spain. We continue to work towards expanding this list even further.\nClick here to create an MDN Plus premium account**.\n* Now available to everyone\n** You will need to subscribe from one of the countries mentioned above to be able to have an MDN Plus premium account at this time\nThe post MDN Plus now available in more countries appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-04-28T10:05:35.000Z",
      "date_modified": "2022-04-28T10:05:35.000Z",
      "_plugin": {
        "pageFilename": "d350c9187bac22b922079ff4295dffe249cbbd3c67e2bbaba334bc4ba3e32823.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47784",
      "url": "https://hacks.mozilla.org/2022/04/adopting-users-design-feedback/",
      "title": "Adopting users’ design feedback",
      "summary": "On March 1st, 2022, MDN Web Docs released a new design and a new brand identity. Overall, the community responded to the redesign enthusiastically and we received many positive messages and kudos. We also received valuable feedback on some of the things we didn’t get quite right, like the browser compatibility table changes as well as some accessibility and readability issues.\nThe post Adopting users’ design feedback appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">On March 1st, 2022, MDN Web Docs released a new design and a new brand identity. Overall, the community responded to the </span><a href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\"><span style=\"font-weight: 400;\">redesign</span></a><span style=\"font-weight: 400;\"> enthusiastically and we received many positive messages and kudos. We also received valuable feedback on some of the things we didn’t get quite right, like the browser compatibility table changes as well as some accessibility and readability issues.</span></p>\n<p><span style=\"font-weight: 400;\">For us, MDN Web Docs has always been synonymous with the term Ubuntu, </span><em>“I am because we are.”</em><span style=\"font-weight: 400;\"> Translated in this context, “MDN Web Docs is the amazing resource it is because of our community’s support, feedback, and contributions.”</span></p>\n<p><span style=\"font-weight: 400;\"> Since the initial launch of the </span><a href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\"><span style=\"font-weight: 400;\">redesign</span></a><span style=\"font-weight: 400;\"> and of </span><a href=\"https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/\"><span style=\"font-weight: 400;\">MDN Plus</span></a><span style=\"font-weight: 400;\"> afterwards, we have been humbled and overwhelmed by the level of support we received from our community of readers. We do our best to listen to what you have to say and to act on suggestions so that together, we make MDN better. </span></p>\n<p><span style=\"font-weight: 400;\">Here is a summary of how we went about addressing the feedback we received.</span></p>\n<p><span style=\"font-weight: 400;\">Eight days after the redesign launch, we started the </span><i><span style=\"font-weight: 400;\">MDN Web Docs Readability Project</span></i><span style=\"font-weight: 400;\">. Our first task was to triage all issues submitted by the community that related to readability and accessibility on MDN Web Docs. Next up, we identified common themes and </span><a href=\"https://github.com/mdn/yari/issues/5546\"><span style=\"font-weight: 400;\">collected them in this meta issue</span></a><span style=\"font-weight: 400;\">. Over time, this grew into 27 unique issues and several related discussions and </span><span style=\"font-weight: 400;\">comments. We collected feedback on GitHub and also from our communities on</span> <a href=\"https://twitter.com/MozDevNet\"><span style=\"font-weight: 400;\">Twitter</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://matrix.to/#/%23mdn:mozilla.org\"><span style=\"font-weight: 400;\">Matrix</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">With the main pain points identified, we </span><a href=\"https://github.com/mdn/yari/discussions/5715\"><span style=\"font-weight: 400;\">opened a discussion on GitHub</span></a><span style=\"font-weight: 400;\">, inviting our readers to follow along and provide feedback on the changes as they were rolled out to a staging instance of the website. Today, roughly six weeks later, we are pleased to announce that all these changes are in production. This was not the effort of any one person but is made up of the work and contributions of people across staff and community.</span></p>\n<p><span style=\"font-weight: 400;\">Below are some of the highlights from this work.</span></p>\n<h2><strong>Dark mode</strong></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47789 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/mdn.png\" alt=\"\" width=\"512\" height=\"171\" srcset=\"https://hacks.mozilla.org/files/2022/04/mdn.png 512w, https://hacks.mozilla.org/files/2022/04/mdn-250x83.png 250w, https://hacks.mozilla.org/files/2022/04/mdn-500x167.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<p><span style=\"font-weight: 400;\">We updated the color palette used in dark mode in particular.</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reworked the initial color palette to use colors that are slightly</span><a href=\"https://github.com/mdn/yari/issues/5378\"><span style=\"font-weight: 400;\"> more subtle in dark mode</span></a><span style=\"font-weight: 400;\"> while ensuring that we still meet AA accessibility guidelines for color contrast.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reconsidered the darkness of the primary background color in dark mode and settled on a compromise that improved the experience for the majority of readers.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We </span><a href=\"https://github.com/mdn/yari/discussions/5715#discussioncomment-2457075\"><span style=\"font-weight: 400;\">cleaned up the notecards</span></a><span style=\"font-weight: 400;\"> that indicate notices such as warnings, experimental features, items not on the standards track, etc.</span></li>\n</ul>\n<h2><strong>Readability</strong></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47793 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/readibility.png\" alt=\"\" width=\"512\" height=\"171\" srcset=\"https://hacks.mozilla.org/files/2022/04/readibility.png 512w, https://hacks.mozilla.org/files/2022/04/readibility-250x83.png 250w, https://hacks.mozilla.org/files/2022/04/readibility-500x167.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<p><span style=\"font-weight: 400;\">We got a clear sense from some of our community folks that readers found it more difficult to skim content and find sections of interest after the redesign. To address these issues, we made the following improvements:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We implemented a clearly defined type-scale </span><a href=\"https://github.com/mdn/yari/issues/5546#issuecomment-1095192000\"><span style=\"font-weight: 400;\">adjusted for mobile</span></a><span style=\"font-weight: 400;\"> to optimize legibility and to effectively use space, especially on smaller screens.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We made the distinction</span><a href=\"https://github.com/mdn/yari/issues/5485\"><span style=\"font-weight: 400;\"> between the different heading levels</span></a><span style=\"font-weight: 400;\"> clearer.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We moved away from changing </span><a href=\"https://github.com/mdn/yari/issues/5755\"><span style=\"font-weight: 400;\">link colors across different areas</span></a><span style=\"font-weight: 400;\"> of MDN Web Docs. We still retain some of the intent of this design decision, but this is now more subtle.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">The font size was bumped up across all pages, </span><a href=\"https://github.com/mdn/yari/issues/5372\"><span style=\"font-weight: 400;\">including the home page</span></a><span style=\"font-weight: 400;\">. We have also optimized letter and line spacing for a more effortless reading experience. This has also improved the reading experience for our Asian readers, for whom </span><a href=\"https://github.com/mdn/yari/issues/5415\"><span style=\"font-weight: 400;\">line heights were much too tight</span></a><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Missing links are </span><a href=\"https://github.com/mdn/yari/issues/5906#issuecomment-1090400185\"><span style=\"font-weight: 400;\">clearly distinguishable</span></a><span style=\"font-weight: 400;\"> from other links and content.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We improved the layout and </span><a href=\"https://github.com/mdn/yari/issues/5632\"><span style=\"font-weight: 400;\">readability of specifications pages</span></a><span style=\"font-weight: 400;\"> across desktop and small screen devices.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We addressed a bug in how the highlighting in the table of contents worked and </span><a href=\"https://github.com/mdn/yari/pull/5852\"><span style=\"font-weight: 400;\">moved to use an IntersectionObserver</span></a><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We made styling for tables consistent across all pages.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">To ensure readers are always oriented regarding which page they are currently viewing, we have </span><a href=\"https://github.com/mdn/yari/issues/5521\"><span style=\"font-weight: 400;\">made the header (which includes the breadcrumbs) sticky</span></a><span style=\"font-weight: 400;\"> on desktop and mobile.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We fixed our </span><a href=\"https://github.com/mdn/yari/issues/5919\"><span style=\"font-weight: 400;\">accessibility skip navigation</span></a><span style=\"font-weight: 400;\"> to now offer skip to content, skip to search, and skip to language selectors.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We </span><a href=\"https://github.com/mdn/yari/pull/5977\"><span style=\"font-weight: 400;\">fixed a tricky issue</span></a><span style=\"font-weight: 400;\"> that caused some elements to flicker when interacting with the page, especially in dark mode. Many thanks to Daniel Holbert for his assistance in diagnosing the problem.</span></li>\n</ul>\n<h2><strong>Browser compatibility tables</strong></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47785 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/browser-compat.png\" alt=\"\" width=\"512\" height=\"171\" srcset=\"https://hacks.mozilla.org/files/2022/04/browser-compat.png 512w, https://hacks.mozilla.org/files/2022/04/browser-compat-250x83.png 250w, https://hacks.mozilla.org/files/2022/04/browser-compat-500x167.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<p><span style=\"font-weight: 400;\">Another area of the site for which we received feedback after the redesign launch was the browser compatibility tables. Almost its own project inside the larger readability effort, the work we invested here resulted, we believe, in a much-improved user experience. All of the changes listed below are now in production:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We restored version numbers in the overview, which are now color-coded across desktop and mobile.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">The font size has been bumped up for easier reading and skimming.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">The line height of rows has been increased for readability.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reduced the table cells to </span><a href=\"https://github.com/mdn/yari/pull/5648\"><span style=\"font-weight: 400;\">one focusable button element</span></a><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://github.com/mdn/yari/pull/5749\"><span style=\"font-weight: 400;\">Browser icons</span></a><span style=\"font-weight: 400;\"> have been restored in the overview header.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reordered </span><a href=\"https://github.com/mdn/yari/pull/5649\"><span style=\"font-weight: 400;\">support history chronologically</span></a><span style=\"font-weight: 400;\"> to make the version range that the support notes refer to visually unambiguous.</span></li>\n</ul>\n<p><span style=\"font-weight: 400;\">We also fixed the following </span><a href=\"https://github.com/mdn/yari/pulls?q=is%3Apr+is%3Aclosed+label%3Abrowser-compat\"><span style=\"font-weight: 400;\">bugs</span></a><span style=\"font-weight: 400;\">:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Color-coded pre-release versions in the overview</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Showing consistent mouseover titles with release dates</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Added the missing </span><a href=\"https://github.com/mdn/yari/pull/5557\"><span style=\"font-weight: 400;\">footnote icon</span></a><span style=\"font-weight: 400;\"> in the overview</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Showing </span><a href=\"https://github.com/mdn/yari/pull/5614\"><span style=\"font-weight: 400;\">correct</span></a><span style=\"font-weight: 400;\"> support status for </span><a href=\"https://github.com/mdn/yari/pull/5616\"><span style=\"font-weight: 400;\">edge cases</span></a><span style=\"font-weight: 400;\"> (e.g., omit prefix symbol if prefixed and unprefixed support)</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Streamlined mobile </span><a href=\"https://github.com/mdn/yari/pull/5933\"><span style=\"font-weight: 400;\">dark mode</span></a></li>\n</ul>\n<p><span style=\"font-weight: 400;\">We believe this is a big step in the right direction but we are not done. We can, and will, continue to </span><span style=\"font-weight: 400;\">improve site-wide readability and functionality of page areas,</span><span style=\"font-weight: 400;\"> such as the sidebars and general accessibility. As with the current improvements, we invite you to </span><a href=\"https://github.com/mdn/yari/issues/new/choose\"><span style=\"font-weight: 400;\">provide us with your feedback</span></a><span style=\"font-weight: 400;\"> and always welcome your pull requests to address </span><a href=\"https://github.com/mdn/yari/issues\"><span style=\"font-weight: 400;\">known issues</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">This was a collective effort, but we&#8217;d like to mention folks who went above and beyond. </span><a href=\"https://github.com/schalkneethling\"><span style=\"font-weight: 400;\">Schalk Neethling</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://github.com/caugner\"><span style=\"font-weight: 400;\">Claas Augner</span></a><span style=\"font-weight: 400;\"> from the MDN Team were responsible for most of the updates. From the community, we’d like to especially thank </span><a href=\"https://github.com/OnkarRuikar\"><span style=\"font-weight: 400;\">Onkar Ruikar</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://github.com/danielhjacobs\"><span style=\"font-weight: 400;\">Daniel Jacobs</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://github.com/iDave2\"><span style=\"font-weight: 400;\">Dave King</span></a><span style=\"font-weight: 400;\">, and </span><a href=\"https://github.com/queengooborg\"><span style=\"font-weight: 400;\">Queen Vinyl Da.i’gyu-Kazotetsu</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/adopting-users-design-feedback/\">Adopting users&#8217; design feedback</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "On March 1st, 2022, MDN Web Docs released a new design and a new brand identity. Overall, the community responded to the redesign enthusiastically and we received many positive messages and kudos. We also received valuable feedback on some of the things we didn’t get quite right, like the browser compatibility table changes as well as some accessibility and readability issues.\nFor us, MDN Web Docs has always been synonymous with the term Ubuntu, “I am because we are.” Translated in this context, “MDN Web Docs is the amazing resource it is because of our community’s support, feedback, and contributions.”\n Since the initial launch of the redesign and of MDN Plus afterwards, we have been humbled and overwhelmed by the level of support we received from our community of readers. We do our best to listen to what you have to say and to act on suggestions so that together, we make MDN better. \nHere is a summary of how we went about addressing the feedback we received.\nEight days after the redesign launch, we started the MDN Web Docs Readability Project. Our first task was to triage all issues submitted by the community that related to readability and accessibility on MDN Web Docs. Next up, we identified common themes and collected them in this meta issue. Over time, this grew into 27 unique issues and several related discussions and comments. We collected feedback on GitHub and also from our communities on Twitter and Matrix.\nWith the main pain points identified, we opened a discussion on GitHub, inviting our readers to follow along and provide feedback on the changes as they were rolled out to a staging instance of the website. Today, roughly six weeks later, we are pleased to announce that all these changes are in production. This was not the effort of any one person but is made up of the work and contributions of people across staff and community.\nBelow are some of the highlights from this work.\nDark mode\n\nWe updated the color palette used in dark mode in particular.\n\nWe reworked the initial color palette to use colors that are slightly more subtle in dark mode while ensuring that we still meet AA accessibility guidelines for color contrast.\nWe reconsidered the darkness of the primary background color in dark mode and settled on a compromise that improved the experience for the majority of readers.\nWe cleaned up the notecards that indicate notices such as warnings, experimental features, items not on the standards track, etc.\n\nReadability\n\nWe got a clear sense from some of our community folks that readers found it more difficult to skim content and find sections of interest after the redesign. To address these issues, we made the following improvements:\n\nWe implemented a clearly defined type-scale adjusted for mobile to optimize legibility and to effectively use space, especially on smaller screens.\nWe made the distinction between the different heading levels clearer.\nWe moved away from changing link colors across different areas of MDN Web Docs. We still retain some of the intent of this design decision, but this is now more subtle.\nThe font size was bumped up across all pages, including the home page. We have also optimized letter and line spacing for a more effortless reading experience. This has also improved the reading experience for our Asian readers, for whom line heights were much too tight.\nMissing links are clearly distinguishable from other links and content.\nWe improved the layout and readability of specifications pages across desktop and small screen devices.\nWe addressed a bug in how the highlighting in the table of contents worked and moved to use an IntersectionObserver.\nWe made styling for tables consistent across all pages.\nTo ensure readers are always oriented regarding which page they are currently viewing, we have made the header (which includes the breadcrumbs) sticky on desktop and mobile.\nWe fixed our accessibility skip navigation to now offer skip to content, skip to search, and skip to language selectors.\nWe fixed a tricky issue that caused some elements to flicker when interacting with the page, especially in dark mode. Many thanks to Daniel Holbert for his assistance in diagnosing the problem.\n\nBrowser compatibility tables\n\nAnother area of the site for which we received feedback after the redesign launch was the browser compatibility tables. Almost its own project inside the larger readability effort, the work we invested here resulted, we believe, in a much-improved user experience. All of the changes listed below are now in production:\n\nWe restored version numbers in the overview, which are now color-coded across desktop and mobile.\nThe font size has been bumped up for easier reading and skimming.\nThe line height of rows has been increased for readability.\nWe reduced the table cells to one focusable button element.\nBrowser icons have been restored in the overview header.\nWe reordered support history chronologically to make the version range that the support notes refer to visually unambiguous.\n\nWe also fixed the following bugs:\n\nColor-coded pre-release versions in the overview\nShowing consistent mouseover titles with release dates\nAdded the missing footnote icon in the overview\nShowing correct support status for edge cases (e.g., omit prefix symbol if prefixed and unprefixed support)\nStreamlined mobile dark mode\n\nWe believe this is a big step in the right direction but we are not done. We can, and will, continue to improve site-wide readability and functionality of page areas, such as the sidebars and general accessibility. As with the current improvements, we invite you to provide us with your feedback and always welcome your pull requests to address known issues.\nThis was a collective effort, but we’d like to mention folks who went above and beyond. Schalk Neethling and Claas Augner from the MDN Team were responsible for most of the updates. From the community, we’d like to especially thank Onkar Ruikar, Daniel Jacobs, Dave King, and Queen Vinyl Da.i’gyu-Kazotetsu.\n \nThe post Adopting users’ design feedback appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-04-21T15:04:05.000Z",
      "date_modified": "2022-04-21T15:04:05.000Z",
      "_plugin": {
        "pageFilename": "782f35a133605e9966e362db200510e7cde192b48457e9521ecf0d9aa0fc8fce.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47775",
      "url": "https://hacks.mozilla.org/2022/04/mozilla-partners-with-the-center-for-humane-technology/",
      "title": "Mozilla partners with the Center for Humane Technology",
      "summary": "We’re pleased to announce that we have partnered with Center for Humane Tech, a nonprofit organization that radically reimagines the digital infrastructure. Its mission is to drive a comprehensive shift toward humane technology that supports the collective well-being, democracy and shared information environment.\nThe post Mozilla partners with the Center for Humane Technology appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">We’re pleased to announce that we have partnered with the </span><a href=\"https://www.humanetech.com/\"><span style=\"font-weight: 400;\">Center for Humane Technology</span></a><span style=\"font-weight: 400;\">, a nonprofit organization that radically reimagines the digital infrastructure. Its mission is to drive a comprehensive shift toward humane technology that supports the collective well-being, democracy and shared information environment. Many of you may remember the Center for Humane Tech from the Netflix documentary </span><a href=\"https://www.youtube.com/watch?v=uaaC57tcci0\"><span style=\"font-weight: 400;\">‘Social Dilemma’,</span></a><span style=\"font-weight: 400;\"> solidifying the saying “If you&#8217;re not paying for the product, then you are the product”. The Social Dilemma, is all about the dark side of technology, focusing on the individual and societal impact of algorithms. </span></p>\n<p><span style=\"font-weight: 400;\">It’s no surprise that this decision to partner was a no brainer and supports our efforts for a safe and open web that is </span>accessible and joyful for all.<span style=\"font-weight: 400;\"> Many people do not understand how AI and algorithms regularly touch our lives and feel powerless in the face of these systems. We are dedicated to making sure the public understands that we can and must have a say in when machines are used to make important decisions – and shape how those decisions are made. </span></p>\n<p><span style=\"font-weight: 400;\">Over the last few years, our work has been increasingly focused on building more trustworthy AI and safe online spaces. From challenging YouTube’s algorithms, where Mozilla </span><a href=\"https://foundation.mozilla.org/en/blog/mozilla-investigation-youtube-algorithm-recommends-videos-that-violate-the-platforms-very-own-policies/\"><span style=\"font-weight: 400;\">research</span></a><span style=\"font-weight: 400;\"> shows that the platform keeps pushing harmful videos and its algorithm is recommending videos with misinformation, violent content, hate speech and scams to its over two billion users to developing Enhanced Tracking Protection in Firefox that automatically protects your privacy while you browse, and </span><a href=\"https://www.mozilla.org/en-US/firefox/pocket/\"><span style=\"font-weight: 400;\">Pocket</span></a><span style=\"font-weight: 400;\"> which recommends high-quality, human-curated articles without collecting your browsing history or sharing your personal information with advertisers.</span></p>\n<p><span style=\"font-weight: 400;\">Let’s face it, most, if not all people, would probably prefer to use social media platforms that are safer and technologists should design products that reflect all users and without bias. As we collectively continue to think about our role in these areas &#8212; now and in the future, this course from the Center for Humane Tech is a great addition to the many tools necessary for change to take place. </span></p>\n<p><span style=\"font-weight: 400;\">The course rightly titled ‘</span><a href=\"https://www.humanetech.com/course\"><i><span style=\"font-weight: 400;\">Foundations of Humane Technology</span></i></a><i><span style=\"font-weight: 400;\">’ </span></i><span style=\"font-weight: 400;\">launched out of beta in March of this year, after rave reviews from hundreds of beta testers! </span></p>\n<p><span style=\"font-weight: 400;\">It explores the personal, societal, and practical challenges of being a humane technologist. Participants will leave the course with a strong conceptual framework, hands-on tools, and an ecosystem of support from peers and experts. Topics range from respecting human nature to minimizing harm to designing technology that deliberately avoids reinforcing inequitable dynamics of the past. </span></p>\n<p><span style=\"font-weight: 400;\">The course is completely free of charge and is centered towards building awareness and self-education through an online, at-your-own pace or binge-worthy set of eight modules. The course is marketed to professionals, with or without a technical background involved in shaping tomorrow&#8217;s technology. </span></p>\n<p><span style=\"font-weight: 400;\">It includes interactive exercises and reflections to help you internalize what you’re learning and regular optional Zoom sessions to discuss course content, connect with like-minded people, learn from experts in the field and even rewards a credential upon completion that can be shared with colleagues and prospective employers.</span></p>\n<p><span style=\"font-weight: 400;\">The problem with tech is not a new one, but this course is a stepping stone in the right direction.</span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/mozilla-partners-with-the-center-for-humane-technology/\">Mozilla partners with the Center for Humane Technology</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "We’re pleased to announce that we have partnered with the Center for Humane Technology, a nonprofit organization that radically reimagines the digital infrastructure. Its mission is to drive a comprehensive shift toward humane technology that supports the collective well-being, democracy and shared information environment. Many of you may remember the Center for Humane Tech from the Netflix documentary ‘Social Dilemma’, solidifying the saying “If you’re not paying for the product, then you are the product”. The Social Dilemma, is all about the dark side of technology, focusing on the individual and societal impact of algorithms. \nIt’s no surprise that this decision to partner was a no brainer and supports our efforts for a safe and open web that is accessible and joyful for all. Many people do not understand how AI and algorithms regularly touch our lives and feel powerless in the face of these systems. We are dedicated to making sure the public understands that we can and must have a say in when machines are used to make important decisions – and shape how those decisions are made. \nOver the last few years, our work has been increasingly focused on building more trustworthy AI and safe online spaces. From challenging YouTube’s algorithms, where Mozilla research shows that the platform keeps pushing harmful videos and its algorithm is recommending videos with misinformation, violent content, hate speech and scams to its over two billion users to developing Enhanced Tracking Protection in Firefox that automatically protects your privacy while you browse, and Pocket which recommends high-quality, human-curated articles without collecting your browsing history or sharing your personal information with advertisers.\nLet’s face it, most, if not all people, would probably prefer to use social media platforms that are safer and technologists should design products that reflect all users and without bias. As we collectively continue to think about our role in these areas — now and in the future, this course from the Center for Humane Tech is a great addition to the many tools necessary for change to take place. \nThe course rightly titled ‘Foundations of Humane Technology’ launched out of beta in March of this year, after rave reviews from hundreds of beta testers! \nIt explores the personal, societal, and practical challenges of being a humane technologist. Participants will leave the course with a strong conceptual framework, hands-on tools, and an ecosystem of support from peers and experts. Topics range from respecting human nature to minimizing harm to designing technology that deliberately avoids reinforcing inequitable dynamics of the past. \nThe course is completely free of charge and is centered towards building awareness and self-education through an online, at-your-own pace or binge-worthy set of eight modules. The course is marketed to professionals, with or without a technical background involved in shaping tomorrow’s technology. \nIt includes interactive exercises and reflections to help you internalize what you’re learning and regular optional Zoom sessions to discuss course content, connect with like-minded people, learn from experts in the field and even rewards a credential upon completion that can be shared with colleagues and prospective employers.\nThe problem with tech is not a new one, but this course is a stepping stone in the right direction.\nThe post Mozilla partners with the Center for Humane Technology appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-04-13T15:02:02.000Z",
      "date_modified": "2022-04-13T15:02:02.000Z",
      "_plugin": {
        "pageFilename": "af4c0a867f853d707ceec586f900e466a29d4c6fbbd81dfdd8546face9920a12.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47723",
      "url": "https://hacks.mozilla.org/2022/03/performance-tool-in-firefox-devtools-reloaded/",
      "title": "Performance Tool in Firefox DevTools Reloaded",
      "summary": "In Firefox 98, we’re shipping a new version of the existing Performance panel. This panel is now based on the Firefox profiler tool that can be used to capture a performance profile for a web page, inspect visualized performance data and analyze it to identify slow areas.\nThe post Performance Tool in Firefox DevTools Reloaded appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>In Firefox 98, we’re shipping a new version of the existing Performance panel. This panel is now based on the <a href=\"https://profiler.firefox.com/docs/#/\">Firefox profiler</a> tool that can be used to capture a performance profile for a web page, inspect visualized performance data and analyze it to identify slow areas.</p>\n<p>The icing on the cake of this already extremely powerful tool is that you can upload collected profile data with a single click and share the resulting link with your teammates (or anyone really). This makes it easier to collaborate on performance issues, especially in a distributed work environment.</p>\n<p>The new Performance panel is available in Firefox DevTools Toolbox by default and can be opened by <strong>Shift+F5</strong> key shortcut.</p>\n<h2>Usage</h2>\n<p>The only thing the user needs to do to start profiling is clicking on the big blue button &#8211; <strong>Start recording</strong>. Check out the screenshot below.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47724\" src=\"https://hacks.mozilla.org/files/2022/03/perf-panel.png\" alt=\"\" width=\"1037\" height=\"619\" srcset=\"https://hacks.mozilla.org/files/2022/03/perf-panel.png 1037w, https://hacks.mozilla.org/files/2022/03/perf-panel-250x149.png 250w, https://hacks.mozilla.org/files/2022/03/perf-panel-500x298.png 500w, https://hacks.mozilla.org/files/2022/03/perf-panel-768x458.png 768w\" sizes=\"(max-width: 1037px) 100vw, 1037px\" /></p>\n<p>As indicated by the onboarding message at the top of the new panel the previous profiler will be available for some time and eventually removed entirely.</p>\n<p>When profiling is started (i.e. the profiler is gathering performance data) the user can see two more buttons:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47728\" src=\"https://hacks.mozilla.org/files/2022/03/perf-panel-buttons.png\" alt=\"\" width=\"1035\" height=\"97\" srcset=\"https://hacks.mozilla.org/files/2022/03/perf-panel-buttons.png 1035w, https://hacks.mozilla.org/files/2022/03/perf-panel-buttons-250x23.png 250w, https://hacks.mozilla.org/files/2022/03/perf-panel-buttons-500x47.png 500w, https://hacks.mozilla.org/files/2022/03/perf-panel-buttons-768x72.png 768w\" sizes=\"(max-width: 1035px) 100vw, 1035px\" /></p>\n<ul>\n<li><strong>Capture recording</strong> &#8211; Stop recording, get what’s been collected so far and visualize it</li>\n<li><strong>Cancel recording</strong> &#8211; Stop recording and throw away all collected data</li>\n</ul>\n<p>When the user clicks on Capture recording all collected data are visualized in a new tab. You should see something like the following:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47740\" src=\"https://hacks.mozilla.org/files/2022/03/perf-data.png\" alt=\"\" width=\"4256\" height=\"2264\" srcset=\"https://hacks.mozilla.org/files/2022/03/perf-data.png 4256w, https://hacks.mozilla.org/files/2022/03/perf-data-250x133.png 250w, https://hacks.mozilla.org/files/2022/03/perf-data-500x266.png 500w, https://hacks.mozilla.org/files/2022/03/perf-data-768x409.png 768w, https://hacks.mozilla.org/files/2022/03/perf-data-1536x817.png 1536w, https://hacks.mozilla.org/files/2022/03/perf-data-2048x1089.png 2048w\" sizes=\"(max-width: 4256px) 100vw, 4256px\" /></p>\n<p>The inspection capabilities of the UI are powerful and let the user inspect every bit of the performance data. You might want to follow this detailed <a href=\"https://profiler.firefox.com/docs/#/./guide-ui-tour\">UI Tour</a> presentation created by the Performance team at Mozilla to learn more about all available features.</p>\n<h2>Customization</h2>\n<p>There are many options that can be used to customize how and what performance data should be collected to optimize specific use cases (see also the <strong>Edit Settings…</strong> link at the bottom of the panel).</p>\n<p>To make customization easier some presets are available and the <strong>Web Developer</strong> preset is selected by default. The profiler can be also used for profiling Firefox itself and Mozilla is extensively using it to make Firefox fast for millions of its users. The WebDeveloper preset is intended for profiling standard web pages and the rest is for profiling Firefox.</p>\n<p>The Profiler can be also used directly from the Firefox toolbar without the DevTools Toolbox being opened. The Profiler button isn’t visible in the toolbar by default, but you can enable it by loading <a href=\"https://profiler.firefox.com/\">https://profiler.firefox.com/</a> and clicking on the “Enable Firefox Profiler Menu Button” on the page.</p>\n<p>This is what the button looks like in the Firefox toolbar.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-large wp-image-47732\" src=\"https://hacks.mozilla.org/files/2022/03/toolbar-500x459.png\" alt=\"\" width=\"500\" height=\"459\" srcset=\"https://hacks.mozilla.org/files/2022/03/toolbar-500x459.png 500w, https://hacks.mozilla.org/files/2022/03/toolbar-250x230.png 250w, https://hacks.mozilla.org/files/2022/03/toolbar.png 611w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p>As you can see from the screenshot above the UI is almost exactly the same (compared to the DevTools Performance panel).</p>\n<h2>Sharing Data</h2>\n<p>Collected performance data can be shared publicly. This is one of the most powerful features of the profiler since it allows the user to upload data to the Firefox Profiler online storage. Before uploading a profile, you can select the data that you want to include, and what you don’t want to include to avoid leaking personal data. The profile link can then be shared in online chats, emails, and bug reports so other people can see and investigate a specific case.</p>\n<p>This is great for team collaboration and that’s something Firefox developers have been doing for years to work on performance. The profile can also be saved as a file on a local machine and imported later from <a href=\"https://profiler.firefox.com/\">https://profiler.firefox.com/</a></p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47736\" src=\"https://hacks.mozilla.org/files/2022/03/share-perf-data.png\" alt=\"\" width=\"930\" height=\"617\" srcset=\"https://hacks.mozilla.org/files/2022/03/share-perf-data.png 930w, https://hacks.mozilla.org/files/2022/03/share-perf-data-250x166.png 250w, https://hacks.mozilla.org/files/2022/03/share-perf-data-500x332.png 500w, https://hacks.mozilla.org/files/2022/03/share-perf-data-768x510.png 768w\" sizes=\"(max-width: 930px) 100vw, 930px\" /></p>\n<p>There are many more powerful features available and you can learn more about them in the extensive <a href=\"https://profiler.firefox.com/docs/#/\">documentation</a>. And of course, just like Firefox itself, the profiler tool is an open source project and you might want to <a href=\"https://github.com/firefox-devtools/profiler\">contribute</a> to it.</p>\n<p>There is also a great <a href=\"https://profiler.firefox.com/docs/#/./bunny\">case study</a> on using the profiler to identify performance issues.</p>\n<p>More is coming to DevTools, so stay tuned!</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/performance-tool-in-firefox-devtools-reloaded/\">Performance Tool in Firefox DevTools Reloaded</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "In Firefox 98, we’re shipping a new version of the existing Performance panel. This panel is now based on the Firefox profiler tool that can be used to capture a performance profile for a web page, inspect visualized performance data and analyze it to identify slow areas.\nThe icing on the cake of this already extremely powerful tool is that you can upload collected profile data with a single click and share the resulting link with your teammates (or anyone really). This makes it easier to collaborate on performance issues, especially in a distributed work environment.\nThe new Performance panel is available in Firefox DevTools Toolbox by default and can be opened by Shift+F5 key shortcut.\nUsage\nThe only thing the user needs to do to start profiling is clicking on the big blue button – Start recording. Check out the screenshot below.\n\nAs indicated by the onboarding message at the top of the new panel the previous profiler will be available for some time and eventually removed entirely.\nWhen profiling is started (i.e. the profiler is gathering performance data) the user can see two more buttons:\n\n\nCapture recording – Stop recording, get what’s been collected so far and visualize it\nCancel recording – Stop recording and throw away all collected data\n\nWhen the user clicks on Capture recording all collected data are visualized in a new tab. You should see something like the following:\n\nThe inspection capabilities of the UI are powerful and let the user inspect every bit of the performance data. You might want to follow this detailed UI Tour presentation created by the Performance team at Mozilla to learn more about all available features.\nCustomization\nThere are many options that can be used to customize how and what performance data should be collected to optimize specific use cases (see also the Edit Settings… link at the bottom of the panel).\nTo make customization easier some presets are available and the Web Developer preset is selected by default. The profiler can be also used for profiling Firefox itself and Mozilla is extensively using it to make Firefox fast for millions of its users. The WebDeveloper preset is intended for profiling standard web pages and the rest is for profiling Firefox.\nThe Profiler can be also used directly from the Firefox toolbar without the DevTools Toolbox being opened. The Profiler button isn’t visible in the toolbar by default, but you can enable it by loading https://profiler.firefox.com/ and clicking on the “Enable Firefox Profiler Menu Button” on the page.\nThis is what the button looks like in the Firefox toolbar.\n\nAs you can see from the screenshot above the UI is almost exactly the same (compared to the DevTools Performance panel).\nSharing Data\nCollected performance data can be shared publicly. This is one of the most powerful features of the profiler since it allows the user to upload data to the Firefox Profiler online storage. Before uploading a profile, you can select the data that you want to include, and what you don’t want to include to avoid leaking personal data. The profile link can then be shared in online chats, emails, and bug reports so other people can see and investigate a specific case.\nThis is great for team collaboration and that’s something Firefox developers have been doing for years to work on performance. The profile can also be saved as a file on a local machine and imported later from https://profiler.firefox.com/\n\nThere are many more powerful features available and you can learn more about them in the extensive documentation. And of course, just like Firefox itself, the profiler tool is an open source project and you might want to contribute to it.\nThere is also a great case study on using the profiler to identify performance issues.\nMore is coming to DevTools, so stay tuned!\nThe post Performance Tool in Firefox DevTools Reloaded appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-30T14:59:00.000Z",
      "date_modified": "2022-03-30T14:59:00.000Z",
      "_plugin": {
        "pageFilename": "fa81033600282f24eed16f4b18b32e49da408c06670241af84e6b2f6a4f1cf89.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47744",
      "url": "https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/",
      "title": "Introducing MDN Plus: Make MDN your own",
      "summary": "MDN is one of the most trusted resources for information about web standards, code samples, tools, and everything you need as a developer to create websites. Today, we are launching MDN Plus, our first step to providing a personalized and more powerful experience while continuing to invest in our always free and open webdocs.\nThe post Introducing MDN Plus: Make MDN your own appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>MDN is one of the most trusted resources for information about web standards, code samples, tools, and everything you need as a developer to create websites. In 2015, we explored how we could expand beyond documentation to provide a structured learning experience. Our first foray was the <a href=\"https://developer.mozilla.org/en-US/docs/Learn\">Learning Area</a>, with the goal of providing a useful addition to the regular MDN reference and guide material. In 2020, we added the first <a href=\"https://developer.mozilla.org/en-US/docs/Learn/Front-end_web_developer\">Front-end developer learning pathway</a>. We saw a lot of interest and engagement from users, and the learning area contributed to about 10% of MDN’s monthly web traffic. These two initiatives were the start of our exploration into how we could offer more learning resources to our community. Today, we are launching MDN Plus, our first step to providing a personalized and more powerful experience while continuing to invest in our always free and open webdocs.</p>\r\n\r\n<p><iframe loading=\"lazy\" title=\"YouTube video player\" src=\"https://www.youtube.com/embed/OBv9qnCesaQ\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe></p>\r\n<h2>Build your own MDN Experience with MDN Plus</h2>\r\n<p>In 2020 and 2021 we surveyed over 60,000 MDN users and learned that many of the respondents  wanted a customized MDN experience. They wanted to organize MDN’s vast library in a way that worked for them. For today’s premium subscription service, MDN Plus, we are releasing three new features that begin to address this need: Notifications, Collections and MDN Offline. More details about the features are listed below:</p>\r\n<ul>\r\n<li><b><i>Notifications: </i></b>Technology is ever changing, and we know how important it is to stay on top of the latest updates and developments. From tutorial pages to API references, you can now get notifications for the latest developments on MDN. When you follow a page, you’ll get notified when the documentation changes, CSS features launch, and APIs ship. Now, you can get a notification for significant events relating to the pages you want to follow. <a href=\"https://developer.mozilla.org/en-US/plus/docs/features/notifications\">Read more about it here</a>.</li>\r\n</ul>\r\n<p><img loading=\"lazy\" class=\"alignnone size-large wp-image-47750\" src=\"https://hacks.mozilla.org/files/2022/03/image-14-500x292.png\" alt=\"Screenshot of a list of notifications on mdn plus\" width=\"500\" height=\"292\" srcset=\"https://hacks.mozilla.org/files/2022/03/image-14-500x292.png 500w, https://hacks.mozilla.org/files/2022/03/image-14-250x146.png 250w, https://hacks.mozilla.org/files/2022/03/image-14-768x448.png 768w, https://hacks.mozilla.org/files/2022/03/image-14-1536x896.png 1536w, https://hacks.mozilla.org/files/2022/03/image-14-2048x1195.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\r\n<ul>\r\n<li><b><i>Collections:</i></b> Find what you need fast with our new collections feature. Not only can you pick the MDN articles you want to save, we also automatically save the pages you visit frequently. Collections help you quickly access the articles that matter the most to you and your work. <a href=\"https://developer.mozilla.org/en-US/plus/docs/features/collections\">Read more about it here</a>.</li>\r\n</ul>\r\n<p><img loading=\"lazy\" class=\"alignnone size-large wp-image-47746\" src=\"https://hacks.mozilla.org/files/2022/03/image-13-500x292.png\" alt=\"Screenshot of a collections list on mdn plus\" width=\"500\" height=\"292\" srcset=\"https://hacks.mozilla.org/files/2022/03/image-13-500x292.png 500w, https://hacks.mozilla.org/files/2022/03/image-13-250x146.png 250w, https://hacks.mozilla.org/files/2022/03/image-13-768x448.png 768w, https://hacks.mozilla.org/files/2022/03/image-13-1536x896.png 1536w, https://hacks.mozilla.org/files/2022/03/image-13-2048x1195.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\r\n<ul>\r\n<li><b><i>MDN offline</i></b>: Sometimes you need to access MDN but don’t have an internet connection. MDN offline leverages a Progressive Web Application (PWA) to give you access to MDN Web Docs even when you lack internet access so you can continue your work without any interruptions. Plus, with MDN offline you can have a faster experience while saving data. <a href=\"https://developer.mozilla.org/en-US/plus/docs/features/offline\">Read more about it here</a>.</li>\r\n</ul>\r\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47760 size-large\" src=\"https://hacks.mozilla.org/files/2022/03/image1-500x299.png\" alt=\"Screenshot of offline settings on mdn plus\" width=\"500\" height=\"299\" srcset=\"https://hacks.mozilla.org/files/2022/03/image1-500x299.png 500w, https://hacks.mozilla.org/files/2022/03/image1-250x150.png 250w, https://hacks.mozilla.org/files/2022/03/image1-768x460.png 768w, https://hacks.mozilla.org/files/2022/03/image1.png 1507w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\r\n<p id=\"countries\">Today, MDN Plus is available in the US and Canada. In the coming months, we will expand to other countries including France, Germany, Italy, Spain, Belgium, Austria, the Netherlands, Ireland, United Kingdom, Switzerland, Malaysia, New Zealand and Singapore. </p>\r\n<h2>Find the right MDN Plus plan for you</h2>\r\n<p>MDN is part of the daily life of millions of web developers. For many of us MDN helped with getting that first job or helped land a promotion. During our research we found many of these users, users who felt so much value from MDN that they wanted to contribute financially. We were both delighted and humbled by this feedback. To provide folks with a few options, we are launching MDN Plus with three plans including a supporter plan for those that want to spend a little extra. Here are the details of those plans:</p>\r\n<ul>\r\n<li aria-level=\"1\"><b><i>MDN Core</i></b>: For those who want to do a test drive before purchasing a plan, we created an option that lets you try a limited version for free.  </li>\r\n<li aria-level=\"1\"><b><i>MDN Plus 5</i></b>:  Offers unlimited access to notifications, collections, and MDN offline with new features added all the time. $5 a month or an annual subscription of $50.</li>\r\n<li aria-level=\"1\"><b><i>MDN Supporter 10</i></b>:  For MDN’s loyal supporters the supporter plan gives you everything under MDN Plus 5 plus early access to new features and a direct feedback channel to  the MDN team. It’s $10 a month or $100 for an annual subscription.  </li>\r\n</ul>\r\n<p>Additionally, we will offer a 20% discount if you subscribe to one of the annual subscription plans.</p>\r\n<p>We invite you to try the <a style=\"color:#e80840;\" href=\"https://developer.mozilla.org/en-US/plus#subscribe\">free trial version or sign up</a> today for a subscription plan that’s right for you. MDN Plus is only <a href=\"#countries\">available in selected countries</a> at this time.</p>\r\n<p>&nbsp;</p><p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/\">Introducing MDN Plus: Make MDN your own</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "MDN is one of the most trusted resources for information about web standards, code samples, tools, and everything you need as a developer to create websites. In 2015, we explored how we could expand beyond documentation to provide a structured learning experience. Our first foray was the Learning Area, with the goal of providing a useful addition to the regular MDN reference and guide material. In 2020, we added the first Front-end developer learning pathway. We saw a lot of interest and engagement from users, and the learning area contributed to about 10% of MDN’s monthly web traffic. These two initiatives were the start of our exploration into how we could offer more learning resources to our community. Today, we are launching MDN Plus, our first step to providing a personalized and more powerful experience while continuing to invest in our always free and open webdocs.\n\n\nBuild your own MDN Experience with MDN Plus\nIn 2020 and 2021 we surveyed over 60,000 MDN users and learned that many of the respondents  wanted a customized MDN experience. They wanted to organize MDN’s vast library in a way that worked for them. For today’s premium subscription service, MDN Plus, we are releasing three new features that begin to address this need: Notifications, Collections and MDN Offline. More details about the features are listed below:\n\nNotifications: Technology is ever changing, and we know how important it is to stay on top of the latest updates and developments. From tutorial pages to API references, you can now get notifications for the latest developments on MDN. When you follow a page, you’ll get notified when the documentation changes, CSS features launch, and APIs ship. Now, you can get a notification for significant events relating to the pages you want to follow. Read more about it here.\n\n\n\nCollections: Find what you need fast with our new collections feature. Not only can you pick the MDN articles you want to save, we also automatically save the pages you visit frequently. Collections help you quickly access the articles that matter the most to you and your work. Read more about it here.\n\n\n\nMDN offline: Sometimes you need to access MDN but don’t have an internet connection. MDN offline leverages a Progressive Web Application (PWA) to give you access to MDN Web Docs even when you lack internet access so you can continue your work without any interruptions. Plus, with MDN offline you can have a faster experience while saving data. Read more about it here.\n\n\nToday, MDN Plus is available in the US and Canada. In the coming months, we will expand to other countries including France, Germany, Italy, Spain, Belgium, Austria, the Netherlands, Ireland, United Kingdom, Switzerland, Malaysia, New Zealand and Singapore. \nFind the right MDN Plus plan for you\nMDN is part of the daily life of millions of web developers. For many of us MDN helped with getting that first job or helped land a promotion. During our research we found many of these users, users who felt so much value from MDN that they wanted to contribute financially. We were both delighted and humbled by this feedback. To provide folks with a few options, we are launching MDN Plus with three plans including a supporter plan for those that want to spend a little extra. Here are the details of those plans:\n\nMDN Core: For those who want to do a test drive before purchasing a plan, we created an option that lets you try a limited version for free.  \nMDN Plus 5:  Offers unlimited access to notifications, collections, and MDN offline with new features added all the time. $5 a month or an annual subscription of $50.\nMDN Supporter 10:  For MDN’s loyal supporters the supporter plan gives you everything under MDN Plus 5 plus early access to new features and a direct feedback channel to  the MDN team. It’s $10 a month or $100 for an annual subscription.  \n\nAdditionally, we will offer a 20% discount if you subscribe to one of the annual subscription plans.\nWe invite you to try the free trial version or sign up today for a subscription plan that’s right for you. MDN Plus is only available in selected countries at this time.\n The post Introducing MDN Plus: Make MDN your own appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-24T16:00:29.000Z",
      "date_modified": "2022-03-24T16:00:29.000Z",
      "_plugin": {
        "pageFilename": "dc9b0ceffc3e39e3e2163b067b294f3485c6e8f2e74848065fb3649b45ad2f21.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47707",
      "url": "https://hacks.mozilla.org/2022/03/mozilla-and-open-web-docs-working-together-on-mdn/",
      "title": "Mozilla and Open Web Docs working together on MDN",
      "summary": "For both MDN and Open Web Docs (OWD), transparency is paramount to our missions. With the upcoming launch of MDN Plus, we believe it’s a good time to talk about how our two organizations work together, and if there is a financial relationship between us. Here is an overview of how our missions overlap and how they differ, and how a premium subscription service fits all this.\nThe post Mozilla and Open Web Docs working together on MDN appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">For both MDN and Open Web Docs (OWD), transparency is paramount to our missions. With the upcoming launch of MDN Plus, we believe it’s a good time to talk about how our two organizations work together and if there is a financial relationship between us. Here is an overview of how our missions overlap, how they differ, and how a premium subscription service fits all this.</span></p>\n<h2><strong>History of our collaboration</strong></h2>\n<p><span style=\"font-weight: 400;\">MDN and Open Web Docs began working together after the creation of Open Web Docs in 2021. Our organizations were born out of the same ethos, and we constantly collaborate on MDN content, contributing to different parts of MDN and even teaming up for shared projects like the conversion to Markdown. We meet on a weekly basis to discuss content strategies and maintain an open dialogue on our respective roadmaps.</span></p>\n<p><span style=\"font-weight: 400;\">MDN and Open Web Docs are different organizations; while our missions and goals frequently overlap, our work is not identical. Open Web Docs is an open collective, with a mission to contribute content to open source projects that are considered important for the future of the Web. MDN is currently the most significant project that Open Web Docs contributes to.</span></p>\n<h2><strong>Separate funding streams, division of labor</strong></h2>\n<p><span style=\"font-weight: 400;\">Mozilla and Open Web Docs collaborate closely on sustaining the Web Docs part of MDN. The Web Docs part is and will remain free and accessible to all. Each organization shoulders part of the costs of this labor, from our distinct budgets and revenue sources.</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Mozilla covers the cost of infrastructure, development and maintenance of the MDN platform including a team of engineers and its own team of dedicated writers.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Open Web Docs receives donations from companies like Google, Microsoft, Meta, Coil and others, and from private individuals. These donations pay for Technical Writing staff and help finance Open Web Docs projects. None of the donations that Open Web Docs receive go to MDN or Mozilla; rather they pay for a team of writers to contribute to MDN. </span></li>\n</ul>\n<h2><strong>Transparency and dialogue but independent decision-making</strong></h2>\n<p><span style=\"font-weight: 400;\">Mozilla and OWD have an open dialogue on content related to MDN. Mozilla sits on the Open Web Docs&#8217; Steering Committee, sharing expertise and experience but does not currently sit on the Open Web Docs’ Governing Committee. Mozilla does not provide direct financial support to Open Web Docs and does not participate in making decisions about Open Web Docs&#8217; overall direction, objectives, hiring and budgeting.</span></p>\n<h2><strong>MDN Plus: How does it fit into the big picture?</strong></h2>\n<p><span style=\"font-weight: 400;\">MDN Plus is a new premium subscription service by Mozilla that allows users to customize their MDN experience. </span></p>\n<p><span style=\"font-weight: 400;\">As with so much of our work, our organizations engaged in a transparent dialogue regarding MDN Plus. When requested, Open Web Docs has provided Mozilla with feedback, but it has not been a part of the development of MDN Plus. The resources Open Web Docs has are used only to improve the free offering of MDN. </span></p>\n<p><span style=\"font-weight: 400;\">The existence of a new subscription model will not detract from MDN&#8217;s current free Web Docs offering in any way. The current experience of accessing web documentation will not change for users who do not wish to sign up for a premium subscription. </span></p>\n<p><span style=\"font-weight: 400;\">Mozilla’s goal with MDN Plus is to help ensure that MDN&#8217;s open source content continues to be supported into the future. While Mozilla has incorporated its partners’ feedback into their vision for the product, MDN Plus has been built only with Mozilla resources. Any revenue generated by MDN Plus will stay within Mozilla. Mozilla is looking into ways to reinvest some of these additional funds into open source projects contributing to MDN but it is still in early stages.</span></p>\n<p><span style=\"font-weight: 400;\">A subscription to MDN Plus gives paying subscribers extra MDN features provided by Mozilla while a donation to Open Web Docs goes to funding writers creating content on MDN Web Docs, and potentially elsewhere. Work produced via OWD will always be publicly available and accessible to all. </span></p>\n<p><span style=\"font-weight: 400;\">Open Web Docs and Mozilla will continue to work closely together on MDN for the best possible web platform documentation for everyone!</span></p>\n<p><span style=\"font-weight: 400;\">Thanks for your continuing feedback and support.</span></p>\n<p>&nbsp;</p>\n<p><img loading=\"lazy\" class=\"wp-image-47717 alignleft\" src=\"https://hacks.mozilla.org/files/2022/03/mdn_owd-250x63.png\" alt=\"\" width=\"365\" height=\"92\" srcset=\"https://hacks.mozilla.org/files/2022/03/mdn_owd-250x63.png 250w, https://hacks.mozilla.org/files/2022/03/mdn_owd-500x125.png 500w, https://hacks.mozilla.org/files/2022/03/mdn_owd.png 600w\" sizes=\"(max-width: 365px) 100vw, 365px\" /></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/mozilla-and-open-web-docs-working-together-on-mdn/\">Mozilla and Open Web Docs working together on MDN</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "For both MDN and Open Web Docs (OWD), transparency is paramount to our missions. With the upcoming launch of MDN Plus, we believe it’s a good time to talk about how our two organizations work together and if there is a financial relationship between us. Here is an overview of how our missions overlap, how they differ, and how a premium subscription service fits all this.\nHistory of our collaboration\nMDN and Open Web Docs began working together after the creation of Open Web Docs in 2021. Our organizations were born out of the same ethos, and we constantly collaborate on MDN content, contributing to different parts of MDN and even teaming up for shared projects like the conversion to Markdown. We meet on a weekly basis to discuss content strategies and maintain an open dialogue on our respective roadmaps.\nMDN and Open Web Docs are different organizations; while our missions and goals frequently overlap, our work is not identical. Open Web Docs is an open collective, with a mission to contribute content to open source projects that are considered important for the future of the Web. MDN is currently the most significant project that Open Web Docs contributes to.\nSeparate funding streams, division of labor\nMozilla and Open Web Docs collaborate closely on sustaining the Web Docs part of MDN. The Web Docs part is and will remain free and accessible to all. Each organization shoulders part of the costs of this labor, from our distinct budgets and revenue sources.\n\nMozilla covers the cost of infrastructure, development and maintenance of the MDN platform including a team of engineers and its own team of dedicated writers.\nOpen Web Docs receives donations from companies like Google, Microsoft, Meta, Coil and others, and from private individuals. These donations pay for Technical Writing staff and help finance Open Web Docs projects. None of the donations that Open Web Docs receive go to MDN or Mozilla; rather they pay for a team of writers to contribute to MDN. \n\nTransparency and dialogue but independent decision-making\nMozilla and OWD have an open dialogue on content related to MDN. Mozilla sits on the Open Web Docs’ Steering Committee, sharing expertise and experience but does not currently sit on the Open Web Docs’ Governing Committee. Mozilla does not provide direct financial support to Open Web Docs and does not participate in making decisions about Open Web Docs’ overall direction, objectives, hiring and budgeting.\nMDN Plus: How does it fit into the big picture?\nMDN Plus is a new premium subscription service by Mozilla that allows users to customize their MDN experience. \nAs with so much of our work, our organizations engaged in a transparent dialogue regarding MDN Plus. When requested, Open Web Docs has provided Mozilla with feedback, but it has not been a part of the development of MDN Plus. The resources Open Web Docs has are used only to improve the free offering of MDN. \nThe existence of a new subscription model will not detract from MDN’s current free Web Docs offering in any way. The current experience of accessing web documentation will not change for users who do not wish to sign up for a premium subscription. \nMozilla’s goal with MDN Plus is to help ensure that MDN’s open source content continues to be supported into the future. While Mozilla has incorporated its partners’ feedback into their vision for the product, MDN Plus has been built only with Mozilla resources. Any revenue generated by MDN Plus will stay within Mozilla. Mozilla is looking into ways to reinvest some of these additional funds into open source projects contributing to MDN but it is still in early stages.\nA subscription to MDN Plus gives paying subscribers extra MDN features provided by Mozilla while a donation to Open Web Docs goes to funding writers creating content on MDN Web Docs, and potentially elsewhere. Work produced via OWD will always be publicly available and accessible to all. \nOpen Web Docs and Mozilla will continue to work closely together on MDN for the best possible web platform documentation for everyone!\nThanks for your continuing feedback and support.\n \n\n \nThe post Mozilla and Open Web Docs working together on MDN appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-17T14:07:34.000Z",
      "date_modified": "2022-03-17T14:07:34.000Z",
      "_plugin": {
        "pageFilename": "95d3545d197d66c977fd54d3fc8eb968d8596af6f05eea50db617e414c296f46.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47622",
      "url": "https://hacks.mozilla.org/2022/03/interop-2022/",
      "title": "Announcing Interop 2022",
      "summary": "Writing high quality standards is a necessary first step to an interoperable web platform, but ensuring that browsers are consistent in their behavior requires an ongoing process. Browsers must work to ensure that they have a shared understanding of web standards, and that their implementation matches that understanding.\nInterop 2022 is a cross-browser initiative to find and address the most important interoperability pain points on the web platform. The end result is a public metric that will assess progress toward fixing these interoperability issues.\nThe post Announcing Interop 2022 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>A key benefit of the web platform is that it&#8217;s defined by standards, rather than by the code of a single implementation. This creates a shared platform that isn&#8217;t tied to specific hardware, a company, or a business model.</p>\n<p>Writing high quality standards is a necessary first step to an interoperable web platform, but ensuring that browsers are consistent in their behavior requires an ongoing process. Browsers must work to ensure that they have a shared understanding of web standards, and that their implementation matches that understanding.</p>\n<h2>Interop 2022</h2>\n<p>Interop 2022 is a cross-browser initiative to find and address the most important interoperability pain points on the web platform. The end result is a public metric that will assess progress toward fixing these interoperability issues.</p>\n<p><a href=\"https://wpt.fyi/interop-2022\"><img loading=\"lazy\" class=\"alignnone\" src=\"https://hacks.mozilla.org/files/2022/02/interop-2022-dashboard.png\" alt=\"Interop 2022 scores. Chrome/Edge 71, Firefox 74, and Safari 73.\" width=\"1200\" height=\"580\" /></a></p>\n<p>In order to identify the areas to include, we looked at two primary sources of data:</p>\n<ul>\n<li>Web developer feedback (e.g., through developer facing surveys including <a href=\"https://insights.developer.mozilla.org/\">MDN’s Web DNA Report</a>) on the most common pain points they experience.</li>\n<li>End user bug reports (e.g., via <a href=\"https://webcompat.com/\">webcompat.com</a>) that could be traced back to implementation differences between browsers.</li>\n</ul>\n<p>During the process of collecting this data, it became clear there are two principal kinds of interoperability problems which affect end users and developers:</p>\n<ul>\n<li>Problems where there&#8217;s a relatively clear and widely accepted standard, but where implementations are incomplete or buggy.</li>\n<li>Problems where the standard is missing, unclear, or doesn&#8217;t match the behavior sites depend on.</li>\n</ul>\n<p>Problems of the first kind have been termed &#8220;focus areas&#8221;. For these we use <a href=\"https://web-platform-tests.org/\">web-platform-tests</a>: a large, shared testsuite that aims to ensure web standards are implemented consistently across browsers. It accepts contributions from anyone, and browsers, including Firefox, contribute tests as part of their process for fixing bugs and shipping new features.</p>\n<p>The path to improvement for these areas is clear: identify or write tests in web-platform-tests that measure conformance to the relevant standard, and update implementations so that they pass those tests.</p>\n<p>Problems of the second kind have been termed “investigate areas”. For these it’s not possible to simply write tests as we&#8217;re not really sure what&#8217;s necessary to reach interoperability. Such unknown unknowns turn out to be extremely common sources of developer and user frustration!</p>\n<p>We’ll make progress here through investigation. And we’ll measure progress with more qualitative goals, e.g., working out what exact behavior sites depend on, and what can be implemented in practice without breaking the web.</p>\n<p>In all cases, the hope is that we can move toward a future in which we know how to make these areas interoperable, update the relevant web standards for them, and measure them with tests as we do with focus areas.</p>\n<h3>Focus areas</h3>\n<p>Interop 2022 has ten new focus areas:</p>\n<ul>\n<li>Cascade Layers</li>\n<li>Color Spaces and Functions</li>\n<li>Containment</li>\n<li>Dialog Element</li>\n<li>Forms</li>\n<li>Scrolling</li>\n<li>Subgrid</li>\n<li>Typography and Encodings</li>\n<li>Viewport Units</li>\n<li>Web Compat</li>\n</ul>\n<p>Unlike the others the Web Compat area doesn&#8217;t represent a specific technology, but is a group of specific known problems with already shipped features, where we see bugs and deviations from standards cause frequent site breakage for end users.</p>\n<p>There are also five additional areas that have been adopted from Google and Microsoft&#8217;s “Compat 2021” effort:</p>\n<ul>\n<li>Aspect Ratio</li>\n<li>Flexbox</li>\n<li>Grid</li>\n<li>Sticky Positioning</li>\n<li>Transforms</li>\n</ul>\n<p>A browser&#8217;s test pass rate in each area contributes 6% — totaling at 90% for fifteen areas — of their score of Interop 2022.</p>\n<p>We believe these are areas where the standards are in good shape for implementation, and where improving interoperability will directly improve the lives of developers and end users.</p>\n<h3>Investigate areas</h3>\n<p>Interop 2022 has three investigate areas:</p>\n<ul>\n<li>Editing, contentEditable, and execCommand</li>\n<li>Pointer and Mouse Events</li>\n<li>Viewport Measurement</li>\n</ul>\n<p>These are areas in which we often see complaints from end users, or reports of site breakage, but where the path toward solving the issues isn&#8217;t clear. Collaboration between vendors is essential to working out how to fix these problem areas, and we believe that Interop 2022 is a unique opportunity to make progress on historically neglected areas of the web platform.</p>\n<p>The overall progress in this area will contribute 10% to the overall score of Interop 2022. This score will be the same across all browsers. This reflects the fact that progress on the web platform requires browsers to collaborate on new or updated web standards and accompanying tests, to achieve the best outcomes for end users and developers.</p>\n<h2>Contributions welcome!</h2>\n<p>Whilst the focus and investigate areas for 2022 are now set, there is still much to do. For the investigate areas, the detailed targets need to be set, and the complex work of understanding the current state of the art, and assessing the options to advance it, are just starting. Additional tests for the focus areas might be needed as well to address particular edge cases.</p>\n<p>If this sounds like something you&#8217;d like to get involved with, follow the instructions on the <a href=\"https://wpt.fyi/interop-2022\">Interop 2022 Dashboard</a>.</p>\n<p>Finally, it&#8217;s also possible that Interop 2022 is missing an area you consider to be a significant pain point. It won&#8217;t be possible to add areas this year, but, if the effort is a success we may end up running further iterations. Feedback on browser differences that are making your life hard as developer or end user are always welcome and will be helpful for identifying the correct focus and investigate areas for any future edition.</p>\n<h2>Partner announcements</h2>\n<p>Bringing Interop 2022 to fruition was a collaborative effort and you might be interested in the other announcements:</p>\n<ul>\n<li>Apple&#8217;s <a href=\"https://webkit.org/blog/12288/working-together-on-interop-2022/\">Working together on Interop 2022</a></li>\n<li><a href=\"https://bocoup.com/blog/interop-2022\">Bocoup and Interop 2022</a></li>\n<li>Google&#8217;s <a href=\"https://web.dev/interop-2022\">Interop 2022: browsers working together to improve the web for developers</a></li>\n<li><a href=\"https://www.igalia.com/news/interop2022.html\">Igalia and Interop 2022</a></li>\n<li><a href=\"https://aka.ms/microsoft-interop2022\">Microsoft and Interop 2022</a></li>\n</ul>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/interop-2022/\">Announcing Interop 2022</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "A key benefit of the web platform is that it’s defined by standards, rather than by the code of a single implementation. This creates a shared platform that isn’t tied to specific hardware, a company, or a business model.\nWriting high quality standards is a necessary first step to an interoperable web platform, but ensuring that browsers are consistent in their behavior requires an ongoing process. Browsers must work to ensure that they have a shared understanding of web standards, and that their implementation matches that understanding.\nInterop 2022\nInterop 2022 is a cross-browser initiative to find and address the most important interoperability pain points on the web platform. The end result is a public metric that will assess progress toward fixing these interoperability issues.\n\nIn order to identify the areas to include, we looked at two primary sources of data:\n\nWeb developer feedback (e.g., through developer facing surveys including MDN’s Web DNA Report) on the most common pain points they experience.\nEnd user bug reports (e.g., via webcompat.com) that could be traced back to implementation differences between browsers.\n\nDuring the process of collecting this data, it became clear there are two principal kinds of interoperability problems which affect end users and developers:\n\nProblems where there’s a relatively clear and widely accepted standard, but where implementations are incomplete or buggy.\nProblems where the standard is missing, unclear, or doesn’t match the behavior sites depend on.\n\nProblems of the first kind have been termed “focus areas”. For these we use web-platform-tests: a large, shared testsuite that aims to ensure web standards are implemented consistently across browsers. It accepts contributions from anyone, and browsers, including Firefox, contribute tests as part of their process for fixing bugs and shipping new features.\nThe path to improvement for these areas is clear: identify or write tests in web-platform-tests that measure conformance to the relevant standard, and update implementations so that they pass those tests.\nProblems of the second kind have been termed “investigate areas”. For these it’s not possible to simply write tests as we’re not really sure what’s necessary to reach interoperability. Such unknown unknowns turn out to be extremely common sources of developer and user frustration!\nWe’ll make progress here through investigation. And we’ll measure progress with more qualitative goals, e.g., working out what exact behavior sites depend on, and what can be implemented in practice without breaking the web.\nIn all cases, the hope is that we can move toward a future in which we know how to make these areas interoperable, update the relevant web standards for them, and measure them with tests as we do with focus areas.\nFocus areas\nInterop 2022 has ten new focus areas:\n\nCascade Layers\nColor Spaces and Functions\nContainment\nDialog Element\nForms\nScrolling\nSubgrid\nTypography and Encodings\nViewport Units\nWeb Compat\n\nUnlike the others the Web Compat area doesn’t represent a specific technology, but is a group of specific known problems with already shipped features, where we see bugs and deviations from standards cause frequent site breakage for end users.\nThere are also five additional areas that have been adopted from Google and Microsoft’s “Compat 2021” effort:\n\nAspect Ratio\nFlexbox\nGrid\nSticky Positioning\nTransforms\n\nA browser’s test pass rate in each area contributes 6% — totaling at 90% for fifteen areas — of their score of Interop 2022.\nWe believe these are areas where the standards are in good shape for implementation, and where improving interoperability will directly improve the lives of developers and end users.\nInvestigate areas\nInterop 2022 has three investigate areas:\n\nEditing, contentEditable, and execCommand\nPointer and Mouse Events\nViewport Measurement\n\nThese are areas in which we often see complaints from end users, or reports of site breakage, but where the path toward solving the issues isn’t clear. Collaboration between vendors is essential to working out how to fix these problem areas, and we believe that Interop 2022 is a unique opportunity to make progress on historically neglected areas of the web platform.\nThe overall progress in this area will contribute 10% to the overall score of Interop 2022. This score will be the same across all browsers. This reflects the fact that progress on the web platform requires browsers to collaborate on new or updated web standards and accompanying tests, to achieve the best outcomes for end users and developers.\nContributions welcome!\nWhilst the focus and investigate areas for 2022 are now set, there is still much to do. For the investigate areas, the detailed targets need to be set, and the complex work of understanding the current state of the art, and assessing the options to advance it, are just starting. Additional tests for the focus areas might be needed as well to address particular edge cases.\nIf this sounds like something you’d like to get involved with, follow the instructions on the Interop 2022 Dashboard.\nFinally, it’s also possible that Interop 2022 is missing an area you consider to be a significant pain point. It won’t be possible to add areas this year, but, if the effort is a success we may end up running further iterations. Feedback on browser differences that are making your life hard as developer or end user are always welcome and will be helpful for identifying the correct focus and investigate areas for any future edition.\nPartner announcements\nBringing Interop 2022 to fruition was a collaborative effort and you might be interested in the other announcements:\n\nApple’s Working together on Interop 2022\nBocoup and Interop 2022\nGoogle’s Interop 2022: browsers working together to improve the web for developers\nIgalia and Interop 2022\nMicrosoft and Interop 2022\n\nThe post Announcing Interop 2022 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-03T17:00:02.000Z",
      "date_modified": "2022-03-03T17:00:02.000Z",
      "_plugin": {
        "pageFilename": "b908dbb3dc5da03d8ed9caf65d494fd5524d8c2087a736247535dc6b123cc564.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47643",
      "url": "https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/",
      "title": "A new year, a new MDN",
      "summary": "If you’ve accessed the MDN website today, you probably noticed that it looks quite different. We hope it’s a good different. Let us explain!\nIn mid-2021 we started to think about modernizing MDN’s design, to create a clean and inviting website that makes navigating our 44,000 articles as easy as possible. We wanted to create a more holistic experience for our users, with an emphasis on improved navigability and a universal look and feel across all our pages. \nThe post A new year, a new MDN appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">If you’ve accessed the MDN website today, you probably noticed that it looks quite different. We hope it’s a good different. Let us explain!</span></p>\n<p><span style=\"font-weight: 400;\">MDN has undergone many changes in its sixteen-year history from its early beginning as a wiki to the recent migration of a static site backed by GitHub. During that time MDN grew organically, with over 45,000 contributors and numerous developers and designers. It’s no surprise that the user experience became somewhat inconsistent throughout the website. </span></p>\n<p><span style=\"font-weight: 400;\">In mid-2021 we started to think about modernizing MDN’s design, to create a clean and inviting website that makes navigating our 44,000 articles as easy as possible. We wanted to create a more holistic experience for our users, with an emphasis on improved navigability and a universal look and feel across all our pages. </span></p>\n<h2><b>A new Homepage, focused on community</b></h2>\n<p><span style=\"font-weight: 400;\">The MDN community is the reason our content can be counted on to be both high quality and trustworthy. MDN content is scrutinized, discussed, and yes, in some cases argued about. Anyone can contribute to MDN, either by writing content, suggesting changes or fixing bugs.</span></p>\n<p><span style=\"font-weight: 400;\">We wanted to acknowledge and celebrate our awesome community and our homepage is the perfect place to do so.</span></p>\n<p><span style=\"font-weight: 400;\">The new homepage was built with a focus on the core concepts of community and simplicity. We made an improved search a central element on the page, while also showing users a selection of the newest and most-read articles. </span></p>\n<p><span style=\"font-weight: 400;\">We will also show the most recent contributions to our GitHub content repo and added a contributor spotlight where we will highlight MDN contributors.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47690 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_mdn_dark-mode.gif\" alt=\"\" width=\"1080\" height=\"675\" /></p>\n<h2><b>Redesigned article pages for improved navigation</b></h2>\n<p><span style=\"font-weight: 400;\">It’s been years—</span><a href=\"https://hacks.mozilla.org/2017/07/the-mdn-redesign-behind-the-scenes/\"><span style=\"font-weight: 400;\">five of them, in fact</span></a><span style=\"font-weight: 400;\">—since MDN’s core content presentation has received a comprehensive design review. In those years, MDN’s content has evolved and changed, with new </span><a href=\"https://hacks.mozilla.org/2018/02/mdn-browser-compatibility-data/\"><span style=\"font-weight: 400;\">ways of structuring content</span></a><span style=\"font-weight: 400;\">, new ways to </span><a href=\"https://hacks.mozilla.org/2020/12/welcome-yari-mdn-web-docs-has-a-new-platform/\"><span style=\"font-weight: 400;\">build</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://github.com/mdn/content/pull/7092\"><span style=\"font-weight: 400;\">write docs</span></a><span style=\"font-weight: 400;\">, and new </span><a href=\"https://github.com/mdn/content/pulse\"><span style=\"font-weight: 400;\">contributors</span></a><span style=\"font-weight: 400;\">. Over time, the documentation’s look and feel had become increasingly disconnected from the way it’s read and written.</span></p>\n<p><span style=\"font-weight: 400;\">While you won’t see a dizzying reinvention of what documentation is, you’ll find that most visual elements on MDN did get love and attention, creating a more coherent view of our docs. This redesign gives MDN content its due, featuring:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">More consistent colors and theming</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Better signposting of major sections, such as HTML, CSS, and JavaScript</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Improved accessibility, such as increased contrast</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Added dark mode toggle for easy switching between modes</span></li>\n</ul>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47694 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_mdn_navigation.2022-02-24-13_14_00.gif\" alt=\"\" width=\"1080\" height=\"675\" /></p>\n<p>&nbsp;</p>\n<p><span style=\"font-weight: 400;\">We’re especially proud of some subtle improvements and conveniences. For example, in-page navigation is always in view to show you where you are in the page as you scroll:</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47698 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_mdn_scrolling.gif\" alt=\"\" width=\"1080\" height=\"675\" /></p>\n<p><span style=\"font-weight: 400;\">We’re also revisiting the way browser compatibility data appears, with better at-a-glance browser support. So you don’t have to keep version numbers in your head, we’ve put more emphasis on </span><i><span style=\"font-weight: 400;\">yes</span></i><span style=\"font-weight: 400;\"> and </span><i><span style=\"font-weight: 400;\">no</span></i><span style=\"font-weight: 400;\"> iconography for browser capabilities, with the option to view the detailed information you’ve come to expect from </span><a href=\"https://github.com/mdn/browser-compat-data\"><span style=\"font-weight: 400;\">our browser compatibility data</span></a><span style=\"font-weight: 400;\">. We think you should check it out. </span></p>\n<p><span style=\"font-weight: 400;\">And we’re not stopping there. The work we’ve done is far-reaching and there are still many opportunities to polish and improve on the design we’re shipping.</span></p>\n<h2><b>A new logo, chosen by our community</b></h2>\n<p><span style=\"font-weight: 400;\">As we began working on both the redesign and expanding MDN beyond WebDocs we realized it was also time for a new logo. We wanted a modern and easily customizable logo that would represent what MDN is today while also strengthening its identity and making it consistent with Mozilla’s current brand.</span></p>\n<p><span style=\"font-weight: 400;\">We worked closely with branding specialist </span><a href=\"https://lucdoucedame.com/\"><span style=\"font-weight: 400;\">Luc Doucedame</span></a><span style=\"font-weight: 400;\">, narrowed down our options to eight potential logos and put out a call to our community of users to help us choose and invited folks to vote on their favorite. We received over 10,000 votes in just three days and are happy to share with you “the MDN people&#8217;s choice.”</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47673 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21.png\" alt=\"\" width=\"643\" height=\"277\" srcset=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21.png 643w, https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21-250x108.png 250w, https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21-500x215.png 500w\" sizes=\"(max-width: 643px) 100vw, 643px\" /></p>\n<p><span style=\"font-weight: 400;\">The winner was Option 4, an M monogram using underscore to convey the process of writing code. Many thanks to everyone who voted!</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47669 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_blog_header_MDN-Intro.png\" alt=\"\" width=\"1920\" height=\"1080\" /></p>\n<h2><b>What you can expect next with MDN</b></h2>\n<h3></h3>\n<h3><b>Bringing content to the places where you need it most</b></h3>\n<p><span style=\"font-weight: 400;\">In recent years, MDN content has grown more sophisticated for authors, such as moving from a wiki to Git and converting from HTML to Markdown. This has been a boon to contributors, who can use more powerful and familiar tools to create more structured and consistent content.</span></p>\n<p><span style=\"font-weight: 400;\">With better tools in place, we’re finally in a position to build more visible and systematic benefits to readers. For example, many of you probably navigate MDN via your favorite search engine, rather than MDN’s own site navigation. We get it. Historically, a wiki made large content architecture efforts impractical. But we’re now closer than ever to making site-wide improvements to structure and navigation.</span></p>\n<p><span style=\"font-weight: 400;\">Looking forward, we have ambitious plans to take advantage of our new tools to explore improved navigation, generated standardization and support summarizes, and embedding MDN documentation in the places where developers need it most: in their IDE, browser tools, and more.</span></p>\n<h2><b>Coming soon: MDN Plus</b></h2>\n<p><span style=\"font-weight: 400;\">MDN has built a reputation as a trusted and central resource for information about standards, codes, tools, and everything you need as a developer to create websites. In 2015, we explored ways to be more than a central resource through creating a </span><a href=\"https://developer.mozilla.org/en-US/docs/Learn\"><span style=\"font-weight: 400;\">Learning Area</span></a><span style=\"font-weight: 400;\">, with the aim of providing a useful counterpart to the regular MDN reference and guide material. </span></p>\n<p><span style=\"font-weight: 400;\">In 2020, we added the first </span><a href=\"https://developer.mozilla.org/en-US/docs/Learn/Front-end_web_developer\"><span style=\"font-weight: 400;\">Front-end developer learning pathway</span></a><span style=\"font-weight: 400;\"> to it.  We saw a lot of interest and engagement from users, the learning area currently being responsible for 10% of MDN’s monthly web traffic. This started us on a path to see what more we can do in this area for our community.</span></p>\n<p><span style=\"font-weight: 400;\">Last year we surveyed users and asked them what they wanted out of their MDN experience. The top requested features included notifications, article collections and an offline experience on MDN. The overall theme we saw was that users wanted to be able to organize MDN’s vast library in a way that worked for them. </span></p>\n<p><span style=\"font-weight: 400;\">We are always looking for ways to meet our users&#8217; needs whether it&#8217;s through MDN’s free web documentation or personalized features. In the coming months, we’ll be expanding MDN to include a premium subscription service based on the feedback we received from web developers who want to customize their MDN experience. Stay tuned for more information on MDN Plus.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47677 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.45.53.png\" alt=\"\" width=\"372\" height=\"430\" srcset=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.45.53.png 372w, https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.45.53-250x289.png 250w\" sizes=\"(max-width: 372px) 100vw, 372px\" /></p>\n<h2><b>Thank you, MDN community</b></h2>\n<p><span style=\"font-weight: 400;\">We appreciate the thousands of people who voted for the new logo as well as everyone who participated in the early beta testing phase since we started this journey. Also, many thanks to our partners from the </span><a href=\"https://openwebdocs.org\"><span style=\"font-weight: 400;\">Open Web Docs</span></a><span style=\"font-weight: 400;\">, who gave us valuable feedback on the redesign and continue to make daily contributions to MDN content. Thanks to you all we could make this a reality and we will continue to invest in improving even further the experience on MDN.</span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\">A new year, a new MDN</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "If you’ve accessed the MDN website today, you probably noticed that it looks quite different. We hope it’s a good different. Let us explain!\nMDN has undergone many changes in its sixteen-year history from its early beginning as a wiki to the recent migration of a static site backed by GitHub. During that time MDN grew organically, with over 45,000 contributors and numerous developers and designers. It’s no surprise that the user experience became somewhat inconsistent throughout the website. \nIn mid-2021 we started to think about modernizing MDN’s design, to create a clean and inviting website that makes navigating our 44,000 articles as easy as possible. We wanted to create a more holistic experience for our users, with an emphasis on improved navigability and a universal look and feel across all our pages. \nA new Homepage, focused on community\nThe MDN community is the reason our content can be counted on to be both high quality and trustworthy. MDN content is scrutinized, discussed, and yes, in some cases argued about. Anyone can contribute to MDN, either by writing content, suggesting changes or fixing bugs.\nWe wanted to acknowledge and celebrate our awesome community and our homepage is the perfect place to do so.\nThe new homepage was built with a focus on the core concepts of community and simplicity. We made an improved search a central element on the page, while also showing users a selection of the newest and most-read articles. \nWe will also show the most recent contributions to our GitHub content repo and added a contributor spotlight where we will highlight MDN contributors.\n\nRedesigned article pages for improved navigation\nIt’s been years—five of them, in fact—since MDN’s core content presentation has received a comprehensive design review. In those years, MDN’s content has evolved and changed, with new ways of structuring content, new ways to build and write docs, and new contributors. Over time, the documentation’s look and feel had become increasingly disconnected from the way it’s read and written.\nWhile you won’t see a dizzying reinvention of what documentation is, you’ll find that most visual elements on MDN did get love and attention, creating a more coherent view of our docs. This redesign gives MDN content its due, featuring:\n\nMore consistent colors and theming\nBetter signposting of major sections, such as HTML, CSS, and JavaScript\nImproved accessibility, such as increased contrast\nAdded dark mode toggle for easy switching between modes\n\n\n \nWe’re especially proud of some subtle improvements and conveniences. For example, in-page navigation is always in view to show you where you are in the page as you scroll:\n\nWe’re also revisiting the way browser compatibility data appears, with better at-a-glance browser support. So you don’t have to keep version numbers in your head, we’ve put more emphasis on yes and no iconography for browser capabilities, with the option to view the detailed information you’ve come to expect from our browser compatibility data. We think you should check it out. \nAnd we’re not stopping there. The work we’ve done is far-reaching and there are still many opportunities to polish and improve on the design we’re shipping.\nA new logo, chosen by our community\nAs we began working on both the redesign and expanding MDN beyond WebDocs we realized it was also time for a new logo. We wanted a modern and easily customizable logo that would represent what MDN is today while also strengthening its identity and making it consistent with Mozilla’s current brand.\nWe worked closely with branding specialist Luc Doucedame, narrowed down our options to eight potential logos and put out a call to our community of users to help us choose and invited folks to vote on their favorite. We received over 10,000 votes in just three days and are happy to share with you “the MDN people’s choice.”\n\nThe winner was Option 4, an M monogram using underscore to convey the process of writing code. Many thanks to everyone who voted!\n\nWhat you can expect next with MDN\n\nBringing content to the places where you need it most\nIn recent years, MDN content has grown more sophisticated for authors, such as moving from a wiki to Git and converting from HTML to Markdown. This has been a boon to contributors, who can use more powerful and familiar tools to create more structured and consistent content.\nWith better tools in place, we’re finally in a position to build more visible and systematic benefits to readers. For example, many of you probably navigate MDN via your favorite search engine, rather than MDN’s own site navigation. We get it. Historically, a wiki made large content architecture efforts impractical. But we’re now closer than ever to making site-wide improvements to structure and navigation.\nLooking forward, we have ambitious plans to take advantage of our new tools to explore improved navigation, generated standardization and support summarizes, and embedding MDN documentation in the places where developers need it most: in their IDE, browser tools, and more.\nComing soon: MDN Plus\nMDN has built a reputation as a trusted and central resource for information about standards, codes, tools, and everything you need as a developer to create websites. In 2015, we explored ways to be more than a central resource through creating a Learning Area, with the aim of providing a useful counterpart to the regular MDN reference and guide material. \nIn 2020, we added the first Front-end developer learning pathway to it.  We saw a lot of interest and engagement from users, the learning area currently being responsible for 10% of MDN’s monthly web traffic. This started us on a path to see what more we can do in this area for our community.\nLast year we surveyed users and asked them what they wanted out of their MDN experience. The top requested features included notifications, article collections and an offline experience on MDN. The overall theme we saw was that users wanted to be able to organize MDN’s vast library in a way that worked for them. \nWe are always looking for ways to meet our users’ needs whether it’s through MDN’s free web documentation or personalized features. In the coming months, we’ll be expanding MDN to include a premium subscription service based on the feedback we received from web developers who want to customize their MDN experience. Stay tuned for more information on MDN Plus.\n\nThank you, MDN community\nWe appreciate the thousands of people who voted for the new logo as well as everyone who participated in the early beta testing phase since we started this journey. Also, many thanks to our partners from the Open Web Docs, who gave us valuable feedback on the redesign and continue to make daily contributions to MDN content. Thanks to you all we could make this a reality and we will continue to invest in improving even further the experience on MDN.\nThe post A new year, a new MDN appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-03-01T14:00:24.000Z",
      "date_modified": "2022-03-01T14:00:24.000Z",
      "_plugin": {
        "pageFilename": "9f12d60757598f94055bec82d7e5e6e497d23bbdb4b9f1f4662ef3fdc155279c.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47607",
      "url": "https://hacks.mozilla.org/2022/02/version-100-in-chrome-and-firefox/",
      "title": "Version 100 in Chrome and Firefox",
      "summary": "Chrome and Firefox will reach version 100 in a couple of months. This has the potential to cause breakage on sites that rely on identifying the browser version to perform business logic.  This post covers the timeline of events, the strategies that Chrome and Firefox are taking to mitigate the impact, and how you can help.\nThe post Version 100 in Chrome and Firefox appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Chrome and Firefox will reach version 100 in a <a href=\"https://developer.chrome.com/blog/force-major-version-to-100/\">couple</a> of <a href=\"https://www.otsukare.info/2021/04/20/ua-three-digits-get-ready\">months</a>. This has the potential to cause breakage on sites that rely on identifying the browser version to perform business logic.  This post covers the timeline of events, the strategies that Chrome and Firefox are taking to mitigate the impact, and how you can help.</p>\n<h2>User-Agent string</h2>\n<p><a href=\"https://www.rfc-editor.org/rfc/rfc7231.html#section-5.5.3\">User-Agent (UA)</a> is a string that browsers send in HTTP headers, so servers can identify the browser.  The string is also accessible through JavaScript with <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/userAgent\">navigator.userAgent</a>. It’s usually formatted as follows:</p>\n<p><code>browserName/majorVersion.minorVersion</code></p>\n<p>For example, the latest release versions of browsers at the time of publishing this post are:</p>\n<ul>\n<li aria-level=\"1\"><code>Chrome: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36</code></li>\n<li aria-level=\"1\"><code>Firefox: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0</code></li>\n<li aria-level=\"1\"><code>Safari: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15</code></li>\n</ul>\n<h2>Major version 100—three-digit version number</h2>\n<p>Major version 100 is a big milestone for both Chrome and Firefox. It also has the potential to cause breakage on websites as we move from a two-digit to a <b>three-digit version number</b>.  Web developers use all kinds of techniques for parsing these strings, from custom code to using User-Agent parsing libraries, which can then be used to determine the corresponding processing logic. The User-Agent and any other version reporting mechanisms will soon report a three-digit version number.</p>\n<h3>Version 100 timelines</h3>\n<p>Version 100 browsers will be first released in experimental versions (Chrome Canary, Firefox Nightly), then beta versions, and then finally on the stable channel.</p>\n<table>\n<tbody>\n<tr>\n<td>Chrome (<a href=\"https://chromiumdash.appspot.com/schedule\">Release Schedule</a>)</td>\n<td>March 29, 2022</td>\n</tr>\n<tr>\n<td>Firefox (<a href=\"https://wiki.mozilla.org/Release_Management/Calendar\">Release Schedule</a>)</td>\n<td>May 3, 2022</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"three-digit\">Why can a three-digit version number be problematic?</h2>\n<p>When browsers first reached version 10 a little over 12 years ago, <a href=\"https://maqentaer.com/devopera-static-backup/http/dev.opera.com/articles/view/opera-ua-string-changes/index.html\">many issues were discovered</a> with User-Agent parsing libraries as the major version number went from one digit to two.</p>\n<p>Without a single specification to follow, <a href=\"https://developer.mozilla.org/docs/Web/HTTP/Headers/User-Agent\">different browsers have different formats</a> for the User-Agent string, and site-specific User-Agent parsing. It’s possible that some parsing libraries may have hard-coded assumptions or bugs that don’t take into account three-digit major version numbers.  Many libraries improved the parsing logic when browsers moved to two-digit version numbers, so hitting the three-digit milestone is expected to cause fewer problems. Mike Taylor, an engineer on the Chrome team, has done a survey of common UA parsing libraries which didn&#8217;t uncover any issues. Running Chrome experiments in the field has surfaced some issues, which are being worked on.</p>\n<h2>What are browsers doing about it?</h2>\n<p>Both Firefox and Chrome have been running experiments where current versions of the browser report being at major version 100 in order to detect possible website breakage. This has led to a few <a href=\"https://github.com/webcompat/web-bugs/labels/version100\">reported</a> <a href=\"https://bugs.chromium.org/p/chromium/issues/detail?id=1273958\">issues</a>, some of which have already been fixed. These experiments will continue to run until the release of version 100.</p>\n<p>There are also backup mitigation strategies in place, in case version 100 release to stable channels causes more damage to websites than anticipated.</p>\n<h2 id=\"firefox-mitigation\">Firefox mitigation</h2>\n<p>In Firefox, the strategy will depend on how important the breakage is. Firefox has a <a href=\"https://wiki.mozilla.org/Compatibility/Interventions_Releases\">site interventions mechanism</a>. Mozilla webcompat team can hot fix broken websites in Firefox using this mechanism. If you type <code>about:compat</code> in the Firefox URL bar, you can see what is currently being fixed. If a site breaks with the major version being 100 on a specific domain, it is possible to fix it by sending version 99 instead.</p>\n<p>If the breakage is widespread and individual site interventions become unmanageable, Mozilla can temporarily freeze Firefox&#8217;s major version at 99 and then test other options.</p>\n<h2 id=\"chrome-mitigation\">Chrome mitigation</h2>\n<p>In Chrome, the backup plan is to use a flag to freeze the major version at 99 and report the real major version number in the minor version part of the User-Agent string (the code has already <a href=\"https://chromium-review.googlesource.com/c/chromium/src/+/3341658\">landed</a>).</p>\n<p>The Chrome version as reported in the User-Agent string follows the pattern &lt;major_version&gt;.&lt;minor_version&gt;.&lt;build_number&gt;.&lt;patch_number&gt;.</p>\n<p>If the backup plan is employed, then the User-Agent string would look like this:</p>\n<p><code>99.101.4988.0</code></p>\n<p>Chrome is also running experiments to ensure that reporting a three-digit value in the minor version part of the string does not result in breakage, since the minor version in the Chrome User-Agent string has reported 0 for a very long time. The Chrome team will decide on whether to resort to the backup option based on the number and severity of the issues reported.</p>\n<h2>What can you do to help?</h2>\n<p>Every strategy that adds complexity to the User-Agent string has a strong impact on the ecosystem. Let’s work together to avoid yet another quirky behavior. In Chrome and Firefox Nightly, you can configure the browser to report the version as 100 right now and report any issues you come across.</p>\n<h3 id=\"firefox-config\">Configure Firefox Nightly to report the major version as 100</h3>\n<ol>\n<li aria-level=\"1\">Open Firefox Nightly’s Settings menu.</li>\n<li aria-level=\"1\">Search for “Firefox 100” and then check the “Firefox 100 User-Agent String” option.</li>\n</ol>\n<h3 id=\"chrome-config\">Configure Chrome to report the major version as 100</h3>\n<ol>\n<li aria-level=\"1\">Go to chrome://flags/#force-major-version-to-100</li>\n<li aria-level=\"1\">Set the option to `Enabled`.</li>\n</ol>\n<h3 id=\"test-report\">Test and file reports</h3>\n<ul>\n<li aria-level=\"1\"><b>If you are a website maintainer</b>, test your website with Chrome and Firefox 100. Review your User-Agent parsing code and libraries, and ensure they are able to handle three-digit version numbers. We have compiled some of the <a href=\"https://www.otsukare.info/2022/01/14/broken-ua-detection\">patterns that are currently breaking</a>.</li>\n<li aria-level=\"1\"><b>If you develop a User-Agent parsing library</b>, add tests to parse versions greater than and equal to 100. Our early tests show that recent versions of libraries can handle it correctly. But the Web is a legacy machine, so if you have old versions of parsing libraries, it’s probably time to check and eventually upgrade.</li>\n<li aria-level=\"1\"><b>If you are browsing the web</b> and notice any issues with the major version 100, <a href=\"https://webcompat.com/issues/new?label=version100\">file a report on webcompat.com</a>.</li>\n</ul>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/version-100-in-chrome-and-firefox/\">Version 100 in Chrome and Firefox</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Chrome and Firefox will reach version 100 in a couple of months. This has the potential to cause breakage on sites that rely on identifying the browser version to perform business logic.  This post covers the timeline of events, the strategies that Chrome and Firefox are taking to mitigate the impact, and how you can help.\nUser-Agent string\nUser-Agent (UA) is a string that browsers send in HTTP headers, so servers can identify the browser.  The string is also accessible through JavaScript with navigator.userAgent. It’s usually formatted as follows:\nbrowserName/majorVersion.minorVersion\nFor example, the latest release versions of browsers at the time of publishing this post are:\n\nChrome: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36\nFirefox: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0\nSafari: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\n\nMajor version 100—three-digit version number\nMajor version 100 is a big milestone for both Chrome and Firefox. It also has the potential to cause breakage on websites as we move from a two-digit to a three-digit version number.  Web developers use all kinds of techniques for parsing these strings, from custom code to using User-Agent parsing libraries, which can then be used to determine the corresponding processing logic. The User-Agent and any other version reporting mechanisms will soon report a three-digit version number.\nVersion 100 timelines\nVersion 100 browsers will be first released in experimental versions (Chrome Canary, Firefox Nightly), then beta versions, and then finally on the stable channel.\n\n\n\nChrome (Release Schedule)\nMarch 29, 2022\n\n\nFirefox (Release Schedule)\nMay 3, 2022\n\n\n\nWhy can a three-digit version number be problematic?\nWhen browsers first reached version 10 a little over 12 years ago, many issues were discovered with User-Agent parsing libraries as the major version number went from one digit to two.\nWithout a single specification to follow, different browsers have different formats for the User-Agent string, and site-specific User-Agent parsing. It’s possible that some parsing libraries may have hard-coded assumptions or bugs that don’t take into account three-digit major version numbers.  Many libraries improved the parsing logic when browsers moved to two-digit version numbers, so hitting the three-digit milestone is expected to cause fewer problems. Mike Taylor, an engineer on the Chrome team, has done a survey of common UA parsing libraries which didn’t uncover any issues. Running Chrome experiments in the field has surfaced some issues, which are being worked on.\nWhat are browsers doing about it?\nBoth Firefox and Chrome have been running experiments where current versions of the browser report being at major version 100 in order to detect possible website breakage. This has led to a few reported issues, some of which have already been fixed. These experiments will continue to run until the release of version 100.\nThere are also backup mitigation strategies in place, in case version 100 release to stable channels causes more damage to websites than anticipated.\nFirefox mitigation\nIn Firefox, the strategy will depend on how important the breakage is. Firefox has a site interventions mechanism. Mozilla webcompat team can hot fix broken websites in Firefox using this mechanism. If you type about:compat in the Firefox URL bar, you can see what is currently being fixed. If a site breaks with the major version being 100 on a specific domain, it is possible to fix it by sending version 99 instead.\nIf the breakage is widespread and individual site interventions become unmanageable, Mozilla can temporarily freeze Firefox’s major version at 99 and then test other options.\nChrome mitigation\nIn Chrome, the backup plan is to use a flag to freeze the major version at 99 and report the real major version number in the minor version part of the User-Agent string (the code has already landed).\nThe Chrome version as reported in the User-Agent string follows the pattern <major_version>.<minor_version>.<build_number>.<patch_number>.\nIf the backup plan is employed, then the User-Agent string would look like this:\n99.101.4988.0\nChrome is also running experiments to ensure that reporting a three-digit value in the minor version part of the string does not result in breakage, since the minor version in the Chrome User-Agent string has reported 0 for a very long time. The Chrome team will decide on whether to resort to the backup option based on the number and severity of the issues reported.\nWhat can you do to help?\nEvery strategy that adds complexity to the User-Agent string has a strong impact on the ecosystem. Let’s work together to avoid yet another quirky behavior. In Chrome and Firefox Nightly, you can configure the browser to report the version as 100 right now and report any issues you come across.\nConfigure Firefox Nightly to report the major version as 100\n\nOpen Firefox Nightly’s Settings menu.\nSearch for “Firefox 100” and then check the “Firefox 100 User-Agent String” option.\n\nConfigure Chrome to report the major version as 100\n\nGo to chrome://flags/#force-major-version-to-100\nSet the option to `Enabled`.\n\nTest and file reports\n\nIf you are a website maintainer, test your website with Chrome and Firefox 100. Review your User-Agent parsing code and libraries, and ensure they are able to handle three-digit version numbers. We have compiled some of the patterns that are currently breaking.\nIf you develop a User-Agent parsing library, add tests to parse versions greater than and equal to 100. Our early tests show that recent versions of libraries can handle it correctly. But the Web is a legacy machine, so if you have old versions of parsing libraries, it’s probably time to check and eventually upgrade.\nIf you are browsing the web and notice any issues with the major version 100, file a report on webcompat.com.\n\nThe post Version 100 in Chrome and Firefox appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-02-15T18:05:20.000Z",
      "date_modified": "2022-02-15T18:05:20.000Z",
      "_plugin": {
        "pageFilename": "91718d0ff887c4fd0181beb5ff01f530881ddd49f4c8fefe1ea2cfef6b960bee.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47599",
      "url": "https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/",
      "title": "Improving the Storage Access API in Firefox",
      "summary": "Before we roll out State Partitioning for all Firefox users, we intend to make a few privacy and ergonomic improvements to the Storage Access API. In this blog post, we’ll detail a few of the new changes we made. \nThe post Improving the Storage Access API in Firefox appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Before we roll out </span><a class=\"editor-rtfLink\" href=\"https://hacks.mozilla.org/2021/02/introducing-state-partitioning/\" target=\"_blank\" rel=\"noopener\"><span data-preserver-spaces=\"true\">State Partitioning</span></a><span data-preserver-spaces=\"true\"> for all Firefox users, we intend to make a few privacy and ergonomic improvements to the </span><a class=\"editor-rtfLink\" href=\"https://privacycg.github.io/storage-access/\" target=\"_blank\" rel=\"noopener\"><span data-preserver-spaces=\"true\">Storage Access API</span></a><span data-preserver-spaces=\"true\">. In this blog post, we’ll detail a few of the new changes we made.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">With State Partitioning, third parties can’t access the same cookie jar when they’re embedded in different sites. Instead, they get a fresh cookie jar for each site they’re embedded in. This isn&#8217;t just limited to cookies either—all storage is partitioned in this way.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">In an ideal world, this would stop trackers from keeping tabs on you wherever they’re embedded because they can&#8217;t keep a unique identifier for you across all of these sites. Unfortunately, the world isn&#8217;t so simple—trackers aren&#8217;t the only third parties that use storage. If you&#8217;ve ever used an authentication provider that requires an embedded resource, you know how important third-party storage can be.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Enter the Storage Access API. This API lets third parties request storage access as if they were a first party. This is called “unpartitioning” and it gives browsers and users control over which third parties can maintain state across first-party origins as well as determine which origins they can access that state from. This is the preferred way for third parties to keep sharing storage across sites.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">The Storage Access API leaves a lot of room for the browser to decide when to allow a third party unrestricted storage access. This is a feature that gives the browser freedom to make decisions it feels are best for the user and decide when to present choices about storage permissions to users directly. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">On the other hand, this means the Storage Access API can vary from browser to browser and version to version. As a result, the developer experience will suffer unless we do two things: 1) Design with the developer experience in mind; and 2) communicate what we’re doing. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">So let’s dive in! Here are four changes we’re making to the Storage Access API that will improve user privacy and maintain a strong developer experience…</span></p>\n<h2 style=\"text-align: left;\"><span data-preserver-spaces=\"true\">Requiring User Consent for Third-Parties the User Never Interacted With</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">With Storage API, the browser determines whether to involve the user in the decision to grant storage access to a third party. Previously, Firefox didn’t involve users until a third party already had access to its storage on five different sites. At that point, the third party&#8217;s storage access requests were presented to users to make a decision. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We’re allowing third parties some leeway to unpartition their storage on a few sites because we’re worried about overwhelming users with popup permission requests. We feel that allowing only a few permission grants per third party would keep the permission frequency down while still preventing any one party from tracking the user on many sites.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We also wanted to improve user privacy in our Storage Access API implementation by reducing the number of times third parties can automatically unpartition themselves without overwhelming the user with storage access requests. The improvement we settled on was requiring the user to have interacted with the third party recently to give them storage access without explicitly asking the user whether or not to allow it. We believe that removing automatic storage access grants for sites the user has never seen before captures the spirit of State Partitioning without having to bother the user too much more.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Careful readers may now be concerned that any embed-only pages, like some authentication services, will be heavily impacted by this. To tip the scales even further toward low user touch, we expanded the definition of “interacting with a site” to support embed-only contexts. Now, whenever a user grants storage access via permission popups or interacts with an iframe with storage access, these both count as user interactions. This change is the result of a lot of careful balancing between preserving legitimate use cases, protecting user privacy, and not annoying users with endless permission prompts. We think we found the sweet spot.</span></p>\n<h2><span data-preserver-spaces=\"true\">Changing the Scope of First-Party Storage Access to Site</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">While rolling out State Partitioning, we’ve seen the emergence of a fair number of use cases for the Storage Access API. One common use is to enable authentication using a third party.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We found on occasion the login portal that gave first-party storage access to the authentication service was a subdomain, like </span><strong><span data-preserver-spaces=\"true\">https://login.example.com</span></strong><span data-preserver-spaces=\"true\">. This caused problems when the user navigated to </span><strong><span data-preserver-spaces=\"true\">https://example.com</span></strong><span data-preserver-spaces=\"true\"> after logging in… they were no longer logged in! This is because the storage access permission was only granted to the login subdomain and not the rest of the site. The authentication provider had access to its cookies on </span><strong><span data-preserver-spaces=\"true\">https://login.example.com</span></strong><span data-preserver-spaces=\"true\">, but not on </span><strong><span data-preserver-spaces=\"true\">https://example.com</span></strong><span data-preserver-spaces=\"true\">. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We fixed this by moving the storage access permission to the Site-scope. This means that when a third party gets storage access on a page, it has access to unpartitioned storage on all pages on that same Site. So in the example above, the authenticating third party would have access to the user&#8217;s login cookie on </span><strong><span data-preserver-spaces=\"true\">https://login.example.com</span></strong><span data-preserver-spaces=\"true\">, </span><strong><span data-preserver-spaces=\"true\">https://example.com</span></strong><span data-preserver-spaces=\"true\">, and </span><strong><span data-preserver-spaces=\"true\">https://any.different.subdomain.example.com</span></strong><span data-preserver-spaces=\"true\">! Yet they still wouldn&#8217;t have access to that login cookie on </span><strong><span data-preserver-spaces=\"true\">http://example.com</span></strong><span data-preserver-spaces=\"true\"> or </span><strong><span data-preserver-spaces=\"true\">https://different-example.com</span></strong><span data-preserver-spaces=\"true\">.</span></p>\n<h2><span data-preserver-spaces=\"true\">Cleaning Up User Interaction Requirements</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Requiring user interaction when requesting storage access was one rough edge of the Storage Access API definition. Let’s talk about that requirement.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">If a third party calls requestStorageAccess as soon as a page loads, it should not get that storage access. It needs to wait until the user interacts with their iframe. Scrolling or clicking are good ways to get this user interaction and it will expire a few seconds after it’s granted. Unfortunately, there were some corner cases in this requirement that we needed to clean up. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">One corner case concerns what to do with the user’s interaction state when they click Accept or Deny on a permission prompt. We decided that when a user clicks Deny on a storage access permission prompt, the third party should lose their user interaction. This prevents the third party from immediately requesting storage access again, bothering the user until they accept. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Conversely, we decided to reset the timer for user interaction if the user clicks Accept to reflect that the user did interact with the third party. This will allow the third party to use APIs that require both storage access and user interaction with only one user interaction in their iframe.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Another corner case concerned how strict to be when requiring user interaction for storage access requests. As we’ve iterated on the Storage Access API, minor changes have been introduced. One of the changes has to do with the case of giving a third party storage access on a page, but then the page is reloaded. Does the third party have to get a user interaction before requesting storage access again? Initially, the answer was no, but now it is yes. We updated our implementation to reflect that change and align with other browsers.</span><span data-preserver-spaces=\"true\"> </span></p>\n<h2><span data-preserver-spaces=\"true\">Integrating User Cookie Preferences</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">In the settings for Firefox </span><a class=\"editor-rtfLink\" href=\"https://support.mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop\" target=\"_blank\" rel=\"noopener\"><span data-preserver-spaces=\"true\">Enhanced Tracking Protection</span></a><span data-preserver-spaces=\"true\">, users can specify how they want the browser to handle cookies. By default, Firefox blocks cookies from known trackers. But we have a few other possible selections, such as allowing all cookies or blocking all third-party cookies. Users can alter this preference to their liking.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We have always respected this user choice when implementing the Storage Access API. However, this wasn&#8217;t clear to developers. For example, users that set Firefox to block all third-party cookies will be relieved to know the Storage Access API in no way weakens their protection; even a storage access permission doesn&#8217;t give a third party any access to storage. But this wasn’t clear to the third party&#8217;s developers. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">The returned promise from requestStorageAccess would resolve, indicating that the third party had access to its unpartitioned storage. We endeavored to fix this. In Firefox 98, when the user has disabled third-party cookies via the preferences, the function requestStorageAccess will always return a rejecting promise and hasStorageAccess will always return false.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\"> </span></p>\n<p><span data-preserver-spaces=\"true\"> </span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/\">Improving the Storage Access API in Firefox</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Before we roll out State Partitioning for all Firefox users, we intend to make a few privacy and ergonomic improvements to the Storage Access API. In this blog post, we’ll detail a few of the new changes we made.\nWith State Partitioning, third parties can’t access the same cookie jar when they’re embedded in different sites. Instead, they get a fresh cookie jar for each site they’re embedded in. This isn’t just limited to cookies either—all storage is partitioned in this way.\nIn an ideal world, this would stop trackers from keeping tabs on you wherever they’re embedded because they can’t keep a unique identifier for you across all of these sites. Unfortunately, the world isn’t so simple—trackers aren’t the only third parties that use storage. If you’ve ever used an authentication provider that requires an embedded resource, you know how important third-party storage can be.\nEnter the Storage Access API. This API lets third parties request storage access as if they were a first party. This is called “unpartitioning” and it gives browsers and users control over which third parties can maintain state across first-party origins as well as determine which origins they can access that state from. This is the preferred way for third parties to keep sharing storage across sites.\nThe Storage Access API leaves a lot of room for the browser to decide when to allow a third party unrestricted storage access. This is a feature that gives the browser freedom to make decisions it feels are best for the user and decide when to present choices about storage permissions to users directly. \nOn the other hand, this means the Storage Access API can vary from browser to browser and version to version. As a result, the developer experience will suffer unless we do two things: 1) Design with the developer experience in mind; and 2) communicate what we’re doing. \nSo let’s dive in! Here are four changes we’re making to the Storage Access API that will improve user privacy and maintain a strong developer experience…\nRequiring User Consent for Third-Parties the User Never Interacted With\nWith Storage API, the browser determines whether to involve the user in the decision to grant storage access to a third party. Previously, Firefox didn’t involve users until a third party already had access to its storage on five different sites. At that point, the third party’s storage access requests were presented to users to make a decision. \nWe’re allowing third parties some leeway to unpartition their storage on a few sites because we’re worried about overwhelming users with popup permission requests. We feel that allowing only a few permission grants per third party would keep the permission frequency down while still preventing any one party from tracking the user on many sites.\nWe also wanted to improve user privacy in our Storage Access API implementation by reducing the number of times third parties can automatically unpartition themselves without overwhelming the user with storage access requests. The improvement we settled on was requiring the user to have interacted with the third party recently to give them storage access without explicitly asking the user whether or not to allow it. We believe that removing automatic storage access grants for sites the user has never seen before captures the spirit of State Partitioning without having to bother the user too much more.\nCareful readers may now be concerned that any embed-only pages, like some authentication services, will be heavily impacted by this. To tip the scales even further toward low user touch, we expanded the definition of “interacting with a site” to support embed-only contexts. Now, whenever a user grants storage access via permission popups or interacts with an iframe with storage access, these both count as user interactions. This change is the result of a lot of careful balancing between preserving legitimate use cases, protecting user privacy, and not annoying users with endless permission prompts. We think we found the sweet spot.\nChanging the Scope of First-Party Storage Access to Site\nWhile rolling out State Partitioning, we’ve seen the emergence of a fair number of use cases for the Storage Access API. One common use is to enable authentication using a third party.\nWe found on occasion the login portal that gave first-party storage access to the authentication service was a subdomain, like https://login.example.com. This caused problems when the user navigated to https://example.com after logging in… they were no longer logged in! This is because the storage access permission was only granted to the login subdomain and not the rest of the site. The authentication provider had access to its cookies on https://login.example.com, but not on https://example.com. \nWe fixed this by moving the storage access permission to the Site-scope. This means that when a third party gets storage access on a page, it has access to unpartitioned storage on all pages on that same Site. So in the example above, the authenticating third party would have access to the user’s login cookie on https://login.example.com, https://example.com, and https://any.different.subdomain.example.com! Yet they still wouldn’t have access to that login cookie on http://example.com or https://different-example.com.\nCleaning Up User Interaction Requirements\nRequiring user interaction when requesting storage access was one rough edge of the Storage Access API definition. Let’s talk about that requirement.\nIf a third party calls requestStorageAccess as soon as a page loads, it should not get that storage access. It needs to wait until the user interacts with their iframe. Scrolling or clicking are good ways to get this user interaction and it will expire a few seconds after it’s granted. Unfortunately, there were some corner cases in this requirement that we needed to clean up. \nOne corner case concerns what to do with the user’s interaction state when they click Accept or Deny on a permission prompt. We decided that when a user clicks Deny on a storage access permission prompt, the third party should lose their user interaction. This prevents the third party from immediately requesting storage access again, bothering the user until they accept. \nConversely, we decided to reset the timer for user interaction if the user clicks Accept to reflect that the user did interact with the third party. This will allow the third party to use APIs that require both storage access and user interaction with only one user interaction in their iframe.\nAnother corner case concerned how strict to be when requiring user interaction for storage access requests. As we’ve iterated on the Storage Access API, minor changes have been introduced. One of the changes has to do with the case of giving a third party storage access on a page, but then the page is reloaded. Does the third party have to get a user interaction before requesting storage access again? Initially, the answer was no, but now it is yes. We updated our implementation to reflect that change and align with other browsers. \nIntegrating User Cookie Preferences\nIn the settings for Firefox Enhanced Tracking Protection, users can specify how they want the browser to handle cookies. By default, Firefox blocks cookies from known trackers. But we have a few other possible selections, such as allowing all cookies or blocking all third-party cookies. Users can alter this preference to their liking.\nWe have always respected this user choice when implementing the Storage Access API. However, this wasn’t clear to developers. For example, users that set Firefox to block all third-party cookies will be relieved to know the Storage Access API in no way weakens their protection; even a storage access permission doesn’t give a third party any access to storage. But this wasn’t clear to the third party’s developers. \nThe returned promise from requestStorageAccess would resolve, indicating that the third party had access to its unpartitioned storage. We endeavored to fix this. In Firefox 98, when the user has disabled third-party cookies via the preferences, the function requestStorageAccess will always return a rejecting promise and hasStorageAccess will always return false.\n \n \nThe post Improving the Storage Access API in Firefox appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-02-08T16:59:21.000Z",
      "date_modified": "2022-02-08T16:59:21.000Z",
      "_plugin": {
        "pageFilename": "4dfb1879c0870b55e08c299c4ee618160f7579b37f741fb3fc64d7b6081dfa79.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47574",
      "url": "https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/",
      "title": "Retrospective and Technical Details on the recent Firefox Outage",
      "summary": "On January 13th 2022, Firefox became unusable for close to two hours for users worldwide. This incident interrupted many people’s workflow. This post highlights the complex series of events and circumstances that, together, triggered a bug deep in the networking code of Firefox. What Happened? Firefox has a number of servers and related infrastructure that […]\nThe post Retrospective and Technical Details on the recent Firefox Outage appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>On January 13th 2022, Firefox became unusable for close to two hours for users worldwide. This incident interrupted many people’s workflow. This post highlights the complex series of events and circumstances that, together, triggered a bug deep in the networking code of Firefox.<span id=\"more-47574\"></span></p>\n<h2>What Happened?</h2>\n<p align=\"justify\">Firefox has a number of servers and related infrastructure that handle several internal services. These include updates, telemetry, certificate management, crash reporting and other similar functionality. This infrastructure is hosted by different cloud service providers that use load balancers to distribute the load evenly across servers. For those services hosted on Google Cloud Platform (GCP) these load balancers have settings related to the HTTP protocol they should advertise and one of these settings is HTTP/3 support with three states: “Enabled”, “Disabled” or “Automatic (default)”. Our load balancers were set to the “Automatic (default)” setting and on January 13, 2022 at 07:28 UTC, GCP deployed an unannounced change to make HTTP/3 the default. As Firefox uses HTTP/3 when supported, from that point forward, some connections that Firefox makes to the services infrastructure would use HTTP/3 instead of the previously used HTTP/2 protocol.<a id=\"footnote1\"></a>¹</p>\n<p align=\"justify\">Shortly after, we noticed a spike in crashes being reported through our crash reporter and also received several reports from inside and outside of Mozilla describing a hang of the browser.</p>\n<p><div id=\"attachment_47575\" style=\"width: 510px\" class=\"wp-caption aligncenter\"><a href=\"https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2.png\"><img aria-describedby=\"caption-attachment-47575\" loading=\"lazy\" class=\"wp-image-47575 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-500x231.png\" alt=\"A graph showing the curve of unprocessed crash reports quickly growing.\" width=\"500\" height=\"231\" srcset=\"https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-500x231.png 500w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-250x115.png 250w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-768x355.png 768w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-1536x709.png 1536w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-2048x946.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a><p id=\"caption-attachment-47575\" class=\"wp-caption-text\">Backlog of pending crash reports building up and reaching close to 300K unprocessed reports.</p></div></p>\n<p align=\"justify\">As part of the incident response process, we quickly discovered that the client was hanging inside a network request to one of the Firefox internal services. However, at this point we neither had an explanation for why this would trigger just now, nor what the scope of the problem was. We continued to look for the “trigger” — some change that must have occurred to start the problem. We found that we had not shipped updates or configuration changes that could have caused this problem. At the same time, we were keeping in mind that HTTP/3 had been enabled since Firefox 88 and was actively used by some popular websites.</p>\n<p align=\"justify\">Although we couldn’t see it, we suspected that there had been some kind of “invisible” change rolled out by one of our cloud providers that somehow modified load balancer behavior. On closer inspection, none of our settings were changed. We then discovered through logs that for some reason, the load balancers for our Telemetry service were serving HTTP/3 connections while they hadn’t done that before. We disabled HTTP/3 explicitly on GCP at 09:12 UTC. This unblocked our users, but we were not yet certain about the root cause and without knowing that, it was impossible for us to tell if this would affect additional HTTP/3 connections.</p>\n<p align=\"justify\"><small><a href=\"#footnote1\">¹</a> <i>Some highly critical services such as updates use a special <code>beConservative</code> flag that prevents the use of any experimental technology for their connections (e.g. HTTP/3).</i></small></p>\n<h2>A Special Mix of Ingredients</h2>\n<p align=\"justify\">It quickly became clear to us that there must be some combination of special circumstances for the hang to occur. We performed a number of tests with various tools and remote services and were not able to reproduce the problem, not even with a regular connection to the Telemetry staging server (a server only used for testing deployments, which we had left in its original configuration for testing purposes). With Firefox itself, however, we were able to reproduce the issue with the staging server.</p>\n<p align=\"justify\">After further debugging, we found the “special ingredient” required for this bug to happen. All HTTP/3 connections go through Necko, our networking stack. However, Rust components that need direct network access are not using Necko directly, but are calling into it through an intermediate library called <a href=\"https://github.com/mozilla/application-services/tree/main/components/viaduct\"><i><code>viaduct</code></i></a>.</p>\n<p align=\"justify\">In order to understand why this mattered, we first need to understand some things about the internals of Necko, in particular about HTTP/3 upload requests. For such requests, the higher-level Necko APIs<a id=\"footnote2\"></a>² check if the <code>Content-Length</code> header is present and if it isn&#8217;t, it will automatically be added. The lower-level HTTP/3 code later relies on this header to determine the request size. This works fine for web content and other requests in our code.</p>\n<p align=\"justify\">When requests pass through <code>viaduct</code> first, however, <code>viaduct</code> will lower-case each header and pass it on to Necko. And here is the problem: the API checks in Necko are case-<b>insensitive</b> while the lower-level HTTP/3 code is case-<b>sensitive</b>. So if any code was to add a <code>Content-Length</code> header and pass the request through <code>viaduct</code>, it would pass the Necko API checks but the HTTP/3 code would not find the header.</p>\n<p align=\"justify\">It just so happens that Telemetry is currently the only Rust-based component in Firefox Desktop that uses the network stack and adds a <code>Content-Length</code> header. This is why users who disabled Telemetry would see this problem resolved even though the problem is not related to Telemetry functionality itself and could have been triggered otherwise.</p>\n<p><div id=\"attachment_47579\" style=\"width: 510px\" class=\"wp-caption aligncenter\"><a href=\"https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4.png\"><img aria-describedby=\"caption-attachment-47579\" loading=\"lazy\" class=\"wp-image-47579 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-500x266.png\" alt=\"A diagram showing the different network components in Firefox.\" width=\"500\" height=\"266\" srcset=\"https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-500x266.png 500w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-250x133.png 250w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-768x409.png 768w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-1536x818.png 1536w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4.png 1826w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a><p id=\"caption-attachment-47579\" class=\"wp-caption-text\">A specific code path was required to trigger the problem in the HTTP/3 protocol implementation.</p></div></p>\n<p align=\"justify\"><small><a href=\"#footnote2\">²</a> <i>These are internal APIs, not accessible to web content.</i></small></p>\n<h2>The Infinite Loop</h2>\n<p align=\"justify\">With the load balancer change in place, and a special code path in a new Rust service now active, the necessary final ingredient to trigger the problem for users was deep in Necko HTTP/3 code.</p>\n<p align=\"justify\">When handling a request, the code <a href=\"https://searchfox.org/mozilla-central/rev/435a77f1a1aaf1a78d30a2aaa81c6158a2f83dba/netwerk/protocol/http/Http3Stream.cpp#71,79-83\">looked up the field in a case-sensitive way</a> and failed to find the header as it had been lower-cased by <code>viaduct</code>. Without the header, the request was determined by the Necko code to be complete, leaving the real request body unsent. However, this code would only terminate when there was no additional content to send. This <a href=\"https://searchfox.org/mozilla-central/rev/435a77f1a1aaf1a78d30a2aaa81c6158a2f83dba/netwerk/protocol/http/Http3Stream.cpp#223,228,272-274\">unexpected state caused the code to loop indefinitely rather than returning an error</a>. Because all network requests go through one <i>socket thread</i>, this loop blocked any further network communication and made Firefox unresponsive, unable to load web content.</p>\n<h2>Lessons Learned</h2>\n<p align=\"justify\">As so often is the case, the issue was a lot more complex than it appeared at first glance and there were many contributing factors working together. Some of the key factors we have identified include:</p>\n<ul>\n<li aria-level=\"1\">\n<p align=\"justify\">GCP’s deployment of HTTP/3 as default was unannounced. We are actively working with them to improve the situation. We realize that an announcement (as is usually sent) might not have entirely mitigated the risk of an incident, but it would likely have triggered more controlled experiments (e.g. in a staging environment) and deployment.</p>\n</li>\n<li aria-level=\"1\">\n<p align=\"justify\">Our setting of “Automatic (default)” on the load balancers instead of a more explicit choice allowed the deployment to take place automatically. We are reviewing all service configurations to avoid similar mistakes in the future.</p>\n</li>\n<li aria-level=\"1\">\n<p align=\"justify\">The particular combination of HTTP/3 and <code>viaduct</code> on Firefox Desktop was not covered in our continuous integration system. While we cannot test every possible combination of configurations and components, the choice of HTTP version is a fairly major change that should have been tested, as well as the use of an additional networking layer like <code>viaduct</code>. Current HTTP/3 tests cover the low-level protocol behavior and the Necko layer as it is used by web content. We should run more system tests with different HTTP versions and doing so could have revealed this problem.</p>\n</li>\n</ul>\n<p align=\"justify\">We are also investigating action points both to make the browser more resilient towards such problems and to make incident response even faster. Learning as much as possible from this incident will help us improve the quality of our products. We’re grateful to all the users who have sent crash reports, worked with us in Bugzilla or helped others to work around the problem.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/\">Retrospective and Technical Details on the recent Firefox Outage</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "On January 13th 2022, Firefox became unusable for close to two hours for users worldwide. This incident interrupted many people’s workflow. This post highlights the complex series of events and circumstances that, together, triggered a bug deep in the networking code of Firefox.\nWhat Happened?\nFirefox has a number of servers and related infrastructure that handle several internal services. These include updates, telemetry, certificate management, crash reporting and other similar functionality. This infrastructure is hosted by different cloud service providers that use load balancers to distribute the load evenly across servers. For those services hosted on Google Cloud Platform (GCP) these load balancers have settings related to the HTTP protocol they should advertise and one of these settings is HTTP/3 support with three states: “Enabled”, “Disabled” or “Automatic (default)”. Our load balancers were set to the “Automatic (default)” setting and on January 13, 2022 at 07:28 UTC, GCP deployed an unannounced change to make HTTP/3 the default. As Firefox uses HTTP/3 when supported, from that point forward, some connections that Firefox makes to the services infrastructure would use HTTP/3 instead of the previously used HTTP/2 protocol.¹\nShortly after, we noticed a spike in crashes being reported through our crash reporter and also received several reports from inside and outside of Mozilla describing a hang of the browser.\nBacklog of pending crash reports building up and reaching close to 300K unprocessed reports.\nAs part of the incident response process, we quickly discovered that the client was hanging inside a network request to one of the Firefox internal services. However, at this point we neither had an explanation for why this would trigger just now, nor what the scope of the problem was. We continued to look for the “trigger” — some change that must have occurred to start the problem. We found that we had not shipped updates or configuration changes that could have caused this problem. At the same time, we were keeping in mind that HTTP/3 had been enabled since Firefox 88 and was actively used by some popular websites.\nAlthough we couldn’t see it, we suspected that there had been some kind of “invisible” change rolled out by one of our cloud providers that somehow modified load balancer behavior. On closer inspection, none of our settings were changed. We then discovered through logs that for some reason, the load balancers for our Telemetry service were serving HTTP/3 connections while they hadn’t done that before. We disabled HTTP/3 explicitly on GCP at 09:12 UTC. This unblocked our users, but we were not yet certain about the root cause and without knowing that, it was impossible for us to tell if this would affect additional HTTP/3 connections.\n¹ Some highly critical services such as updates use a special beConservative flag that prevents the use of any experimental technology for their connections (e.g. HTTP/3).\nA Special Mix of Ingredients\nIt quickly became clear to us that there must be some combination of special circumstances for the hang to occur. We performed a number of tests with various tools and remote services and were not able to reproduce the problem, not even with a regular connection to the Telemetry staging server (a server only used for testing deployments, which we had left in its original configuration for testing purposes). With Firefox itself, however, we were able to reproduce the issue with the staging server.\nAfter further debugging, we found the “special ingredient” required for this bug to happen. All HTTP/3 connections go through Necko, our networking stack. However, Rust components that need direct network access are not using Necko directly, but are calling into it through an intermediate library called viaduct.\nIn order to understand why this mattered, we first need to understand some things about the internals of Necko, in particular about HTTP/3 upload requests. For such requests, the higher-level Necko APIs² check if the Content-Length header is present and if it isn’t, it will automatically be added. The lower-level HTTP/3 code later relies on this header to determine the request size. This works fine for web content and other requests in our code.\nWhen requests pass through viaduct first, however, viaduct will lower-case each header and pass it on to Necko. And here is the problem: the API checks in Necko are case-insensitive while the lower-level HTTP/3 code is case-sensitive. So if any code was to add a Content-Length header and pass the request through viaduct, it would pass the Necko API checks but the HTTP/3 code would not find the header.\nIt just so happens that Telemetry is currently the only Rust-based component in Firefox Desktop that uses the network stack and adds a Content-Length header. This is why users who disabled Telemetry would see this problem resolved even though the problem is not related to Telemetry functionality itself and could have been triggered otherwise.\nA specific code path was required to trigger the problem in the HTTP/3 protocol implementation.\n² These are internal APIs, not accessible to web content.\nThe Infinite Loop\nWith the load balancer change in place, and a special code path in a new Rust service now active, the necessary final ingredient to trigger the problem for users was deep in Necko HTTP/3 code.\nWhen handling a request, the code looked up the field in a case-sensitive way and failed to find the header as it had been lower-cased by viaduct. Without the header, the request was determined by the Necko code to be complete, leaving the real request body unsent. However, this code would only terminate when there was no additional content to send. This unexpected state caused the code to loop indefinitely rather than returning an error. Because all network requests go through one socket thread, this loop blocked any further network communication and made Firefox unresponsive, unable to load web content.\nLessons Learned\nAs so often is the case, the issue was a lot more complex than it appeared at first glance and there were many contributing factors working together. Some of the key factors we have identified include:\n\n\nGCP’s deployment of HTTP/3 as default was unannounced. We are actively working with them to improve the situation. We realize that an announcement (as is usually sent) might not have entirely mitigated the risk of an incident, but it would likely have triggered more controlled experiments (e.g. in a staging environment) and deployment.\n\n\nOur setting of “Automatic (default)” on the load balancers instead of a more explicit choice allowed the deployment to take place automatically. We are reviewing all service configurations to avoid similar mistakes in the future.\n\n\nThe particular combination of HTTP/3 and viaduct on Firefox Desktop was not covered in our continuous integration system. While we cannot test every possible combination of configurations and components, the choice of HTTP version is a fairly major change that should have been tested, as well as the use of an additional networking layer like viaduct. Current HTTP/3 tests cover the low-level protocol behavior and the Necko layer as it is used by web content. We should run more system tests with different HTTP versions and doing so could have revealed this problem.\n\n\nWe are also investigating action points both to make the browser more resilient towards such problems and to make incident response even faster. Learning as much as possible from this incident will help us improve the quality of our products. We’re grateful to all the users who have sent crash reports, worked with us in Bugzilla or helped others to work around the problem.\nThe post Retrospective and Technical Details on the recent Firefox Outage appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-02-02T09:00:50.000Z",
      "date_modified": "2022-02-02T09:00:50.000Z",
      "_plugin": {
        "pageFilename": "30d427542a946a9d41c300459f38abecebf217acc2e45146d18fe16c799dfebd.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47585",
      "url": "https://hacks.mozilla.org/2022/01/hacks-decoded-adewale-adetona/",
      "title": "Hacks Decoded: Adewale Adetona",
      "summary": "Adetona Adewale Akeem, more popularly known as iSlimfit, is a Nigeria-born revered digital technologist and marketing expert. He is the co-founder of Menopays, a fintech startup offering another Buy Now Pay Later (BNPL) option across Africa. We chatted with him about founding Menopays and the impact of tech solutions developed in Nigeria. \nThe post Hacks Decoded: Adewale Adetona appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><i>Welcome to our Hacks: Decoded Interview series!</i></p>\n<p><i>Once a month, </i><a href=\"https://foundation.mozilla.org/\" target=\"_blank\" rel=\"noopener\"><i>Mozilla Foundation</i></a><i>’s </i><a href=\"https://www.xavierharding.com/\" target=\"_blank\" rel=\"noopener\"><i>Xavier Harding</i></a><i> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </i><a href=\"https://hacks.mozilla.org/\"><i>Hacks</i></a><i> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.</i></p>\n<p><strong>Meet Adetona Adewale Akeem!</strong></p>\n<p><strong><img loading=\"lazy\" class=\"aligncenter\" src=\"https://cdn.vanguardngr.com/wp-content/uploads/2021/07/WA-683x1024.jpeg\" alt=\"Adewale Adetona\" width=\"395\" height=\"593\" /></strong></p>\n<p><span style=\"font-weight: 400;\">Adetona Adewale Akeem, more popularly known as iSlimfit, is a Nigeria-born revered digital technologist and marketing expert. He is the co-founder of <a href=\"https://menopays.com/\">Menopays,</a> a fintech startup offering another Buy Now Pay Later (BNPL) option across Africa. </span></p>\n<p><b>So, I’ve got to ask — where does the name iSlimfit come from?</b></p>\n<p><span style=\"font-weight: 400;\">“Slimfit” is a nickname from my University days. But when I wanted to join social media, Twitter, in particular, I figured out the username Slimfit was already taken. All efforts to reach and plead with the user — who even up until now has never posted anything on the account — to release the username for me proved abortive. Then I came up with another username by adding “i” (which signifies referring to myself) to the front of Slimfit. </span><span style=\"font-weight: 400;\"><br />\n</span><span style=\"font-weight: 400;\"><br />\n</span><b>How did you get started in the tech industry, iSlimfit?</b><b></b></p>\n<p><span style=\"font-weight: 400;\">My journey into tech started as far back as 2014, when I made the switch from working at a Media &amp; Advertising Agency in Lagos Nigeria to working as a Digital Marketing Executive in a Fintech Company called SystemSpecs in Nigeria. Being someone that loved combining data with tech, I have always had a knack for growth marketing. So the opportunity to work in a fintech company in that capacity wasn’t something I could let slide.</span></p>\n<p><b>Where are you based currently? And where are you from originally? How does where you&#8217;re from affect how you move through the tech industry?</b></p>\n<p><span style=\"font-weight: 400;\">I am currently based in Leeds, United Kingdom after recently getting a <a href=\"https://technation.io/visa/\">Tech Nation Global Talent</a> endorsement by the UK government. I am from Ogun State, Nigeria. </span></p>\n<p><span style=\"font-weight: 400;\">There is actually no negative impact from my background or where I am from as regards my work in tech. The Nigerian tech space is huge and the opportunities are enormous. Strategic positioning and working with a goal in mind has helped me in navigating my career in tech so far.</span></p>\n<p><b>What brought about the idea of your new vlog Tech Chat with iSlimfit?</b></p>\n<p><b></b><b></b><span style=\"font-weight: 400;\">My desire to make an impact and contribute to the growth of upcoming tech professionals birthed </span><a href=\"https://www.youtube.com/c/AdetonaAdewaleSlimfit\"><span style=\"font-weight: 400;\">the vlog</span></a><span style=\"font-weight: 400;\">. Also, I wanted to replicate what I do offline with Lagos Digital Summit, in an online manner. The vlog is basically a series of YouTube chat series where I bring various people in tech — growth marketers, UI/UX designers, product managers, startup founders, mobile app developers, etc. — to share their career journey, background, transitioning, their career journey, learnings, and general questions about their day-to-day job so that Tech enthusiasts can learn from their expertise.</span></p>\n<p><b>I have to bring up the fact that in 2021, you were endorsed by Tech Nation as an Exceptional agent in Digital Tech. What’s it feel like to achieve something like that?</b><b><br />\n</b><b><br />\n</b><span style=\"font-weight: 400;\">The Tech Nation endorsement by the UK government is one of my biggest achievements. It made me realize how important my impact on the Nigerian tech industry over the years has been. The endorsement was granted based on my significant contribution to the Nigerian Digital Tech sector, my mentorship &amp; leadership capabilities, and also the potential contribution my talent &amp; expertise would add to the UK digital economy. I am particularly grateful for the opportunity to positively make an impact to the digital economy of the United Kingdom.</span></p>\n<p><b>What&#8217;s something folks may not immediately realize about the tech sector in Nigeria if they’re not from there?</b><b></b></p>\n<p><span style=\"font-weight: 400;\">Easy: the fact that the tech sector in Nigeria is the biggest in Africa, and the impact of tech solutions developed in Nigeria is felt all over Africa. Also, as we can see from a recent </span><a href=\"https://www.linkedin.com/posts/islimfit_startups-activity-6871734350942085120-Jh0V\"><b>report</b></a><span style=\"font-weight: 400;\">, Nigerian startups lead the list of African Startups that received funding in 2021.</span></p>\n<p><b>What digital policy or policies do you think Nigeria (your home country) should pursue in order to accelerate digital development in the country?</b></p>\n<p><span style=\"font-weight: 400;\">The Nigerian government need to come to terms with the fact that digital technology is the bedrock for the development of the Nation. They need to develop policies that will shape the Nation’s digital economy and design a roadmap for grassroots digital Tech empowerment of Nigeria’s agile population. </span></p>\n<p><span style=\"font-weight: 400;\">We also need more people to champion and improve on our quest for digital entrepreneurship development through various platforms.</span></p>\n<p><b>You helped co-found a company called Menopays. What were some of the hurdles when it comes to getting a tech company off the ground over there? What about the opposite? What are the ways those in tech benefit from founding and working in Nigeria?</b></p>\n<p><span style=\"font-weight: 400;\">Some hurdles in starting a tech company is putting together the right team for the job. This cuts across legal, product, marketing, and the tech itself. The idea could be great but without the right team, execution is challenging. </span></p>\n<p><span style=\"font-weight: 400;\">A great benefit is that the continent of Africa is gaining in popularity and the world is watching, so a genuine team founding a business will get the benefits of foreign investments which is great in terms of dollar value.</span></p>\n<p><b>Some take issue with </b><a href=\"https://www.nerdwallet.com/article/loans/personal-loans/buy-now-pay-later-apps\"><b>Buy Now Pay Later apps</b></a><b> and services like Menopays in how they may profit off of buyers who may have less. How is Menopays different? How does the company make money? What measures are in place to make sure you aren’t taking advantage of people?</b></p>\n<p><span style=\"font-weight: 400;\">Menopays is different because our focus goes beyond the profitability of the industry. We tailored a minimum spendable amount with a decent repayment period for the minimum wage in Nigeria. Our vision stands in the middle of every decision we make both business-wise and/or product development-wise. </span></p>\n<p><span style=\"font-weight: 400;\">The measure in place is that decisions are guided by why we started Menopays, which is “to fight poverty”. We don’t charge customers exorbitant interest as it goes against what we are preaching as a brand. So our Vision is imprinted in the heart of all the team members working towards making Menopays a family brand.</span></p>\n<p><b>You’ve </b><a href=\"https://technext.ng/2021/12/03/lagos-digital-summit-has-inspired-the-birth-of-similar-digital-gatherings-across-nigeria-adewale-adetona/\"><b>mentioned</b></a><b> Menopays is fighting poverty in Nigeria and eventually all of Africa, how so?</b><b><br />\n</b><b><br />\n</b><span style=\"font-weight: 400;\">Thinking about one of the incidents that happened to one of our co-founders, Reuben Olawale Odumosu, about eight years back. He lost his best friend because of a substandard malaria medication. His best friend in high school died because his parents couldn’t afford NGN2,500 malaria medication at the time and point of need which led to them going for a cheaper drug that eventually led to his death. Menopays exists to prevent such situations by making basic needs like healthcare, groceries and clothing available to our customers even when they don’t have the money to pay at that moment.</span></p>\n<p><span style=\"font-weight: 400;\">So in light of this, at Menopays, we believe that if some particular things are taken care of, individuals stand a lot more chances of survival. Take for instance, someone earns NGN18,000, spends NGN5,000 on transport, NGN7,000 on food and rent and some other miscellaneous of NGN6,000; with Menopays, we take out the cost of transportation and food (by providing you access to our merchants) and we give them more time to pay over the next three months. Which means each month the customer is positive cash flow of NGN6,000. We turn a negative cash flow into a positive cash flow and savings, thereby fighting poverty.</span></p>\n<p><b>If you didn’t help found Menopays, what would you be doing now instead?</b></p>\n<p><span style=\"font-weight: 400;\">I would probably be working on founding another tech startup doing something for the greater good of the world and helping brands achieve their desired marketing objectives.</span></p>\n<p><b>How can the African tech diaspora help startups similar to Menopays?</b></p>\n<p><span style=\"font-weight: 400;\">One way African tech diaspora can help startups similar to Menopays is by promoting their services, sharing with potential users, and also by investing in it.</span></p>\n<p><b>How did you come up with the idea for Lagos Digital Summit?</b><b></b></p>\n<p><span style=\"font-weight: 400;\">Lagos Digital Summit started in 2017 with just an idea in my small shared apartment back then in Lagos with my friend who is now in Canada. The goal back then was simply to facilitate a platform for the convergence of 50 to 60 digital marketing professionals and business thought leaders for the advancement of SMEs and Digital Media enthusiasts within our network.</span><span style=\"font-weight: 400;\"><br />\n</span><span style=\"font-weight: 400;\"><br />\n</span><span style=\"font-weight: 400;\">Five years down the line, despite being faced with plenty of challenges, it&#8217;s been a big success story. We have had the privilege of empowering over 5,000 businesses and individuals with diverse digital marketing skills. </span></p>\n<p><b>What’s it been like arranging that sort of summit in the midst of a pandemic?</b></p>\n<p><span style=\"font-weight: 400;\">Lagos Digital Summit 2020 has been the only edition that we’ve had to do full virtual because it was in the peak of the COVID-19 pandemic. Every other edition before then had been physical with fully packed attendees of an average of 1,000. For the 2021 edition, it was hybrid because Covid-19 restrictions were relaxed, where we had just 300 people attend physically and every other people watched online.</span></p>\n<p><b>What&#8217;s something you see everywhere in tech that you wish more people would talk about?</b></p>\n<p><span style=\"font-weight: 400;\">I wish more people would talk about the struggle, the disappointments, the challenges and the numerous sacrifices that comes with building a tech startup. A lot of times, the media only portray the success stories, especially when a startup raises funds; the headlines are always very inspiring and rosy. </span></p>\n<p><b>What’s been the most impactful thing you’ve done since working in tech? What’s been the most memorable?</b><b><br />\n</b><b><br />\n</b><span style=\"font-weight: 400;\">That should be founding Lagos Digital Summit; the kind of sponsors, corporate organisations, high-profiled speakers, volunteers and attendees that the Summit has been able to attract has been a memorable and proud feeling.</span></p>\n<p><b>What sort of lasting impact do you want to have on the industry and the world? What keeps you going?</b></p>\n<p><span style=\"font-weight: 400;\">Waking up every day, knowing that a lot of people would have a smile on their faces because I have chosen to impact lives and make the world a better place through relevant tech solutions and platforms is the best feeling for me. The fact that I can read through reports and data and see the number of people using Menopays as a Buy Now Pay Later (BNPL) payment option to ease their lifestyle is a big motivation for me. </span></p>\n<p><b>What’s some advice you’d give to others hoping to enter the tech world or hoping to start up a company?</b></p>\n<p><span style=\"font-weight: 400;\">Venturing into Tech or building a Startup takes a whole lot of concerted effort and determination. Getting the right set of partner(s) would however make the journey easier for you. Just have partners or cofounders with similar vision and complementing skills.</span></p>\n<p>—</p>\n<p><em>You can keep up with Adewale’s work by following him <a href=\"https://twitter.com/iSlimfit\" target=\"_blank\" rel=\"noopener\">here.</a> Stay tuned for more Hacks Decoded Q&amp;A’s!</em></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/01/hacks-decoded-adewale-adetona/\">Hacks Decoded: Adewale Adetona</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Welcome to our Hacks: Decoded Interview series!\nOnce a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.\nMeet Adetona Adewale Akeem!\n\nAdetona Adewale Akeem, more popularly known as iSlimfit, is a Nigeria-born revered digital technologist and marketing expert. He is the co-founder of Menopays, a fintech startup offering another Buy Now Pay Later (BNPL) option across Africa. \nSo, I’ve got to ask — where does the name iSlimfit come from?\n“Slimfit” is a nickname from my University days. But when I wanted to join social media, Twitter, in particular, I figured out the username Slimfit was already taken. All efforts to reach and plead with the user — who even up until now has never posted anything on the account — to release the username for me proved abortive. Then I came up with another username by adding “i” (which signifies referring to myself) to the front of Slimfit. \n\nHow did you get started in the tech industry, iSlimfit?\nMy journey into tech started as far back as 2014, when I made the switch from working at a Media & Advertising Agency in Lagos Nigeria to working as a Digital Marketing Executive in a Fintech Company called SystemSpecs in Nigeria. Being someone that loved combining data with tech, I have always had a knack for growth marketing. So the opportunity to work in a fintech company in that capacity wasn’t something I could let slide.\nWhere are you based currently? And where are you from originally? How does where you’re from affect how you move through the tech industry?\nI am currently based in Leeds, United Kingdom after recently getting a Tech Nation Global Talent endorsement by the UK government. I am from Ogun State, Nigeria. \nThere is actually no negative impact from my background or where I am from as regards my work in tech. The Nigerian tech space is huge and the opportunities are enormous. Strategic positioning and working with a goal in mind has helped me in navigating my career in tech so far.\nWhat brought about the idea of your new vlog Tech Chat with iSlimfit?\nMy desire to make an impact and contribute to the growth of upcoming tech professionals birthed the vlog. Also, I wanted to replicate what I do offline with Lagos Digital Summit, in an online manner. The vlog is basically a series of YouTube chat series where I bring various people in tech — growth marketers, UI/UX designers, product managers, startup founders, mobile app developers, etc. — to share their career journey, background, transitioning, their career journey, learnings, and general questions about their day-to-day job so that Tech enthusiasts can learn from their expertise.\nI have to bring up the fact that in 2021, you were endorsed by Tech Nation as an Exceptional agent in Digital Tech. What’s it feel like to achieve something like that?\n\nThe Tech Nation endorsement by the UK government is one of my biggest achievements. It made me realize how important my impact on the Nigerian tech industry over the years has been. The endorsement was granted based on my significant contribution to the Nigerian Digital Tech sector, my mentorship & leadership capabilities, and also the potential contribution my talent & expertise would add to the UK digital economy. I am particularly grateful for the opportunity to positively make an impact to the digital economy of the United Kingdom.\nWhat’s something folks may not immediately realize about the tech sector in Nigeria if they’re not from there?\nEasy: the fact that the tech sector in Nigeria is the biggest in Africa, and the impact of tech solutions developed in Nigeria is felt all over Africa. Also, as we can see from a recent report, Nigerian startups lead the list of African Startups that received funding in 2021.\nWhat digital policy or policies do you think Nigeria (your home country) should pursue in order to accelerate digital development in the country?\nThe Nigerian government need to come to terms with the fact that digital technology is the bedrock for the development of the Nation. They need to develop policies that will shape the Nation’s digital economy and design a roadmap for grassroots digital Tech empowerment of Nigeria’s agile population. \nWe also need more people to champion and improve on our quest for digital entrepreneurship development through various platforms.\nYou helped co-found a company called Menopays. What were some of the hurdles when it comes to getting a tech company off the ground over there? What about the opposite? What are the ways those in tech benefit from founding and working in Nigeria?\nSome hurdles in starting a tech company is putting together the right team for the job. This cuts across legal, product, marketing, and the tech itself. The idea could be great but without the right team, execution is challenging. \nA great benefit is that the continent of Africa is gaining in popularity and the world is watching, so a genuine team founding a business will get the benefits of foreign investments which is great in terms of dollar value.\nSome take issue with Buy Now Pay Later apps and services like Menopays in how they may profit off of buyers who may have less. How is Menopays different? How does the company make money? What measures are in place to make sure you aren’t taking advantage of people?\nMenopays is different because our focus goes beyond the profitability of the industry. We tailored a minimum spendable amount with a decent repayment period for the minimum wage in Nigeria. Our vision stands in the middle of every decision we make both business-wise and/or product development-wise. \nThe measure in place is that decisions are guided by why we started Menopays, which is “to fight poverty”. We don’t charge customers exorbitant interest as it goes against what we are preaching as a brand. So our Vision is imprinted in the heart of all the team members working towards making Menopays a family brand.\nYou’ve mentioned Menopays is fighting poverty in Nigeria and eventually all of Africa, how so?\n\nThinking about one of the incidents that happened to one of our co-founders, Reuben Olawale Odumosu, about eight years back. He lost his best friend because of a substandard malaria medication. His best friend in high school died because his parents couldn’t afford NGN2,500 malaria medication at the time and point of need which led to them going for a cheaper drug that eventually led to his death. Menopays exists to prevent such situations by making basic needs like healthcare, groceries and clothing available to our customers even when they don’t have the money to pay at that moment.\nSo in light of this, at Menopays, we believe that if some particular things are taken care of, individuals stand a lot more chances of survival. Take for instance, someone earns NGN18,000, spends NGN5,000 on transport, NGN7,000 on food and rent and some other miscellaneous of NGN6,000; with Menopays, we take out the cost of transportation and food (by providing you access to our merchants) and we give them more time to pay over the next three months. Which means each month the customer is positive cash flow of NGN6,000. We turn a negative cash flow into a positive cash flow and savings, thereby fighting poverty.\nIf you didn’t help found Menopays, what would you be doing now instead?\nI would probably be working on founding another tech startup doing something for the greater good of the world and helping brands achieve their desired marketing objectives.\nHow can the African tech diaspora help startups similar to Menopays?\nOne way African tech diaspora can help startups similar to Menopays is by promoting their services, sharing with potential users, and also by investing in it.\nHow did you come up with the idea for Lagos Digital Summit?\nLagos Digital Summit started in 2017 with just an idea in my small shared apartment back then in Lagos with my friend who is now in Canada. The goal back then was simply to facilitate a platform for the convergence of 50 to 60 digital marketing professionals and business thought leaders for the advancement of SMEs and Digital Media enthusiasts within our network.\n\nFive years down the line, despite being faced with plenty of challenges, it’s been a big success story. We have had the privilege of empowering over 5,000 businesses and individuals with diverse digital marketing skills. \nWhat’s it been like arranging that sort of summit in the midst of a pandemic?\nLagos Digital Summit 2020 has been the only edition that we’ve had to do full virtual because it was in the peak of the COVID-19 pandemic. Every other edition before then had been physical with fully packed attendees of an average of 1,000. For the 2021 edition, it was hybrid because Covid-19 restrictions were relaxed, where we had just 300 people attend physically and every other people watched online.\nWhat’s something you see everywhere in tech that you wish more people would talk about?\nI wish more people would talk about the struggle, the disappointments, the challenges and the numerous sacrifices that comes with building a tech startup. A lot of times, the media only portray the success stories, especially when a startup raises funds; the headlines are always very inspiring and rosy. \nWhat’s been the most impactful thing you’ve done since working in tech? What’s been the most memorable?\n\nThat should be founding Lagos Digital Summit; the kind of sponsors, corporate organisations, high-profiled speakers, volunteers and attendees that the Summit has been able to attract has been a memorable and proud feeling.\nWhat sort of lasting impact do you want to have on the industry and the world? What keeps you going?\nWaking up every day, knowing that a lot of people would have a smile on their faces because I have chosen to impact lives and make the world a better place through relevant tech solutions and platforms is the best feeling for me. The fact that I can read through reports and data and see the number of people using Menopays as a Buy Now Pay Later (BNPL) payment option to ease their lifestyle is a big motivation for me. \nWhat’s some advice you’d give to others hoping to enter the tech world or hoping to start up a company?\nVenturing into Tech or building a Startup takes a whole lot of concerted effort and determination. Getting the right set of partner(s) would however make the journey easier for you. Just have partners or cofounders with similar vision and complementing skills.\n—\nYou can keep up with Adewale’s work by following him here. Stay tuned for more Hacks Decoded Q&A’s!\nThe post Hacks Decoded: Adewale Adetona appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-01-31T17:44:10.000Z",
      "date_modified": "2022-01-31T17:44:10.000Z",
      "_plugin": {
        "pageFilename": "9fb82410659e819dbec442d9df0f04c4df387e2a7a5205273156a3038181925e.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47555",
      "url": "https://hacks.mozilla.org/2022/01/contributing-to-mdn-meet-the-contributors/",
      "title": "Contributing to MDN: Meet the Contributors",
      "summary": "If you’ve ever built anything with web technologies, you’re probably familiar with MDN Web Docs. With about 13,000 pages documenting how to use programming languages such as HTML, CSS and JavaScript, the site has about 8,000 people using it at any given moment. MDN relies on contributors to help maintain its ever-expanding and up to date documentation. We reached out to 4 long-time community contributors to talk about how and why they started contributing, why they kept going, and ask what advice they have for new contributors.\nThe post Contributing to MDN: Meet the Contributors appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">If you’ve ever built anything with web technologies, you’re probably familiar with MDN Web Docs. With about 13,000 pages documenting how to use programming languages such as HTML, CSS and JavaScript, the site has about 8,000 people using it at any given moment.</span></p>\n<p><span style=\"font-weight: 400;\">MDN relies on contributors to help maintain its ever-expanding and up to date documentation. Supported by companies such as Open Web Docs, Google, w3c, Microsoft, Samsung and Igalia (to name a few), contributions also come from community members. These contributions take many different forms, from fixing issues to contributing code to helping newcomers and localizing content.</span></p>\n<p><span style=\"font-weight: 400;\">We reached out to 4 long-time community contributors to talk about how and why they started contributing, why they kept going, and ask what advice they have for new contributors.</span></p>\n<h2><b>Meet the contributors</b></h2>\n<p><span style=\"font-weight: 400;\">MDN contributors come from all over the world, have different backgrounds, and contribute in different ways. </span></p>\n<p><span style=\"font-weight: 400;\">Irvin and Julien&#8217;s main area of contribution is localizations. They are part of a diverse team of volunteers that ensure that MDN is translated in seven different languages (Discover </span><a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute/Localize\"><span style=\"font-weight: 400;\">here how translations of MDN content happens</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">Since the end of 2020, the translation of MDN articles happen on the new GitHub based platform.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47557 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-500x334.jpg\" alt=\"Irvin\" width=\"500\" height=\"334\" srcset=\"https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-500x334.jpg 500w, https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-250x167.jpg 250w, https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-768x513.jpg 768w, https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c.jpg 799w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p style=\"text-align: center;\"><b><i>Irvin, @irvinfly, volunteer from Mozilla Taiwan Community</i></b></p>\n<p><i><span style=\"font-weight: 400;\">I had been a front-end engineer for more than a decade. I had been a leisure contributor on MDN for a long time. I check MDN all the time when writing websites, but only made some simple contributions, like fixing typos.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">In early 2020, the MDN team asked us if zh (Chinese) locale would like to join the early stage of the localization system on </span></i><a href=\"https://github.com/mdn/yari)\"><i><span style=\"font-weight: 400;\">Yari</span></i></a><i><span style=\"font-weight: 400;\">, the new Github-based platform. We accepted the invitation and formed the </span></i><a href=\"https://github.com/mdn/translated-content/blob/main/PEERS_GUIDELINES.md#review-teams\"><i><span style=\"font-weight: 400;\">zh-review-tea</span></i></a><i><span style=\"font-weight: 400;\">m. Since then, I have begun to contribute to MDN every week.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">My primary work is collaboration with other zh reviewers to check and review the open </span></i><a href=\"https://github.com/mdn/translated-content/pulls?q=is%3Apr+label%3Al10n-zh+)\"><i><span style=\"font-weight: 400;\">pull requests</span></i></a><i><span style=\"font-weight: 400;\"> on both Traditional Chinese and Simplified Chinese locales. Our goal is to ensure that all the changes to the zh docs are well done, both regarding the file format and translations. </span></i></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47569 size-full\" src=\"https://hacks.mozilla.org/files/2022/01/ensemble.png\" alt=\"Julien\" width=\"354\" height=\"286\" srcset=\"https://hacks.mozilla.org/files/2022/01/ensemble.png 354w, https://hacks.mozilla.org/files/2022/01/ensemble-250x202.png 250w\" sizes=\"(max-width: 354px) 100vw, 354px\" /></p>\n<p style=\"text-align: center;\"><b><i>Sphinx  (Julien) (he / him), @</i></b><a href=\"https://twitter.com/Sphinx_Twitt\"><b><i>Sphinx_Twitt</i></b></a><b><i> </i></b></p>\n<p><i><span style=\"font-weight: 400;\">Most of my contributions revolve around localizing MDN content in French (translating new articles and also maintaining existing pages). Since MDN moved to GitHub, contributing also encompasses reviewing other&#8217;s contributions. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">I started to contribute when, having time as a student, I joined a collaborative translation project led by Framasoft. After a few discussions, I joined a mailing list and IRC. One of the first contribution proposals I saw was about improving the translation of the MDN Glossary in French to help newcomers. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">I started helping and was welcomed by the team and community at that time. One thing led to another, and I started helping to translate other areas of MDN in French.</span></i></p>\n<p><span style=\"font-weight: 400;\">Tanner and Kenrick are also longtime volunteers. Their main areas of activity are contributing code, solving issues in MDN repositories, as well as reviewing and assisting the submissions of other contributors.</span></p>\n<p><span style=\"font-weight: 400;\">In MDN, all users can </span><a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute\"><span style=\"font-weight: 400;\">add issues to the issue tracker, as well as contributing fixes, and reviewing other people fixes</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47561 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/tanner-casual-med-500x666.jpg\" alt=\"Tanner\" width=\"500\" height=\"666\" srcset=\"https://hacks.mozilla.org/files/2022/01/tanner-casual-med-500x666.jpg 500w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-250x333.jpg 250w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-768x1022.jpg 768w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-1154x1536.jpg 1154w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-1539x2048.jpg 1539w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-scaled.jpg 1923w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p style=\"text-align: center;\"><b><i>Tanner Dolby, @tannerdolby </i></b></p>\n<p><i><span style=\"font-weight: 400;\"> I contribute to MDN by being active in the issue tracker of MDN repositories. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">I tend to look through the issues and search for one I understand, then I read the conversation in the issue thread for context. If I have any questions or notice that the conversation wasn’t resolved, I comment in the thread to get clarification before moving forward. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">From there, I test my proposed changes locally and then submit a pull request to fix the issue on GitHub. The changes I submit are then reviewed by project maintainers. After the review, I implement recommended changes. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Outside of this, I contribute to MDN by spotting bugs and creating new issues, fixing existing issues, making feature requests for things I’d like to see on the site, assisting in the completion of a feature request, participating in code review and interacting with other contributors on existing issues.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">I started contributing to MDN by creating an issue in the mdn/yari repository. I was referencing documentation and wanted to clarify a bit of information that could be a typo. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">The MDN Web Docs team was welcoming of me resolving the issue, so I opened and reviewed/merged a PR I submitted, which fixed things. The Yari project maintainers explained things in detail, helping me to understand that the content for MDN Web Docs lived in mdn/content and not directly in mdn/yari source. The issue I originally opened was transferred to mdn/content and the corresponding fix was merged. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">My first OSS experience with MDN was really fun. It helped me to branch out and explore other issues/pull requests in MDN repositories to better understand how MDN Web Docs worked, so I could contribute again in the future.</span></i></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47565 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040-500x500.jpeg\" alt=\"Kenrick\" width=\"500\" height=\"500\" srcset=\"https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040-500x500.jpeg 500w, https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040-250x250.jpeg 250w, https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040.jpeg 512w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p style=\"text-align: center;\"><b><i>Kenrick, @kenrick95</i></b></p>\n<p><i><span style=\"font-weight: 400;\">I’ve edited content and contributed codes to MDN repositories: browser-compat-data, interactive-examples, and yari.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">My first contribution to content was a long time ago, when we could directly edit on MDN. I can no longer recall what it was, probably fixing a typo. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">My first code contribution was to the “interactive-examples” repo. I noticed that the editor had some bugs, and I found the GitHub issue. After I read the codes, it seemed to me that the bug could be easily fixed, so I went ahead and sent a pull request</span></i></p>\n<h2><b>Why contribute?</b></h2>\n<p><span style=\"font-weight: 400;\">Contributions are essential to the MDN project. When talking about why they deem contribution to MDN a critical task, contributors underlined different facets, stressing its importance as an open, reliable and easily accessible resource to programmers, web developers and learners. </span></p>\n<p><span style=\"font-weight: 400;\">Contributions to MDN documentation and infrastructure help insure the constant improvement of this resource. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">Contributions to MDN are important because it helps to provide a reliable and accessible </span></i><i><span style=\"font-weight: 400;\">source of information on the Web for developers. MDN Web Docs being open source allows for bugs to quickly be spotted by contributors and for feature requests to be readily prototyped. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Building in the open creates an environment that allows for contributors from all over the world to help make MDN a better resource for everyone and that is incredible. </span></i><span style=\"font-weight: 400;\">(Tanner)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">Contributions to the platform and tools that powers MDN are important to enhance users experience (</span></i><span style=\"font-weight: 400;\">Kenrick)</span></p></blockquote>\n<p><span style=\"font-weight: 400;\">Small and big contributions are all significant and have a real impact. A common misconception about contributing to MDN is that you can only contribute code, but that is not the case! </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">MDN is the primary place for people to check any references on web-dev tech. As small as fixing one typo, any contribution to MDN can always help thousands of programmers and learners. (Irvin)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">Contribution to localization allows learners and developers to access this resource in languages other than English, making it more accessible. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">Especially for those who are struggling with reading English docs, localization can enable them to access the latest and solid knowledge (Irvin)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">Contributing to localization help beginners on the Web finding quality documentation and explanations so that they can build sites, apps and so on without having to know English. MDN is a technical reference, but also a fantastic learning ground to educate newcomers. From basic concepts to complex techniques, language should not be a barrier to build something on the Web. (Julien)</span></i></p></blockquote>\n<h2></h2>\n<h2><b>Contributing is a rewarding experience</b></h2>\n<p><span style=\"font-weight: 400;\">We asked contributors why they find contributing to MDN a rewarding experience. </span><span style=\"font-weight: 400;\">They told us that contribution is a way to help others, but also to learn new things. They spoke about the relationship that volunteers build with other people while contributing, and the possibility to learn from and help others. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">The part of contributing that I enjoy most is providing a fix for something that positively </span></i><i><span style=\"font-weight: 400;\">impacts the experience for users browsing MDN Web Docs. This could be an update to </span></i><i><span style=\"font-weight: 400;\">documentation to help provide developers with accurate docs, or helping to land a new feature on the site that will provide users new or improved functionality. Before I started contributing to MDN, I referenced MDN Web Docs very often and really appreciated the hard work that was put into the site. To this day, I’m motivated to continue help making MDN Web Docs the best resource it can be through open source contributions. (</span></i><span style=\"font-weight: 400;\">Tanner)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">I enjoy finding different points of view on how to achieve the same things. This is natural, since the people I interact comes from different part of the world and we all are influenced by our local cultures (Kenrick)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">The part of contributing I most enjoy is definitely the part when I&#8217;m learning and discovering from what I&#8217;m translating (&#8230;). </span></i><i><span style=\"font-weight: 400;\">My best memory to contribute to MDN is that </span></i><i><span style=\"font-weight: 400;\">I had the great privilege of spending an evening watching a sunset of lava and sea with people related to MDN for whom I have the deepest esteem. (Julien)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">The journey of contribution itself is important. The support of MDN maintainers and the exchange of ideas is essential. Contribution does not happen in a silo but is a collaborative effort between volunteers and the MDN team.</span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">My best memory of contributing to MDN would have to be the journey of creating the </span></i><i><span style=\"font-weight: 400;\">copy-to-clipboard functionality for code snippets on MDN Web Docs. I remember prototyping the feature in mdn/yari locally and then beginning to see it come to life really quickly, which was wonderful to see. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">The code review process for this feature was such a joy and incredibly motivating. Each step of the feature was tested thoroughly and every win was celebrated. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Each morning, I would wake up and eagerly check my email and see if any “Re: [mdn/yari]” labelled emails were there because it meant I could get back to collaborating with the MDN Web Docs team. This contribution really opened my eyes to how incredibly fun and rewarding open source software can be. (Tanner)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">My best memory of contributing to MDN was working on </span></i><a href=\"https://github.com/mdn/yari/pull/172\"><i><span style=\"font-weight: 400;\">https://github.com/mdn/yari/pull/172</span></i></a><i><span style=\"font-weight: 400;\">. The change in itself wasn’t big, but the solution changed several times after lengthy discussion. I’m amazed on how open the maintainers are in accepting different point of views for achieving the end goal (Kenrick</span></i><i><span style=\"font-weight: 400;\">)</span></i></p></blockquote>\n<h2><b>Contributions to be proud of</b></h2>\n<p><span style=\"font-weight: 400;\">All contributions are important, but some hold a special place with each volunteer.</span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">The contribution that I’m most proud of is adding copy-to-clipboard functionality to all code snippets for documentation pages on MDN Web Docs. I use this utility very often while browsing pages on MDN Web Docs and seeing a feature I helped build live on the site for other people to use is an amazing feeling. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">This contribution was something I wanted to see on the site and after discussing the feature with the Yari team, I began prototyping and participating in code review until the feature was merged into the live site. This utility was one of the first “large” feature requests that I contributed to mdn/yari and is something I’m very proud of.</span></i><span style=\"font-weight: 400;\"> (Tanner)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">The contribution I am most proud of is having the HTML, CSS, and JavaScript section complete and up-to-date in French in 2017 after being told this would be impossible :) . More recently, helping rebuilding tools for localizers on the new MDN platform with a tracking dashboard</span></i> <span style=\"font-weight: 400;\">(Julien)</span></p></blockquote>\n<p><span style=\"font-weight: 400;\">Kenrick was most proud of adding a feature that marks the page you are looking at in the sidebar. This change makes a significant difference for visual learners. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">It was a simple change, but I felt that this UX improvement is important because it serves as a guide to the reader to check what are the documents related to the one they are reading. </span></i></p></blockquote>\n<h2></h2>\n<h2><b>Getting started </b></h2>\n<p><span style=\"font-weight: 400;\">There are many ways to contribute to MDN! Our seasoned contributors suggest starting with reporting issues and trying to fix them, follow the issue trackers and getting familiarized with GitHub. Don’t be afraid to ask questions, and to make mistakes, there are people that will help you and review your work.</span></p>\n<blockquote><p><span style=\"font-weight: 400;\"> G</span><i><span style=\"font-weight: 400;\">o at your own pace, don&#8217;t hesitate to ask questions. If you can, try to hack things to fix the issues you encounter on a project. If you are eager to learn things about the Web, check MDN as a way to contribute to open source</span></i><span style=\"font-weight: 400;\"> (Julien)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">Suppose you become aware of a bug in any MDN doc (such as a typo), you are welcome to fix them directly by clicking the &#8220;Edit on Github&#8221; button. The review team will ensure it&#8217;s good, so you don&#8217;t need to worry about making any mistakes. (Irvin)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">From taking the first steps, contributors can then progress to more difficult issues and contributions. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">Don’t be afraid of reading code. Pick up any issue from GitHub, and you can easily start contributing code! (Kenrick)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">My advice for new contributors or those getting started with open source is to get familiarized with the project that they wish to contribute in and then begin staying up-to-date with the issue tracker. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Start being active in the project by looking through issues and reading through the comments, this is a sure-fire way to learn about the project. If there is something that you aren’t ready to contribute but want to have a conversation about, drop a comment in the issue thread or create a discussion in the repository for a great way to inspire conversation about a topic. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Lastly, understanding a version control software like Git is recommended for those that are considering starting to contribute to open source software. Be open to help in any way you can when first getting started in open source, I started small with documentation fixes on MDN Web Docs and then gradually worked my way into more complex contributions as I became more familiar with the project. (Tanner)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">If you want to start contributing, please check out these resources:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute\"><span style=\"font-weight: 400;\">Contributing to MDN</span></a></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://community.mozilla.org/en/activities/contribute-to-mdn-web-docs/\"><span style=\"font-weight: 400;\">MDN activity</span></a></li>\n</ul>\n<p><span style=\"font-weight: 400;\">If you have any questions, join the</span><a href=\"https://chat.mozilla.org/#/room/#mdn:mozilla.org\"> <span style=\"font-weight: 400;\">matrix chat room</span></a><span style=\"font-weight: 400;\"> for MDN.</span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/01/contributing-to-mdn-meet-the-contributors/\">Contributing to MDN: Meet the Contributors</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "If you’ve ever built anything with web technologies, you’re probably familiar with MDN Web Docs. With about 13,000 pages documenting how to use programming languages such as HTML, CSS and JavaScript, the site has about 8,000 people using it at any given moment.\nMDN relies on contributors to help maintain its ever-expanding and up to date documentation. Supported by companies such as Open Web Docs, Google, w3c, Microsoft, Samsung and Igalia (to name a few), contributions also come from community members. These contributions take many different forms, from fixing issues to contributing code to helping newcomers and localizing content.\nWe reached out to 4 long-time community contributors to talk about how and why they started contributing, why they kept going, and ask what advice they have for new contributors.\nMeet the contributors\nMDN contributors come from all over the world, have different backgrounds, and contribute in different ways. \nIrvin and Julien’s main area of contribution is localizations. They are part of a diverse team of volunteers that ensure that MDN is translated in seven different languages (Discover here how translations of MDN content happens. \nSince the end of 2020, the translation of MDN articles happen on the new GitHub based platform.\n\nIrvin, @irvinfly, volunteer from Mozilla Taiwan Community\nI had been a front-end engineer for more than a decade. I had been a leisure contributor on MDN for a long time. I check MDN all the time when writing websites, but only made some simple contributions, like fixing typos.\nIn early 2020, the MDN team asked us if zh (Chinese) locale would like to join the early stage of the localization system on Yari, the new Github-based platform. We accepted the invitation and formed the zh-review-team. Since then, I have begun to contribute to MDN every week.\nMy primary work is collaboration with other zh reviewers to check and review the open pull requests on both Traditional Chinese and Simplified Chinese locales. Our goal is to ensure that all the changes to the zh docs are well done, both regarding the file format and translations. \n\nSphinx  (Julien) (he / him), @Sphinx_Twitt \nMost of my contributions revolve around localizing MDN content in French (translating new articles and also maintaining existing pages). Since MDN moved to GitHub, contributing also encompasses reviewing other’s contributions. \nI started to contribute when, having time as a student, I joined a collaborative translation project led by Framasoft. After a few discussions, I joined a mailing list and IRC. One of the first contribution proposals I saw was about improving the translation of the MDN Glossary in French to help newcomers. \nI started helping and was welcomed by the team and community at that time. One thing led to another, and I started helping to translate other areas of MDN in French.\nTanner and Kenrick are also longtime volunteers. Their main areas of activity are contributing code, solving issues in MDN repositories, as well as reviewing and assisting the submissions of other contributors.\nIn MDN, all users can add issues to the issue tracker, as well as contributing fixes, and reviewing other people fixes. \n\nTanner Dolby, @tannerdolby \n I contribute to MDN by being active in the issue tracker of MDN repositories. \nI tend to look through the issues and search for one I understand, then I read the conversation in the issue thread for context. If I have any questions or notice that the conversation wasn’t resolved, I comment in the thread to get clarification before moving forward. \nFrom there, I test my proposed changes locally and then submit a pull request to fix the issue on GitHub. The changes I submit are then reviewed by project maintainers. After the review, I implement recommended changes. \nOutside of this, I contribute to MDN by spotting bugs and creating new issues, fixing existing issues, making feature requests for things I’d like to see on the site, assisting in the completion of a feature request, participating in code review and interacting with other contributors on existing issues.\nI started contributing to MDN by creating an issue in the mdn/yari repository. I was referencing documentation and wanted to clarify a bit of information that could be a typo. \nThe MDN Web Docs team was welcoming of me resolving the issue, so I opened and reviewed/merged a PR I submitted, which fixed things. The Yari project maintainers explained things in detail, helping me to understand that the content for MDN Web Docs lived in mdn/content and not directly in mdn/yari source. The issue I originally opened was transferred to mdn/content and the corresponding fix was merged. \nMy first OSS experience with MDN was really fun. It helped me to branch out and explore other issues/pull requests in MDN repositories to better understand how MDN Web Docs worked, so I could contribute again in the future.\n\nKenrick, @kenrick95\nI’ve edited content and contributed codes to MDN repositories: browser-compat-data, interactive-examples, and yari.\nMy first contribution to content was a long time ago, when we could directly edit on MDN. I can no longer recall what it was, probably fixing a typo. \nMy first code contribution was to the “interactive-examples” repo. I noticed that the editor had some bugs, and I found the GitHub issue. After I read the codes, it seemed to me that the bug could be easily fixed, so I went ahead and sent a pull request\nWhy contribute?\nContributions are essential to the MDN project. When talking about why they deem contribution to MDN a critical task, contributors underlined different facets, stressing its importance as an open, reliable and easily accessible resource to programmers, web developers and learners. \nContributions to MDN documentation and infrastructure help insure the constant improvement of this resource. \nContributions to MDN are important because it helps to provide a reliable and accessible source of information on the Web for developers. MDN Web Docs being open source allows for bugs to quickly be spotted by contributors and for feature requests to be readily prototyped. \nBuilding in the open creates an environment that allows for contributors from all over the world to help make MDN a better resource for everyone and that is incredible. (Tanner)\nContributions to the platform and tools that powers MDN are important to enhance users experience (Kenrick)\nSmall and big contributions are all significant and have a real impact. A common misconception about contributing to MDN is that you can only contribute code, but that is not the case! \nMDN is the primary place for people to check any references on web-dev tech. As small as fixing one typo, any contribution to MDN can always help thousands of programmers and learners. (Irvin)\nContribution to localization allows learners and developers to access this resource in languages other than English, making it more accessible. \nEspecially for those who are struggling with reading English docs, localization can enable them to access the latest and solid knowledge (Irvin)\nContributing to localization help beginners on the Web finding quality documentation and explanations so that they can build sites, apps and so on without having to know English. MDN is a technical reference, but also a fantastic learning ground to educate newcomers. From basic concepts to complex techniques, language should not be a barrier to build something on the Web. (Julien)\n\nContributing is a rewarding experience\nWe asked contributors why they find contributing to MDN a rewarding experience. They told us that contribution is a way to help others, but also to learn new things. They spoke about the relationship that volunteers build with other people while contributing, and the possibility to learn from and help others. \nThe part of contributing that I enjoy most is providing a fix for something that positively impacts the experience for users browsing MDN Web Docs. This could be an update to documentation to help provide developers with accurate docs, or helping to land a new feature on the site that will provide users new or improved functionality. Before I started contributing to MDN, I referenced MDN Web Docs very often and really appreciated the hard work that was put into the site. To this day, I’m motivated to continue help making MDN Web Docs the best resource it can be through open source contributions. (Tanner)\nI enjoy finding different points of view on how to achieve the same things. This is natural, since the people I interact comes from different part of the world and we all are influenced by our local cultures (Kenrick)\nThe part of contributing I most enjoy is definitely the part when I’m learning and discovering from what I’m translating (…). My best memory to contribute to MDN is that I had the great privilege of spending an evening watching a sunset of lava and sea with people related to MDN for whom I have the deepest esteem. (Julien)\nThe journey of contribution itself is important. The support of MDN maintainers and the exchange of ideas is essential. Contribution does not happen in a silo but is a collaborative effort between volunteers and the MDN team.\nMy best memory of contributing to MDN would have to be the journey of creating the copy-to-clipboard functionality for code snippets on MDN Web Docs. I remember prototyping the feature in mdn/yari locally and then beginning to see it come to life really quickly, which was wonderful to see. \nThe code review process for this feature was such a joy and incredibly motivating. Each step of the feature was tested thoroughly and every win was celebrated. \nEach morning, I would wake up and eagerly check my email and see if any “Re: [mdn/yari]” labelled emails were there because it meant I could get back to collaborating with the MDN Web Docs team. This contribution really opened my eyes to how incredibly fun and rewarding open source software can be. (Tanner)\nMy best memory of contributing to MDN was working on https://github.com/mdn/yari/pull/172. The change in itself wasn’t big, but the solution changed several times after lengthy discussion. I’m amazed on how open the maintainers are in accepting different point of views for achieving the end goal (Kenrick)\nContributions to be proud of\nAll contributions are important, but some hold a special place with each volunteer.\nThe contribution that I’m most proud of is adding copy-to-clipboard functionality to all code snippets for documentation pages on MDN Web Docs. I use this utility very often while browsing pages on MDN Web Docs and seeing a feature I helped build live on the site for other people to use is an amazing feeling. \nThis contribution was something I wanted to see on the site and after discussing the feature with the Yari team, I began prototyping and participating in code review until the feature was merged into the live site. This utility was one of the first “large” feature requests that I contributed to mdn/yari and is something I’m very proud of. (Tanner)\nThe contribution I am most proud of is having the HTML, CSS, and JavaScript section complete and up-to-date in French in 2017 after being told this would be impossible :) . More recently, helping rebuilding tools for localizers on the new MDN platform with a tracking dashboard (Julien)\nKenrick was most proud of adding a feature that marks the page you are looking at in the sidebar. This change makes a significant difference for visual learners. \nIt was a simple change, but I felt that this UX improvement is important because it serves as a guide to the reader to check what are the documents related to the one they are reading. \n\nGetting started \nThere are many ways to contribute to MDN! Our seasoned contributors suggest starting with reporting issues and trying to fix them, follow the issue trackers and getting familiarized with GitHub. Don’t be afraid to ask questions, and to make mistakes, there are people that will help you and review your work.\n Go at your own pace, don’t hesitate to ask questions. If you can, try to hack things to fix the issues you encounter on a project. If you are eager to learn things about the Web, check MDN as a way to contribute to open source (Julien)\nSuppose you become aware of a bug in any MDN doc (such as a typo), you are welcome to fix them directly by clicking the “Edit on Github” button. The review team will ensure it’s good, so you don’t need to worry about making any mistakes. (Irvin)\nFrom taking the first steps, contributors can then progress to more difficult issues and contributions. \nDon’t be afraid of reading code. Pick up any issue from GitHub, and you can easily start contributing code! (Kenrick)\nMy advice for new contributors or those getting started with open source is to get familiarized with the project that they wish to contribute in and then begin staying up-to-date with the issue tracker. \nStart being active in the project by looking through issues and reading through the comments, this is a sure-fire way to learn about the project. If there is something that you aren’t ready to contribute but want to have a conversation about, drop a comment in the issue thread or create a discussion in the repository for a great way to inspire conversation about a topic. \nLastly, understanding a version control software like Git is recommended for those that are considering starting to contribute to open source software. Be open to help in any way you can when first getting started in open source, I started small with documentation fixes on MDN Web Docs and then gradually worked my way into more complex contributions as I became more familiar with the project. (Tanner)\nIf you want to start contributing, please check out these resources:\n\nContributing to MDN\nMDN activity\n\nIf you have any questions, join the matrix chat room for MDN.\nThe post Contributing to MDN: Meet the Contributors appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2022-01-18T16:07:59.000Z",
      "date_modified": "2022-01-18T16:07:59.000Z",
      "_plugin": {
        "pageFilename": "dad028eee6e10c5289e61134c55675b27d2a336e410460ea18be28fc28bfa58e.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47506",
      "url": "https://hacks.mozilla.org/2021/12/hacks-decoded-sara-soueidan-award-winning-ui-design-engineer-and-author/",
      "title": "Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author",
      "summary": "Sara Soueidan is an independent Web UI and design engineer, author, speaker, and trainer from Lebanon. Currently, she’s working on a new course, \"Practical Accessibility,\" meant to teach devs and designers ways to make their products accessible. We chatted with Sara about front-end web development, the importance of design and her appreciation of birds.\nThe post Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><i>Welcome to our Hacks: Decoded Interview series! </i></p>\n<p><i>Once a month, </i><a href=\"https://foundation.mozilla.org/\"><i>Mozilla Foundation</i></a><i>’s </i><a href=\"https://www.xavierharding.com/\"><i>Xavier Harding</i></a><i> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </i><a href=\"https://hacks.mozilla.org/\"><i>Hacks</i></a><i> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.</i></p>\n<p>&nbsp;</p>\n<p><strong>Meet Sara Soueidan!</strong></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47511 size-large\" src=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-Mozilla-Hacks-Decoded-interview-QA-500x466.jpg\" alt=\"\" width=\"500\" height=\"466\" /></p>\n<div class=\"block docBlock Block_container__39pfw Block_readOnly__aAcdv rootBlock rect-definition-node\" data-block-id=\"5c056e39-ea06-4755-a174-53f031ca6d3e\" data-full-hit=\"false\" data-text=\"false\" data-frame=\"false\" data-positioned=\"false\" data-editing=\"false\" data-indent=\"0\">\n<div class=\"ContextMenuHandler_wrapper__2D0Q8\">\n<div class=\"BlockType_block__Szmra\">\n<p>Sara Soueidan is an independent Web UI and design engineer, author, speaker, and trainer from Lebanon.</p>\n<p>Sara has worked with companies around the world, building web user interfaces, designing systems, and creating digital products that focus on responsive design and accessibility. She’s worked with companies like SuperFriendly, Herman Miller, Khan Academy, and has given workshops within companies like Netflix and Telus that focus on building scalable, resilient design.</p>\n<p>When Sara isn’t offering keynote speeches at conferences (she’s done so a dozen times) she’s writing books like “Codrops CSS Reference” and “Smashing Book 5.” Currently, she’s working on a new course, &#8220;Practical Accessibility,&#8221; meant to teach devs and designers ways to make their products accessible.</p>\n<p>In 2015, Sara was voted Developer of the Year in the net awards, and shortlisted for the Outstanding Contribution of the Year award. She also won an O’Reilly Web Platform Award for “exceptional leadership, creativity, and collaboration in the development of JavaScript, HTML, CSS, and the supporting Web ecosystem.”</p>\n<p>We chatted with Sara about front-end web development, the importance of design and her appreciation of birds.</p>\n</div>\n</div>\n</div>\n<p><b>Where did you get your start? How did you end up working in tech?</b></p>\n<p>I took my first HTML class in eighth grade. I instantly fell in love with it. It just made sense; and it felt like a second language that I found myself speaking fluently. But back then, it was just another class. As I continued my journey through high school, I considered architecture as a major. I never thought I&#8217;d major in anything even remotely related to tech. I always thought I’d choose a career that had nothing to do with computers. In fact, before choosing computer science as a major, I was preparing to study architecture in the Faculty of Arts.</p>\n<p>Then, life happened. A series of events had me choosing CS as a major. And even after I did, I didn’t really think I’d make a career in tech. I spent 18 months after college pondering what I could do for a living with a CS major in Lebanon, but I didn’t find my calling anywhere.</p>\n<p>My love for the web was rekindled when someone suggested I learn web development and try making websites for a living. The appeal of that was two-fold: I&#8217;d get to work remotely from the comfort of my home, and I&#8217;d get to be my own boss, and have full control over my time and the work that I choose.</p>\n<p>After a few weeks of learning modern HTML and CSS, and dipping my feet into JavaScript, I was hooked. I found myself spending more time learning and practicing. <a href=\"https://codepen.io/\">Codepen</a> was new back then, and it was a great place to do quick code exercises and experiments. I also created a one-page Web site — because if you&#8217;re going to work freelance and accept work requests, you gotta have that!</p>\n<p>As I continued learning and experimenting for a few months, I started sharing what I learned as articles on a blog that I started in 2013. A few weeks after I published my first article, I got my first client request to create the UI for a Facebook-like Web application. And over the course of the first year, I got one small client project after another.</p>\n<p>My career really kicked off though in 2014. By then, I was writing more, getting more client work, and writing a CSS reference for Codrops. Conference speaking invitations started flooding in after I delivered my first talk at CSSConf in 2014. I gave my first workshop in LA in 2015. And I have been doing what I do now since.</p>\n<p>I am grateful things didn’t work out the way I wanted them to after high school.</p>\n<p><b>You’ve been programming for a while now, you’ve co-authored a book about the craft, you’ve created guides like the Codrops CSS Reference — what drives you?</b></p>\n<p>A thirst for knowledge and a craving for variety in work. I don’t think I’d be inspired enough to do <i>any</i> kind of work that doesn’t satisfy both. I also need to feel like I’m doing something meaningful, like helping others. And I&#8217;ve been able to fulfill all of these needs in this field. That&#8217;s why I fell in love with it.</p>\n<p>Being independent, I have full control over my time and the type of work I spend it on. While building websites is my main work and source of income, I do spend a large portion of my time switching between writing, editing, giving talks, running workshops (in-house and at events), making courses (this one’s new!) and working on personal projects.</p>\n<p>Everything I do complements one another: I learn, to write, to teach; I code, to write, to speak; I code, to learn, to share. It’s a wonderful circle of creative work! This variety helps keep the spark alive, and helps me rekindle my passion for the web even after frequent burnouts.</p>\n<p>I like that I must keep learning for a living! And that I get to also teach (another passion and — dare I say — talent of mine) as part of my job. I teach through writing, through speaking, through running workshops, and even through direct collaboration with designers and engineers on client projects.</p>\n<p>I always think that even if I end up changing careers, I would still make some time to fiddle with code and make web projects on the side of whatever else I&#8217;d be doing for a living.</p>\n<p><b>When it comes to front-end versus back-end versus full stack, you seem to be #TeamFrontEnd. What is it about front-end web and app development that calls your name (more so than back-end)?</b></p>\n<p>I love working at the intersection of design and engineering! This is the area of the front end typically referred to as “the front of the front end.” It is the perfect sweet spot between design and engineering. It stimulates both parts of my brain, and keeps me inspired and challenged — a combination my brain needs to stay creative.</p>\n<p>I find building interfaces fascinating. I love the fact that the interfaces I build are the bridge between people and the information they access online.</p>\n<p>That comes with great responsibility, of course. Building for people is not easy because people are so diverse and so are the ways they access the Web. And it&#8217;s the interfaces they use that determine whether they can!</p>\n<p>It is <i>our</i> responsibility as front-end developers and designers to ensure that what we create is inclusive of as many people as possible.</p>\n<p>While this may sound intimidating and maybe even scary, I find it inspiring. It is what gives more meaning to what I do, and what pushes me to keep learning and trying to do better. The front of the front end is where I found my sweet spot: a place where I can be challenged and inspired.</p>\n<p>A couple of years ago, I was feeling this so much that <a href=\"https://twitter.com/DNABeast/status/1150326370007842816\">I shared that moment on Twitter</a>. Among the many replies I got, this quote by Douglas Adams stuck with me:</p>\n<p>“<i>We all like to congregate, at boundary conditions. Where land meets water. Where earth meets air. Where body meets mind. Where space meets time.”</i></p>\n<p><b>What do you love about coding? What’s your least favorite part?</b></p>\n<p>My favorite part is the satisfaction of seeing my code “come to life”. The idea that I can write a few lines of code that computers understand, and that so many people can consume and interact with it using various technologies — present and in the future.</p>\n<p>I also appreciate the short feedback loop in modern code environments: you write code or make changes to existing one, and see the results immediately in the browser. It is almost magical. And who doesn’t like a little bit of magic in their lives?</p>\n<p>My least favorite part, however, is that it requires so little movement. There is life in movement! One of my favorite yoga teachers once said: &#8220;Once you stop moving, you start dying.&#8221; And I felt that. Spending so much time in front of a screen is very taxing.</p>\n<p>Regular exercise is crucial for my ability to continue doing what I do. But I still sometimes feel like I need more movement <i>during</i> my work sessions. So I got a standing desk a couple of years ago.</p>\n<p>Switching between standing and sitting gives my body short &#8220;breathers&#8221; throughout the day and allows for better blood flow. A balanced lifestyle is crucial to maintaining a good health when you spend as much time in front of a screen. Try to move, drink lots of water, and go outside more.</p>\n<p><b>You’re based out of Lebanon. What’s something many folks may not realize about the tech scene there?</b></p>\n<p>I know this isn&#8217;t the answer you&#8217;re expecting, but I think what many people don&#8217;t realize about the tech scene here is how challenging it is! In Lebanon, we live in a country that has a massive, serious, and ongoing power crisis.</p>\n<p>This crisis, as you can imagine, affects almost every facet of our lives, including the digital. You need power to do work. And you need an internet connection to do work. We’ve always had problems with internet speed. And with the fuel shortage, full power outages, and reception problems, having a <i>reliable</i> connection is less likely than before.</p>\n<p>But there are some incredibly talented designers and developers still making it work through this all. Living in Lebanon brings daily challenges, but being challenged in life is inevitable.</p>\n<p>I try to look on the bright side of everything. Working on a slow connection has its upsides, you know. You learn to appreciate performance more and strive to make better, faster Web sites. You appreciate tech like Service Worker more, and learn to use it to <a href=\"https://www.sarasoueidan.com/blog/going-offline/\">make content available offline</a>. If anything, living here has made many of us more resilient to change, and more creative with our solutions in the face of crisis.</p>\n<p><b>How do you find (tech) supporting communities in Lebanon, if not where does your community live?</b></p>\n<p>I don’t. But that’s mainly because I live in an area with no active tech community. And I live far from where any tech meetups happen. I also don’t know any front-end focused developers in Lebanon. I’m sure they exist; it’s just that, being the introvert that I am, I don’t happen to know any. So my community is mainly online — on Twitter, and in a couple of not-very-busy Slack channels.</p>\n<p><b>Ok, random question. We’ve gotta know about the birds. You’ve raised at least a dozen. What’s the story there?</b></p>\n<p>It all started back in 2009, I think. A close friend had, for whatever reason, decided that I might enjoy taking care of baby birds. So, he got me a baby <a href=\"https://www.flickr.com/photos/raed_shorrosh/30669614184/\">White-spectacled Bulbul</a> (my favorite bird species currently), with all the bird food I needed to start. He taught me what I needed to know to take care of it. And he told me that, when it grows up, it won’t need to live in a cage because <i>I</i> would be its home. I had no idea back then how much I’d fall in love with that bird.</p>\n<p>I&#8217;ve raised 10+ birds since. Not a single one of them was kept in a cage. I would raise them and train them so that, when they grew up, they would fly out in the morning — making friends, living like they were meant to, and return home before the end of the day.</p>\n<p>They would drink from my tea cup, share my sandwiches, eat out of my plate (mainly rice) and spend most of the day either sitting on my shoulder and head, or napping on my arm. Friends have always told me that I was like a Disney princess with my birds. I&#8217;m not sure about that, but it did sometimes <i>feel</i> that way. x)</p>\n<p>Here’s a photo of my last two baby birds from a couple of years ago. I took them out in a car drive to &#8220;explore the outside world&#8221; for the first time.</p>\n<p>They just sat there chilling on my arm, as they watched the world (cars, mainly) pass by.</p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47515 size-large\" src=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397-500x754.jpg\" alt=\"\" width=\"500\" height=\"754\" srcset=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397-500x754.jpg 500w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397-250x377.jpg 250w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397.jpg 679w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p>Years after my friend got me my first bird, I asked him why he did, and whether he knew about the connection that was going to happen. His answer was short. He said: &#8220;<i>You have the heart of a bird. I knew you&#8217;d love creatures that are like you.</i>&#8221;</p>\n<p><b>Another random question: In an interview, you mentioned mainly working in the morning (6am-10am), and slowing down after lunch. You’re like me! How important is a flexible work day to your workflow? (And how do we convince more people that 9-to-5 work isn’t realistic for everyone? How do we normalize hard work in the morning, meetings and calls in the afternoon?) </b></p>\n<p>I can’t imagine myself working on a 9-to-5 schedule! That’s actually one of the few reasons I never took a full-time job. As I mentioned earlier, flexibility was a key factor in choosing a freelance career.</p>\n<p>I am an early bird. On <a href=\"https://www.sarasoueidan.com/desk/typical-day/\">a typical day</a>, I wake up no later than 5:30 in the morning. So my day starts very early. My brain’s information retention powers are at their highest early in the morning. So I get my best work done during that time. With my brain firing on all cylinders, I make quite a bit of headway with the day’s tasks. What makes this time even more productive is the fact that there are no expectations, nor interruptions: no emails, no client communication, not even any IRL interruptions.</p>\n<p>The earlier you start in the day, and knowing that most people are only really productive for about 4.5 hours a day, I believe it makes a lot of sense to slow down after lunch.</p>\n<p>I realize this is easier said than done, though. Being freelance gives me this flexibility but I realize others may not have that working full time. But with more companies going fully or partially remote now, I think more people will hopefully get to choose when they work during the day.</p>\n<p><b>You’re working on an accessibility course, can you talk a bit about why you decided to develop this course and the importance of creating more accessible web interfaces?</b></p>\n<p>Before COVID-19 hit, I traveled to run workshops at conferences and in-house at companies. The lockdown had us all, well, locked down, so that was put on temporary hold.</p>\n<p>Over the years, I collected some amazing feedback to my accessibility workshop from former attendees. I knew I had useful content that many others would find helpful.</p>\n<p>As many events went online, running the workshop online was the sensible plan B. But the fact that my Internet was unreliable made that a little risky — I wouldn’t want my internet connection to fail in the middle of an online workshop! So that plan was put on hold too.</p>\n<p>On the other hand, working with designers and engineers on client projects made me realize that there was a big accessibility knowledge gap in most companies I’ve worked with. I love to teach teams I work with about accessibility at every chance I get, but there’s only so much you can share in Zoom meetings and Slack channels. In-house workshops were not always an option, and online training was not feasible at the time.</p>\n<p>And last but not least, I noticed that there is quite a bit of misinformation and bad advice circulating the web community around accessibility. You can cover a good amount of information in articles, but I already had a good bunch of content I could start with from the accessibility workshop that I can use as a foundation for a more comprehensive series of teaching materials — sort of like a mini curriculum.</p>\n<p>By developing this course I am scratching my own itch. All the reasons mentioned above had me wishing I had created a course that I could share around, especially with client teams, and then with members of the community. So with the time I have in between client projects and speaking, I started working on it!</p>\n<p>The course is called <a href=\"https://practical-accessibility.today\">Practical Accessibility</a>, and is <b>still under active development</b>, coming in 2022. The content of the course is going to be much more comprehensive than that of the workshop, and it will cover much more ground, and hopefully be a great foundation for anyone wanting to learn how to create more accessible websites.</p>\n<p><b>Of everything you worked on, what’s your favorite?</b></p>\n<p>Out of all the projects I’ve worked on, probably the one that stood out for me is a project for <a href=\"http://hermanmiller.com\">Herman Miller</a> that I collaborated with <a href=\"https://superfriendlydesign.systems\">SuperFriendly</a> on. The project was under NDA, and was discontinued a few weeks after COVID-19 hit and the world realized it was going to change moving forward; so I, unfortunately, don’t have any details to share about the project itself.</p>\n<p>But what made this opportunity so special is that this was the first and only project that I was involved in from the very start— from early kick-off meetings and ideation, through research and user testing, UX and UI design, and development. I learned so much working with an amazing group of SuperFriends. The trip to the Herman Miller showroom in Atlanta, where we ran a workshop with the team at Herman Miller, was the last trip most of us took before the big lockdown.</p>\n<p>Herman Miller is a furniture company. And what many people don’t know about me is how much I <i>love</i> interior design. I even took an interior design course last year! So, on this project, I got to (1) work with an amazing team (who I get to call my friends now <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f495.png\" alt=\"💕\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" />), (2) on a creative project, (3) for a company specializing in making modern furniture, (4) in the field of interior design! How could I not love that?!</p>\n<p>The cherry on top of the cake was that I got a generous discount which I used to upgrade my office chair and desk to an ergonomic Herman Miller chair and standing desk. So even my body and health were thankful for this opportunity!</p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47519 size-large\" src=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-500x667.jpg\" alt=\"Sara Soueidan desk - Sara' favorite project was working with SuperFriendly and Herman Miller. &quot;The cherry on top of the cake was that I got a generous discount which I used to upgrade my office chair and desk to an ergonomic Herman Miller chair and standing desk. So even my body and health were thankful for this opportunity!&quot;\" width=\"500\" height=\"667\" srcset=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-500x667.jpg 500w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-250x333.jpg 250w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-768x1024.jpg 768w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-1152x1536.jpg 1152w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-1536x2048.jpg 1536w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-scaled.jpg 1920w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p><strong>Final question:</strong> <b>W</b><b>hat would you tell folks learning a programming language or aspiring to be a front end developer, or any sort of developer. What advice would you give them?</b></p>\n<p>Learn the fundamentals — HTML, accessibility, CSS, and just enough vanilla JavaScript to get started. Build upon those skills with tools and frameworks as your work needs.</p>\n<p>Don‘t get intimidated or overwhelmed by what everybody else is doing. Learn what you need when you need it. And practice as much as you can. Practice won’t make you perfect because there is no Perfect in this field, but it will make you better!</p>\n<p>This probably should have been the first piece of advice though: <b>Put the user first.</b> User experience should trump developer convenience. Once you let that guide your work, you’re already halfway through to being a better developer than many others.</p>\n<p>Oh and last but certainly not least: Create a personal website! Own your content. And share your work with the world!</p>\n<p>&#8212;</p>\n<p><em>You can keep up with Sara&#8217;s work by following her blog on her personal site <a href=\"https://www.sarasoueidan.com/blog/\">here.</a> Stay tuned for more Hacks Decoded Q&amp;A&#8217;s!</em></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/12/hacks-decoded-sara-soueidan-award-winning-ui-design-engineer-and-author/\">Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Welcome to our Hacks: Decoded Interview series! \nOnce a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.\n \nMeet Sara Soueidan!\n\n\n\n\nSara Soueidan is an independent Web UI and design engineer, author, speaker, and trainer from Lebanon.\nSara has worked with companies around the world, building web user interfaces, designing systems, and creating digital products that focus on responsive design and accessibility. She’s worked with companies like SuperFriendly, Herman Miller, Khan Academy, and has given workshops within companies like Netflix and Telus that focus on building scalable, resilient design.\nWhen Sara isn’t offering keynote speeches at conferences (she’s done so a dozen times) she’s writing books like “Codrops CSS Reference” and “Smashing Book 5.” Currently, she’s working on a new course, “Practical Accessibility,” meant to teach devs and designers ways to make their products accessible.\nIn 2015, Sara was voted Developer of the Year in the net awards, and shortlisted for the Outstanding Contribution of the Year award. She also won an O’Reilly Web Platform Award for “exceptional leadership, creativity, and collaboration in the development of JavaScript, HTML, CSS, and the supporting Web ecosystem.”\nWe chatted with Sara about front-end web development, the importance of design and her appreciation of birds.\n\n\n\nWhere did you get your start? How did you end up working in tech?\nI took my first HTML class in eighth grade. I instantly fell in love with it. It just made sense; and it felt like a second language that I found myself speaking fluently. But back then, it was just another class. As I continued my journey through high school, I considered architecture as a major. I never thought I’d major in anything even remotely related to tech. I always thought I’d choose a career that had nothing to do with computers. In fact, before choosing computer science as a major, I was preparing to study architecture in the Faculty of Arts.\nThen, life happened. A series of events had me choosing CS as a major. And even after I did, I didn’t really think I’d make a career in tech. I spent 18 months after college pondering what I could do for a living with a CS major in Lebanon, but I didn’t find my calling anywhere.\nMy love for the web was rekindled when someone suggested I learn web development and try making websites for a living. The appeal of that was two-fold: I’d get to work remotely from the comfort of my home, and I’d get to be my own boss, and have full control over my time and the work that I choose.\nAfter a few weeks of learning modern HTML and CSS, and dipping my feet into JavaScript, I was hooked. I found myself spending more time learning and practicing. Codepen was new back then, and it was a great place to do quick code exercises and experiments. I also created a one-page Web site — because if you’re going to work freelance and accept work requests, you gotta have that!\nAs I continued learning and experimenting for a few months, I started sharing what I learned as articles on a blog that I started in 2013. A few weeks after I published my first article, I got my first client request to create the UI for a Facebook-like Web application. And over the course of the first year, I got one small client project after another.\nMy career really kicked off though in 2014. By then, I was writing more, getting more client work, and writing a CSS reference for Codrops. Conference speaking invitations started flooding in after I delivered my first talk at CSSConf in 2014. I gave my first workshop in LA in 2015. And I have been doing what I do now since.\nI am grateful things didn’t work out the way I wanted them to after high school.\nYou’ve been programming for a while now, you’ve co-authored a book about the craft, you’ve created guides like the Codrops CSS Reference — what drives you?\nA thirst for knowledge and a craving for variety in work. I don’t think I’d be inspired enough to do any kind of work that doesn’t satisfy both. I also need to feel like I’m doing something meaningful, like helping others. And I’ve been able to fulfill all of these needs in this field. That’s why I fell in love with it.\nBeing independent, I have full control over my time and the type of work I spend it on. While building websites is my main work and source of income, I do spend a large portion of my time switching between writing, editing, giving talks, running workshops (in-house and at events), making courses (this one’s new!) and working on personal projects.\nEverything I do complements one another: I learn, to write, to teach; I code, to write, to speak; I code, to learn, to share. It’s a wonderful circle of creative work! This variety helps keep the spark alive, and helps me rekindle my passion for the web even after frequent burnouts.\nI like that I must keep learning for a living! And that I get to also teach (another passion and — dare I say — talent of mine) as part of my job. I teach through writing, through speaking, through running workshops, and even through direct collaboration with designers and engineers on client projects.\nI always think that even if I end up changing careers, I would still make some time to fiddle with code and make web projects on the side of whatever else I’d be doing for a living.\nWhen it comes to front-end versus back-end versus full stack, you seem to be #TeamFrontEnd. What is it about front-end web and app development that calls your name (more so than back-end)?\nI love working at the intersection of design and engineering! This is the area of the front end typically referred to as “the front of the front end.” It is the perfect sweet spot between design and engineering. It stimulates both parts of my brain, and keeps me inspired and challenged — a combination my brain needs to stay creative.\nI find building interfaces fascinating. I love the fact that the interfaces I build are the bridge between people and the information they access online.\nThat comes with great responsibility, of course. Building for people is not easy because people are so diverse and so are the ways they access the Web. And it’s the interfaces they use that determine whether they can!\nIt is our responsibility as front-end developers and designers to ensure that what we create is inclusive of as many people as possible.\nWhile this may sound intimidating and maybe even scary, I find it inspiring. It is what gives more meaning to what I do, and what pushes me to keep learning and trying to do better. The front of the front end is where I found my sweet spot: a place where I can be challenged and inspired.\nA couple of years ago, I was feeling this so much that I shared that moment on Twitter. Among the many replies I got, this quote by Douglas Adams stuck with me:\n“We all like to congregate, at boundary conditions. Where land meets water. Where earth meets air. Where body meets mind. Where space meets time.”\nWhat do you love about coding? What’s your least favorite part?\nMy favorite part is the satisfaction of seeing my code “come to life”. The idea that I can write a few lines of code that computers understand, and that so many people can consume and interact with it using various technologies — present and in the future.\nI also appreciate the short feedback loop in modern code environments: you write code or make changes to existing one, and see the results immediately in the browser. It is almost magical. And who doesn’t like a little bit of magic in their lives?\nMy least favorite part, however, is that it requires so little movement. There is life in movement! One of my favorite yoga teachers once said: “Once you stop moving, you start dying.” And I felt that. Spending so much time in front of a screen is very taxing.\nRegular exercise is crucial for my ability to continue doing what I do. But I still sometimes feel like I need more movement during my work sessions. So I got a standing desk a couple of years ago.\nSwitching between standing and sitting gives my body short “breathers” throughout the day and allows for better blood flow. A balanced lifestyle is crucial to maintaining a good health when you spend as much time in front of a screen. Try to move, drink lots of water, and go outside more.\nYou’re based out of Lebanon. What’s something many folks may not realize about the tech scene there?\nI know this isn’t the answer you’re expecting, but I think what many people don’t realize about the tech scene here is how challenging it is! In Lebanon, we live in a country that has a massive, serious, and ongoing power crisis.\nThis crisis, as you can imagine, affects almost every facet of our lives, including the digital. You need power to do work. And you need an internet connection to do work. We’ve always had problems with internet speed. And with the fuel shortage, full power outages, and reception problems, having a reliable connection is less likely than before.\nBut there are some incredibly talented designers and developers still making it work through this all. Living in Lebanon brings daily challenges, but being challenged in life is inevitable.\nI try to look on the bright side of everything. Working on a slow connection has its upsides, you know. You learn to appreciate performance more and strive to make better, faster Web sites. You appreciate tech like Service Worker more, and learn to use it to make content available offline. If anything, living here has made many of us more resilient to change, and more creative with our solutions in the face of crisis.\nHow do you find (tech) supporting communities in Lebanon, if not where does your community live?\nI don’t. But that’s mainly because I live in an area with no active tech community. And I live far from where any tech meetups happen. I also don’t know any front-end focused developers in Lebanon. I’m sure they exist; it’s just that, being the introvert that I am, I don’t happen to know any. So my community is mainly online — on Twitter, and in a couple of not-very-busy Slack channels.\nOk, random question. We’ve gotta know about the birds. You’ve raised at least a dozen. What’s the story there?\nIt all started back in 2009, I think. A close friend had, for whatever reason, decided that I might enjoy taking care of baby birds. So, he got me a baby White-spectacled Bulbul (my favorite bird species currently), with all the bird food I needed to start. He taught me what I needed to know to take care of it. And he told me that, when it grows up, it won’t need to live in a cage because I would be its home. I had no idea back then how much I’d fall in love with that bird.\nI’ve raised 10+ birds since. Not a single one of them was kept in a cage. I would raise them and train them so that, when they grew up, they would fly out in the morning — making friends, living like they were meant to, and return home before the end of the day.\nThey would drink from my tea cup, share my sandwiches, eat out of my plate (mainly rice) and spend most of the day either sitting on my shoulder and head, or napping on my arm. Friends have always told me that I was like a Disney princess with my birds. I’m not sure about that, but it did sometimes feel that way. x)\nHere’s a photo of my last two baby birds from a couple of years ago. I took them out in a car drive to “explore the outside world” for the first time.\nThey just sat there chilling on my arm, as they watched the world (cars, mainly) pass by.\n\nYears after my friend got me my first bird, I asked him why he did, and whether he knew about the connection that was going to happen. His answer was short. He said: “You have the heart of a bird. I knew you’d love creatures that are like you.”\nAnother random question: In an interview, you mentioned mainly working in the morning (6am-10am), and slowing down after lunch. You’re like me! How important is a flexible work day to your workflow? (And how do we convince more people that 9-to-5 work isn’t realistic for everyone? How do we normalize hard work in the morning, meetings and calls in the afternoon?) \nI can’t imagine myself working on a 9-to-5 schedule! That’s actually one of the few reasons I never took a full-time job. As I mentioned earlier, flexibility was a key factor in choosing a freelance career.\nI am an early bird. On a typical day, I wake up no later than 5:30 in the morning. So my day starts very early. My brain’s information retention powers are at their highest early in the morning. So I get my best work done during that time. With my brain firing on all cylinders, I make quite a bit of headway with the day’s tasks. What makes this time even more productive is the fact that there are no expectations, nor interruptions: no emails, no client communication, not even any IRL interruptions.\nThe earlier you start in the day, and knowing that most people are only really productive for about 4.5 hours a day, I believe it makes a lot of sense to slow down after lunch.\nI realize this is easier said than done, though. Being freelance gives me this flexibility but I realize others may not have that working full time. But with more companies going fully or partially remote now, I think more people will hopefully get to choose when they work during the day.\nYou’re working on an accessibility course, can you talk a bit about why you decided to develop this course and the importance of creating more accessible web interfaces?\nBefore COVID-19 hit, I traveled to run workshops at conferences and in-house at companies. The lockdown had us all, well, locked down, so that was put on temporary hold.\nOver the years, I collected some amazing feedback to my accessibility workshop from former attendees. I knew I had useful content that many others would find helpful.\nAs many events went online, running the workshop online was the sensible plan B. But the fact that my Internet was unreliable made that a little risky — I wouldn’t want my internet connection to fail in the middle of an online workshop! So that plan was put on hold too.\nOn the other hand, working with designers and engineers on client projects made me realize that there was a big accessibility knowledge gap in most companies I’ve worked with. I love to teach teams I work with about accessibility at every chance I get, but there’s only so much you can share in Zoom meetings and Slack channels. In-house workshops were not always an option, and online training was not feasible at the time.\nAnd last but not least, I noticed that there is quite a bit of misinformation and bad advice circulating the web community around accessibility. You can cover a good amount of information in articles, but I already had a good bunch of content I could start with from the accessibility workshop that I can use as a foundation for a more comprehensive series of teaching materials — sort of like a mini curriculum.\nBy developing this course I am scratching my own itch. All the reasons mentioned above had me wishing I had created a course that I could share around, especially with client teams, and then with members of the community. So with the time I have in between client projects and speaking, I started working on it!\nThe course is called Practical Accessibility, and is still under active development, coming in 2022. The content of the course is going to be much more comprehensive than that of the workshop, and it will cover much more ground, and hopefully be a great foundation for anyone wanting to learn how to create more accessible websites.\nOf everything you worked on, what’s your favorite?\nOut of all the projects I’ve worked on, probably the one that stood out for me is a project for Herman Miller that I collaborated with SuperFriendly on. The project was under NDA, and was discontinued a few weeks after COVID-19 hit and the world realized it was going to change moving forward; so I, unfortunately, don’t have any details to share about the project itself.\nBut what made this opportunity so special is that this was the first and only project that I was involved in from the very start— from early kick-off meetings and ideation, through research and user testing, UX and UI design, and development. I learned so much working with an amazing group of SuperFriends. The trip to the Herman Miller showroom in Atlanta, where we ran a workshop with the team at Herman Miller, was the last trip most of us took before the big lockdown.\nHerman Miller is a furniture company. And what many people don’t know about me is how much I love interior design. I even took an interior design course last year! So, on this project, I got to (1) work with an amazing team (who I get to call my friends now ), (2) on a creative project, (3) for a company specializing in making modern furniture, (4) in the field of interior design! How could I not love that?!\nThe cherry on top of the cake was that I got a generous discount which I used to upgrade my office chair and desk to an ergonomic Herman Miller chair and standing desk. So even my body and health were thankful for this opportunity!\n\nFinal question: What would you tell folks learning a programming language or aspiring to be a front end developer, or any sort of developer. What advice would you give them?\nLearn the fundamentals — HTML, accessibility, CSS, and just enough vanilla JavaScript to get started. Build upon those skills with tools and frameworks as your work needs.\nDon‘t get intimidated or overwhelmed by what everybody else is doing. Learn what you need when you need it. And practice as much as you can. Practice won’t make you perfect because there is no Perfect in this field, but it will make you better!\nThis probably should have been the first piece of advice though: Put the user first. User experience should trump developer convenience. Once you let that guide your work, you’re already halfway through to being a better developer than many others.\nOh and last but certainly not least: Create a personal website! Own your content. And share your work with the world!\n—\nYou can keep up with Sara’s work by following her blog on her personal site here. Stay tuned for more Hacks Decoded Q&A’s!\nThe post Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-12-30T15:13:51.000Z",
      "date_modified": "2021-12-30T15:13:51.000Z",
      "_plugin": {
        "pageFilename": "1a823ca5ab96872c62b01e5669cb49e146d8e04ea9f287eec8a5a793c15cf3e3.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47492",
      "url": "https://hacks.mozilla.org/2021/12/webassembly-and-back-again-fine-grained-sandboxing-in-firefox-95/",
      "title": "WebAssembly and Back Again: Fine-Grained Sandboxing in Firefox 95",
      "summary": "In Firefox 95, we're shipping a novel sandboxing technology called RLBox — developed in collaboration with researchers at the University of California San Diego and the University of Texas — that makes it easy and efficient to isolate subcomponents to make the browser more secure. This technology opens up new opportunities beyond what's been possible with traditional process-based sandboxing, and we look forward to expanding its usage and (hopefully) seeing it adopted in other browsers and software projects.\nThe post WebAssembly and Back Again: Fine-Grained Sandboxing in Firefox 95 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">In Firefox 95, we&#8217;re shipping a novel sandboxing technology called </span><a href=\"https://plsyssec.github.io/rlbox_sandboxing_api/sphinx/\"><span style=\"font-weight: 400;\">RLBox</span></a><span style=\"font-weight: 400;\"> — developed in collaboration with researchers at the University of California San Diego and the University of Texas — that makes it easy and efficient to isolate subcomponents to make the browser more secure. </span><span style=\"font-weight: 400;\">This technology opens up new opportunities beyond what&#8217;s been possible with traditional process-based sandboxing, and we look forward to expanding its usage and (hopefully) seeing it adopted in other browsers and software projects.</span></p>\n<p><span style=\"font-weight: 400;\">This technique, which uses WebAssembly to isolate potentially-buggy code, builds on the </span><a href=\"https://hacks.mozilla.org/2020/02/securing-firefox-with-webassembly/\"><span style=\"font-weight: 400;\">prototype</span></a><span style=\"font-weight: 400;\"> we shipped last year to Mac and Linux users. Now, we’re bringing that technology to all supported Firefox platforms (desktop and mobile), and isolating five different modules: </span><a href=\"https://scripts.sil.org/cms/scripts/page.php?site_id=projects&amp;item_id=graphite_home\"><span style=\"font-weight: 400;\">Graphite</span></a><span style=\"font-weight: 400;\">, </span><a href=\"http://hunspell.github.io/\"><span style=\"font-weight: 400;\">Hunspell</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://xiph.org/ogg/\"><span style=\"font-weight: 400;\">Ogg</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://libexpat.github.io/\"><span style=\"font-weight: 400;\">Expat</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://github.com/google/woff2\"><span style=\"font-weight: 400;\">Woff2</span></a> [1]<span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">Going forward, we can treat these modules as untrusted code, and — assuming we did it right — even a zero-day vulnerability in any of them should pose no threat to Firefox. Accordingly, we’ve updated our </span><a href=\"https://www.mozilla.org/en-US/security/client-bug-bounty/#exploit-mitigation-bounty\"><span style=\"font-weight: 400;\">bug bounty program</span></a><span style=\"font-weight: 400;\"> to pay researchers for bypassing the sandbox even without a vulnerability in the isolated library.</span></p>\n<h2><strong>The Limits of Process Sandboxing</strong></h2>\n<p><span style=\"font-weight: 400;\">All major browsers run Web content in its own sandboxed process, in theory preventing it from exploiting a browser vulnerability to compromise your computer. On desktop operating systems, Firefox also isolates each site in its own process in order to protect sites from each other. </span></p>\n<p><span style=\"font-weight: 400;\">Unfortunately, threat actors routinely attack users by chaining together two vulnerabilities — one to compromise the sandboxed process containing the malicious site, and another to escape the sandbox [2]</span><span style=\"font-weight: 400;\">. To keep our users secure against the most well-funded adversaries, we need multiple layers of protection.</span></p>\n<p><span style=\"font-weight: 400;\">Having already isolated things along trust boundaries, the next logical step is to isolate across functional boundaries. Historically, this has meant hoisting a subcomponent into its own process. For example, Firefox runs audio and video codecs in a dedicated, locked-down process with a limited interface to the rest of the system. However, there are some serious limitations to this approach. </span><span style=\"font-weight: 400;\">First, it requires decoupling the code and making it asynchronous, which is usually time-consuming and may impose a performance cost. Second, processes have a fixed memory overhead, and adding more of them increases the memory footprint of the application. </span></p>\n<p><span style=\"font-weight: 400;\">For all of these reasons, nobody would seriously consider hoisting something like the XML parser into its own process. To isolate at that level of granularity, we need a different approach.</span></p>\n<h2><strong>Isolating with RLBox</strong></h2>\n<p><span style=\"font-weight: 400;\">This is where RLBox comes in. Rather than hoisting the code into a separate process, we instead compile it into WebAssembly and then compile that WebAssembly into native code. This doesn’t result in us shipping any .wasm files in Firefox, since the WebAssembly step is only an intermediate representation in our build process. </span></p>\n<p><span style=\"font-weight: 400;\">However, the transformation places two key restrictions on the target code: it can’t jump to unexpected parts of the rest of the program, and it can’t access memory outside of a specified region. Together, these restrictions </span><a href=\"http://www.cse.psu.edu/~gxt29/papers/sfi-final.pdf\"><span style=\"font-weight: 400;\">make it safe to share an address space</span></a><span style=\"font-weight: 400;\"> (</span><a href=\"https://arxiv.org/abs/2105.00033\"><span style=\"font-weight: 400;\">including the stack</span></a><span style=\"font-weight: 400;\">) between trusted and untrusted code, allowing us to run them in the same process largely as we were doing before. </span><span style=\"font-weight: 400;\">This, in turn, makes it easy to apply without major refactoring: the programmer only needs to sanitize any values that come from the sandbox (since they could be maliciously-crafted), a task which RLBox makes easy with a <a href=\"https://hacks.mozilla.org/2020/02/securing-firefox-with-webassembly/\">tainting layer</a></span><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">The first step in this transformation is straightforward: we use </span><a href=\"https://clang.llvm.org/\"><span style=\"font-weight: 400;\">Clang</span></a><span style=\"font-weight: 400;\"> to compile Firefox, and Clang knows how to emit WebAssembly, so we simply need to switch the output format for the given module from native code to wasm. For the second step, our prototype implementation used </span><a href=\"https://github.com/bytecodealliance/wasmtime/tree/main/cranelift\"><span style=\"font-weight: 400;\">Cranelift</span></a><span style=\"font-weight: 400;\">. Cranelift is excellent, but a second native code generator added complexity — and we realized that it would be simpler to just map the WebAssembly back into something that our existing build system could ingest. </span></p>\n<p><span style=\"font-weight: 400;\">We accomplished this with </span><a href=\"https://github.com/WebAssembly/wabt/tree/main/wasm2c\"><span style=\"font-weight: 400;\">wasm2c</span></a><span style=\"font-weight: 400;\">, which performs a straightforward translation of WebAssembly into equivalent C code, which we can then feed back into Clang along with the rest of the Firefox source code. This approach is very simple, and automatically enables a number of important features that we support for regular Firefox code: profile-guided optimization, inlining across sandbox boundaries, crash reporting, debugger support, source-code indexing, and likely other things that we have yet to appreciate.</span></p>\n<h2><strong>Next Steps</strong></h2>\n<p><span style=\"font-weight: 400;\">RLBox is a big win for us on several fronts: it protects our users from accidental defects as well as supply-chain attacks, and it reduces the need for us to scramble when such issues are disclosed upstream. </span><span style=\"font-weight: 400;\">As such, we intend to continue applying to more components going forward. Some components are not a good fit for this approach — either because they depend too much on sharing memory with the rest of the program, or because they’re too performance-sensitive to accept the modest overhead incurred — but we’ve identified a number of other good candidates. </span></p>\n<p><span style=\"font-weight: 400;\">Moreover, we hope to see this technology make its way into other browsers and software projects to make the ecosystem safer. </span><a href=\"https://github.com/PLSysSec/rlbox_sandboxing_api\"><span style=\"font-weight: 400;\">RLBox</span></a><span style=\"font-weight: 400;\"> is a standalone project that’s designed to be very modular and easy-to-use, and the team behind it would welcome other use-cases.</span></p>\n<p><span style=\"font-weight: 400;\">Speaking of the team: I’d like to thank </span><a href=\"https://shravanrn.com/\"><span style=\"font-weight: 400;\">Shravan Narayan</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://cseweb.ucsd.edu/~dstefan/\"><span style=\"font-weight: 400;\">Deian Stefan</span></a><span style=\"font-weight: 400;\">, and </span><a href=\"https://www.cs.utexas.edu/~hovav/\"><span style=\"font-weight: 400;\">Hovav Shacham</span></a><span style=\"font-weight: 400;\"> for their tireless work in bringing this work from research concept to production. Shipping to hundreds of millions of users is hard, and </span><span style=\"font-weight: 400;\">they did some seriously impressive work.</span></p>\n<p>Read more about RLBox and this announcement on the <a class=\"c-link\" tabindex=\"-1\" href=\"https://jacobsschool.ucsd.edu/news/release/3374\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://jacobsschool.ucsd.edu/news/release/3374\" data-sk=\"tooltip_parent\" data-remove-tab-index=\"true\">UC San Diego Jacobs School of Engineering website</a>.</p>\n<hr />\n<p>[1] Cross-platform sandboxing for Graphite, Hunspell, and Ogg is shipping in Firefox 95, while Expat and Woff2 will ship in Firefox 96.</p>\n<p>[2] By using a syscall to to exploit a vulnerability in the OS, or by using an IPC message to exploit a vulnerability in a process hosting more-privileged parts of the browser.</p>\n<hr />\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/12/webassembly-and-back-again-fine-grained-sandboxing-in-firefox-95/\">WebAssembly and Back Again: Fine-Grained Sandboxing in Firefox 95</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "In Firefox 95, we’re shipping a novel sandboxing technology called RLBox — developed in collaboration with researchers at the University of California San Diego and the University of Texas — that makes it easy and efficient to isolate subcomponents to make the browser more secure. This technology opens up new opportunities beyond what’s been possible with traditional process-based sandboxing, and we look forward to expanding its usage and (hopefully) seeing it adopted in other browsers and software projects.\nThis technique, which uses WebAssembly to isolate potentially-buggy code, builds on the prototype we shipped last year to Mac and Linux users. Now, we’re bringing that technology to all supported Firefox platforms (desktop and mobile), and isolating five different modules: Graphite, Hunspell, Ogg, Expat and Woff2 [1]. \nGoing forward, we can treat these modules as untrusted code, and — assuming we did it right — even a zero-day vulnerability in any of them should pose no threat to Firefox. Accordingly, we’ve updated our bug bounty program to pay researchers for bypassing the sandbox even without a vulnerability in the isolated library.\nThe Limits of Process Sandboxing\nAll major browsers run Web content in its own sandboxed process, in theory preventing it from exploiting a browser vulnerability to compromise your computer. On desktop operating systems, Firefox also isolates each site in its own process in order to protect sites from each other. \nUnfortunately, threat actors routinely attack users by chaining together two vulnerabilities — one to compromise the sandboxed process containing the malicious site, and another to escape the sandbox [2]. To keep our users secure against the most well-funded adversaries, we need multiple layers of protection.\nHaving already isolated things along trust boundaries, the next logical step is to isolate across functional boundaries. Historically, this has meant hoisting a subcomponent into its own process. For example, Firefox runs audio and video codecs in a dedicated, locked-down process with a limited interface to the rest of the system. However, there are some serious limitations to this approach. First, it requires decoupling the code and making it asynchronous, which is usually time-consuming and may impose a performance cost. Second, processes have a fixed memory overhead, and adding more of them increases the memory footprint of the application. \nFor all of these reasons, nobody would seriously consider hoisting something like the XML parser into its own process. To isolate at that level of granularity, we need a different approach.\nIsolating with RLBox\nThis is where RLBox comes in. Rather than hoisting the code into a separate process, we instead compile it into WebAssembly and then compile that WebAssembly into native code. This doesn’t result in us shipping any .wasm files in Firefox, since the WebAssembly step is only an intermediate representation in our build process. \nHowever, the transformation places two key restrictions on the target code: it can’t jump to unexpected parts of the rest of the program, and it can’t access memory outside of a specified region. Together, these restrictions make it safe to share an address space (including the stack) between trusted and untrusted code, allowing us to run them in the same process largely as we were doing before. This, in turn, makes it easy to apply without major refactoring: the programmer only needs to sanitize any values that come from the sandbox (since they could be maliciously-crafted), a task which RLBox makes easy with a tainting layer.\nThe first step in this transformation is straightforward: we use Clang to compile Firefox, and Clang knows how to emit WebAssembly, so we simply need to switch the output format for the given module from native code to wasm. For the second step, our prototype implementation used Cranelift. Cranelift is excellent, but a second native code generator added complexity — and we realized that it would be simpler to just map the WebAssembly back into something that our existing build system could ingest. \nWe accomplished this with wasm2c, which performs a straightforward translation of WebAssembly into equivalent C code, which we can then feed back into Clang along with the rest of the Firefox source code. This approach is very simple, and automatically enables a number of important features that we support for regular Firefox code: profile-guided optimization, inlining across sandbox boundaries, crash reporting, debugger support, source-code indexing, and likely other things that we have yet to appreciate.\nNext Steps\nRLBox is a big win for us on several fronts: it protects our users from accidental defects as well as supply-chain attacks, and it reduces the need for us to scramble when such issues are disclosed upstream. As such, we intend to continue applying to more components going forward. Some components are not a good fit for this approach — either because they depend too much on sharing memory with the rest of the program, or because they’re too performance-sensitive to accept the modest overhead incurred — but we’ve identified a number of other good candidates. \nMoreover, we hope to see this technology make its way into other browsers and software projects to make the ecosystem safer. RLBox is a standalone project that’s designed to be very modular and easy-to-use, and the team behind it would welcome other use-cases.\nSpeaking of the team: I’d like to thank Shravan Narayan, Deian Stefan, and Hovav Shacham for their tireless work in bringing this work from research concept to production. Shipping to hundreds of millions of users is hard, and they did some seriously impressive work.\nRead more about RLBox and this announcement on the UC San Diego Jacobs School of Engineering website.\n\n[1] Cross-platform sandboxing for Graphite, Hunspell, and Ogg is shipping in Firefox 95, while Expat and Woff2 will ship in Firefox 96.\n[2] By using a syscall to to exploit a vulnerability in the OS, or by using an IPC message to exploit a vulnerability in a process hosting more-privileged parts of the browser.\n\nThe post WebAssembly and Back Again: Fine-Grained Sandboxing in Firefox 95 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-12-06T13:05:44.000Z",
      "date_modified": "2021-12-06T13:05:44.000Z",
      "_plugin": {
        "pageFilename": "81be85273b5a1a98d016b999ab60a6f68dd461804af327aa21da5a18b6962685.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47474",
      "url": "https://hacks.mozilla.org/2021/11/hacks-decoded-seyi-akiwowo-founder-of-glitch/",
      "title": "Hacks Decoded: Seyi Akiwowo, Founder of Glitch",
      "summary": "Seyi Akiwowo’s reputation precedes her. Akiwowo is the founder of Glitch, an organization that seeks to end online abuse. We spoke with Seyi over video chat to learn about what drives her, why she does what she does and what she’d be doing if not battling trolls online for a living. \nThe post Hacks Decoded: Seyi Akiwowo, Founder of Glitch appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><i><span style=\"font-weight: 400;\">Welcome to our Hacks: Decoded Interview series! </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Once a month, </span></i><a href=\"https://foundation.mozilla.org/\"><i><span style=\"font-weight: 400;\">Mozilla Foundation</span></i></a><i><span style=\"font-weight: 400;\">’s </span></i><a href=\"https://www.xavierharding.com/\"><i><span style=\"font-weight: 400;\">Xavier Harding</span></i></a><i><span style=\"font-weight: 400;\"> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </span></i><a href=\"https://hacks.mozilla.org/\"><i><span style=\"font-weight: 400;\">Hacks</span></i></a><i><span style=\"font-weight: 400;\"> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.</span></i></p>\n<p><b>Meet Seyi Akiwowo (<em>pronounced Shay-ee Aki-wo-wo</em>)</b></p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47480 size-large\" src=\"https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-500x750.jpg\" alt=\"seyi akiwowo\" width=\"500\" height=\"750\" srcset=\"https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-500x750.jpg 500w, https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-250x375.jpg 250w, https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-768x1152.jpg 768w, https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-1024x1536.jpg 1024w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p><span style=\"font-weight: 400;\">Seyi Akiwowo’s reputation precedes her. Akiwowo is the founder of </span><a href=\"https://glitchcharity.co.uk/\"><span style=\"font-weight: 400;\">Glitch</span></a><span style=\"font-weight: 400;\">, an organization that seeks to end online abuse. Akiwowo is a graduate of the London School of Economics. She’s delivered talks at TED, European Parliament, the U.N. and more and was elected a councillor for the Labor Party in East London — the youngest Black woman ever to do so. </span><i></i></p>\n<p><span style=\"font-weight: 400;\">We spoke with Seyi over video chat to learn about what drives her, why she does what she does and what she’d be doing if not battling trolls online for a living. All that in this month’s Hacks: Decoded.</span></p>\n<p><b>Where do we even begin? What do you consider to be the starting point of your story?</b><i></i></p>\n<p><span style=\"font-weight: 400;\">I’d start with my love for the internet. When you&#8217;re in this anti-troll, human rights space, you’re pitched as anti-tech and anti-change and it&#8217;s simply not true. It’s because I’m such a lover and fan of the internet, I do what I do. I’m part of the ’90s, Microsoft-PC-with-the-big-back-and-a-CD-drive generation. </span></p>\n<p><span style=\"font-weight: 400;\">I had dial-up internet and spent huge amounts of time on MSN and MySpace. There were times I was home alone, my mum out trying to make ends meet, my dad not around, and the computer was my outlet. I adored all of it. The worldwide web was my friend. It was my connection to the rest of the world, all while in a small council flat in East London. </span></p>\n<p><span style=\"font-weight: 400;\">It was when I appeared in a </span><a href=\"https://www.ted.com/talks/seyi_akiwowo_how_to_fix_the_glitch_in_our_online_communities\"><span style=\"font-weight: 400;\">video</span></a><span style=\"font-weight: 400;\"> that went viral that I learned what it meant to be a black woman online. I realized, “Oh, there are people that don’t know me and do not like me and are telling me that they don&#8217;t like me in very violent ways.” So I’d say my journey starts with the love of the internet and innovation. When you&#8217;re in this kind of anti-troll, human rights space, you&#8217;re kind of pitched as anti-tech and anti-change and it&#8217;s not [true], it&#8217;s because I&#8217;m such a lover of the internet is the reason I do what I do.</span></p>\n<p><b>How does where you’re from influence what you do now?</b></p>\n<p><span style=\"font-weight: 400;\">I went to a very good university but the mix of social classes felt almost like a class war! Going to university was the first time I ever felt othered. I didn’t feel it growing up. I grew up in Newham in East London and I didn&#8217;t really realize I was from a poor area. There’s such beauty and safety in that, but there’s also a glass ceiling without you really realizing it.</span></p>\n<p><span style=\"font-weight: 400;\">I grew up with an entrepreneurial spirit and we just made do with what we had and we just were still excellent with the minimum we had. I see how that translates to nowadays. I can really make £1 of funding go far at Glitch! My working-class bargain hunting roots really helped me be frugal with money.</span></p>\n<p><b>It’s interesting how when you throw a Black woman into the mix, people don’t know how to act. Off the internet but specifically on the internet too.</b></p>\n<p><span style=\"font-weight: 400;\">It is, something I’ve been thinking about a lot is that the internet doesn’t need to be this bad if we just listen to Black women many many years ago. If we think about some of the Black activists or campaigners or even the Black women that were just minding their business but got forced into this issue because of their lived experience, like myself. Folks at Facebook, Twitter, Google who weren’t listening — you have </span><a href=\"https://twitter.com/Blackamazon\"><span style=\"font-weight: 400;\">Sydette</span></a><span style=\"font-weight: 400;\"> and Michelle Ferrier. Michelle is a journalist who, after a hate campaign, started </span><a href=\"http://www.troll-busters.com/\"><span style=\"font-weight: 400;\">Troll Busters</span></a><span style=\"font-weight: 400;\"> to help other journalists and women.</span></p>\n<p><span style=\"font-weight: 400;\"> You’ve got </span><a href=\"https://twitter.com/AngryBlackLady\"><span style=\"font-weight: 400;\">Angry Black Lady</span></a><span style=\"font-weight: 400;\"> on Twitter who I remember learning a lot about her experience on social media and she just wasn&#8217;t listened to. I think white men not only have the privilege to make things and get a lot of venture capital and raise a lot of money to make these huge products and break and fail. That’s one. But they&#8217;re also privileged in their echo chamber bubble that they didn&#8217;t have to listen to Black women. And it was only until it started affecting white middle-class upper-class Hollywood that we started paying closer attention to this issue.</span></p>\n<p><b>You’re saying something a lot of us know. Black women encounter harms on these platforms that a white guy may not necessarily encounter. And yet, most of the leadership at these companies are not black women, they’re mostly white men — a group of folks who may not realize how bad these problems are. Why do you think that is? When will it change?</b></p>\n<p><span style=\"font-weight: 400;\">I really don&#8217;t know and I think it&#8217;s a conundrum that isn’t unique to the tech space. You see it everywhere. You see it in conversations in policy discussions about domestic abuse or refugees and you do not have the community that faces it the most, in those conversations. </span></p>\n<p><span style=\"font-weight: 400;\">That&#8217;s the whole reason I went into politics because decisions were being made about my community that’s predominantly people of colour, we have such a high transient population, one of the most diverse boroughs in the world, and yet the council did not look like its community. It&#8217;s a phenomenon that&#8217;s existed in so many places and it&#8217;s even worse in tech because you&#8217;re seeing such the direct harm it&#8217;s having tenfold. </span></p>\n<p><span style=\"font-weight: 400;\">But it&#8217;s everywhere. The erasure and the lack of dignity and respect Black folks and people of colour are given. Issues have to become mainstream enough for people to act on that. Black Lives Matter had to become mainstream enough for people to finally listen. It&#8217;s a lesson for all of us. How do we make sure there is someone in the room who is more, in relative terms, a more minoritized community than you. </span></p>\n<p><span style=\"font-weight: 400;\">How do we make sure that we’re allies offline and online? How are we making sure we’re building community and sharing that legacy and our knowledge and our playbooks and our capital? So that more people from minoritized communities come together.</span></p>\n<p><b>What’s been the most challenging thing about founding and running Glitch?</b></p>\n<p><span style=\"font-weight: 400;\">When you’re a Black founder CEO in a predominantly white charity sector and tech sector, things are just different. I’ve taken meetings where it’s supposed to be a prospective funding meeting about our work and before we can even give the pitch the first thing the prospective funder says is, “If Black lives really matter—” </span><i></i></p>\n<p><b><em>raises eyebrows, confusedly</em></b></p>\n<p><span style=\"font-weight: 400;\">Exactly. So he says, “If Black lives really matter, why are you all not getting the vaccine?” It feels like someone is putting you in an ice bucket or flushing your head down a toilet, comments like that, microaggressions too, are a jarring reminder that you don&#8217;t belong here. It’s like you’re finally at the table, and someone has banged your head against that very table as if to say “you’re stupid for thinking you can be here.” </span></p>\n<p><span style=\"font-weight: 400;\">I think those are moments that really bungee-cord pull you back to reality, That’s what’s really tough. Really, really, really, tough. And I think I got lost in negative thoughts this summer where I thought, “I don’t belong, I don’t know what I’m doing. This privilege of being CEO — how do I use it?” and overall just a massive loss of confidence. And everything. And I think that&#8217;s been really tough.</span></p>\n<p><b>Wow. I’m still stuck on this “If Black Lives really matter—” guy. </b></p>\n<p><b>Seyi, what did you want to be when you grew up? Trolls attacked you online because you made a viral video online so you rose to the occasion and fought back. There are folks out there who don’t grapple with trolls, out there living their truth without a care in the world. What did that look like for you? What did you want to be when you grew up?</b></p>\n<p><span style=\"font-weight: 400;\">I wanted to be a dancer. I wanted to be the next Ciara. I wanted to be in Missy Elliot’s videos. I was like, ‘Move over, sis!’</span><i></i></p>\n<p><b>What’s your favorite Ciara song? </b></p>\n<p><span style=\"font-weight: 400;\">Goodies.</span></p>\n<p><b>Classic. </b></p>\n<p><b></b><b>Back to trolls, what’s a topic regarding online abuse that you wish you saw people talk about more?</b></p>\n<p><span style=\"font-weight: 400;\">The topic of social media whistleblowers and how we should be less worried about making Facebook and Twitter look bad and more about holding them accountable. More specifically, what does this do in the way of changing the system? We shouldn’t have to keep relying on brave individuals who generally tend to be women and women of colour who really put themselves out on the line.</span></p>\n<p><span style=\"font-weight: 400;\"> I don&#8217;t want this to be the trend where we get small bits of reform. We need to have the media holding companies to account, not looking to make Mark Zuckerberg the bad guy because then the narrative becomes ‘when he leaves everything will be sorted out&#8217; and that is not the case.</span><i></i></p>\n<p><b>Seyi, you have the longest resume I’ve ever seen in my life. What motivates you to keep going?</b></p>\n<p><span style=\"font-weight: 400;\">I’m not, I don’t think I want to “keep going” anymore. I grew my organization by 50% in terms of income and more in terms of staff and diversified our income streams before we hit the two-year mark — during a pandemic! I’m ready to rest, I’m ready to sleep more, I’m ready to do work that is still great with minimum viable effort. That’s the sweet spot I’m looking for. </span></p>\n<p>&#8212;</p>\n<p><i>You can keep up with Seyi and Glitch&#8217;s work right </i><a href=\"https://glitchcharity.co.uk/\" target=\"_blank\" rel=\"noopener\"><i>here</i></a><i> and support their special Christmas fundraiser for a safe internet <a href=\"https://www.justgiving.com/campaign/GlitchChristmas\">here</a>.</i></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/11/hacks-decoded-seyi-akiwowo-founder-of-glitch/\">Hacks Decoded: Seyi Akiwowo, Founder of Glitch</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Welcome to our Hacks: Decoded Interview series! \nOnce a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.\nMeet Seyi Akiwowo (pronounced Shay-ee Aki-wo-wo)\n\nSeyi Akiwowo’s reputation precedes her. Akiwowo is the founder of Glitch, an organization that seeks to end online abuse. Akiwowo is a graduate of the London School of Economics. She’s delivered talks at TED, European Parliament, the U.N. and more and was elected a councillor for the Labor Party in East London — the youngest Black woman ever to do so. \nWe spoke with Seyi over video chat to learn about what drives her, why she does what she does and what she’d be doing if not battling trolls online for a living. All that in this month’s Hacks: Decoded.\nWhere do we even begin? What do you consider to be the starting point of your story?\nI’d start with my love for the internet. When you’re in this anti-troll, human rights space, you’re pitched as anti-tech and anti-change and it’s simply not true. It’s because I’m such a lover and fan of the internet, I do what I do. I’m part of the ’90s, Microsoft-PC-with-the-big-back-and-a-CD-drive generation. \nI had dial-up internet and spent huge amounts of time on MSN and MySpace. There were times I was home alone, my mum out trying to make ends meet, my dad not around, and the computer was my outlet. I adored all of it. The worldwide web was my friend. It was my connection to the rest of the world, all while in a small council flat in East London. \nIt was when I appeared in a video that went viral that I learned what it meant to be a black woman online. I realized, “Oh, there are people that don’t know me and do not like me and are telling me that they don’t like me in very violent ways.” So I’d say my journey starts with the love of the internet and innovation. When you’re in this kind of anti-troll, human rights space, you’re kind of pitched as anti-tech and anti-change and it’s not [true], it’s because I’m such a lover of the internet is the reason I do what I do.\nHow does where you’re from influence what you do now?\nI went to a very good university but the mix of social classes felt almost like a class war! Going to university was the first time I ever felt othered. I didn’t feel it growing up. I grew up in Newham in East London and I didn’t really realize I was from a poor area. There’s such beauty and safety in that, but there’s also a glass ceiling without you really realizing it.\nI grew up with an entrepreneurial spirit and we just made do with what we had and we just were still excellent with the minimum we had. I see how that translates to nowadays. I can really make £1 of funding go far at Glitch! My working-class bargain hunting roots really helped me be frugal with money.\nIt’s interesting how when you throw a Black woman into the mix, people don’t know how to act. Off the internet but specifically on the internet too.\nIt is, something I’ve been thinking about a lot is that the internet doesn’t need to be this bad if we just listen to Black women many many years ago. If we think about some of the Black activists or campaigners or even the Black women that were just minding their business but got forced into this issue because of their lived experience, like myself. Folks at Facebook, Twitter, Google who weren’t listening — you have Sydette and Michelle Ferrier. Michelle is a journalist who, after a hate campaign, started Troll Busters to help other journalists and women.\n You’ve got Angry Black Lady on Twitter who I remember learning a lot about her experience on social media and she just wasn’t listened to. I think white men not only have the privilege to make things and get a lot of venture capital and raise a lot of money to make these huge products and break and fail. That’s one. But they’re also privileged in their echo chamber bubble that they didn’t have to listen to Black women. And it was only until it started affecting white middle-class upper-class Hollywood that we started paying closer attention to this issue.\nYou’re saying something a lot of us know. Black women encounter harms on these platforms that a white guy may not necessarily encounter. And yet, most of the leadership at these companies are not black women, they’re mostly white men — a group of folks who may not realize how bad these problems are. Why do you think that is? When will it change?\nI really don’t know and I think it’s a conundrum that isn’t unique to the tech space. You see it everywhere. You see it in conversations in policy discussions about domestic abuse or refugees and you do not have the community that faces it the most, in those conversations. \nThat’s the whole reason I went into politics because decisions were being made about my community that’s predominantly people of colour, we have such a high transient population, one of the most diverse boroughs in the world, and yet the council did not look like its community. It’s a phenomenon that’s existed in so many places and it’s even worse in tech because you’re seeing such the direct harm it’s having tenfold. \nBut it’s everywhere. The erasure and the lack of dignity and respect Black folks and people of colour are given. Issues have to become mainstream enough for people to act on that. Black Lives Matter had to become mainstream enough for people to finally listen. It’s a lesson for all of us. How do we make sure there is someone in the room who is more, in relative terms, a more minoritized community than you. \nHow do we make sure that we’re allies offline and online? How are we making sure we’re building community and sharing that legacy and our knowledge and our playbooks and our capital? So that more people from minoritized communities come together.\nWhat’s been the most challenging thing about founding and running Glitch?\nWhen you’re a Black founder CEO in a predominantly white charity sector and tech sector, things are just different. I’ve taken meetings where it’s supposed to be a prospective funding meeting about our work and before we can even give the pitch the first thing the prospective funder says is, “If Black lives really matter—” \nraises eyebrows, confusedly\nExactly. So he says, “If Black lives really matter, why are you all not getting the vaccine?” It feels like someone is putting you in an ice bucket or flushing your head down a toilet, comments like that, microaggressions too, are a jarring reminder that you don’t belong here. It’s like you’re finally at the table, and someone has banged your head against that very table as if to say “you’re stupid for thinking you can be here.” \nI think those are moments that really bungee-cord pull you back to reality, That’s what’s really tough. Really, really, really, tough. And I think I got lost in negative thoughts this summer where I thought, “I don’t belong, I don’t know what I’m doing. This privilege of being CEO — how do I use it?” and overall just a massive loss of confidence. And everything. And I think that’s been really tough.\nWow. I’m still stuck on this “If Black Lives really matter—” guy. \nSeyi, what did you want to be when you grew up? Trolls attacked you online because you made a viral video online so you rose to the occasion and fought back. There are folks out there who don’t grapple with trolls, out there living their truth without a care in the world. What did that look like for you? What did you want to be when you grew up?\nI wanted to be a dancer. I wanted to be the next Ciara. I wanted to be in Missy Elliot’s videos. I was like, ‘Move over, sis!’\nWhat’s your favorite Ciara song? \nGoodies.\nClassic. \nBack to trolls, what’s a topic regarding online abuse that you wish you saw people talk about more?\nThe topic of social media whistleblowers and how we should be less worried about making Facebook and Twitter look bad and more about holding them accountable. More specifically, what does this do in the way of changing the system? We shouldn’t have to keep relying on brave individuals who generally tend to be women and women of colour who really put themselves out on the line.\n I don’t want this to be the trend where we get small bits of reform. We need to have the media holding companies to account, not looking to make Mark Zuckerberg the bad guy because then the narrative becomes ‘when he leaves everything will be sorted out’ and that is not the case.\nSeyi, you have the longest resume I’ve ever seen in my life. What motivates you to keep going?\nI’m not, I don’t think I want to “keep going” anymore. I grew my organization by 50% in terms of income and more in terms of staff and diversified our income streams before we hit the two-year mark — during a pandemic! I’m ready to rest, I’m ready to sleep more, I’m ready to do work that is still great with minimum viable effort. That’s the sweet spot I’m looking for. \n—\nYou can keep up with Seyi and Glitch’s work right here and support their special Christmas fundraiser for a safe internet here.\nThe post Hacks Decoded: Seyi Akiwowo, Founder of Glitch appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-11-30T16:02:27.000Z",
      "date_modified": "2021-11-30T16:02:27.000Z",
      "_plugin": {
        "pageFilename": "8cec6c35f0a29e4b7bcc548eb8308b35f038b64846525ad9d4abeb32f40c0053.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47440",
      "url": "https://hacks.mozilla.org/2021/10/hacks-decoded-thomas-park-founder-of-codepip/",
      "title": "Hacks Decoded: Thomas Park, Founder of Codepip",
      "summary": "Welcome to our Hacks: Decoded Interview series! We spoke with Thomas Park over email about coding, his favourite apps and his past life at Mozilla. Thomas is the founder of Codepip, a platform he created for coding games that helps people learn HTML, CSS, JavaScript, etc. The most popular game is Flexbox Froggy.\nThe post Hacks Decoded: Thomas Park, Founder of Codepip appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><i><span style=\"font-weight: 400;\">Welcome to our Hacks: Decoded Interview series! </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Once a month, </span></i><a href=\"https://foundation.mozilla.org/\"><i><span style=\"font-weight: 400;\">Mozilla Foundation</span></i></a><i><span style=\"font-weight: 400;\">’s </span></i><a href=\"https://www.xavierharding.com/\"><i><span style=\"font-weight: 400;\">Xavier Harding</span></i></a><i><span style=\"font-weight: 400;\"> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </span></i><a href=\"https://hacks.mozilla.org/\"><i><span style=\"font-weight: 400;\">Hacks</span></i></a><i><span style=\"font-weight: 400;\"> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work. </span></i></p>\n<h4><strong>Meet Thomas Park </strong></h4>\n<p><span style=\"font-weight: 400;\">Thomas Park is a software developer based in the U.S. (Philadelphia, specifically). Previously, he was a teacher and researcher at Drexel University and even worked at Mozilla Foundation for a stint. Now, he’s the founder of </span><a href=\"https://codepip.com/\"><span style=\"font-weight: 400;\">Codepip</span></a><span style=\"font-weight: 400;\">, a platform that offers games that teach players how to code. Park has made a couple games himself: Flexbox Froggy and Grid Garden.</span></p>\n<p><span style=\"font-weight: 400;\">We spoke with Thomas over email about coding, his favourite apps and his past life at Mozilla. Check it out below and welcome to Hacks: Decoded.</span></p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47454 size-full\" src=\"https://hacks.mozilla.org/files/2021/10/image3.jpg\" alt=\"\" width=\"1999\" height=\"1347\" srcset=\"https://hacks.mozilla.org/files/2021/10/image3.jpg 1999w, https://hacks.mozilla.org/files/2021/10/image3-250x168.jpg 250w, https://hacks.mozilla.org/files/2021/10/image3-500x337.jpg 500w, https://hacks.mozilla.org/files/2021/10/image3-768x518.jpg 768w, https://hacks.mozilla.org/files/2021/10/image3-1536x1035.jpg 1536w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /></p>\n<p><b>Where’d you get your start, Thomas? How did you end up working in tech, what was the first piece of code you wrote, what’s the Thomas Park origin story?</b></p>\n<p><span style=\"font-weight: 400;\">The very first piece of code I wrote was in elementary school. We were introduced to </span><a href=\"https://en.wikipedia.org/wiki/Logo_(programming_language)\"><span style=\"font-weight: 400;\">Logo</span></a><span style=\"font-weight: 400;\">, an educational programming language that was used to draw graphics with a turtle (a little cursor that was shaped like the animal). I drew a rudimentary weapon that shot an animated laser beam, with the word &#8220;LAZER&#8221; misspelled under it.</span></p>\n<p><span style=\"font-weight: 400;\">Afterwards, I took an extremely long hiatus from coding. Dabbled with HyperCard and HTML here and there, but didn&#8217;t pick it up in earnest until college.</span></p>\n<p><span style=\"font-weight: 400;\">Post-college, I worked in the distance education department at the Center for Talented Youth at Johns Hopkins University, designing and teaching online courses. It was there I realized how much the technology we used mediated the experience of our students. I also realized how much better the design of this tech should be. That motivated me to go to grad school to study human-computer interaction, with a focus on educational technology. I wrote a decent amount of code to build prototypes and analyze data during my time there.</span></p>\n<p><b>What is Codepip? What made you want to create it? </b></p>\n<p><a href=\"https://codepip.com/\"><span style=\"font-weight: 400;\">Codepip</span></a><span style=\"font-weight: 400;\"> is a platform I created for coding games that help people learn HTML, CSS, JavaScript, etc. The most popular game is </span><a href=\"https://codepip.com/games/flexbox-froggy/\"><span style=\"font-weight: 400;\">Flexbox Froggy</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47446\" src=\"https://hacks.mozilla.org/files/2021/10/image1.gif\" alt=\"\" width=\"660\" height=\"504\" /></p>\n<p><span style=\"font-weight: 400;\">Codepip actually has its roots in Mozilla. During grad school, I did an internship with the Mozilla Foundation. At the time, they had a code editor geared toward teachers and students called Thimble. For my internship, I worked with Mozilla employees to </span><a href=\"https://thomaspark.co/2013/10/weaving-tutorials-into-mozilla-thimble/\"><span style=\"font-weight: 400;\">integrate a tutorial feature into Thimble</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">Anyway, through this internship I got to attend </span><a href=\"https://www.mozillafestival.org/\"><span style=\"font-weight: 400;\">Mozilla Festival</span></a><span style=\"font-weight: 400;\">. And there I met many people who did brilliant work inside and outside of Mozilla. One was an extremely talented designer named </span><a href=\"https://twitter.com/flukeout\"><span style=\"font-weight: 400;\">Luke Pacholski</span></a><span style=\"font-weight: 400;\">. By that time, he had created </span><a href=\"https://flukeout.github.io/\"><span style=\"font-weight: 400;\">CSS Diner</span></a><span style=\"font-weight: 400;\">, a game about CSS selectors. And we got to chatting about other game ideas.</span></p>\n<p><span style=\"font-weight: 400;\">After I returned from MozFest, I worked weekends for about a month to create Flexbox Froggy. I was blown away by the reception, from both beginners who wanted to learn CSS, to more experienced devs curious about this powerful new CSS module called flexbox. To me, this affirmed that coding games could make a good complement to more traditional ways of learning. Since then, I&#8217;ve made other games that touch on CSS grid, JS math, HTML shortcuts with Emmet, and more.</span></p>\n<p><b>Gamified online learning has become quite popular in the past couple of years, what are some old school methods that you still recommend and use?</b></p>\n<p><span style=\"font-weight: 400;\">Consulting the docs, if you can call that old school. I often visit the </span><a href=\"https://developer.mozilla.org/docs/\"><span style=\"font-weight: 400;\">MDN Web Docs</span></a><span style=\"font-weight: 400;\"> to learn some aspect of CSS or JS. The articles are detailed, with plenty of examples.</span></p>\n<p><span style=\"font-weight: 400;\">On occasion I find myself doing a deep dive into the </span><a href=\"https://www.w3.org/Style/CSS/\"><span style=\"font-weight: 400;\">W3C standards</span></a><span style=\"font-weight: 400;\">, though navigating the site can be tricky.</span></p>\n<p><span style=\"font-weight: 400;\">Same goes for any third-party library or framework you&#8217;re working with — read the docs!</span></p>\n<p><b>What&#8217;s one thing you wish you knew when you first started to code?</b></p>\n<p><span style=\"font-weight: 400;\">I wish I knew git when I first started to code. Actually, I wish I knew git now.</span></p>\n<p><span style=\"font-weight: 400;\">It&#8217;s never too early to start version controlling your projects. Sign up for a free GitHub account, install GitHub&#8217;s client or learn a handful of basic git commands, and backup your code. You can opt for your code to be public if you&#8217;re comfortable with it, private if not. There&#8217;s no excuse.</span></p>\n<p><span style=\"font-weight: 400;\">Plus, years down the line when you&#8217;ve mastered your craft, you can get some entertainment value from looking back at your old code.</span></p>\n<p><b>Whose work do you admire right now? Who should more people be paying attention to?</b></p>\n<p><span style=\"font-weight: 400;\">I&#8217;m curious how other people answer this. I feel like I&#8217;m out of the loop on this one.</span></p>\n<p><span style=\"font-weight: 400;\">But since you asked, I will say that when it comes to web design with high stakes, the teams at Stripe and Apple have been the gold standard for years. I&#8217;ll browse their sites and get inspired by the many small, almost imperceptible details that add up to something magical. Or something in your face that blows my mind.</span></p>\n<p><span style=\"font-weight: 400;\">On a more personal front, there&#8217;s the art of </span><a href=\"https://diana-adrianne.com/\"><span style=\"font-weight: 400;\">Diana Smith</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://codepen.io/ivorjetski\"><span style=\"font-weight: 400;\">Ben Evans</span></a><span style=\"font-weight: 400;\">, which pushes the boundaries of what&#8217;s possible with pure CSS. I love how </span><a href=\"https://lynnandtonic.com/\"><span style=\"font-weight: 400;\">Lynn Fisher</span></a><span style=\"font-weight: 400;\"> commits to weird side projects. And I admire the approachability of </span><a href=\"https://www.joshwcomeau.com/\"><span style=\"font-weight: 400;\">Josh Comeau&#8217;s</span></a><span style=\"font-weight: 400;\"> writings on technical subjects.</span></p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47450\" src=\"https://hacks.mozilla.org/files/2021/10/image2.png\" alt=\"\" width=\"1999\" height=\"1170\" srcset=\"https://hacks.mozilla.org/files/2021/10/image2.png 1999w, https://hacks.mozilla.org/files/2021/10/image2-250x146.png 250w, https://hacks.mozilla.org/files/2021/10/image2-500x293.png 500w, https://hacks.mozilla.org/files/2021/10/image2-768x450.png 768w, https://hacks.mozilla.org/files/2021/10/image2-1536x899.png 1536w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /></p>\n<p><b>What’s a part of your journey that many may not realize when they look at your resume or LinkedIn page?</b></p>\n<p><span style=\"font-weight: 400;\">My resume tells a cohesive story that connects the dots of my education and employment. As if there was a master plan that guided me to where I am.</span></p>\n<p><span style=\"font-weight: 400;\">The truth is I never had it all figured out. I tried some things I enjoyed, tried other things which I learned I did not, and discovered whole new industries that I didn&#8217;t even realize existed. On the whole, the journey has been rewarding, and I feel fortunate to be doing work right now that I love and feel passionate about. But that took time and is subject to change.</span></p>\n<p><span style=\"font-weight: 400;\">Some beginners may feel discouraged that they don&#8217;t have their career mapped out from A to Z, like everyone else seemingly does. But all of us are on our own journeys of self-discovery, even if the picture we paint for prospective employers, or family and friends, is one of a singular path.</span></p>\n<p><b>What’s something you’ve realized since we’ve been in this pandemic? Tech-related or otherwise?</b></p>\n<p><span style=\"font-weight: 400;\">Outside of tech, I&#8217;ve realized how grateful I am for all the healthcare workers, teachers, caretakers, sanitation workers, and food service workers who put themselves at risk to keep things going. At times I got a glimpse of what happens without them and it wasn&#8217;t pretty.</span></p>\n<p><span style=\"font-weight: 400;\">Tech-related, the pandemic has accelerated a lot of tech trends by years or even decades. Not everything is as stark as, say, Blockbuster getting replaced by Netflix, but industries are irreversibly changing and new technology is making that happen. It really underscores how in order to survive and flourish, we as tech workers have to always be ready to learn and adapt in a fast-changing world.</span></p>\n<p><b>Okay a random one — you’re stranded on a desert island with nothing but a smartphone. Which three apps could you not live without?</b></p>\n<p><span style=\"font-weight: 400;\">Assuming I&#8217;ll be stuck there for a while, I&#8217;d definitely need my podcasts. My podcast app of choice has long been </span><a href=\"https://overcast.fm/\"><span style=\"font-weight: 400;\">Overcast</span></a><span style=\"font-weight: 400;\">. I&#8217;d load it up with some </span><a href=\"https://99percentinvisible.org/\"><span style=\"font-weight: 400;\">99% Invisible</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://www.npr.org/podcasts/510289/planet-money\"><span style=\"font-weight: 400;\">Planet Money</span></a><span style=\"font-weight: 400;\">. Although I&#8217;d probably only need a single episode of </span><a href=\"https://www.dancarlin.com/hardcore-history-series/\"><span style=\"font-weight: 400;\">Hardcore History</span></a><span style=\"font-weight: 400;\"> to last me before I got rescued.</span></p>\n<p><span style=\"font-weight: 400;\">I&#8217;d also have </span><a href=\"https://simplenote.com/\"><span style=\"font-weight: 400;\">Simplenote</span></a><span style=\"font-weight: 400;\"> for all my note-taking needs. When it comes to notes, I prefer the minimalist, low-friction approach of Simplenote to manage my to-dos and projects. Or count days and nights in this case.</span></p>\n<p><span style=\"font-weight: 400;\">Assuming I have bars, my last app is Reddit. The larger subs get most of the attention, but there are plenty of smaller ones with strong communities and thoughtful discussion. Just avoid the financial investing advice from there.</span></p>\n<p><b>Last question — what’s next for you?</b></p>\n<p><span style=\"font-weight: 400;\">I&#8217;m putting the finishing touches on a new coding game called </span><a href=\"https://codepip.com/games/disarray/\"><span style=\"font-weight: 400;\">Disarray</span></a><span style=\"font-weight: 400;\">. You play a cleaning expert who organizes arrays of household objects using JavaScript methods like <code>push</code>, <code>sort</code>, <code>splice</code>, and <code>map</code>, sparking joy in the homeowner.<br />\n</span></p>\n<p><span style=\"font-weight: 400;\">And planning for a sequel. Maybe a game about databases…</span></p>\n<p><i><span style=\"font-weight: 400;\">Thomas Park is a software developer living in Philly. You can keep up with his work right </span></i><a href=\"https://twitter.com/thomashpark\"><i><span style=\"font-weight: 400;\">here</span></i></a><i><span style=\"font-weight: 400;\"> and keep up with Mozilla on </span></i><a href=\"https://twitter.com/mozilla\"><i><span style=\"font-weight: 400;\">Twitter</span></i></a><i><span style=\"font-weight: 400;\"> and </span></i><a href=\"https://www.instagram.com/mozilla/\"><i><span style=\"font-weight: 400;\">Instagram</span></i></a><i><span style=\"font-weight: 400;\">. Tune into future articles in the Hacks: Decoded series on this very blog.</span></i></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/10/hacks-decoded-thomas-park-founder-of-codepip/\">Hacks Decoded: Thomas Park, Founder of Codepip</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Welcome to our Hacks: Decoded Interview series! \nOnce a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work. \nMeet Thomas Park \nThomas Park is a software developer based in the U.S. (Philadelphia, specifically). Previously, he was a teacher and researcher at Drexel University and even worked at Mozilla Foundation for a stint. Now, he’s the founder of Codepip, a platform that offers games that teach players how to code. Park has made a couple games himself: Flexbox Froggy and Grid Garden.\nWe spoke with Thomas over email about coding, his favourite apps and his past life at Mozilla. Check it out below and welcome to Hacks: Decoded.\n\nWhere’d you get your start, Thomas? How did you end up working in tech, what was the first piece of code you wrote, what’s the Thomas Park origin story?\nThe very first piece of code I wrote was in elementary school. We were introduced to Logo, an educational programming language that was used to draw graphics with a turtle (a little cursor that was shaped like the animal). I drew a rudimentary weapon that shot an animated laser beam, with the word “LAZER” misspelled under it.\nAfterwards, I took an extremely long hiatus from coding. Dabbled with HyperCard and HTML here and there, but didn’t pick it up in earnest until college.\nPost-college, I worked in the distance education department at the Center for Talented Youth at Johns Hopkins University, designing and teaching online courses. It was there I realized how much the technology we used mediated the experience of our students. I also realized how much better the design of this tech should be. That motivated me to go to grad school to study human-computer interaction, with a focus on educational technology. I wrote a decent amount of code to build prototypes and analyze data during my time there.\nWhat is Codepip? What made you want to create it? \nCodepip is a platform I created for coding games that help people learn HTML, CSS, JavaScript, etc. The most popular game is Flexbox Froggy.\n\nCodepip actually has its roots in Mozilla. During grad school, I did an internship with the Mozilla Foundation. At the time, they had a code editor geared toward teachers and students called Thimble. For my internship, I worked with Mozilla employees to integrate a tutorial feature into Thimble.\nAnyway, through this internship I got to attend Mozilla Festival. And there I met many people who did brilliant work inside and outside of Mozilla. One was an extremely talented designer named Luke Pacholski. By that time, he had created CSS Diner, a game about CSS selectors. And we got to chatting about other game ideas.\nAfter I returned from MozFest, I worked weekends for about a month to create Flexbox Froggy. I was blown away by the reception, from both beginners who wanted to learn CSS, to more experienced devs curious about this powerful new CSS module called flexbox. To me, this affirmed that coding games could make a good complement to more traditional ways of learning. Since then, I’ve made other games that touch on CSS grid, JS math, HTML shortcuts with Emmet, and more.\nGamified online learning has become quite popular in the past couple of years, what are some old school methods that you still recommend and use?\nConsulting the docs, if you can call that old school. I often visit the MDN Web Docs to learn some aspect of CSS or JS. The articles are detailed, with plenty of examples.\nOn occasion I find myself doing a deep dive into the W3C standards, though navigating the site can be tricky.\nSame goes for any third-party library or framework you’re working with — read the docs!\nWhat’s one thing you wish you knew when you first started to code?\nI wish I knew git when I first started to code. Actually, I wish I knew git now.\nIt’s never too early to start version controlling your projects. Sign up for a free GitHub account, install GitHub’s client or learn a handful of basic git commands, and backup your code. You can opt for your code to be public if you’re comfortable with it, private if not. There’s no excuse.\nPlus, years down the line when you’ve mastered your craft, you can get some entertainment value from looking back at your old code.\nWhose work do you admire right now? Who should more people be paying attention to?\nI’m curious how other people answer this. I feel like I’m out of the loop on this one.\nBut since you asked, I will say that when it comes to web design with high stakes, the teams at Stripe and Apple have been the gold standard for years. I’ll browse their sites and get inspired by the many small, almost imperceptible details that add up to something magical. Or something in your face that blows my mind.\nOn a more personal front, there’s the art of Diana Smith and Ben Evans, which pushes the boundaries of what’s possible with pure CSS. I love how Lynn Fisher commits to weird side projects. And I admire the approachability of Josh Comeau’s writings on technical subjects.\n\nWhat’s a part of your journey that many may not realize when they look at your resume or LinkedIn page?\nMy resume tells a cohesive story that connects the dots of my education and employment. As if there was a master plan that guided me to where I am.\nThe truth is I never had it all figured out. I tried some things I enjoyed, tried other things which I learned I did not, and discovered whole new industries that I didn’t even realize existed. On the whole, the journey has been rewarding, and I feel fortunate to be doing work right now that I love and feel passionate about. But that took time and is subject to change.\nSome beginners may feel discouraged that they don’t have their career mapped out from A to Z, like everyone else seemingly does. But all of us are on our own journeys of self-discovery, even if the picture we paint for prospective employers, or family and friends, is one of a singular path.\nWhat’s something you’ve realized since we’ve been in this pandemic? Tech-related or otherwise?\nOutside of tech, I’ve realized how grateful I am for all the healthcare workers, teachers, caretakers, sanitation workers, and food service workers who put themselves at risk to keep things going. At times I got a glimpse of what happens without them and it wasn’t pretty.\nTech-related, the pandemic has accelerated a lot of tech trends by years or even decades. Not everything is as stark as, say, Blockbuster getting replaced by Netflix, but industries are irreversibly changing and new technology is making that happen. It really underscores how in order to survive and flourish, we as tech workers have to always be ready to learn and adapt in a fast-changing world.\nOkay a random one — you’re stranded on a desert island with nothing but a smartphone. Which three apps could you not live without?\nAssuming I’ll be stuck there for a while, I’d definitely need my podcasts. My podcast app of choice has long been Overcast. I’d load it up with some 99% Invisible and Planet Money. Although I’d probably only need a single episode of Hardcore History to last me before I got rescued.\nI’d also have Simplenote for all my note-taking needs. When it comes to notes, I prefer the minimalist, low-friction approach of Simplenote to manage my to-dos and projects. Or count days and nights in this case.\nAssuming I have bars, my last app is Reddit. The larger subs get most of the attention, but there are plenty of smaller ones with strong communities and thoughtful discussion. Just avoid the financial investing advice from there.\nLast question — what’s next for you?\nI’m putting the finishing touches on a new coding game called Disarray. You play a cleaning expert who organizes arrays of household objects using JavaScript methods like push, sort, splice, and map, sparking joy in the homeowner.\n\nAnd planning for a sequel. Maybe a game about databases…\nThomas Park is a software developer living in Philly. You can keep up with his work right here and keep up with Mozilla on Twitter and Instagram. Tune into future articles in the Hacks: Decoded series on this very blog.\nThe post Hacks Decoded: Thomas Park, Founder of Codepip appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-10-20T14:35:47.000Z",
      "date_modified": "2021-10-20T14:35:47.000Z",
      "_plugin": {
        "pageFilename": "7931309fff9cd3f92fb110c29dd355ead397949deec3889ac79e59d3be8b5d7c.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47407",
      "url": "https://hacks.mozilla.org/2021/10/lots-to-see-in-firefox-93/",
      "title": "Lots to see in Firefox 93!",
      "summary": "Firefox 93 comes with lots of lovely updates including AVIF image format support, filling of XFA-based forms in its PDF viewer and protection against insecure downloads by blocking downloads relying on insecure connections.\nThe post Lots to see in Firefox 93! appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Firefox 93 comes with lots of lovely updates including AVIF image format support, filling of XFA-based forms in its PDF viewer and protection against insecure downloads by blocking downloads relying on insecure connections.</p>\r\n<p>Web developers are now able to use <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes/Class_static_initialization_blocks\">static initialization blocks</a> within JavaScript classes, and there are some Shadow DOM and Custom Elements updates. The SHA-256 algorithm is now supported for <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication\">HTTP Authentication</a> using digests. This allows much more secure authentication than previously available using the MD5 algorithm.</p>\r\n<p>This blog post provides merely a set of highlights; for all the details, check out the following:</p>\r\n<ul>\r\n<li aria-level=\"1\"><a href=\"https://developer.mozilla.org/docs/Mozilla/Firefox/Releases/93\">Firefox 93 for developers on MDN</a></li>\r\n<li aria-level=\"1\"><a href=\"https://www.mozilla.org/en-US/firefox/93.0/releasenotes/\">Firefox 93 end-user release notes</a></li>\r\n</ul>\r\n<h2><b>AVIF Image Support</b></h2>\r\n<p>The AV1 Image File Format (AVIF) is a powerful, open source, royalty-free file format. AVIF has the potential to become the &#8220;next big thing&#8221; for sharing images in web content. It offers state-of-the-art features and performance, without the encumbrance of complicated licensing and patent royalties that have hampered comparable alternatives.</p>\r\n<p>It offers much better lossless compression compared to PNG or JPEG formats, with support for higher color depths and transparency. As support is not yet comprehensive, you should include fallbacks to formats with better browser support (i.e. using the <code>&lt;picture&gt;</code> element).</p>\r\n<p>Read more about the AVIF image format in the<a href=\"https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Image_types#avif_image\"> Image file type and format guide on MDN</a>.</p>\r\n<h2><b>Static initialization blocks</b></h2>\r\n<p>Support for static initialization blocks in JavaScript classes is now available in Firefox 93. This enables more flexibility as it allows developers to run blocks of code when initializing static fields. This is handy if you want to set multiple fields from a single value or evaluate statements.</p>\r\n<p>You can have multiple static blocks within a class and they come with their own scope. As they are declared within a class, they have access to a class&#8217;s private fields. You can find more information about<a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes/Class_static_initialization_blocks\"> static initialization blocks on MDN</a>.</p>\r\n<h2><b>Custom Elements &amp; </b><b>Shadow DOM</b><b></b></h2>\r\n<p>In Firefox 92 the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLSlotElement/assign\">Imperative Slotting API</a> was implemented giving developers more control over assigning slots within a custom element. Firefox 93 included support for the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLSlotElement/slotchange_event\"><code>slotchange</code></a> event that fires when the nodes within a slot change.</p>\r\n<p>Also implemented in Firefox 93 is the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/attachInternals\"><code>HTMLElement.attachInternals()</code></a> method. This returns an instance of <code>ElementInternals</code>, allowing control over an HTML element&#8217;s internal features. The <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ElementInternals/shadowRoot\"><code>ElementInternals.shadowRoot</code></a> property was also added, meaning developers can gain access to the shadow root of elements, even if they themselves didn&#8217;t create the element.</p>\r\n<p>If you want to learn more about Custom Elements and the Shadow DOM, check out<a href=\"https://developer.mozilla.org/en-US/docs/Web/Web_Components/Using_custom_elements\"> MDN&#8217;s guides on the topics</a>.</p>\r\n<h2><b>Other highlights</b></h2>\r\n<p>A few other features worth noting include:</p>\r\n<ul>\r\n<li aria-level=\"1\">The <a href=\"https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Roles/meter_role\">ARIA meter role</a> has been implemented.</li>\r\n<li aria-level=\"1\">The UI for <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/datetime-local\"><code>&lt;input type=\"datetime-local\"&gt;</code></a> has been added.</li>\r\n<li aria-level=\"1\">In CSS, the <code>small-caps</code> keyword is is now supported for the <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/font-synthesis\"><code>font-synthesis</code></a> property.</li>\r\n<li aria-level=\"1\">The <code>options</code> object parameter for the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/createImageBitmap\"><code>createImageBitmap()</code></a> method now supports <code>imageOrientation</code> and <code>premultiplyAlpha</code> properties.</li>\r\n</ul>\r\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/10/lots-to-see-in-firefox-93/\">Lots to see in Firefox 93!</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Firefox 93 comes with lots of lovely updates including AVIF image format support, filling of XFA-based forms in its PDF viewer and protection against insecure downloads by blocking downloads relying on insecure connections.\nWeb developers are now able to use static initialization blocks within JavaScript classes, and there are some Shadow DOM and Custom Elements updates. The SHA-256 algorithm is now supported for HTTP Authentication using digests. This allows much more secure authentication than previously available using the MD5 algorithm.\nThis blog post provides merely a set of highlights; for all the details, check out the following:\n\nFirefox 93 for developers on MDN\nFirefox 93 end-user release notes\n\nAVIF Image Support\nThe AV1 Image File Format (AVIF) is a powerful, open source, royalty-free file format. AVIF has the potential to become the “next big thing” for sharing images in web content. It offers state-of-the-art features and performance, without the encumbrance of complicated licensing and patent royalties that have hampered comparable alternatives.\nIt offers much better lossless compression compared to PNG or JPEG formats, with support for higher color depths and transparency. As support is not yet comprehensive, you should include fallbacks to formats with better browser support (i.e. using the <picture> element).\nRead more about the AVIF image format in the Image file type and format guide on MDN.\nStatic initialization blocks\nSupport for static initialization blocks in JavaScript classes is now available in Firefox 93. This enables more flexibility as it allows developers to run blocks of code when initializing static fields. This is handy if you want to set multiple fields from a single value or evaluate statements.\nYou can have multiple static blocks within a class and they come with their own scope. As they are declared within a class, they have access to a class’s private fields. You can find more information about static initialization blocks on MDN.\nCustom Elements & Shadow DOM\nIn Firefox 92 the Imperative Slotting API was implemented giving developers more control over assigning slots within a custom element. Firefox 93 included support for the slotchange event that fires when the nodes within a slot change.\nAlso implemented in Firefox 93 is the HTMLElement.attachInternals() method. This returns an instance of ElementInternals, allowing control over an HTML element’s internal features. The ElementInternals.shadowRoot property was also added, meaning developers can gain access to the shadow root of elements, even if they themselves didn’t create the element.\nIf you want to learn more about Custom Elements and the Shadow DOM, check out MDN’s guides on the topics.\nOther highlights\nA few other features worth noting include:\n\nThe ARIA meter role has been implemented.\nThe UI for <input type=\"datetime-local\"> has been added.\nIn CSS, the small-caps keyword is is now supported for the font-synthesis property.\nThe options object parameter for the createImageBitmap() method now supports imageOrientation and premultiplyAlpha properties.\n\nThe post Lots to see in Firefox 93! appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-10-08T16:45:37.000Z",
      "date_modified": "2021-10-08T16:45:37.000Z",
      "_plugin": {
        "pageFilename": "ee947b180a91c068c2f6752eb76d336afb18e0b5da98adda46c5da55996e77bd.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47372",
      "url": "https://hacks.mozilla.org/2021/10/implementing-form-filling-and-accessibility-in-the-firefox-pdf-viewer/",
      "title": "Implementing form filling and accessibility in the Firefox PDF viewer",
      "summary": "Last year, during lockdown, many discovered the importance of PDF forms when having to deal remotely with administrations and large organizations like banks. Firefox supported displaying PDF forms, but it didn’t support filling them: users had to print them, fill them by hand, and scan them back to digital form. We decided it was time to reinvest in the PDF viewer (PDF.js) and support filling PDF forms within Firefox to make our users' lives easier.\nThe post Implementing form filling and accessibility in the Firefox PDF viewer appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<h2 class=\"c1\"><span class=\"c8\">Intro</span></h2>\n<p class=\"c1\"><span class=\"c0\">Last year, during lockdown, many discovered the importance of PDF forms when having to deal remotely with administrations and large organizations like banks. Firefox supported displaying PDF forms, but it didn’t support filling them: users had to print them, fill them by hand, and scan them back to digital form. </span><span class=\"c0\">We decided it was time to reinvest in the PDF viewer (PDF.js) and support filling PDF forms within Firefox to make our users&#8217; lives easier.</span></p>\n<p class=\"c1\"><span class=\"c0\">While we invested more time in the PDF viewer, we also went through the backlog of work and prioritized improving the accessibility of </span><span class=\"c0\">our PDF reader for users of assistive technologies. Below we&#8217;ll describe how we implemented the form support, improved accessibility, and made sure we had no regressions along the way.<br />\n</span></p>\n<h2 class=\"c1\"><span class=\"c8\">Brief Summary of the PDF.js Architecture</span></h2>\n<p class=\"c1\"><span class=\"c0\"><a href=\"https://hacks.mozilla.org/files/2021/09/pdfjs_architecture.png\"><img loading=\"lazy\" class=\"alignnone wp-image-47373 size-large\" src=\"https://hacks.mozilla.org/files/2021/09/pdfjs_architecture-500x245.png\" alt=\"Overview of the PDF.js Architecture\" width=\"500\" height=\"245\" srcset=\"https://hacks.mozilla.org/files/2021/09/pdfjs_architecture-500x245.png 500w, https://hacks.mozilla.org/files/2021/09/pdfjs_architecture-250x122.png 250w, https://hacks.mozilla.org/files/2021/09/pdfjs_architecture-768x376.png 768w, https://hacks.mozilla.org/files/2021/09/pdfjs_architecture-1536x752.png 1536w, https://hacks.mozilla.org/files/2021/09/pdfjs_architecture.png 1999w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a>To understand how we added support for forms and tagged PDFs, it’s first important to understand some basics about how the PDF viewer (PDF.js) works in Firefox.</span></p>\n<p class=\"c1\"><span class=\"c0\">First, PDF.js will fetch and parse the document in a web worker. The parsed document will then generate drawing instructions. PDF.js sends them to the main thread and draws them on an HTML5 canvas element.<br />\n</span></p>\n<p class=\"c1\">Besides the canvas, PDF.js potentially creates three more layers that are displayed on top of it. The first layer, the <em><strong>text layer</strong>,</em> enables text selection and search<span class=\"c0\">. It contains span elements that are transparent and line up with the text drawn below them on the canvas. The other two layers are the <strong><em>Annotation/AcroForm layer</em></strong> and the <strong><em>XFA form layer</em></strong>. They support form filling and we will describe them in more detail below.</span></p>\n<h2 class=\"c1\"><span class=\"c8\">Filling Forms (AcroForms)</span></h2>\n<p class=\"c1\"><span class=\"c0\">AcroForms are one of two types of forms that PDF supports, the most common type of form.</span></p>\n<h3>AcroForm structure</h3>\n<p class=\"c1\"><span class=\"c0\">Within a PDF file, the form elements are stored in the annotation data. Annotations in PDF are separate elements from the main content of a document. They are often used for things like taking notes on a document or drawing on top of a document. AcroForm annotation elements support user input similar to HTML input e.g. text, check boxes, radio buttons.</span></p>\n<h3>AcroForm implementation</h3>\n<p class=\"c1\"><span class=\"c0\">In PDF.js, we parse a PDF file and create the annotations in a web worker. Then, we send them out from the worker and render them in the main process using HTML elements inserted in a div (annotation layer). We render this annotation layer, composed of HTML elements, on top of the canvas layer.</span></p>\n<p class=\"c1\"><span class=\"c0\">The annotation layer works well for displaying the form elements in the browser, but it was not compatible with the way PDF.js supports printing. When printing a PDF, we draw its contents on a special printing canvas, insert it into the current document and send it to the printer. To support printing form elements with user input, we needed to draw them on the canvas.</span></p>\n<p class=\"c1\">By inspecting (with the help of the <span class=\"c4\"><a class=\"c6\" href=\"https://github.com/qpdf/qpdf\">qpdf</a></span> tool) the raw PDF data of forms saved using other tools, we discovered that we needed to save the appearance of a filled field by using some PDF <span class=\"c0\">drawing instructions, and that we could support both saving and printing with a common implementation.</span></p>\n<p class=\"c1\">To generate the field appearance, we needed to get the values entered by the user. We introduced an object called annotationStorage to store those values by using callback functions in the corresponding HTML elements. The annotationStorage is then passed to the worker when saving or printing<span class=\"c0\">, and the values for each annotation are used to create an appearance.</span></p>\n<p class=\"c1\"><img loading=\"lazy\" class=\"alignnone size-large wp-image-47377\" src=\"https://hacks.mozilla.org/files/2021/09/1040-500x313.png\" alt=\"Example PDF.js Form Rendering\" width=\"500\" height=\"313\" srcset=\"https://hacks.mozilla.org/files/2021/09/1040-500x313.png 500w, https://hacks.mozilla.org/files/2021/09/1040-250x157.png 250w, https://hacks.mozilla.org/files/2021/09/1040-768x481.png 768w, https://hacks.mozilla.org/files/2021/09/1040.png 1023w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p class=\"c1\"><span class=\"c0\">On top a filled form in Firefox and on bottom the printed PDF opened in Evince.</span></p>\n<h2 class=\"c1\"><span class=\"c8\">Safely Executing JavaScript within PDFs<br />\n</span></h2>\n<p class=\"c1\">Thanks to our Telemetry, we discovered that many forms contain and use embedded JavaScript code (yes, that&#8217;s a thing!).</p>\n<p class=\"c1\">JavaScript in PDFs can be used for many things, but is most commonly used to validate data entered by the user or automatically calculate formulas. For example, in <span class=\"c4\"><a class=\"c6\" href=\"https://impotsdirects.public.lu/dam-assets/fr/formulaires/fiches_d_impot/2020/160F-2020.pdf\">this PDF</a></span><span class=\"c0\">, tax calculations are performed automatically starting from user input. Since this feature is common and helpful to users, we set out to implement it in PDF.js.</span></p>\n<h3>The alternatives</h3>\n<p class=\"c1\"><span class=\"c0\">From the start of our JavaScript implementation, our main concern was security. We did not want PDF files to become a new vector for attacks. Embedded JS code must be executed when a PDF is loaded or on events generated by form elements (focus, input, …).</span></p>\n<p class=\"c1\">We investigated using the following<span class=\"c8\">:</span></p>\n<ol class=\"c5 lst-kix_n33gp6i2lgcq-0 start\" start=\"1\">\n<li class=\"c1 c7 li-bullet-0\">JS <span class=\"c9\">eval </span><span class=\"c0\">function</span></li>\n<li class=\"c1 c7 li-bullet-0\">JS engine compiled in <span class=\"c4\"><a class=\"c6\" href=\"https://developer.mozilla.org/en-US/docs/WebAssembly\">WebAssembly</a></span> with <span class=\"c4\"><a class=\"c6\" href=\"https://github.com/emscripten-core/emscripten\">emscripten</a></span></li>\n<li class=\"c1 c7 li-bullet-0\">Firefox JS engine <span class=\"c4\"><a class=\"c6\" href=\"https://web.archive.org/web/20210506210158/https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XPCOM/Language_Bindings/Components.utils.Sandbox\">ComponentUtils.Sandbox</a></span></li>\n</ol>\n<p class=\"c1\">The first option, while simple, was immediately discarded since running untrusted code in <span class=\"c9\">eval</span> is very unsafe.</p>\n<p class=\"c1\">Option two, using a JS engine compiled with WebAssembly, was a strong contender since it would work with the built-in Firefox PDF viewer and the version of PDF.js that can be used in regular websites. However, it would have been a large new attack surface to audit. It would have also considerably increased the size of PDF.js and it would have been slower.</p>\n<p class=\"c1\">The third option, sandboxes, is a feature exposed to privileged code in Firefox that allows <em><strong>JS execution in a special isolated environment</strong></em>. The sandbox is created with a <span class=\"c4\"><a class=\"c6\" href=\"https://web.archive.org/web/20210531054716/https://developer.mozilla.org/en-US/docs/Mozilla/Gecko/Script_security\">null principal</a></span><span class=\"c0\">, which means that everything within the sandbox can only be accessed by it and can only access other things within the sandbox itself (and by privileged Firefox code).</span></p>\n<h3>Our final choice</h3>\n<p class=\"c1\">We settled on using a ComponentUtils.Sandbox for the Firefox built-in viewer. ComponentUtils.Sandbox has been used for years now in WebExtensions, so this implementation is battle tested and very safe: executing a script from a PDF is at least as safe as executing one from a normal web page.</p>\n<p class=\"c1\">For the generic web viewer (where we can only use standard web APIs, so we know nothing about ComponentUtils.Sandbox) and the <span class=\"c4\"><a class=\"c6\" href=\"https://github.com/mozilla/pdf.js\">pdf.js</a></span> test suite we used a WebAssembly version of <span class=\"c4\"><a class=\"c6\" href=\"https://github.com/bellard/quickjs\">QuickJS</a></span> (see <span class=\"c4\"><a class=\"c6\" href=\"https://github.com/mozilla/pdf.js.quickjs\">pdf.js.quickjs</a></span> for details)<span class=\"c0\">.</span></p>\n<p class=\"c1\">The <span class=\"c4\"><a class=\"c6\" href=\"https://searchfox.org/mozilla-central/source/toolkit/components/pdfjs/content/PdfSandbox.jsm\">implementation</a></span><span class=\"c0\"> of the PDF sandbox in Firefox works as follows:</span></p>\n<ul class=\"c5 lst-kix_663mt1xlbe3z-0 start\">\n<li class=\"c1 c7 li-bullet-0\"><span class=\"c0\">We collect all the fields and their properties (including the JS actions associated with them) and then clone them into the sandbox;</span></li>\n<li class=\"c1 c7 li-bullet-0\">At build time, we generate a bundle with the <span class=\"c4\"><a class=\"c6\" href=\"https://github.com/mozilla/pdf.js/tree/master/src/scripting_api\">JS code</a></span><span class=\"c0\"> to implement the PDF JS API (totally different from the web API we are accustomed to!). We load it in the sandbox and then execute it with the data collected during the first step;</span></li>\n<li class=\"c1 c7 li-bullet-0\">In the<span class=\"c0\"> HTML representation of the fields we added callbacks to handle the events (focus, input, …). The callbacks simply dispatch them into the sandbox through an object containing the field identifier and linked parameters. We execute the corresponding JS actions in the sandbox using eval (it’s safe in this case: we’re in a sandbox). Then, we clone the result and dispatch it outside the sandbox to update the states in the HTML representations of the fields.</span></li>\n</ul>\n<p class=\"c1\"><span class=\"c0\">We decided not to implement the PDF APIs related to I/O (network, disk, …) to avoid any security concerns.</span></p>\n<h2 class=\"c1\"><span class=\"c9\">Yet Another Form Format: XFA</span></h2>\n<p class=\"c1\">Our Telemetry also informed us that another type of PDF forms, <span class=\"c4\"><a class=\"c6\" href=\"https://en.wikipedia.org/wiki/XFA\">XFA</a></span><span class=\"c0\">, was fairly common. This format has been removed from the official PDF specification, but many PDFs with XFA still exist and are viewed by our users so we decided to implement it as well.</span></p>\n<h3>The XFA format</h3>\n<p class=\"c1\"><span class=\"c0\">The XFA format is very different from what is usually in PDF files. A normal PDF is typically a list of drawing commands with all layout statically defined by the PDF generator. However, XFA is much closer to HTML and has a more dynamic layout that the PDF viewer must generate. In reality XFA is a totally different format that was bolted on to PDF.</span></p>\n<p class=\"c1\">The XFA entry in a PDF contains multiple <span class=\"c4\"><a class=\"c6\" href=\"https://en.wikipedia.org/wiki/XML\">XML</a></span> streams: the most important being the template and datasets. The <span class=\"c9\">template</span> XML contains all the information required to render the form: it contains the UI elements (e.g. text fields, checkboxes, …) and containers (subform, draw, …) which can have static or dynamic layouts. The <span class=\"c9\">datasets</span><span class=\"c0\"> XML contains all the data used by the form itself (e.g. text field content, checkbox state, …). All these data are bound into the template (before layout) to set the values of the different UI elements.<br />\n</span></p>\n<h4>Example Template</h4>\n<pre class=\"xml\"><code>&lt;template xmlns=\"http://www.xfa.org/schema/xfa-template/3.6/\"&gt;\n  &lt;subform&gt;\n    &lt;pageSet name=\"ps\"&gt;\n      &lt;pageArea name=\"page1\" id=\"Page1\"&gt;\n        &lt;contentArea x=\"7.62mm\" y=\"30.48mm\" w=\"200.66mm\" h=\"226.06mm\"/&gt;\n        &lt;medium stock=\"default\" short=\"215.9mm\" long=\"279.4mm\"/&gt;\n      &lt;/pageArea&gt;\n    &lt;/pageSet&gt;\n    &lt;subform&gt;\n      &lt;draw name=\"Text1\" y=\"10mm\" x=\"50mm\" w=\"200mm\" h=\"7mm\"&gt;\n        &lt;font size=\"15pt\" typeface=\"Helvetica\"/&gt;\n        &lt;value&gt;\n          &lt;text&gt;Hello XFA &amp; PDF.js world !&lt;/text&gt;\n        &lt;/value&gt;\n      &lt;/ draw&gt;\n    &lt;/subform&gt;\n  &lt;/subform&gt;\n&lt;/template&gt;</code></pre>\n<h4>Output From Template</h4>\n<p><img loading=\"lazy\" class=\"alignnone size-large wp-image-47392\" src=\"https://hacks.mozilla.org/files/2021/09/xfa_rendering-500x195.png\" alt=\"Rendering of XFA Document\" width=\"500\" height=\"195\" srcset=\"https://hacks.mozilla.org/files/2021/09/xfa_rendering-500x195.png 500w, https://hacks.mozilla.org/files/2021/09/xfa_rendering-250x97.png 250w, https://hacks.mozilla.org/files/2021/09/xfa_rendering-768x299.png 768w, https://hacks.mozilla.org/files/2021/09/xfa_rendering.png 1172w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<h3>The XFA implementation</h3>\n<p class=\"c1\">In PDF.js we already had a pretty good XML parser to retrieve metadata about PDFs: it was a good start.</p>\n<p class=\"c1\">We decided to map every XML node to a JavaScript object, whose structure is used to validate the node (e.g. possible children and their different numbers). Once the XML is parsed and validated, the form data needs to be bound in the form template and some prototypes can be used with the help of <span class=\"c4\"><a class=\"c6\" href=\"https://www.w3.org/1999/05/XFA/xfa-template.html#prose-som\">SOM</a></span> expressions (kind of <span class=\"c4\"><a class=\"c6\" href=\"https://en.wikipedia.org/wiki/XPath\">XPath</a></span><span class=\"c0\"> expressions).</span></p>\n<h4>The layout engine</h4>\n<p class=\"c1\">In XFA, we can have different kinds of layouts and the final layout depends on the contents. We initially planned to piggyback on the Firefox layout engine, but we discovered that unfortunately we would need to lay everything out ourselves because XFA uses some layout features which don’t exist in Firefox<span class=\"c0\">. For example, when a container is overflowing the extra contents can be put in another container (often on a new page, but sometimes also in another subform).  Moreover, some template elements don’t have any dimensions, which must be inferred based on their contents.</span></p>\n<p class=\"c1\"><span class=\"c0\">In the end <em>we implemented a custom layout engine</em>: we traverse the template tree from top to bottom and, following layout rules, check if an element fits into the available space. If it doesn’t, we flush all the elements layed out so far into the current content area, and we move to the next one.</span></p>\n<p>During layout, we convert all the XML elements into JavaScript objects with a tree structure. Then, we send them to the main process to be converted into HTML elements and placed in the XFA layer.</p>\n<h4>The missing font problem</h4>\n<p class=\"c1\">As mentioned above, the dimensions of some elements are not specified. We must compute them ourselves based on the font used in them. This is even more challenging because sometimes fonts are not embedded in the PDF file.</p>\n<p class=\"c1\">Not embedding fonts in a PDF is considered bad practice, but in reality many PDFs do not include some well-known fonts (e.g. the ones shipped by Acrobat or Windows: Arial, Calibri, &#8230;) as PDF creators simply expected them to be always available.</p>\n<p class=\"c1\">To have our output more closely match Adobe Acrobat, we decided to ship the <span class=\"c4\"><a class=\"c6\" href=\"https://en.wikipedia.org/wiki/Liberation_fonts\">Liberation</a></span><span class=\"c0\"> fonts and glyph widths of well-known fonts. We used the widths to rescale the glyph drawing to have compatible font substitutions for all the well-known fonts.</span></p>\n<p><a href=\"https://hacks.mozilla.org/files/2021/10/glyph_rescale.png\"><img loading=\"lazy\" class=\"alignnone wp-image-47403 size-large\" src=\"https://hacks.mozilla.org/files/2021/10/glyph_rescale-500x214.png\" alt=\"Comparing glyph rescaling\" width=\"500\" height=\"214\" srcset=\"https://hacks.mozilla.org/files/2021/10/glyph_rescale-500x214.png 500w, https://hacks.mozilla.org/files/2021/10/glyph_rescale-250x107.png 250w, https://hacks.mozilla.org/files/2021/10/glyph_rescale-768x329.png 768w, https://hacks.mozilla.org/files/2021/10/glyph_rescale-1536x659.png 1536w, https://hacks.mozilla.org/files/2021/10/glyph_rescale.png 1602w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a></p>\n<p>On the left: default font without glyph rescaling. On the right: Liberation font with glyph rescaling to emulate <span class=\"mx_EventTile_body\" dir=\"auto\">MyriadPro</span>.</p>\n<h4>The result</h4>\n<p class=\"c1\">In the end the result turned out quite good, for example, you can now open PDFs such as <span class=\"c4\"><a class=\"c6\" href=\"https://inspection.canada.ca/DAM/DAM-food-aliments/STAGING/text-texte/c5704_re_1357758804123_eng.pdf\">5704 &#8211; APPLICATION FOR A FISH EXPORT LICENCE</a></span> in Firefox 93!</p>\n<h2 class=\"c1\"><span class=\"c8\">Making PDFs accessible</span></h2>\n<h3 class=\"c1\"><span class=\"c9\">What is a Tagged PDF?<br />\n</span></h3>\n<p class=\"c1\"><span class=\"c0\">Early versions of PDFs were not a friendly format for accessibility tools such as screen readers. This was mainly because within a document, all text on a page is more or less absolutely positioned and there’s not a notion of a logical structure such as paragraphs, headings or sentences. There was also no way to provide a text description of images or figures. For example, some pseudo code for how a PDF may draw text:</span></p>\n<pre class=\"js\"><code>showText(“This”, 0 /*x*/, 60 /*y*/);\nshowText(“is”, 0, 40);\nshowText(“a”, 0, 20);\nshowText(“Heading!”, 0, 0);</code></pre>\n<p class=\"c1\"><span class=\"c0\">This would draw text as four separate lines, but a screen reader would have no idea that they were all part of one heading. To help with accessibility, later versions of the PDF specification introduced “Tagged PDF.” This allowed PDFs to create a logical structure that screen readers could then use. One can think of this as a similar concept to an HTML hierarchy of DOM nodes. Using the example above, one could add tags:</span></p>\n<pre class=\"js\"><code>beginTag(“heading 1”);\nshowText(“This”, 0 /*x*/, 60 /*y*/);\nshowText(“is”, 0, 40);\nshowText(“a”, 0, 20);\nshowText(“Heading!”, 0, 0);\nendTag(“heading 1”);</code></pre>\n<p class=\"c1\"><span class=\"c0\">With the extra tag information, a screen reader knows that all of the lines are part of “heading 1” and can read it in a more natural fashion. The structure also allows screen readers to easily navigate to different parts of the document.</span></p>\n<p class=\"c1\"><span class=\"c0\">The above example is only about text, but tagged PDFs support many more features than this e.g. alt text for images, table data, lists, etc.</span></p>\n<h3 class=\"c1\"><span class=\"c9\">How we supported Tagged PDFs in PDF.js</span></h3>\n<p class=\"c1\">For tagged PDFs we leveraged the existing “text layer” and the browsers built in <span class=\"c4\"><a class=\"c6\" href=\"https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA\">HTML ARIA accessibility features</a></span><span class=\"c0\">. We can easily see this by a simple PDF example with one heading and one paragraph. First, we generate the logical structure and insert it into the canvas:</span></p>\n<pre class=\"html\"><code>&lt;canvas id=\"page1\"&gt;\n  &lt;!-- This content is not visible, \n  but available to screen readers   --&gt;\n  &lt;span role=\"heading\" aria-level=\"1\" aria-owns=\"heading_id\"&gt;&lt;/span&gt;\n  &lt;span aria_owns=\"some_paragraph\"&gt;&lt;/span&gt;\n&lt;/canvas&gt;</code></pre>\n<p>In the text layer that overlays the canvas:</p>\n<pre class=\"html\"><code>&lt;div id=\"text_layer\"&gt;\n  &lt;span id=\"heading_id\"&gt;Some Heading&lt;/span&gt;\n  &lt;span id=\"some_paragaph\"&gt;Hello world!&lt;/span&gt;\n&lt;/div&gt;</code></pre>\n<p class=\"c1\"><span class=\"c0\">A screen reader would then walk the DOM accessibility tree in the canvas and use the `aria-owns` attributes to find the text content for each node. For the above example, a screen reader would announce:</span></p>\n<p><code>Heading Level 1 Some Heading<br />\nHello World!</code></p>\n<p class=\"c1\"><span class=\"c0\">For those not familiar with screen readers, having this extra structure also makes navigating around the PDF much easier: you can jump from heading to heading and read paragraphs without unneeded pauses.</span></p>\n<h2 class=\"c1\"><span class=\"c8\">Ensure there are no regressions at scale, meet reftests</span></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47385 size-large\" src=\"https://hacks.mozilla.org/files/2021/09/reftest_analyzer.pn_-500x252.png\" alt=\"Reference Test Analyzer\" width=\"500\" height=\"252\" srcset=\"https://hacks.mozilla.org/files/2021/09/reftest_analyzer.pn_-500x252.png 500w, https://hacks.mozilla.org/files/2021/09/reftest_analyzer.pn_-250x126.png 250w, https://hacks.mozilla.org/files/2021/09/reftest_analyzer.pn_-768x388.png 768w, https://hacks.mozilla.org/files/2021/09/reftest_analyzer.pn_-1536x775.png 1536w, https://hacks.mozilla.org/files/2021/09/reftest_analyzer.pn_-2048x1034.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<h3>Crawling for PDFs</h3>\n<p class=\"c1\"><span class=\"c0\">Over the past few months, we have built a web crawler to retrieve PDFs from the web and, using a set of heuristics, collect statistics about them (e.g. are they XFA? What fonts are they using? What formats of images do they include?).</span></p>\n<p class=\"c1\">We have also used the crawler with its heuristics to retrieve PDFs of interest from the <span class=\"c4\"><a class=\"c6\" href=\"https://www.pdfa.org/a-new-stressful-pdf-corpus/\">“stressful PDF corpus” published by the PDF association</a></span><span class=\"c0\">, which proved particularly interesting as they contained many corner cases we did not think could exist.</span></p>\n<p class=\"c1\">With the crawler, we were able to build a large corpus of Tagged PDFs (around 32000), PDFs using JS (around 1900), XFA PDFs (around 1200) which we could use for manual and automated testing. Kudos to our QA team for going through so many PDFs! They now know everything about asking for a fishing license in Canada, life skills!</p>\n<h3>Reftests for the win</h3>\n<p class=\"c1\"><span class=\"c0\">We did not only use the corpus for manual QA, but also added some of those PDFs to our list of reftests (reference tests).</span></p>\n<p class=\"c1\"><span class=\"c0\">A <em><strong>reftest</strong></em> is a test consisting of a test file and a reference file. The test file uses the pdf.js rendering engine, while the reference file doesn’t (to make sure it is consistent and can’t be affected by changes in the patch the test is validating). The reference file is simply a screenshot of the rendering of a given PDF from the “master” branch of pdf.js.</span></p>\n<h4>The reftest process</h4>\n<p class=\"c1\"><span class=\"c0\">When a developer submits a change to the PDF.js repo, we run the reftests and ensure the rendering of the test file is exactly the same as the reference screenshot. If there are differences, we ensure that the differences are improvements rather than regressions.</span></p>\n<p class=\"c1\"><span class=\"c0\">After accepting and merging a change, we regenerate the references.</span></p>\n<h4>The reftest shortcomings</h4>\n<p class=\"c1\"><span class=\"c0\">In some situations a test may have subtle differences in rendering compared to the reference due to, e.g., anti-aliasing. This introduces noise in the results, with “fake” regressions the developer and reviewer have to sift through. Sometimes, it is possible to miss real regressions because of the large number of differences to look at.</span></p>\n<p class=\"c1\"><span class=\"c0\">Another shortcoming of reftests is that they are often big. A regression in a reftest is not as easy to investigate as a failure of a unit test.</span></p>\n<p class=\"c1\"><span class=\"c0\">Despite these shortcomings, <strong><em>reftests are a very powerful regression prevention weapon in the pdf.js arsenal</em></strong>. The large number of reftests we have boosts our confidence when applying changes.</span></p>\n<h2 class=\"c1\"><span class=\"c8\">Conclusion<br />\n</span></h2>\n<p class=\"c1\">Support for AcroForms landed in Firefox v84. JavaScript execution in v88. Tagged PDFs in v89. XFA forms in v93 (tomorrow, October 5th, 2021!).</p>\n<p class=\"c1\">While all of these features have greatly improved form usability and accessibility, there are still more features we’d like to add. If you’re interested in helping, we’re always looking for more contributors and you can join us on <a href=\"https://chat.mozilla.org/#/room/#pdfjs:mozilla.org\">element</a> or <a href=\"https://github.com/mozilla/pdf.js\">github</a>.</p>\n<p>We also want to say a big thanks to two of our contributors <a href=\"https://github.com/Snuffleupagus\">Jonas Jenwald</a> and <a href=\"https://github.com/timvandermeij\">Tim van der Meij</a> for their on going help with the above projects.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/10/implementing-form-filling-and-accessibility-in-the-firefox-pdf-viewer/\">Implementing form filling and accessibility in the Firefox PDF viewer</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Intro\nLast year, during lockdown, many discovered the importance of PDF forms when having to deal remotely with administrations and large organizations like banks. Firefox supported displaying PDF forms, but it didn’t support filling them: users had to print them, fill them by hand, and scan them back to digital form. We decided it was time to reinvest in the PDF viewer (PDF.js) and support filling PDF forms within Firefox to make our users’ lives easier.\nWhile we invested more time in the PDF viewer, we also went through the backlog of work and prioritized improving the accessibility of our PDF reader for users of assistive technologies. Below we’ll describe how we implemented the form support, improved accessibility, and made sure we had no regressions along the way.\n\nBrief Summary of the PDF.js Architecture\nTo understand how we added support for forms and tagged PDFs, it’s first important to understand some basics about how the PDF viewer (PDF.js) works in Firefox.\nFirst, PDF.js will fetch and parse the document in a web worker. The parsed document will then generate drawing instructions. PDF.js sends them to the main thread and draws them on an HTML5 canvas element.\n\nBesides the canvas, PDF.js potentially creates three more layers that are displayed on top of it. The first layer, the text layer, enables text selection and search. It contains span elements that are transparent and line up with the text drawn below them on the canvas. The other two layers are the Annotation/AcroForm layer and the XFA form layer. They support form filling and we will describe them in more detail below.\nFilling Forms (AcroForms)\nAcroForms are one of two types of forms that PDF supports, the most common type of form.\nAcroForm structure\nWithin a PDF file, the form elements are stored in the annotation data. Annotations in PDF are separate elements from the main content of a document. They are often used for things like taking notes on a document or drawing on top of a document. AcroForm annotation elements support user input similar to HTML input e.g. text, check boxes, radio buttons.\nAcroForm implementation\nIn PDF.js, we parse a PDF file and create the annotations in a web worker. Then, we send them out from the worker and render them in the main process using HTML elements inserted in a div (annotation layer). We render this annotation layer, composed of HTML elements, on top of the canvas layer.\nThe annotation layer works well for displaying the form elements in the browser, but it was not compatible with the way PDF.js supports printing. When printing a PDF, we draw its contents on a special printing canvas, insert it into the current document and send it to the printer. To support printing form elements with user input, we needed to draw them on the canvas.\nBy inspecting (with the help of the qpdf tool) the raw PDF data of forms saved using other tools, we discovered that we needed to save the appearance of a filled field by using some PDF drawing instructions, and that we could support both saving and printing with a common implementation.\nTo generate the field appearance, we needed to get the values entered by the user. We introduced an object called annotationStorage to store those values by using callback functions in the corresponding HTML elements. The annotationStorage is then passed to the worker when saving or printing, and the values for each annotation are used to create an appearance.\n\nOn top a filled form in Firefox and on bottom the printed PDF opened in Evince.\nSafely Executing JavaScript within PDFs\n\nThanks to our Telemetry, we discovered that many forms contain and use embedded JavaScript code (yes, that’s a thing!).\nJavaScript in PDFs can be used for many things, but is most commonly used to validate data entered by the user or automatically calculate formulas. For example, in this PDF, tax calculations are performed automatically starting from user input. Since this feature is common and helpful to users, we set out to implement it in PDF.js.\nThe alternatives\nFrom the start of our JavaScript implementation, our main concern was security. We did not want PDF files to become a new vector for attacks. Embedded JS code must be executed when a PDF is loaded or on events generated by form elements (focus, input, …).\nWe investigated using the following:\n\nJS eval function\nJS engine compiled in WebAssembly with emscripten\nFirefox JS engine ComponentUtils.Sandbox\n\nThe first option, while simple, was immediately discarded since running untrusted code in eval is very unsafe.\nOption two, using a JS engine compiled with WebAssembly, was a strong contender since it would work with the built-in Firefox PDF viewer and the version of PDF.js that can be used in regular websites. However, it would have been a large new attack surface to audit. It would have also considerably increased the size of PDF.js and it would have been slower.\nThe third option, sandboxes, is a feature exposed to privileged code in Firefox that allows JS execution in a special isolated environment. The sandbox is created with a null principal, which means that everything within the sandbox can only be accessed by it and can only access other things within the sandbox itself (and by privileged Firefox code).\nOur final choice\nWe settled on using a ComponentUtils.Sandbox for the Firefox built-in viewer. ComponentUtils.Sandbox has been used for years now in WebExtensions, so this implementation is battle tested and very safe: executing a script from a PDF is at least as safe as executing one from a normal web page.\nFor the generic web viewer (where we can only use standard web APIs, so we know nothing about ComponentUtils.Sandbox) and the pdf.js test suite we used a WebAssembly version of QuickJS (see pdf.js.quickjs for details).\nThe implementation of the PDF sandbox in Firefox works as follows:\n\nWe collect all the fields and their properties (including the JS actions associated with them) and then clone them into the sandbox;\nAt build time, we generate a bundle with the JS code to implement the PDF JS API (totally different from the web API we are accustomed to!). We load it in the sandbox and then execute it with the data collected during the first step;\nIn the HTML representation of the fields we added callbacks to handle the events (focus, input, …). The callbacks simply dispatch them into the sandbox through an object containing the field identifier and linked parameters. We execute the corresponding JS actions in the sandbox using eval (it’s safe in this case: we’re in a sandbox). Then, we clone the result and dispatch it outside the sandbox to update the states in the HTML representations of the fields.\n\nWe decided not to implement the PDF APIs related to I/O (network, disk, …) to avoid any security concerns.\nYet Another Form Format: XFA\nOur Telemetry also informed us that another type of PDF forms, XFA, was fairly common. This format has been removed from the official PDF specification, but many PDFs with XFA still exist and are viewed by our users so we decided to implement it as well.\nThe XFA format\nThe XFA format is very different from what is usually in PDF files. A normal PDF is typically a list of drawing commands with all layout statically defined by the PDF generator. However, XFA is much closer to HTML and has a more dynamic layout that the PDF viewer must generate. In reality XFA is a totally different format that was bolted on to PDF.\nThe XFA entry in a PDF contains multiple XML streams: the most important being the template and datasets. The template XML contains all the information required to render the form: it contains the UI elements (e.g. text fields, checkboxes, …) and containers (subform, draw, …) which can have static or dynamic layouts. The datasets XML contains all the data used by the form itself (e.g. text field content, checkbox state, …). All these data are bound into the template (before layout) to set the values of the different UI elements.\n\nExample Template\n<template xmlns=\"http://www.xfa.org/schema/xfa-template/3.6/\">\n  <subform>\n    <pageSet name=\"ps\">\n      <pageArea name=\"page1\" id=\"Page1\">\n        <contentArea x=\"7.62mm\" y=\"30.48mm\" w=\"200.66mm\" h=\"226.06mm\"/>\n        <medium stock=\"default\" short=\"215.9mm\" long=\"279.4mm\"/>\n      </pageArea>\n    </pageSet>\n    <subform>\n      <draw name=\"Text1\" y=\"10mm\" x=\"50mm\" w=\"200mm\" h=\"7mm\">\n        <font size=\"15pt\" typeface=\"Helvetica\"/>\n        <value>\n          <text>Hello XFA & PDF.js world !</text>\n        </value>\n      </ draw>\n    </subform>\n  </subform>\n</template>\nOutput From Template\n\nThe XFA implementation\nIn PDF.js we already had a pretty good XML parser to retrieve metadata about PDFs: it was a good start.\nWe decided to map every XML node to a JavaScript object, whose structure is used to validate the node (e.g. possible children and their different numbers). Once the XML is parsed and validated, the form data needs to be bound in the form template and some prototypes can be used with the help of SOM expressions (kind of XPath expressions).\nThe layout engine\nIn XFA, we can have different kinds of layouts and the final layout depends on the contents. We initially planned to piggyback on the Firefox layout engine, but we discovered that unfortunately we would need to lay everything out ourselves because XFA uses some layout features which don’t exist in Firefox. For example, when a container is overflowing the extra contents can be put in another container (often on a new page, but sometimes also in another subform).  Moreover, some template elements don’t have any dimensions, which must be inferred based on their contents.\nIn the end we implemented a custom layout engine: we traverse the template tree from top to bottom and, following layout rules, check if an element fits into the available space. If it doesn’t, we flush all the elements layed out so far into the current content area, and we move to the next one.\nDuring layout, we convert all the XML elements into JavaScript objects with a tree structure. Then, we send them to the main process to be converted into HTML elements and placed in the XFA layer.\nThe missing font problem\nAs mentioned above, the dimensions of some elements are not specified. We must compute them ourselves based on the font used in them. This is even more challenging because sometimes fonts are not embedded in the PDF file.\nNot embedding fonts in a PDF is considered bad practice, but in reality many PDFs do not include some well-known fonts (e.g. the ones shipped by Acrobat or Windows: Arial, Calibri, …) as PDF creators simply expected them to be always available.\nTo have our output more closely match Adobe Acrobat, we decided to ship the Liberation fonts and glyph widths of well-known fonts. We used the widths to rescale the glyph drawing to have compatible font substitutions for all the well-known fonts.\n\nOn the left: default font without glyph rescaling. On the right: Liberation font with glyph rescaling to emulate MyriadPro.\nThe result\nIn the end the result turned out quite good, for example, you can now open PDFs such as 5704 – APPLICATION FOR A FISH EXPORT LICENCE in Firefox 93!\nMaking PDFs accessible\nWhat is a Tagged PDF?\n\nEarly versions of PDFs were not a friendly format for accessibility tools such as screen readers. This was mainly because within a document, all text on a page is more or less absolutely positioned and there’s not a notion of a logical structure such as paragraphs, headings or sentences. There was also no way to provide a text description of images or figures. For example, some pseudo code for how a PDF may draw text:\nshowText(“This”, 0 /*x*/, 60 /*y*/);\nshowText(“is”, 0, 40);\nshowText(“a”, 0, 20);\nshowText(“Heading!”, 0, 0);\nThis would draw text as four separate lines, but a screen reader would have no idea that they were all part of one heading. To help with accessibility, later versions of the PDF specification introduced “Tagged PDF.” This allowed PDFs to create a logical structure that screen readers could then use. One can think of this as a similar concept to an HTML hierarchy of DOM nodes. Using the example above, one could add tags:\nbeginTag(“heading 1”);\nshowText(“This”, 0 /*x*/, 60 /*y*/);\nshowText(“is”, 0, 40);\nshowText(“a”, 0, 20);\nshowText(“Heading!”, 0, 0);\nendTag(“heading 1”);\nWith the extra tag information, a screen reader knows that all of the lines are part of “heading 1” and can read it in a more natural fashion. The structure also allows screen readers to easily navigate to different parts of the document.\nThe above example is only about text, but tagged PDFs support many more features than this e.g. alt text for images, table data, lists, etc.\nHow we supported Tagged PDFs in PDF.js\nFor tagged PDFs we leveraged the existing “text layer” and the browsers built in HTML ARIA accessibility features. We can easily see this by a simple PDF example with one heading and one paragraph. First, we generate the logical structure and insert it into the canvas:\n<canvas id=\"page1\">\n  <!-- This content is not visible, \n  but available to screen readers   -->\n  <span role=\"heading\" aria-level=\"1\" aria-owns=\"heading_id\"></span>\n  <span aria_owns=\"some_paragraph\"></span>\n</canvas>\nIn the text layer that overlays the canvas:\n<div id=\"text_layer\">\n  <span id=\"heading_id\">Some Heading</span>\n  <span id=\"some_paragaph\">Hello world!</span>\n</div>\nA screen reader would then walk the DOM accessibility tree in the canvas and use the `aria-owns` attributes to find the text content for each node. For the above example, a screen reader would announce:\nHeading Level 1 Some Heading\nHello World!\nFor those not familiar with screen readers, having this extra structure also makes navigating around the PDF much easier: you can jump from heading to heading and read paragraphs without unneeded pauses.\nEnsure there are no regressions at scale, meet reftests\n\nCrawling for PDFs\nOver the past few months, we have built a web crawler to retrieve PDFs from the web and, using a set of heuristics, collect statistics about them (e.g. are they XFA? What fonts are they using? What formats of images do they include?).\nWe have also used the crawler with its heuristics to retrieve PDFs of interest from the “stressful PDF corpus” published by the PDF association, which proved particularly interesting as they contained many corner cases we did not think could exist.\nWith the crawler, we were able to build a large corpus of Tagged PDFs (around 32000), PDFs using JS (around 1900), XFA PDFs (around 1200) which we could use for manual and automated testing. Kudos to our QA team for going through so many PDFs! They now know everything about asking for a fishing license in Canada, life skills!\nReftests for the win\nWe did not only use the corpus for manual QA, but also added some of those PDFs to our list of reftests (reference tests).\nA reftest is a test consisting of a test file and a reference file. The test file uses the pdf.js rendering engine, while the reference file doesn’t (to make sure it is consistent and can’t be affected by changes in the patch the test is validating). The reference file is simply a screenshot of the rendering of a given PDF from the “master” branch of pdf.js.\nThe reftest process\nWhen a developer submits a change to the PDF.js repo, we run the reftests and ensure the rendering of the test file is exactly the same as the reference screenshot. If there are differences, we ensure that the differences are improvements rather than regressions.\nAfter accepting and merging a change, we regenerate the references.\nThe reftest shortcomings\nIn some situations a test may have subtle differences in rendering compared to the reference due to, e.g., anti-aliasing. This introduces noise in the results, with “fake” regressions the developer and reviewer have to sift through. Sometimes, it is possible to miss real regressions because of the large number of differences to look at.\nAnother shortcoming of reftests is that they are often big. A regression in a reftest is not as easy to investigate as a failure of a unit test.\nDespite these shortcomings, reftests are a very powerful regression prevention weapon in the pdf.js arsenal. The large number of reftests we have boosts our confidence when applying changes.\nConclusion\n\nSupport for AcroForms landed in Firefox v84. JavaScript execution in v88. Tagged PDFs in v89. XFA forms in v93 (tomorrow, October 5th, 2021!).\nWhile all of these features have greatly improved form usability and accessibility, there are still more features we’d like to add. If you’re interested in helping, we’re always looking for more contributors and you can join us on element or github.\nWe also want to say a big thanks to two of our contributors Jonas Jenwald and Tim van der Meij for their on going help with the above projects.\nThe post Implementing form filling and accessibility in the Firefox PDF viewer appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-10-07T15:13:02.000Z",
      "date_modified": "2021-10-07T15:13:02.000Z",
      "_plugin": {
        "pageFilename": "50bfb771dbd2c10e839fc694951c12ab9e73f424a4c4e2a84ec370e3893cad5b.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47420",
      "url": "https://hacks.mozilla.org/2021/10/control-your-data-for-good-with-rally/",
      "title": "Control your data for good with Rally",
      "summary": "In a world where data and AI are reshaping society, people currently have no tangible way to put their data to work for the causes they believe in. To address this, we built the Rally platform, a first-of-its-kind tool that enables you to contribute your data to specific studies and exercise consent at a granular level. Mozilla Rally puts you in control of your data while building a better Internet and a better society. \nThe post Control your data for good with Rally appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><span style=\"font-weight: 400;\">Let’s face it, if you have ever used the internet or signed up for an online account, or even read a blog post like this one, chances are that your data has left a permanent mark on the interwebs and online services have exploited your data without your awareness for a very long time. </span></p>\n<h2>The Fight for Privacy</h2>\n<p><span style=\"font-weight: 400;\">The fight for privacy is compounded by the rise in misinformation and platforms like Facebook willingly sharing information that is untrustworthy, shutting down platforms like </span><a href=\"https://firstdraftnews.org/articles/%E2%80%A8what-facebook-gutting-crowdtangle-means-for-misinformation/\"><span style=\"font-weight: 400;\">Crowdtangle</span></a><span style=\"font-weight: 400;\"> and recently </span><a href=\"https://knightcolumbia.org/content/researchers-nyu-knight-institute-condemn-facebooks-effort-to-squelch-independent-research-about-misinformation\"><span style=\"font-weight: 400;\">terminating the accounts</span></a><span style=\"font-weight: 400;\"> of New York University researchers that built Ad Observer, an extension dedicated to bringing greater transparency to political advertising. We think a better internet is one where people have more control over their data. </span></p>\n<h2>Contribute your data for good</h2>\n<p><span style=\"font-weight: 400;\">In a world where data and AI are reshaping society, people currently have no tangible way to put their data to work for the causes they believe in. To address this, we built the <a href=\"https://rally.mozilla.org/?utm_medium=blog&amp;utm_source=mozilla-hacks&amp;utm_campaign=20211006\">Rally</a> platform, a first-of-its-kind tool that enables you to contribute your data to specific studies and exercise consent at a granular level. Mozilla Rally puts you in control of your data while building a better Internet and a better society. </span></p>\n<h2>Mozilla Rally</h2>\n<p><span style=\"font-weight: 400;\">Like Mozilla, <a href=\"https://rally.mozilla.org/?utm_medium=blog&amp;utm_source=mozilla-hacks&amp;utm_campaign=20211006\">Rally</a> is a community-driven open source project and we publish our code on </span><a href=\"https://github.com/mozilla-rally\"><span style=\"font-weight: 400;\">GitHub</span></a><span style=\"font-weight: 400;\">, ensuring that it’s open-source and freely available for you to audit. Privacy, control and transparency are foundational to Rally. Participating is voluntary, meaning we won&#8217;t collect data unless you agree to it first, and we’ll provide you with a clear understanding of what we have access to at every step of the way.</span></p>\n<p><div style=\"width: 560px;\" class=\"wp-video\"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->\n<video class=\"wp-video-shortcode\" id=\"video-47420-1\" width=\"560\" height=\"350\" poster=\"https://hacks.mozilla.org/files/2021/10/Logo-Illustrations-Banner-1536x806-1.png\" preload=\"metadata\" controls=\"controls\"><source type=\"video/mp4\" src=\"https://hacks.mozilla.org/files/2021/10/Rally-v6.mp4?_=1\" /><a href=\"https://hacks.mozilla.org/files/2021/10/Rally-v6.mp4\">https://hacks.mozilla.org/files/2021/10/Rally-v6.mp4</a></video></div></p>\n<p>&nbsp;</p>\n<p><span style=\"font-weight: 400;\">With your help, we can create a safer, more transparent, and more equitable internet that protects people, not Big Tech. </span></p>\n<h2><b>Interested?</b></h2>\n<p><span style=\"font-weight: 400;\">Rally needs users and is currently available on Firefox. In the future, we will expand to other web browsers. We’re currently looking for users who are residents in the United States, age 19 and older. </span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://rally.mozilla.org/?utm_medium=blog&amp;utm_source=mozilla-hacks&amp;utm_campaign=20211006\"><span style=\"font-weight: 400;\">Go to the Rally webpage </span></a><span style=\"font-weight: 400;\">for all the details!</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Install the Firefox Rally add-on</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://rally.mozilla.org/current-studies/\"><span style=\"font-weight: 400;\">Chose a study</span></a><span style=\"font-weight: 400;\"> you want to participate in</span></li>\n</ul>\n<p><span style=\"font-weight: 400;\">Protecting the internet and its users is hard work!  We’re also </span><a href=\"https://rally.mozilla.org/careers/apply/\"><span style=\"font-weight: 400;\">hiring</span></a><span style=\"font-weight: 400;\"> to grow our Rally Team.</span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/10/control-your-data-for-good-with-rally/\">Control your data for good with Rally</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Let’s face it, if you have ever used the internet or signed up for an online account, or even read a blog post like this one, chances are that your data has left a permanent mark on the interwebs and online services have exploited your data without your awareness for a very long time. \nThe Fight for Privacy\nThe fight for privacy is compounded by the rise in misinformation and platforms like Facebook willingly sharing information that is untrustworthy, shutting down platforms like Crowdtangle and recently terminating the accounts of New York University researchers that built Ad Observer, an extension dedicated to bringing greater transparency to political advertising. We think a better internet is one where people have more control over their data. \nContribute your data for good\nIn a world where data and AI are reshaping society, people currently have no tangible way to put their data to work for the causes they believe in. To address this, we built the Rally platform, a first-of-its-kind tool that enables you to contribute your data to specific studies and exercise consent at a granular level. Mozilla Rally puts you in control of your data while building a better Internet and a better society. \nMozilla Rally\nLike Mozilla, Rally is a community-driven open source project and we publish our code on GitHub, ensuring that it’s open-source and freely available for you to audit. Privacy, control and transparency are foundational to Rally. Participating is voluntary, meaning we won’t collect data unless you agree to it first, and we’ll provide you with a clear understanding of what we have access to at every step of the way.\n\nhttps://hacks.mozilla.org/files/2021/10/Rally-v6.mp4\n \nWith your help, we can create a safer, more transparent, and more equitable internet that protects people, not Big Tech. \nInterested?\nRally needs users and is currently available on Firefox. In the future, we will expand to other web browsers. We’re currently looking for users who are residents in the United States, age 19 and older. \n\nGo to the Rally webpage for all the details!\nInstall the Firefox Rally add-on\nChose a study you want to participate in\n\nProtecting the internet and its users is hard work!  We’re also hiring to grow our Rally Team.\n \nThe post Control your data for good with Rally appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-10-06T14:55:48.000Z",
      "date_modified": "2021-10-06T14:55:48.000Z",
      "_plugin": {
        "pageFilename": "dfc759fe76e019fdd1be2a9c1ce57c64da5ad7b126089696d5a96836a4c9eec8.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47413",
      "url": "https://hacks.mozilla.org/2021/10/tab-unloading-in-firefox-93/",
      "title": "Tab Unloading in Firefox 93",
      "summary": "Starting with Firefox 93, Firefox will monitor available system memory and, should it ever become so critically low that a crash is imminent, Firefox will respond by unloading memory-heavy but not actively used tabs. This feature is currently enabled on Windows and will be deployed later for macOS and Linux as well.\nThe post Tab Unloading in Firefox 93 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Starting with Firefox 93, Firefox will monitor available system memory and, should it ever become so critically low that a crash is imminent, Firefox will respond by unloading memory-heavy but not actively used tabs. This feature is currently enabled on Windows and will be deployed later for macOS and Linux as well. When a tab is unloaded, the tab remains in the tab bar and will be automatically reloaded when it is next selected. The tab’s scroll position and form data are restored just like when the browser is restarted with the <i>restore previous windows</i> browser option.</p>\n<p>On Windows, out-of-memory (OOM) situations are responsible for a significant number of the browser and content process crashes reported by our users. Unloading tabs allows Firefox to save memory leading to fewer crashes and avoids the associated interruption in using the browser.</p>\n<p>We believe this may especially benefit people who are doing heavy browsing work with many tabs on resource-constrained machines. Or perhaps those users simply trying to play a memory-intensive game or using a website that goes a little crazy. And of course, there are the tab hoarders, (no judgement here). Firefox is now better at surviving these situations.</p>\n<p>We have experimented with tab unloading on Windows in the past, but a problem we could not get past was that finding a balance between decreasing the browser’s memory usage and annoying the user because there&#8217;s a slight delay as the tab gets reloaded, is a rather difficult exercise, and we never got satisfactory results.</p>\n<p>We have now approached the problem again by refining our low-memory detection and tab selection algorithm and narrowing the action to the case where we are sure we&#8217;re providing a user benefit: if the browser is about to crash. Recently we have been conducting an experiment on our Nightly channel to monitor how tab unloading affects browser use and the number of crashes our users encounter. We’ve seen encouraging results with that experiment. We’ll continue to monitor the results as the feature ships in Firefox 93.</p>\n<p>With our experiment on the Nightly channel, we hoped to see a decrease in the number of OOM crashes hit by our users. However, after the month-long experiment, we found an overall significant decrease in browser crashes and content process crashes. Of those remaining crashes, we saw an <i>increase</i> in OOM crashes. Most encouragingly, people who had tab unloading enabled were able to use the browser for longer periods of time. We also found that average memory usage of the browser <i>increased.</i></p>\n<p>The latter may seem very counter-intuitive, but is easily explained by survivorship bias. Much like in the <a href=\"https://en.wikipedia.org/wiki/Survivorship_bias#In_the_military\">archetypal example of the Allied WWII bombers with bullet holes</a>, browser sessions that had such high memory usage would have crashed and burned in the past, but are now able to survive by unloading tabs just before hitting the critical threshold.</p>\n<p>The increase in OOM crashes, also very counter-intuitive, is harder to explain. Before tab unloading was introduced, Firefox already responded to Windows memory-pressure by triggering an internal memory-pressure event, allowing subsystems to reduce their memory use. With tab unloading, this event is fired after all possible unloadable tabs have been unloaded.</p>\n<p>This may account for the difference. Another hypothesis is that it&#8217;s possible our tab unloading sometimes kicks in a fraction too late and finds the tabs in a state where they can&#8217;t even be safely unloaded any more.</p>\n<p>For example, unloading a tab requires a garbage collection pass over its JavaScript heap. This needs some additional temporary storage that is not available, leading to the tab crashing instead of being unloaded but still saving the entire browser from going down.</p>\n<p>We&#8217;re working on improving our understanding of this problem and the relevant heuristics. But given the clearly improved outcomes for users, we felt there was no point in holding back the feature.</p>\n<h2>When does Firefox automatically unload tabs?</h2>\n<p>When system memory is critically low, Firefox will begin automatically unloading tabs. Unloading tabs could disturb users’ browsing sessions so the approach aims to unload tabs only when necessary to avoid crashes. On Windows, Firefox gets a notification from the operating system (setup using <a href=\"https://docs.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-creatememoryresourcenotification\">CreateMemoryResourceNotification</a>) indicating that the available physical memory is running low. The threshold for low physical memory is not documented, but appears to be around 6%. Once that occurs, Firefox starts periodically checking the commit space (<a href=\"https://docs.microsoft.com/en-us/windows/win32/api/sysinfoapi/ns-sysinfoapi-memorystatusex\">MEMORYSTATUSEX.ullAvailPageFile</a>).</p>\n<p>When the commit space reaches a low-memory threshold, which is defined with the preference “browser.low_commit_space_threshold_mb”, Firefox will unload one tab, or if there are no unloadable tabs, trigger the Firefox-internal memory-pressure warning allowing subsystems in the browser to reduce their memory use. The browser then waits for a short period of time before checking commit space again and then repeating this process until available commit space is above the threshold.</p>\n<p>We found the checks on commit space to be essential for predicting when a real out-of-memory situation is happening. As long as there is still swap AND physical memory available, there is no problem. If we run out of physical memory and there is swap, performance will crater due to paging, but we won&#8217;t crash.</p>\n<p>On Windows, allocations fail and applications will crash if there is low commit space in the system even though there is physical memory available because Windows does not overcommit memory and can refuse to allocate virtual memory to the process in this case. In other words, unlike Linux, Windows always requires commit space to allocate memory.</p>\n<p>How do we end up in this situation? If some applications allocate memory but do not touch it, Windows does not assign the physical memory to such untouched memory. We have observed graphics drivers doing this, leading to low swap space when plenty of physical memory is available.</p>\n<p>In addition, <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1711610\">crash data</a> we collected indicated that a surprising number of users with beefy machines were in this situation, some perhaps thinking that because they had a lot of memory in their machine, the Windows swap could be reduced to the bare minimum. You can see why this is not a good idea!</p>\n<h2>How does Firefox choose which tabs to unload first?</h2>\n<p>Ideally, only tabs that are no longer needed will be unloaded and the user will eventually restart the browser or close unloaded tabs before ever reloading them. A natural metric is to consider when the user has last used a tab. Firefox unloads tabs in least-recently-used order.</p>\n<p>Tabs playing sound, using picture-in-picture, pinned tabs, or tabs using WebRTC (which is used for video and audio conferencing sites) are weighted more heavily so they are less likely to be unloaded. Tabs in the foreground are never unloaded. We plan to do more experiments and continue to tune the algorithm, aiming to reduce crashes while maintaining performance and being unobtrusive to the user.</p>\n<h2>about:unloads</h2>\n<p>For diagnostic and testing purposes, a new page about:unloads has been added to display the tabs in their unload-priority-order and to manually trigger tab unloading. This feature is currently in beta and will ship with Firefox 94.</p>\n<p><div id=\"attachment_47414\" style=\"width: 260px\" class=\"wp-caption alignnone\"><img aria-describedby=\"caption-attachment-47414\" loading=\"lazy\" class=\"size-medium wp-image-47414\" src=\"https://hacks.mozilla.org/files/2021/10/AboutUnloads-250x199.png\" alt=\"Screenshot of the about:unloads page in beta planned for Firefox 94.\" width=\"250\" height=\"199\" srcset=\"https://hacks.mozilla.org/files/2021/10/AboutUnloads-250x199.png 250w, https://hacks.mozilla.org/files/2021/10/AboutUnloads-500x397.png 500w, https://hacks.mozilla.org/files/2021/10/AboutUnloads-768x610.png 768w, https://hacks.mozilla.org/files/2021/10/AboutUnloads.png 1177w\" sizes=\"(max-width: 250px) 100vw, 250px\" /><p id=\"caption-attachment-47414\" class=\"wp-caption-text\">Screenshot of the about:unloads page in beta planned for Firefox 94.</p></div></p>\n<h2></h2>\n<h2>Browser Extensions</h2>\n<p>Some browser extensions already offer users the ability to unload tabs. We expect these extensions to interoperate with automatic tab unloading as they use the same underlying <a href=\"https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/API/tabs/discard\">tabs.discard() API</a>. Although it may change in the future, today automatic tab unloading only occurs when system memory is critically low, which is a low-level system metric that is not exposed by the WebExtensions API. (Note: an extension could use the <a href=\"https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Native_messaging\">native messaging support</a> in the WebExtensions API to accomplish this with a separate application.) Users will still be able to benefit from tab unloading extensions and those extensions may offer more control over when tabs are unloaded, or deploy more aggressive heuristics to save more memory.</p>\n<p>Let us know how it works for you by leaving feedback on <a href=\"https://ideas.mozilla.org\">ideas.mozilla.org</a> or <a href=\"https://bugzilla.mozilla.org/enter_bug.cgi?product=Firefox&amp;component=Tabbed+Browser\">reporting a bug</a>. For support, visit <a href=\"https://support.mozilla.org\">support.mozilla.org</a>.Firefox crash reporting and telemetry adheres to our <a href=\"https://www.mozilla.org/en-US/privacy/principles/\">data privacy principles</a>. See the <a href=\"https://www.mozilla.org/en-US/privacy/\">Mozilla Privacy Policy</a> for more information.</p>\n<p><em>Thanks to Gian-Carlo Pascutto, Toshihito Kikuchi, Gabriele Svelto, Neil Deakin, Kris Wright, and Chris Peterson, for their contributions to this blog post and their work on developing tab unloading in Firefox.</em></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/10/tab-unloading-in-firefox-93/\">Tab Unloading in Firefox 93</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Starting with Firefox 93, Firefox will monitor available system memory and, should it ever become so critically low that a crash is imminent, Firefox will respond by unloading memory-heavy but not actively used tabs. This feature is currently enabled on Windows and will be deployed later for macOS and Linux as well. When a tab is unloaded, the tab remains in the tab bar and will be automatically reloaded when it is next selected. The tab’s scroll position and form data are restored just like when the browser is restarted with the restore previous windows browser option.\nOn Windows, out-of-memory (OOM) situations are responsible for a significant number of the browser and content process crashes reported by our users. Unloading tabs allows Firefox to save memory leading to fewer crashes and avoids the associated interruption in using the browser.\nWe believe this may especially benefit people who are doing heavy browsing work with many tabs on resource-constrained machines. Or perhaps those users simply trying to play a memory-intensive game or using a website that goes a little crazy. And of course, there are the tab hoarders, (no judgement here). Firefox is now better at surviving these situations.\nWe have experimented with tab unloading on Windows in the past, but a problem we could not get past was that finding a balance between decreasing the browser’s memory usage and annoying the user because there’s a slight delay as the tab gets reloaded, is a rather difficult exercise, and we never got satisfactory results.\nWe have now approached the problem again by refining our low-memory detection and tab selection algorithm and narrowing the action to the case where we are sure we’re providing a user benefit: if the browser is about to crash. Recently we have been conducting an experiment on our Nightly channel to monitor how tab unloading affects browser use and the number of crashes our users encounter. We’ve seen encouraging results with that experiment. We’ll continue to monitor the results as the feature ships in Firefox 93.\nWith our experiment on the Nightly channel, we hoped to see a decrease in the number of OOM crashes hit by our users. However, after the month-long experiment, we found an overall significant decrease in browser crashes and content process crashes. Of those remaining crashes, we saw an increase in OOM crashes. Most encouragingly, people who had tab unloading enabled were able to use the browser for longer periods of time. We also found that average memory usage of the browser increased.\nThe latter may seem very counter-intuitive, but is easily explained by survivorship bias. Much like in the archetypal example of the Allied WWII bombers with bullet holes, browser sessions that had such high memory usage would have crashed and burned in the past, but are now able to survive by unloading tabs just before hitting the critical threshold.\nThe increase in OOM crashes, also very counter-intuitive, is harder to explain. Before tab unloading was introduced, Firefox already responded to Windows memory-pressure by triggering an internal memory-pressure event, allowing subsystems to reduce their memory use. With tab unloading, this event is fired after all possible unloadable tabs have been unloaded.\nThis may account for the difference. Another hypothesis is that it’s possible our tab unloading sometimes kicks in a fraction too late and finds the tabs in a state where they can’t even be safely unloaded any more.\nFor example, unloading a tab requires a garbage collection pass over its JavaScript heap. This needs some additional temporary storage that is not available, leading to the tab crashing instead of being unloaded but still saving the entire browser from going down.\nWe’re working on improving our understanding of this problem and the relevant heuristics. But given the clearly improved outcomes for users, we felt there was no point in holding back the feature.\nWhen does Firefox automatically unload tabs?\nWhen system memory is critically low, Firefox will begin automatically unloading tabs. Unloading tabs could disturb users’ browsing sessions so the approach aims to unload tabs only when necessary to avoid crashes. On Windows, Firefox gets a notification from the operating system (setup using CreateMemoryResourceNotification) indicating that the available physical memory is running low. The threshold for low physical memory is not documented, but appears to be around 6%. Once that occurs, Firefox starts periodically checking the commit space (MEMORYSTATUSEX.ullAvailPageFile).\nWhen the commit space reaches a low-memory threshold, which is defined with the preference “browser.low_commit_space_threshold_mb”, Firefox will unload one tab, or if there are no unloadable tabs, trigger the Firefox-internal memory-pressure warning allowing subsystems in the browser to reduce their memory use. The browser then waits for a short period of time before checking commit space again and then repeating this process until available commit space is above the threshold.\nWe found the checks on commit space to be essential for predicting when a real out-of-memory situation is happening. As long as there is still swap AND physical memory available, there is no problem. If we run out of physical memory and there is swap, performance will crater due to paging, but we won’t crash.\nOn Windows, allocations fail and applications will crash if there is low commit space in the system even though there is physical memory available because Windows does not overcommit memory and can refuse to allocate virtual memory to the process in this case. In other words, unlike Linux, Windows always requires commit space to allocate memory.\nHow do we end up in this situation? If some applications allocate memory but do not touch it, Windows does not assign the physical memory to such untouched memory. We have observed graphics drivers doing this, leading to low swap space when plenty of physical memory is available.\nIn addition, crash data we collected indicated that a surprising number of users with beefy machines were in this situation, some perhaps thinking that because they had a lot of memory in their machine, the Windows swap could be reduced to the bare minimum. You can see why this is not a good idea!\nHow does Firefox choose which tabs to unload first?\nIdeally, only tabs that are no longer needed will be unloaded and the user will eventually restart the browser or close unloaded tabs before ever reloading them. A natural metric is to consider when the user has last used a tab. Firefox unloads tabs in least-recently-used order.\nTabs playing sound, using picture-in-picture, pinned tabs, or tabs using WebRTC (which is used for video and audio conferencing sites) are weighted more heavily so they are less likely to be unloaded. Tabs in the foreground are never unloaded. We plan to do more experiments and continue to tune the algorithm, aiming to reduce crashes while maintaining performance and being unobtrusive to the user.\nabout:unloads\nFor diagnostic and testing purposes, a new page about:unloads has been added to display the tabs in their unload-priority-order and to manually trigger tab unloading. This feature is currently in beta and will ship with Firefox 94.\nScreenshot of the about:unloads page in beta planned for Firefox 94.\n\nBrowser Extensions\nSome browser extensions already offer users the ability to unload tabs. We expect these extensions to interoperate with automatic tab unloading as they use the same underlying tabs.discard() API. Although it may change in the future, today automatic tab unloading only occurs when system memory is critically low, which is a low-level system metric that is not exposed by the WebExtensions API. (Note: an extension could use the native messaging support in the WebExtensions API to accomplish this with a separate application.) Users will still be able to benefit from tab unloading extensions and those extensions may offer more control over when tabs are unloaded, or deploy more aggressive heuristics to save more memory.\nLet us know how it works for you by leaving feedback on ideas.mozilla.org or reporting a bug. For support, visit support.mozilla.org.Firefox crash reporting and telemetry adheres to our data privacy principles. See the Mozilla Privacy Policy for more information.\nThanks to Gian-Carlo Pascutto, Toshihito Kikuchi, Gabriele Svelto, Neil Deakin, Kris Wright, and Chris Peterson, for their contributions to this blog post and their work on developing tab unloading in Firefox.\nThe post Tab Unloading in Firefox 93 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-10-05T16:52:07.000Z",
      "date_modified": "2021-10-05T16:52:07.000Z",
      "_plugin": {
        "pageFilename": "7d984afd2cf18ec56ae9e027d4e79dc515acedaaa00193a085952f19e9cdc882.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47391",
      "url": "https://hacks.mozilla.org/2021/10/mdn-web-docs-at-write-the-docs-prague-2021/",
      "title": "MDN Web Docs at Write the Docs Prague 2021",
      "summary": "The MDN Web Docs team is pleased to sponsor Write the Docs Prague 2021, which is being held remotely this year. We’re excited to join hundreds of documentarians to learn more about collaborating with writers, developers, and readers to make better documentation. We plan to take part in all that the conference has to offer, including the Writing Day, Job Fair, and the virtual hallway track.\nThe post MDN Web Docs at Write the Docs Prague 2021 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>The MDN Web Docs team is pleased to sponsor <a href=\"https://www.writethedocs.org/conf/prague/2021/sponsors/\">Write the Docs Prague 2021</a>, which is being held remotely this year. We’re excited to join hundreds of documentarians to learn more about collaborating with writers, developers, and readers to make better documentation. We plan to take part in all that the conference has to offer, including the Writing Day, Job Fair, and the virtual hallway track.</p>\n<p>In particular, we’re looking forward to taking part in the <a href=\"https://www.writethedocs.org/conf/prague/2021/writing-day/#open-web-docs-and-mdn\">Writing Day on Sunday, October 3</a>, where we’ll be joining our friends from Open Web Docs (OWD) to work on MDN content updates together. We’re planning to invite our fellow conference attendees to take part in making open source documentation. OWD is also sponsoring ​​Write the Docs; <a href=\"https://opencollective.com/open-web-docs/updates/join-open-web-docs-at-write-the-docs-prague-online\">read their announcement</a> to learn more.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/10/mdn-web-docs-at-write-the-docs-prague-2021/\">MDN Web Docs at Write the Docs Prague 2021</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "The MDN Web Docs team is pleased to sponsor Write the Docs Prague 2021, which is being held remotely this year. We’re excited to join hundreds of documentarians to learn more about collaborating with writers, developers, and readers to make better documentation. We plan to take part in all that the conference has to offer, including the Writing Day, Job Fair, and the virtual hallway track.\nIn particular, we’re looking forward to taking part in the Writing Day on Sunday, October 3, where we’ll be joining our friends from Open Web Docs (OWD) to work on MDN content updates together. We’re planning to invite our fellow conference attendees to take part in making open source documentation. OWD is also sponsoring ​​Write the Docs; read their announcement to learn more.\nThe post MDN Web Docs at Write the Docs Prague 2021 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-10-01T12:46:37.000Z",
      "date_modified": "2021-10-01T12:46:37.000Z",
      "_plugin": {
        "pageFilename": "94e9ad971aa1b0de3bda0c503548e433885af445531cd51197047c02ed2b39a2.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47367",
      "url": "https://hacks.mozilla.org/2021/09/time-for-a-review-of-firefox-92/",
      "title": "Time for a review of Firefox 92",
      "summary": "Release time comes around so quickly! This month we have quite a few CSS updates, along with the new Object.hasOwn() static method for JavaScript.\nThe post Time for a review of Firefox 92 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Release time comes around so quickly! This month we have quite a few CSS updates, along with the new <code>Object.hasOwn()</code> static method for JavaScript.</p>\n<p>This blog post provides merely a set of highlights; for all the details, check out the following:</p>\n<ul>\n<li aria-level=\"1\"><a href=\"https://developer.mozilla.org/docs/Mozilla/Firefox/Releases/92\">Firefox 92 for developers on MDN</a></li>\n<li aria-level=\"1\"><a href=\"https://www.mozilla.org/en-US/firefox/92.0/releasenotes/\">Firefox 92 end-user release notes</a></li>\n</ul>\n<h2><b>CSS Updates</b></h2>\n<p>A couple of CSS features have moved from behind a preference and are now available by default: <code>accent-color</code> and <code>size-adjust.</code></p>\n<h3><b>accent-color</b></h3>\n<p>The <code>accent-color</code> CSS property sets the color of an element&#8217;s accent. Accents appear in elements such as a checkbox or radio input. It&#8217;s default value is <code>auto</code> which represents a UA-chosen color, which should match the accent color of the platform. You can also specify a color value. <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/accent-color\">Read more about the accent-color property here</a>.</p>\n<h3><b>size-adjust</b></h3>\n<p>The <code>size-adjust</code> descriptor for <code>@font-face</code> takes a percentage value which acts as a multiplier for glyph outlines and metrics. Another tool in the CSS box for controlling fonts, it can help to harmonize the designs of various fonts when rendered at the same font size. <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/@font-face/size-adjust\">Check out some examples on the size-adjust descriptor page on MDN</a>.</p>\n<h3><b>And more&#8230;</b></h3>\n<p>Along with both of those, the <code>break-inside</code> property now has support for values <code>avoid-page</code> and <code>avoid-column</code>, the <code>font-size-adjust</code> property accepts two values <i>and</i> if that wasn&#8217;t enough <code>system-ui</code> as a generic font family name for the <code>font-family</code> property is now supported.</p>\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/break-inside\">break-inside property on MDN</a></p>\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/font-size-adjust\">font-size-adjust property on MDN</a></p>\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/font-family\">font-family property on MDN</a></p>\n<h2><b>Object.hasOwn arrives</b></h2>\n<p>A nice addition to JavaScript is the <code>Object.hasOwn()</code> static method. This returns <code>true</code> if the specified property is a direct property of the object (even if that property&#8217;s value is <code>null</code> or <code>undefined</code>). <code>false</code> is returned if the specified property is inherited or not declared. Unlike the <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/in\"><code>in</code></a> operator, this method does not check for the specified property in the object&#8217;s prototype chain.</p>\n<p><code>Object.hasOwn()</code> is recommended over <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/hasOwnProperty\"><code>Object.hasOwnProperty()</code></a> as it works for objects created using <code>Object.create(null)</code> and with objects that have overridden the inherited <code>hasOwnProperty()</code> method.</p>\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/hasOwn\">Read more about Object.hasOwn() on MDN</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/09/time-for-a-review-of-firefox-92/\">Time for a review of Firefox 92</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Release time comes around so quickly! This month we have quite a few CSS updates, along with the new Object.hasOwn() static method for JavaScript.\nThis blog post provides merely a set of highlights; for all the details, check out the following:\n\nFirefox 92 for developers on MDN\nFirefox 92 end-user release notes\n\nCSS Updates\nA couple of CSS features have moved from behind a preference and are now available by default: accent-color and size-adjust.\naccent-color\nThe accent-color CSS property sets the color of an element’s accent. Accents appear in elements such as a checkbox or radio input. It’s default value is auto which represents a UA-chosen color, which should match the accent color of the platform. You can also specify a color value. Read more about the accent-color property here.\nsize-adjust\nThe size-adjust descriptor for @font-face takes a percentage value which acts as a multiplier for glyph outlines and metrics. Another tool in the CSS box for controlling fonts, it can help to harmonize the designs of various fonts when rendered at the same font size. Check out some examples on the size-adjust descriptor page on MDN.\nAnd more…\nAlong with both of those, the break-inside property now has support for values avoid-page and avoid-column, the font-size-adjust property accepts two values and if that wasn’t enough system-ui as a generic font family name for the font-family property is now supported.\nbreak-inside property on MDN\nfont-size-adjust property on MDN\nfont-family property on MDN\nObject.hasOwn arrives\nA nice addition to JavaScript is the Object.hasOwn() static method. This returns true if the specified property is a direct property of the object (even if that property’s value is null or undefined). false is returned if the specified property is inherited or not declared. Unlike the in operator, this method does not check for the specified property in the object’s prototype chain.\nObject.hasOwn() is recommended over Object.hasOwnProperty() as it works for objects created using Object.create(null) and with objects that have overridden the inherited hasOwnProperty() method.\nRead more about Object.hasOwn() on MDN\nThe post Time for a review of Firefox 92 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-09-08T15:17:15.000Z",
      "date_modified": "2021-09-08T15:17:15.000Z",
      "_plugin": {
        "pageFilename": "1a9e843033754dd18001958aa3e0ba60339157ae801e2199e4c542e427306815.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47356",
      "url": "https://hacks.mozilla.org/2021/08/spring-cleaning-mdn-part-2/",
      "title": "Spring cleaning MDN: Part 2",
      "summary": "Last month we removed a bunch of content from MDN. MDN is 16 years old (and yes it can drink in some countries), all that time ago it was a great place for all of Mozilla to document all of their things. As MDN evolved and the web reference became our core content, other areas became less relevant to the overall site. We have ~11k active pages on MDN, so keeping them up to date is a big task and we feel our focus should be there.\nThe post Spring cleaning MDN: Part 2 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><div id=\"attachment_47361\" style=\"width: 610px\" class=\"wp-caption alignnone\"><img aria-describedby=\"caption-attachment-47361\" loading=\"lazy\" class=\"wp-image-47361 size-full\" src=\"https://hacks.mozilla.org/files/2021/08/mdn_spring_cleaning_ii.png\" alt=\"An illustration of a blue coloured dinosaur sweeping with a broom\" width=\"600\" height=\"400\" srcset=\"https://hacks.mozilla.org/files/2021/08/mdn_spring_cleaning_ii.png 600w, https://hacks.mozilla.org/files/2021/08/mdn_spring_cleaning_ii-250x167.png 250w, https://hacks.mozilla.org/files/2021/08/mdn_spring_cleaning_ii-500x333.png 500w\" sizes=\"(max-width: 600px) 100vw, 600px\" /><p id=\"caption-attachment-47361\" class=\"wp-caption-text\">Illustration by <a href=\"https://www.darylalexsy.net/\">Daryl Alexsy</a></p></div></p>\n<p>&nbsp;</p>\n<p>The bags have been filled up with all the things we&#8217;re ready to let go of and it&#8217;s time to take them to the charity shop.</p>\n<h2><b>Archiving content</b></h2>\n<p>Last month we removed a bunch of content from MDN. MDN is 16 years old (and yes it can drink in some countries), all that time ago it was a great place for all of Mozilla to document all of their things. As MDN evolved and the web reference became our core content, other areas became less relevant to the overall site. We have ~11k active pages on MDN, so keeping them up to date is a big task and we feel our focus should be there.</p>\n<p>This was a big decision and had been in the works for over a year. It actually started before we moved MDN content to GitHub. You may have noticed a banner every now and again, saying certain pages weren&#8217;t maintained. Various topics were removed including all Firefox (inc. Gecko) docs, which you <a href=\"https://firefox-source-docs.mozilla.org/\">can now find here</a>. Mercurial, Spidermonkey, Thunderbird, Rhino and XUL were also included in the archive.</p>\n<h2><b>So where is the content now?</b></h2>\n<p>It&#8217;s saved &#8211; it&#8217;s <a href=\"https://github.com/mdn/archived-content/\">in this repo</a>. We haven&#8217;t actually deleted it completely. Some of it is being re-hosted by various teams and we have the ability to redirect to those new places. It&#8217;s saved in both it&#8217;s rendered state and the raw wiki form. Just. In. Case.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/08/spring-cleaning-mdn-part-2/\">Spring cleaning MDN: Part 2</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Illustration by Daryl Alexsy\n \nThe bags have been filled up with all the things we’re ready to let go of and it’s time to take them to the charity shop.\nArchiving content\nLast month we removed a bunch of content from MDN. MDN is 16 years old (and yes it can drink in some countries), all that time ago it was a great place for all of Mozilla to document all of their things. As MDN evolved and the web reference became our core content, other areas became less relevant to the overall site. We have ~11k active pages on MDN, so keeping them up to date is a big task and we feel our focus should be there.\nThis was a big decision and had been in the works for over a year. It actually started before we moved MDN content to GitHub. You may have noticed a banner every now and again, saying certain pages weren’t maintained. Various topics were removed including all Firefox (inc. Gecko) docs, which you can now find here. Mercurial, Spidermonkey, Thunderbird, Rhino and XUL were also included in the archive.\nSo where is the content now?\nIt’s saved – it’s in this repo. We haven’t actually deleted it completely. Some of it is being re-hosted by various teams and we have the ability to redirect to those new places. It’s saved in both it’s rendered state and the raw wiki form. Just. In. Case.\nThe post Spring cleaning MDN: Part 2 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-08-18T15:05:06.000Z",
      "date_modified": "2021-08-18T15:05:06.000Z",
      "_plugin": {
        "pageFilename": "9a9fde127cf3601ac3e614e78a2412a34d18d518796095cfd568fefb065815bb.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47357",
      "url": "https://hacks.mozilla.org/2021/08/hopping-on-firefox-91/",
      "title": "Hopping on Firefox 91",
      "summary": "August is already here, which means so is Firefox 91! For developers, Firefox 91 supports the Visual Viewport API and Intl.DateTimeFormat object additions.\nThe post Hopping on Firefox 91 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<h1><b>Hopping on Firefox 91</b></h1>\n<p>August is already here, which means so is Firefox 91! This release has a Scottish locale added and, if the &#8216;increased contrast&#8217; setting is checked, auto enables High Contrast mode on macOS.</p>\n<p>Private browsing windows have an HTTPS-first policy and will automatically attempt to make all connections to websites secure. Connections will fall back to HTTP if the website does not support HTTPS.</p>\n<p>For developers Firefox 91 supports the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Visual_Viewport_API\">Visual Viewport API</a> and adds some more additions to the <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl/DateTimeFormat\">Intl.DateTimeFormat</a> object.</p>\n<p>This blog post provides merely a set of highlights; for all the details, check out the following:</p>\n<ul>\n<li aria-level=\"1\"><a href=\"https://developer.mozilla.org/docs/Mozilla/Firefox/Releases/91\">Firefox 91 for developers on MDN</a></li>\n<li aria-level=\"1\"><a href=\"https://www.mozilla.org/en-US/firefox/91.0/releasenotes/\">Firefox 91 end-user release notes</a></li>\n</ul>\n<h2><b>Visual Viewport API</b></h2>\n<p>Implemented back in Firefox 63, the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Visual_Viewport_API\">Visual Viewport API</a> was behind the pref <code>dom.visualviewport.enabled</code> in the desktop release. It is now no longer behind that pref and enabled by default, meaning the API is now supported in all major browsers.</p>\n<p>There are two viewports on the mobile web, the layout viewport and the visual viewport. The layout viewport covers all the elements on a page and the visual viewport represents what is actually visible on screen. If a keyboard appears on screen, the visual viewport dimensions will shrink, but the layout viewport will remain the same.</p>\n<p>This API gives you information about the size, offset and scale of the visual viewport and allows you to listen for resize and scroll events. You access it via the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window/visualViewport\">visualViewport</a> property of the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window\">window</a> interface.</p>\n<p>In this simple example the <code>resize</code> event is listened for and when a user zooms in, hides an element in the layout, so as not to clutter the interface.</p>\n<pre><code class=\"”language-js”\">const elToHide = document.getElementById('to-hide');\n\nvar viewport = window.visualViewport;\n\nfunction resizeHandler() {\n\n   if (viewport.scale &gt; 1.3)\n     elToHide.style.display = \"none\";\n   else\n     elToHide.style.display = \"block\";\n}\n\nwindow.visualViewport.addEventListener('resize', resizeHandler);\n</code></pre>\n<h2><b>New formats for Intl.DateTimeFormat</b></h2>\n<p>A couple of updates to the <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl/DateTimeFormat\">Intl.DateTimeFormat</a> object include new <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl/DateTimeFormat/DateTimeFormat\">timeZoneName</a> options for formatting how a timezone is displayed. These include the localized GMT formats <code>shortOffset</code> and <code>longOffset</code>, and generic non-location formats <code>shortGeneric</code> and <code>longGeneric</code>. The below code shows all the different options for the <code>timeZoneName</code> and their format.</p>\n<pre><code class=\"”language-js”\">var date = Date.UTC(2021, 11, 17, 3, 0, 42);\nconst timezoneNames = ['short', 'long', 'shortOffset', 'longOffset', 'shortGeneric', 'longGeneric']\n\nfor (const zoneName of timezoneNames) {\n\n  // Do something with currentValue\n  var formatter = new Intl.DateTimeFormat('en-US', {\n    timeZone: 'America/Los_Angeles',\n    timeZoneName: zoneName,\n  });\n\nconsole.log(zoneName + \": \" + formatter.format(date) );\n\n}\n\n// expected output:\n// &gt; \"short: 12/16/2021, PST\"\n// &gt; \"long: 12/16/2021, Pacific Standard Time\"\n// &gt; \"shortOffset: 12/16/2021, GMT-8\"\n// &gt; \"longOffset: 12/16/2021, GMT-08:00\"\n// &gt; \"shortGeneric: 12/16/2021, PT\"\n// &gt; \"longGeneric: 12/16/2021, Pacific Time\"</code></pre>\n<p>You can now format date ranges as well with the new <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl/DateTimeFormat/formatRange\">formatRange()</a> and <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl/DateTimeFormat/formatRangeToParts\">formatRangeToParts()</a> methods. The former returns a localized and formatted string for the range between two <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date\">Date objects</a>:</p>\n<pre><code class=\"”language-js”\">const options = { weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' };\n\nconst startDate = new Date(Date.UTC(2007, 0, 10, 10, 0, 0));\nconst endDate = new Date(Date.UTC(2008, 0, 10, 11, 0, 0));\n\nconst dateTimeFormat = new Intl.DateTimeFormat('en', options1);\nconsole.log(dateTimeFormat.formatRange(startDate, endDate));\n\n// expected output: Wednesday, January 10, 2007 – Thursday, January 10, 2008</code></pre>\n<p>And the latter returns an array containing the locale-specific parts of a date range:</p>\n<pre><code class=\"”language-js”\">const startDate = new Date(Date.UTC(2007, 0, 10, 10, 0, 0)); // &gt; 'Wed, 10 Jan 2007 10:00:00 GMT'\nconst endDate = new Date(Date.UTC(2007, 0, 10, 11, 0, 0));   // &gt; 'Wed, 10 Jan 2007 11:00:00 GMT'\n\nconst dateTimeFormat = new Intl.DateTimeFormat('en', {\n  hour: 'numeric',\n  minute: 'numeric'\n});\nconst parts = dateTimeFormat.formatRangeToParts(startDate, endDate);\n\nfor (const part of parts) {\n\n  console.log(part);\n\n}\n\n// expected output (in GMT timezone):\n// Object { type: \"hour\", value: \"2\", source: \"startRange\" }\n// Object { type: \"literal\", value: \":\", source: \"startRange\" }\n// Object { type: \"minute\", value: \"00\", source: \"startRange\" }\n// Object { type: \"literal\", value: \" – \", source: \"shared\" }\n// Object { type: \"hour\", value: \"3\", source: \"endRange\" }\n// Object { type: \"literal\", value: \":\", source: \"endRange\" }\n// Object { type: \"minute\", value: \"00\", source: \"endRange\" }\n// Object { type: \"literal\", value: \" \", source: \"shared\" }\n// Object { type: \"dayPeriod\", value: \"AM\", source: \"shared\" }\n</code></pre>\n<h2><b>Securing the Gamepad API</b></h2>\n<p>There have been a few updates to the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Gamepad_API\">Gamepad API</a> to fall in line with the spec. It is now only available in secure contexts (HTTPS) and is protected by <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Feature-Policy/gamepad\">Feature Policy: gamepad</a>. If access to gamepads is <i>disallowed,</i> calls to <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/getGamepads\">Navigator.getGamepads()</a> will throw an error and the <code>gamepadconnected</code> and <code>gamepaddisconnected</code> events will not fire.</p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/08/hopping-on-firefox-91/\">Hopping on Firefox 91</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Hopping on Firefox 91\nAugust is already here, which means so is Firefox 91! This release has a Scottish locale added and, if the ‘increased contrast’ setting is checked, auto enables High Contrast mode on macOS.\nPrivate browsing windows have an HTTPS-first policy and will automatically attempt to make all connections to websites secure. Connections will fall back to HTTP if the website does not support HTTPS.\nFor developers Firefox 91 supports the Visual Viewport API and adds some more additions to the Intl.DateTimeFormat object.\nThis blog post provides merely a set of highlights; for all the details, check out the following:\n\nFirefox 91 for developers on MDN\nFirefox 91 end-user release notes\n\nVisual Viewport API\nImplemented back in Firefox 63, the Visual Viewport API was behind the pref dom.visualviewport.enabled in the desktop release. It is now no longer behind that pref and enabled by default, meaning the API is now supported in all major browsers.\nThere are two viewports on the mobile web, the layout viewport and the visual viewport. The layout viewport covers all the elements on a page and the visual viewport represents what is actually visible on screen. If a keyboard appears on screen, the visual viewport dimensions will shrink, but the layout viewport will remain the same.\nThis API gives you information about the size, offset and scale of the visual viewport and allows you to listen for resize and scroll events. You access it via the visualViewport property of the window interface.\nIn this simple example the resize event is listened for and when a user zooms in, hides an element in the layout, so as not to clutter the interface.\nconst elToHide = document.getElementById('to-hide');\n\nvar viewport = window.visualViewport;\n\nfunction resizeHandler() {\n\n   if (viewport.scale > 1.3)\n     elToHide.style.display = \"none\";\n   else\n     elToHide.style.display = \"block\";\n}\n\nwindow.visualViewport.addEventListener('resize', resizeHandler);\n\nNew formats for Intl.DateTimeFormat\nA couple of updates to the Intl.DateTimeFormat object include new timeZoneName options for formatting how a timezone is displayed. These include the localized GMT formats shortOffset and longOffset, and generic non-location formats shortGeneric and longGeneric. The below code shows all the different options for the timeZoneName and their format.\nvar date = Date.UTC(2021, 11, 17, 3, 0, 42);\nconst timezoneNames = ['short', 'long', 'shortOffset', 'longOffset', 'shortGeneric', 'longGeneric']\n\nfor (const zoneName of timezoneNames) {\n\n  // Do something with currentValue\n  var formatter = new Intl.DateTimeFormat('en-US', {\n    timeZone: 'America/Los_Angeles',\n    timeZoneName: zoneName,\n  });\n\nconsole.log(zoneName + \": \" + formatter.format(date) );\n\n}\n\n// expected output:\n// > \"short: 12/16/2021, PST\"\n// > \"long: 12/16/2021, Pacific Standard Time\"\n// > \"shortOffset: 12/16/2021, GMT-8\"\n// > \"longOffset: 12/16/2021, GMT-08:00\"\n// > \"shortGeneric: 12/16/2021, PT\"\n// > \"longGeneric: 12/16/2021, Pacific Time\"\nYou can now format date ranges as well with the new formatRange() and formatRangeToParts() methods. The former returns a localized and formatted string for the range between two Date objects:\nconst options = { weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' };\n\nconst startDate = new Date(Date.UTC(2007, 0, 10, 10, 0, 0));\nconst endDate = new Date(Date.UTC(2008, 0, 10, 11, 0, 0));\n\nconst dateTimeFormat = new Intl.DateTimeFormat('en', options1);\nconsole.log(dateTimeFormat.formatRange(startDate, endDate));\n\n// expected output: Wednesday, January 10, 2007 – Thursday, January 10, 2008\nAnd the latter returns an array containing the locale-specific parts of a date range:\nconst startDate = new Date(Date.UTC(2007, 0, 10, 10, 0, 0)); // > 'Wed, 10 Jan 2007 10:00:00 GMT'\nconst endDate = new Date(Date.UTC(2007, 0, 10, 11, 0, 0));   // > 'Wed, 10 Jan 2007 11:00:00 GMT'\n\nconst dateTimeFormat = new Intl.DateTimeFormat('en', {\n  hour: 'numeric',\n  minute: 'numeric'\n});\nconst parts = dateTimeFormat.formatRangeToParts(startDate, endDate);\n\nfor (const part of parts) {\n\n  console.log(part);\n\n}\n\n// expected output (in GMT timezone):\n// Object { type: \"hour\", value: \"2\", source: \"startRange\" }\n// Object { type: \"literal\", value: \":\", source: \"startRange\" }\n// Object { type: \"minute\", value: \"00\", source: \"startRange\" }\n// Object { type: \"literal\", value: \" – \", source: \"shared\" }\n// Object { type: \"hour\", value: \"3\", source: \"endRange\" }\n// Object { type: \"literal\", value: \":\", source: \"endRange\" }\n// Object { type: \"minute\", value: \"00\", source: \"endRange\" }\n// Object { type: \"literal\", value: \" \", source: \"shared\" }\n// Object { type: \"dayPeriod\", value: \"AM\", source: \"shared\" }\n\nSecuring the Gamepad API\nThere have been a few updates to the Gamepad API to fall in line with the spec. It is now only available in secure contexts (HTTPS) and is protected by Feature Policy: gamepad. If access to gamepads is disallowed, calls to Navigator.getGamepads() will throw an error and the gamepadconnected and gamepaddisconnected events will not fire.\n \nThe post Hopping on Firefox 91 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-08-10T15:04:27.000Z",
      "date_modified": "2021-08-10T15:04:27.000Z",
      "_plugin": {
        "pageFilename": "82e31040f1a7837b8949947d4d05dffb477763a16757f52969a94a2739b9a222.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47344",
      "url": "https://hacks.mozilla.org/2021/08/mdns-autocomplete-search/",
      "title": "How MDN’s autocomplete search works",
      "summary": "Last month, Gregor Weber and Peter Bengtsson added an autocomplete search to MDN Web Docs, that allows you to quickly jump straight to the document you're looking for by typing parts of the document title. This is the story about how that's implemented. \nThe post How MDN’s autocomplete search works appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Last month, <a href=\"https://github.com/Gregoor/\">Gregor Weber</a> and I added an autocomplete search to <a href=\"https://developer.mozilla.org\">MDN Web Docs</a>, that allows you to quickly jump straight to the document you&#8217;re looking for by typing parts of the document title. This is the story about how that&#8217;s implemented. If you stick around to the end, I&#8217;ll share an &#8220;easter egg&#8221; feature that, once you&#8217;ve learned it, will make you look really cool at dinner parties. Or, perhaps you just want to navigate MDN faster than mere mortals.</p>\n<p><img loading=\"lazy\" class=\"alignnone size-medium wp-image-47345\" src=\"https://hacks.mozilla.org/files/2021/07/mdn-autocomplete-1-250x462.png\" alt=\"MDN's autocomplete search in action\" width=\"250\" height=\"462\" srcset=\"https://hacks.mozilla.org/files/2021/07/mdn-autocomplete-1-250x462.png 250w, https://hacks.mozilla.org/files/2021/07/mdn-autocomplete-1-500x924.png 500w, https://hacks.mozilla.org/files/2021/07/mdn-autocomplete-1-768x1419.png 768w, https://hacks.mozilla.org/files/2021/07/mdn-autocomplete-1.png 816w\" sizes=\"(max-width: 250px) 100vw, 250px\" /></p>\n<p>In its simplest form, the input field has an <code>onkeypress</code> event listener that filters through a complete list of every single document title (per locale). At the time of writing, there are 11,690 different document titles (and their URLs) for English US. You can see a preview by opening <a href=\"https://developer.mozilla.org/en-US/search-index.json\">https://developer.mozilla.org/en-US/search-index.json</a>. Yes, it&#8217;s huge, but it&#8217;s not too huge to load all into memory. After all, together with the code that does the searching, it&#8217;s only loaded when the user has indicated intent to type something. And speaking of size, because the file is compressed with <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/brotli_compression\">Brotli</a>, the file is only 144KB over the network.</p>\n<h2>Implementation details</h2>\n<p>By default, the only JavaScript code that&#8217;s loaded is a small shim that watches for <code>onmouseover</code> and <code>onfocus</code> for the search <code>&lt;input&gt;</code> field. There&#8217;s also an event listener on the whole <code>document</code> that looks for a certain keystroke. Pressing <code>/</code> at any point, acts the same as if you had used your mouse cursor to put focus into the <code>&lt;input&gt;</code> field. As soon as focus is triggered, the first thing it does is download two JavaScript bundles which turns the <code>&lt;input&gt;</code> field into something much more advanced. In its simplest (pseudo) form, here&#8217;s how it works:</p>\n<pre class=\"html\"><code>&lt;input \n type=\"search\" \n name=\"q\"\n onfocus=\"startAutocomplete()\" \n onmouseover=\"startAutocomplete()\"\n placeholder=\"Site search...\" \n value=\"q\"&gt;</code></pre>\n<pre class=\"js\"><code>let started = false;\nfunction startAutocomplete() {\n  if (started) {\n    return false;\n  }\n  const script = document.createElement(\"script\");\n  script.src = \"/static/js/autocomplete.js\";\n  document.head.appendChild(script);\n}</code></pre>\n<p>Then it loads <code>/static/js/autocomplete.js</code> which is where the real magic happens. Let&#8217;s dig deeper with the pseudo code:</p>\n<pre class=\"js\"><code>(async function() {\n  const response = await fetch('/en-US/search-index.json');\n  const documents = await response.json();\n  \n  const inputValue = document.querySelector(\n    'input[type=\"search\"]'\n  ).value;\n  const flex = FlexSearch.create();\n  documents.forEach(({ title }, i) =&gt; {\n    flex.add(i, title);\n  });\n\n  const indexResults = flex.search(inputValue);\n  const foundDocuments = indexResults.map((index) =&gt; documents[index]);\n  displayFoundDocuments(foundDocuments.slice(0, 10));\n})();</code></pre>\n<p>As you can probably see, this is an oversimplification of how it actually works, but it&#8217;s not yet time to dig into the details. The next step is to display the matches. We use (TypeScript) React to do this, but the following pseudo code is easier to follow:</p>\n<pre class=\"js\"><code>function displayFoundResults(documents) {\n  const container = document.createElement(\"ul\");\n  documents.forEach(({url, title}) =&gt; {\n    const row = document.createElement(\"li\");\n    const link = document.createElement(\"a\");\n    link.href = url;\n    link.textContent = title;\n    row.appendChild(link);\n    container.appendChild(row);\n  });\n  document.querySelector('#search').appendChild(container);\n}\n</code></pre>\n<p>Then with some CSS, we just display this as an overlay just beneath the <code>&lt;input&gt;</code> field. For example, we highlight each <code>title</code> according to the <code>inputValue</code> and various keystroke event handlers take care of highlighting the relevant row when you navigate up and down.</p>\n<h2>Ok, let&#8217;s dig deeper into the implementation details</h2>\n<p>We create the <code>FlexSearch</code> index just <i>once</i> and <i>re-use it for every new keystroke</i>. Because the user might type more while waiting for the network, it&#8217;s actually reactive so executes the actual search once all the JavaScript and the JSON XHR have arrived.</p>\n<p>Before we dig into what this <code>FlexSearch</code> is, let&#8217;s talk about how the display actually works. For that we use a React library called <a href=\"https://www.npmjs.com/package/downshift\">downshift</a> which handles all the interactions, displays, and makes sure the displayed search results are accessible. <code>downshift</code> is a mature library that handles a myriad of challenges with building a widget like that, especially the aspects of making it accessible.</p>\n<p>So, what is this <a href=\"https://www.npmjs.com/package/flexsearch\"><code>FlexSearch</code> library</a>? It&#8217;s another third party that makes sure that searching on titles is done with natural language in mind. It describes itself as the &#8220;Web&#8217;s fastest and most memory-flexible full-text search library with zero dependencies.&#8221; which is a lot more performant and accurate than attempting to simply look for one string in a long list of other strings.</p>\n<h2>Deciding which result to show first</h2>\n<p>In fairness, if the user types <code>foreac</code>, it&#8217;s not that hard to reduce a list of 10,000+ document titles down to only those that contain <code>foreac</code> in the title, then we decide which result to show first. The way we implement that is relying on pageview stats. We record, for every single MDN URL, which one gets the most pageviews as a form of determining &#8220;popularity&#8221;. The documents that most people decide to arrive on are most probably what the user was searching for.</p>\n<p>Our <a href=\"https://github.com/mdn/yari\">build-process</a> that generates the <code>search-index.json</code> file knows about each URLs number of pageviews. We actually don&#8217;t care about absolute numbers, but what we do care about is the relative differences. For example, we know that <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/forEach\"><code>Array.prototype.forEach()</code></a> (that&#8217;s one of the document titles) is a more popular page than <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray/forEach\"><code>TypedArray.prototype.forEach()</code></a>, so we leverage that and sort the entries in <code>search-index.json</code> accordingly. Now, with <code>FlexSearch</code> doing the reduction, we use the &#8220;natural order&#8221; of the array as the trick that tries to give users the document they were probably looking for. It&#8217;s actually the same technique we use for <code>Elasticsearch</code> in our full site-search. More about that in: <a href=\"https://hacks.mozilla.org/2021/03/how-mdns-site-search-works/\">How MDN’s site-search works</a>.</p>\n<h2>The easter egg: How to search by URL</h2>\n<p>Actually, it&#8217;s not a whimsical easter egg, but a feature that came from the fact that this autocomplete needs to work for our content creators. You see, when you work on the <a href=\"https://github.com/mdn/content\">content in MDN</a> you start a local &#8220;preview server&#8221; which is a complete copy of all documents but all running locally, as a static site, under <code>http://localhost:5000</code>. There, you don&#8217;t want to rely on a server to do searches. Content authors need to quickly move between documents, so much of the reason why the autocomplete search is done entirely in the client is because of that.</p>\n<p>Commonly implemented in tools like the VSCode and Atom IDEs, you can do &#8220;fuzzy searches&#8221; to find and open files simply by typing portions of the file path. For example, searching for <code>whmlemvo</code> should find the file <code>files/<b>w</b>eb/<b>h</b>t<b>ml</b>/<b>e</b>lement/<b>v</b>ide<b>o</b></code>. You can do that with MDN&#8217;s autocomplete search too. The way you do it is by typing <code>/</code> as the first input character.</p>\n<p><img loading=\"lazy\" class=\"alignnone size-medium wp-image-47349\" src=\"https://hacks.mozilla.org/files/2021/07/Screen-Shot-2021-07-30-at-3.47.26-PM-250x217.png\" alt=\"Activate &quot;fuzzy search&quot; on MDN\" width=\"250\" height=\"217\" srcset=\"https://hacks.mozilla.org/files/2021/07/Screen-Shot-2021-07-30-at-3.47.26-PM-250x217.png 250w, https://hacks.mozilla.org/files/2021/07/Screen-Shot-2021-07-30-at-3.47.26-PM-500x435.png 500w, https://hacks.mozilla.org/files/2021/07/Screen-Shot-2021-07-30-at-3.47.26-PM-768x667.png 768w, https://hacks.mozilla.org/files/2021/07/Screen-Shot-2021-07-30-at-3.47.26-PM.png 794w\" sizes=\"(max-width: 250px) 100vw, 250px\" /></p>\n<p>It makes it really quick to jump straight to a document if you know its URL but don&#8217;t want to spell it out exactly.<br />\nIn fact, there&#8217;s another way to navigate and that is to first press <code>/</code> anywhere when browsing MDN, which activates the autocomplete search. Then you type <code>/</code> again, and you&#8217;re off to the races!</p>\n<h2>How to get really deep into the implementation details</h2>\n<p>The code for all of this is in the <a href=\"https://github.com/mdn/yari\">Yari repo</a> which is the project that builds and previews all of the <a href=\"https://github.com/mdn/content\">MDN content</a>. To find the exact code, click into the <a href=\"https://github.com/mdn/yari/blob/main/client/src/search.tsx\"><code>client/src/search.tsx</code></a> source code and you&#8217;ll find all the code for lazy-loading, searching, preloading, and displaying autocomplete searches.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/08/mdns-autocomplete-search/\">How MDN&#8217;s autocomplete search works</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Last month, Gregor Weber and I added an autocomplete search to MDN Web Docs, that allows you to quickly jump straight to the document you’re looking for by typing parts of the document title. This is the story about how that’s implemented. If you stick around to the end, I’ll share an “easter egg” feature that, once you’ve learned it, will make you look really cool at dinner parties. Or, perhaps you just want to navigate MDN faster than mere mortals.\n\nIn its simplest form, the input field has an onkeypress event listener that filters through a complete list of every single document title (per locale). At the time of writing, there are 11,690 different document titles (and their URLs) for English US. You can see a preview by opening https://developer.mozilla.org/en-US/search-index.json. Yes, it’s huge, but it’s not too huge to load all into memory. After all, together with the code that does the searching, it’s only loaded when the user has indicated intent to type something. And speaking of size, because the file is compressed with Brotli, the file is only 144KB over the network.\nImplementation details\nBy default, the only JavaScript code that’s loaded is a small shim that watches for onmouseover and onfocus for the search <input> field. There’s also an event listener on the whole document that looks for a certain keystroke. Pressing / at any point, acts the same as if you had used your mouse cursor to put focus into the <input> field. As soon as focus is triggered, the first thing it does is download two JavaScript bundles which turns the <input> field into something much more advanced. In its simplest (pseudo) form, here’s how it works:\n<input \n type=\"search\" \n name=\"q\"\n onfocus=\"startAutocomplete()\" \n onmouseover=\"startAutocomplete()\"\n placeholder=\"Site search...\" \n value=\"q\">\nlet started = false;\nfunction startAutocomplete() {\n  if (started) {\n    return false;\n  }\n  const script = document.createElement(\"script\");\n  script.src = \"/static/js/autocomplete.js\";\n  document.head.appendChild(script);\n}\nThen it loads /static/js/autocomplete.js which is where the real magic happens. Let’s dig deeper with the pseudo code:\n(async function() {\n  const response = await fetch('/en-US/search-index.json');\n  const documents = await response.json();\n  \n  const inputValue = document.querySelector(\n    'input[type=\"search\"]'\n  ).value;\n  const flex = FlexSearch.create();\n  documents.forEach(({ title }, i) => {\n    flex.add(i, title);\n  });\n\n  const indexResults = flex.search(inputValue);\n  const foundDocuments = indexResults.map((index) => documents[index]);\n  displayFoundDocuments(foundDocuments.slice(0, 10));\n})();\nAs you can probably see, this is an oversimplification of how it actually works, but it’s not yet time to dig into the details. The next step is to display the matches. We use (TypeScript) React to do this, but the following pseudo code is easier to follow:\nfunction displayFoundResults(documents) {\n  const container = document.createElement(\"ul\");\n  documents.forEach(({url, title}) => {\n    const row = document.createElement(\"li\");\n    const link = document.createElement(\"a\");\n    link.href = url;\n    link.textContent = title;\n    row.appendChild(link);\n    container.appendChild(row);\n  });\n  document.querySelector('#search').appendChild(container);\n}\n\nThen with some CSS, we just display this as an overlay just beneath the <input> field. For example, we highlight each title according to the inputValue and various keystroke event handlers take care of highlighting the relevant row when you navigate up and down.\nOk, let’s dig deeper into the implementation details\nWe create the FlexSearch index just once and re-use it for every new keystroke. Because the user might type more while waiting for the network, it’s actually reactive so executes the actual search once all the JavaScript and the JSON XHR have arrived.\nBefore we dig into what this FlexSearch is, let’s talk about how the display actually works. For that we use a React library called downshift which handles all the interactions, displays, and makes sure the displayed search results are accessible. downshift is a mature library that handles a myriad of challenges with building a widget like that, especially the aspects of making it accessible.\nSo, what is this FlexSearch library? It’s another third party that makes sure that searching on titles is done with natural language in mind. It describes itself as the “Web’s fastest and most memory-flexible full-text search library with zero dependencies.” which is a lot more performant and accurate than attempting to simply look for one string in a long list of other strings.\nDeciding which result to show first\nIn fairness, if the user types foreac, it’s not that hard to reduce a list of 10,000+ document titles down to only those that contain foreac in the title, then we decide which result to show first. The way we implement that is relying on pageview stats. We record, for every single MDN URL, which one gets the most pageviews as a form of determining “popularity”. The documents that most people decide to arrive on are most probably what the user was searching for.\nOur build-process that generates the search-index.json file knows about each URLs number of pageviews. We actually don’t care about absolute numbers, but what we do care about is the relative differences. For example, we know that Array.prototype.forEach() (that’s one of the document titles) is a more popular page than TypedArray.prototype.forEach(), so we leverage that and sort the entries in search-index.json accordingly. Now, with FlexSearch doing the reduction, we use the “natural order” of the array as the trick that tries to give users the document they were probably looking for. It’s actually the same technique we use for Elasticsearch in our full site-search. More about that in: How MDN’s site-search works.\nThe easter egg: How to search by URL\nActually, it’s not a whimsical easter egg, but a feature that came from the fact that this autocomplete needs to work for our content creators. You see, when you work on the content in MDN you start a local “preview server” which is a complete copy of all documents but all running locally, as a static site, under http://localhost:5000. There, you don’t want to rely on a server to do searches. Content authors need to quickly move between documents, so much of the reason why the autocomplete search is done entirely in the client is because of that.\nCommonly implemented in tools like the VSCode and Atom IDEs, you can do “fuzzy searches” to find and open files simply by typing portions of the file path. For example, searching for whmlemvo should find the file files/web/html/element/video. You can do that with MDN’s autocomplete search too. The way you do it is by typing / as the first input character.\n\nIt makes it really quick to jump straight to a document if you know its URL but don’t want to spell it out exactly.\nIn fact, there’s another way to navigate and that is to first press / anywhere when browsing MDN, which activates the autocomplete search. Then you type / again, and you’re off to the races!\nHow to get really deep into the implementation details\nThe code for all of this is in the Yari repo which is the project that builds and previews all of the MDN content. To find the exact code, click into the client/src/search.tsx source code and you’ll find all the code for lazy-loading, searching, preloading, and displaying autocomplete searches.\nThe post How MDN’s autocomplete search works appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-08-03T15:49:30.000Z",
      "date_modified": "2021-08-03T15:49:30.000Z",
      "_plugin": {
        "pageFilename": "39f78184143f1c68bcfe883305b9e9e5c35cbdae1d6312ae0b23cd66a1bcdc42.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47324",
      "url": "https://hacks.mozilla.org/2021/07/spring-cleaning-mdn-part-1/",
      "title": "Spring Cleaning MDN: Part 1",
      "summary": "As we’re all aware by now, we made some big platform changes at the end of 2020. Whilst the big move has happened, it’s given us a great opportunity to clear out the cupboards and closets. \nThe post Spring Cleaning MDN: Part 1 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>As we’re all aware by now, <a href=\"https://hacks.mozilla.org/2020/12/welcome-yari-mdn-web-docs-has-a-new-platform\">we made some big platform changes at the end of 2020</a>. Whilst the big move has happened, its given us a great opportunity to clear out the cupboards and closets.</p>\r\n<div id=\"attachment_47339\" style=\"width: 610px\" class=\"wp-caption alignnone\"><img aria-describedby=\"caption-attachment-47339\" loading=\"lazy\" class=\"wp-image-47339 size-full\" src=\"https://hacks.mozilla.org/files/2021/07/mdn_spring_cleaning.png\" alt=\"An illustration of a salmon coloured dinosaur sweeping with a broom\" width=\"600\" height=\"401\" srcset=\"https://hacks.mozilla.org/files/2021/07/mdn_spring_cleaning.png 600w, https://hacks.mozilla.org/files/2021/07/mdn_spring_cleaning-250x167.png 250w, https://hacks.mozilla.org/files/2021/07/mdn_spring_cleaning-500x334.png 500w\" sizes=\"(max-width: 600px) 100vw, 600px\" /><p id=\"caption-attachment-47339\" class=\"wp-caption-text\"><em>                                  Illustration by <a href=\"https://www.darylalexsy.net/\">Daryl Alexsy</a></em></p></div>\r\n<p>&nbsp;</p>\r\n<p>Most notably MDN now manages its content from a <a href=\"https://github.com/mdn/content\">repository on GitHub</a>. Prior to this, the content was stored in a database and edited by logging in to the site and modifying content via an in-page (WYSIWYG) editor, aka ‘The Wiki’. Since the big move, we have determined that MDN accounts are no longer functional for our users. If you want to edit or contribute content, you need to sign in to GitHub, not MDN.</p>\r\n<p>Due to this, we’ll be removing the account functionality and removing all of the account data from our database. This is consistent with our <a href=\"https://www.mozilla.org/en-US/about/policy/lean-data/\">Lean Data Practices principles</a> and our commitment to user privacy. We also have the perfect opportunity to be doing this now, as we’re moving our database from MySQL to PostgreSQL this week.</p>\r\n<p>Accounts will be disabled on MDN on Thursday, 22nd July.</p>\r\n<p>Don’t worry though &#8211; you can still contribute to MDN! That hasn’t changed. All the information on how to help is <a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute\">here in this guide</a>.</p>\r\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/07/spring-cleaning-mdn-part-1/\">Spring Cleaning MDN: Part 1</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "As we’re all aware by now, we made some big platform changes at the end of 2020. Whilst the big move has happened, its given us a great opportunity to clear out the cupboards and closets.\n                                  Illustration by Daryl Alexsy\n \nMost notably MDN now manages its content from a repository on GitHub. Prior to this, the content was stored in a database and edited by logging in to the site and modifying content via an in-page (WYSIWYG) editor, aka ‘The Wiki’. Since the big move, we have determined that MDN accounts are no longer functional for our users. If you want to edit or contribute content, you need to sign in to GitHub, not MDN.\nDue to this, we’ll be removing the account functionality and removing all of the account data from our database. This is consistent with our Lean Data Practices principles and our commitment to user privacy. We also have the perfect opportunity to be doing this now, as we’re moving our database from MySQL to PostgreSQL this week.\nAccounts will be disabled on MDN on Thursday, 22nd July.\nDon’t worry though – you can still contribute to MDN! That hasn’t changed. All the information on how to help is here in this guide.\nThe post Spring Cleaning MDN: Part 1 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-07-20T16:00:48.000Z",
      "date_modified": "2021-07-20T16:00:48.000Z",
      "_plugin": {
        "pageFilename": "eb35cd6a83bdd1ce7200b98c9ce2bd35b3b4ecb59d99d6676a8b116473efe682.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47323",
      "url": "https://hacks.mozilla.org/2021/07/getting-lively-with-firefox-90/",
      "title": "Getting lively with Firefox 90",
      "summary": "As the summer rolls around for those of us in the northern hemisphere, temperatures are high and unwinding with a cool ice tea is high on the agenda. Isn't it lucky then that Background Update is here for Windows, which means Firefox can update even if it's not running. We can just sit back and relax!\nAlso this release we see a few nice JavaScript additions, including private fields and methods for classes, and the at() method for Array, String and TypedArray global objects.\nThis blog post just provides a set of highlights.\nThe post Getting lively with Firefox 90 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<h1><b>Getting lively with Firefox 90</b></h1>\n<p>As the summer rolls around for those of us in the northern hemisphere, temperatures are high and unwinding with a cool ice tea is high on the agenda. Isn&#8217;t it lucky then that Background Update is here for Windows, which means Firefox can update even if it&#8217;s not running. We can just sit back and relax!</p>\n<p>Also this release we see a few nice JavaScript additions, including <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes/Private_class_fields\">private fields and methods for classes</a>, and the at() method for <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/at\">Array</a>, <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/at\">String</a> and <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray/at\">TypedArray</a> global objects.</p>\n<p>This blog post just provides a set of highlights; for all the details, check out the following:</p>\n<ul>\n<li aria-level=\"1\"><a href=\"https://developer.mozilla.org/docs/Mozilla/Firefox/Releases/90\">Firefox 90 for developers on MDN</a></li>\n<li aria-level=\"1\"><a href=\"https://www.mozilla.org/en-US/firefox/90.0/releasenotes/\">Firefox 90 end-user release notes</a></li>\n</ul>\n<h2><b>Classes go private</b></h2>\n<p>A feature JavaScript has lacked since its inception,<a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes/Private_class_fields\"> private fields and methods</a> are now enabled by default In Firefox 90. This allows you to declare private properties within a class. You can not reference these private properties from outside of the class; they can only be read or written within the class body.</p>\n<p>Private names must be prefixed with a &#8216;hash mark&#8217; (#) to distinguish them from any public properties a class might hold.</p>\n<p>This shows how to declare private fields as opposed to public ones within a class:</p>\n<pre><code class=\"language-js\">class ClassWithPrivateProperties {\n\n  #privateField;\n  publicField;\n\n  constructor() {\n\n    // can be referenced within the class, but not accessed outside\n    this.#privateField = 42;\n\n    // can be referenced within the class aswell as outside\n    this.publicField = 52;\n}\n\n  // again, can only be used within the class\n  #privateMethod() {\n    return 'hello world';\n  }\n\n  // can be called when using the class\n  getPrivateMessage() {\n    return this.#privateMethod();\n  }\n}</code></pre>\n<p>Static fields and methods can also be private. For a more detailed overview and explanation, check out the great guide:<a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Working_With_Private_Class_Features\"> Working with private class features</a>. You can also read what it takes to implement such a feature in our <a href=\"https://hacks.mozilla.org/2021/06/implementing-private-fields-for-javascript/\">previous blog post Implementing Private Fields for JavaScript</a>.</p>\n<h2><b>JavaScript at() method</b></h2>\n<p>The relative indexing method at() has been added to the <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/at\">Array</a>, <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/at\">String</a> and <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray/at\">TypedArray</a> global objects.</p>\n<p>Passing a positive integer to the method returns the item or character at that position. However the highlight with this method, is that it also accepts negative integers. These count back from the end of the array or string. For example, 1 would return the second item or character and -1 would return the last item or character.</p>\n<p>This example declares an array of values and uses the at() method to select an item in that array <i>from the end</i>.</p>\n<pre><code class=\"language-js\">const myArray = [5, 12, 8, 130, 44];\n\nlet arrItem = myArray.at(-2);\n\n// arrItem = 130\n</code></pre>\n<p>It&#8217;s worth mentioning there <i>are</i> other common ways of doing this, however this one looks quite neat.</p>\n<h2><b>Conic gradients for Canvas</b></h2>\n<p>The <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API\">2D Canvas API</a> has a new <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/createConicGradient\">createConicGradient()</a> method, which creates a gradient <i>around</i> a point (rather than from it, like <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/createRadialGradient\">createRadialGradient()</a> ). This feature allows you to specify where you want the center to be and in which direction the gradient should start. You then add the colours you want and where they should begin (and end).</p>\n<p>This example creates a conic gradient with 5 colour stops, which we use to fill a rectangle.</p>\n<pre><code class=\"language-js\">var canvas = document.getElementById('canvas');\n\nvar ctx = canvas.getContext('2d');\n\n// Create a conic gradient\n// The start angle is 0\n// The centre position is 100, 100\nvar gradient = ctx.createConicGradient(0, 100, 100);\n\n// Add five color stops\ngradient.addColorStop(0, \"red\");\ngradient.addColorStop(0.25, \"orange\");\ngradient.addColorStop(0.5, \"yellow\");\ngradient.addColorStop(0.75, \"green\");\ngradient.addColorStop(1, \"blue\");\n\n// Set the fill style and draw a rectangle\nctx.fillStyle = gradient;\nctx.fillRect(20, 20, 200, 200);\n</code></pre>\n<p>The result looks like this:</p>\n<p><img loading=\"lazy\" class=\"alignnone size-medium wp-image-47325\" src=\"https://hacks.mozilla.org/files/2021/07/Screenshot-2021-07-08-at-10.38.43-250x183.png\" alt=\"Rainbow radial gradient\" width=\"250\" height=\"183\" srcset=\"https://hacks.mozilla.org/files/2021/07/Screenshot-2021-07-08-at-10.38.43-250x183.png 250w, https://hacks.mozilla.org/files/2021/07/Screenshot-2021-07-08-at-10.38.43.png 492w\" sizes=\"(max-width: 250px) 100vw, 250px\" /></p>\n<h2><b>New Request Headers</b></h2>\n<p>Fetch metadata request headers provide information about the context from which a request originated. This allows the server to make decisions about whether a request should be allowed based on where the request came from and how the resource will be used. Firefox 90 enables the following by default:</p>\n<ul>\n<li aria-level=\"1\"><a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Sec-Fetch-Site\">Sec-Fetch-Site</a></li>\n<li aria-level=\"1\"><a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Sec-Fetch-Mode\">Sec-Fetch-Mode</a></li>\n<li aria-level=\"1\"><a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Sec-Fetch-User\">Sec-Fetch-User</a></li>\n<li aria-level=\"1\"><a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Sec-Fetch-Dest\">Sec-Fetch-Dest</a></li>\n</ul>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/07/getting-lively-with-firefox-90/\">Getting lively with Firefox 90</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Getting lively with Firefox 90\nAs the summer rolls around for those of us in the northern hemisphere, temperatures are high and unwinding with a cool ice tea is high on the agenda. Isn’t it lucky then that Background Update is here for Windows, which means Firefox can update even if it’s not running. We can just sit back and relax!\nAlso this release we see a few nice JavaScript additions, including private fields and methods for classes, and the at() method for Array, String and TypedArray global objects.\nThis blog post just provides a set of highlights; for all the details, check out the following:\n\nFirefox 90 for developers on MDN\nFirefox 90 end-user release notes\n\nClasses go private\nA feature JavaScript has lacked since its inception, private fields and methods are now enabled by default In Firefox 90. This allows you to declare private properties within a class. You can not reference these private properties from outside of the class; they can only be read or written within the class body.\nPrivate names must be prefixed with a ‘hash mark’ (#) to distinguish them from any public properties a class might hold.\nThis shows how to declare private fields as opposed to public ones within a class:\nclass ClassWithPrivateProperties {\n\n  #privateField;\n  publicField;\n\n  constructor() {\n\n    // can be referenced within the class, but not accessed outside\n    this.#privateField = 42;\n\n    // can be referenced within the class aswell as outside\n    this.publicField = 52;\n}\n\n  // again, can only be used within the class\n  #privateMethod() {\n    return 'hello world';\n  }\n\n  // can be called when using the class\n  getPrivateMessage() {\n    return this.#privateMethod();\n  }\n}\nStatic fields and methods can also be private. For a more detailed overview and explanation, check out the great guide: Working with private class features. You can also read what it takes to implement such a feature in our previous blog post Implementing Private Fields for JavaScript.\nJavaScript at() method\nThe relative indexing method at() has been added to the Array, String and TypedArray global objects.\nPassing a positive integer to the method returns the item or character at that position. However the highlight with this method, is that it also accepts negative integers. These count back from the end of the array or string. For example, 1 would return the second item or character and -1 would return the last item or character.\nThis example declares an array of values and uses the at() method to select an item in that array from the end.\nconst myArray = [5, 12, 8, 130, 44];\n\nlet arrItem = myArray.at(-2);\n\n// arrItem = 130\n\nIt’s worth mentioning there are other common ways of doing this, however this one looks quite neat.\nConic gradients for Canvas\nThe 2D Canvas API has a new createConicGradient() method, which creates a gradient around a point (rather than from it, like createRadialGradient() ). This feature allows you to specify where you want the center to be and in which direction the gradient should start. You then add the colours you want and where they should begin (and end).\nThis example creates a conic gradient with 5 colour stops, which we use to fill a rectangle.\nvar canvas = document.getElementById('canvas');\n\nvar ctx = canvas.getContext('2d');\n\n// Create a conic gradient\n// The start angle is 0\n// The centre position is 100, 100\nvar gradient = ctx.createConicGradient(0, 100, 100);\n\n// Add five color stops\ngradient.addColorStop(0, \"red\");\ngradient.addColorStop(0.25, \"orange\");\ngradient.addColorStop(0.5, \"yellow\");\ngradient.addColorStop(0.75, \"green\");\ngradient.addColorStop(1, \"blue\");\n\n// Set the fill style and draw a rectangle\nctx.fillStyle = gradient;\nctx.fillRect(20, 20, 200, 200);\n\nThe result looks like this:\n\nNew Request Headers\nFetch metadata request headers provide information about the context from which a request originated. This allows the server to make decisions about whether a request should be allowed based on where the request came from and how the resource will be used. Firefox 90 enables the following by default:\n\nSec-Fetch-Site\nSec-Fetch-Mode\nSec-Fetch-User\nSec-Fetch-Dest\n\nThe post Getting lively with Firefox 90 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-07-13T15:02:10.000Z",
      "date_modified": "2021-07-13T15:02:10.000Z",
      "_plugin": {
        "pageFilename": "1c339cbd6235841fa16f9d2ceec98ff7736e43717ed95bc9959c672d106c0b5e.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47312",
      "url": "https://hacks.mozilla.org/2021/06/implementing-private-fields-for-javascript/",
      "title": "Implementing Private Fields for JavaScript",
      "summary": "When implementing a language feature for JavaScript, an implementer must make decisions about how the language in the specification maps to the implementation. Private fields is an example of where the specification language and implementation reality diverge, at least in SpiderMonkey– the JavaScript engine which powers Firefox. To understand more, I’ll explain what private fields are, a couple of models for thinking about them, and explain why our implementation diverges from the specification language.\nThe post Implementing Private Fields for JavaScript appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p><em>This post is <a href=\"https://www.mgaudet.ca/technical/2021/5/4/implementing-private-fields-for-javascript\">cross-posted from Matthew Gaudet’s blog</a></em></p>\n<p>When implementing a language feature for JavaScript, an implementer must make decisions about how the language in the specification maps to the implementation. Sometimes this is fairly simple, where the specification and implementation can share much of the same terminology and algorithms. Other times, pressures in the implementation make it more challenging, requiring or pressuring the implementation strategy diverge to diverge from the language specification.</p>\n<p>Private fields is an example of where the specification language and implementation reality diverge, at least in <a href=\"https://spidermonkey.dev/\">SpiderMonkey</a>– the JavaScript engine which powers Firefox. To understand more, I’ll explain what private fields are, a couple of models for thinking about them, and explain why our implementation diverges from the specification language.</p>\n<h2 id=\"private-fields\">Private Fields</h2>\n<p>Private fields are a language feature being added to the JavaScript language through the <a href=\"https://tc39.es/\">TC39</a> <a href=\"https://tc39.es/process-document/\">proposal process</a>, as part of the <a href=\"https://github.com/tc39/proposal-class-fields\">class fields proposal</a>, which is at Stage 4 in the TC39 process. We will ship private fields and private methods in Firefox 90.</p>\n<p>The private fields proposal adds a strict notion of ‘private state’ to the language. In the following example, <code>#x</code> may only be accessed by instances of class <code>A</code>:</p>\n<pre class=\"js\"><code>class A {\n  #x = 10;\n}</code></pre>\n<p>This means that outside of the class, it is impossible to access that field. Unlike public fields for example, as the following example shows:</p>\n<pre class=\"js\"><code>class A {\n  #x = 10; // Private field\n  y = 12; // Public Field\n}\n\nvar a = new A();\na.y; // Accessing public field y: OK\na.#x; // Syntax error: reference to undeclared private field</code></pre>\n<p>Even various other tools that JavaScript gives you for interrogating objects are prevented from accessing private fields (e.g. <code>Object.getOwnProperty{Symbols,Names}</code> don’t list private fields; there’s no way to use <code>Reflect.get</code> to access them).</p>\n<h2 id=\"a-feature-three-ways\">A Feature Three Ways</h2>\n<p>When talking about a feature in JavaScript, there are often three different aspects in play: the mental model, the specification, and the implementation.</p>\n<p>The mental model provides the high-level thinking that we expect programmers to use mostly. The specification in turn provides the detail of the semantics required by the feature. The implementation can look wildly different from the specification text, so long as the specification semantics are maintained.</p>\n<p>These three aspects shouldn’t produce different results for people reasoning through things (though, sometimes a ‘mental model’ is shorthand, and doesn’t accurately capture semantics in edge case scenarios).</p>\n<p>We can look at private fields using these three aspects:</p>\n<h3 id=\"mental-model\">Mental Model</h3>\n<p>The most basic mental model one can have for private fields is what it says on the tin: <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes/Public_class_fields\">fields</a>, but private. Now, JS fields become properties on objects, so the mental model is perhaps ‘properties that can’t be accessed from outside the class’.</p>\n<p>However, when we encounter proxies, this mental model breaks down a bit; trying to specify the semantics for ‘hidden properties’ and proxies <a href=\"https://github.com/zenparsing/es-abstract-refs/issues/11#issuecomment-65723350\">is challenging</a> (what happens when a Proxy is trying to provide access control to properties, if you aren’t supposed to be able see private fields with Proxies? Can subclasses access private fields? Do private fields participate in prototype inheritance?) . In order to preserve the desired privacy properties an alternative mental model became the way the committee thinks about private fields.</p>\n<p>This alternative model is called the ‘WeakMap’ model. In this mental model you imagine that each class has a hidden weak map associated with each private field, such that you could hypothetically <a href=\"https://en.wikipedia.org/wiki/Syntactic_sugar\">‘desugar’</a></p>\n<pre class=\"js\"><code>class A {\n  #x = 15;\n  g() {\n    return this.#x;\n  }\n}</code></pre>\n<p>into something like</p>\n<pre class=\"js\"><code>class A_desugared {\n  static InaccessibleWeakMap_x = new WeakMap();\n  constructor() {\n    A_desugared.InaccessibleWeakMap_x.set(this, 15);\n  }\n\n  g() {\n    return A_desugared.InaccessibleWeakMap_x.get(this);\n  }\n}</code></pre>\n<p>The <code>WeakMap</code> model is, surprisingly, not how the feature is written in the specification, but is an important part of the design intention is behind them. I will cover a bit later how this mental model shows up in places later.</p>\n<h3 id=\"specification\">Specification</h3>\n<p>The actual specification changes are provided by the <a href=\"https://github.com/tc39/proposal-class-fields\">class fields proposal</a>, specifically the <a href=\"https://tc39.es/proposal-class-fields/\">changes to the specification text</a>. I won’t cover every piece of this specification text, but I’ll call out specific aspects to help elucidate the differences between specification text and implementation.</p>\n<p>First, the specification adds the notion of <a href=\"https://tc39.es/proposal-class-fields/#sec-private-names\"><code>[[PrivateName]]</code></a>, which is a globally unique field identifier. This global uniqueness is to ensure that two classes cannot access each other’s fields merely by having the same name.</p>\n<pre class=\"js\"><code>function createClass() {\n  return class {\n    #x = 1;\n    static getX(o) {\n      return o.#x;\n    }\n  };\n}\n\nlet [A, B] = [0, 1].map(createClass);\nlet a = new A();\nlet b = new B();\n\nA.getX(a); // Allowed: Same class\nA.getX(b); // Type Error, because different class.</code></pre>\n<p>The specification also adds a new <a href=\"https://tc39.es/ecma262/#sec-object-internal-methods-and-internal-slots\">‘internal slot’</a>, which is a specification level piece of internal state associated with an object in the spec, called <a href=\"https://tc39.es/proposal-class-fields/#sec-private-names\"><code>[[PrivateFieldValues]]</code></a> to all objects. <code>[[PrivateFieldValues]]</code> is a list of records of the form:</p>\n<pre><code>{\n  [[PrivateName]]: Private Name,\n  [[PrivateFieldValue]]: ECMAScript value\n}</code></pre>\n<p>To manipulate this list, the specification adds four new algorithms:</p>\n<ol type=\"1\">\n<li><a href=\"https://tc39.es/proposal-class-fields/#sec-privatefieldfind\"><code>PrivateFieldFind</code></a></li>\n<li><a href=\"https://tc39.es/proposal-class-fields/#sec-privatefieldadd\"><code>PrivateFieldAdd</code></a></li>\n<li><a href=\"https://tc39.es/proposal-class-fields/#sec-privatefieldget\"><code>PrivateFieldGet</code></a></li>\n<li><a href=\"https://tc39.es/proposal-class-fields/#sec-privatefieldset\"><code>PrivateFieldSet</code></a></li>\n</ol>\n<p>These algorithms largely work as you would expect: <code>PrivateFieldAdd</code> appends an entry to the list (though, in the interest of trying to provide errors eagerly, if a matching Private Name already exists in the list, it will throw a <code>TypeError</code>. I’ll show how that can happen later). <code>PrivateFieldGet</code> retrieves a value stored in the list, keyed by a given Private name, etc.</p>\n<h4 id=\"the-constructor-override-trick\">The Constructor Override Trick</h4>\n<p>When I first started to read the specification, I was surprised to see that <code>PrivateFieldAdd</code> could throw. Given that it was only called from a constructor on the object being constructed, I had fully expected that the object would be freshly created, and therefore you’d not need to worry about a field already being there.</p>\n<p>This turns out to be possible, <a href=\"https://www.mgaudet.ca/technical/2020/7/24/investigating-the-return-behaviour-of-js-constructors\">a side effect of some of the specification’s handling of constructor return values</a>. To be more concrete, the following is an example provided to me by André Bargull, which shows this in action.</p>\n<pre class=\"js\"><code>class Base {\n  constructor(o) {\n    return o; // Note: We are returning the argument!\n  }\n}\n\nclass Stamper extends Base {\n  #x = \"stamped\";\n  static getX(o) {\n    return o.#x;\n  }\n}</code></pre>\n<p><code>Stamper</code> is a class which can ‘stamp’ its private field onto any object:</p>\n<pre class=\"js\"><code>let obj = {};\nnew Stamper(obj); // obj now has private field #x\nStamper.getX(obj); // =&gt; \"stamped\"</code></pre>\n<p>This means that when we add private fields to an object we cannot assume it doesn’t have them already. This is where the pre-existence check in <code>PrivateFieldAdd</code> comes into play:</p>\n<pre class=\"js\"><code>let obj2 = {};\nnew Stamper(obj2);\nnew Stamper(obj2); // Throws 'TypeError' due to pre-existence of private field</code></pre>\n<p>This ability to stamp private fields into arbitrary objects interacts with the WeakMap model a bit here as well. For example, given that you can stamp private fields onto any object, that means you could also stamp a private field onto a sealed object:</p>\n<pre><code>var obj3 = {};\nObject.seal(obj3);\nnew Stamper(obj3);\nStamper.getX(obj3); // =&gt; \"stamped\"</code></pre>\n<p>If you imagine private fields as properties, this is uncomfortable, because it means you’re modifying an object that was sealed by a programmer to future modification. However, using the weak map model, it is totally acceptable, as you’re only using the sealed object as a key in the weak map.</p>\n<p>PS: Just because you <em>can</em> stamp private fields into arbitrary objects, doesn’t mean you <em>should</em>: Please don’t do this.</p>\n<h3 id=\"implementing-the-specification\">Implementing the Specification</h3>\n<p>When faced with implementing the specification, there is a tension between following the letter of the specification, and doing something different to improve the implementation on some dimension.</p>\n<p>Where it is possible to implement the steps of the specification directly, we prefer to do that, as it makes maintenance of features easier as specification changes are made. SpiderMonkey does this in many places. You will see sections of code that are transcriptions of specification algorithms, <a href=\"https://searchfox.org/mozilla-central/rev/3434a9df60373a997263107e6f124fb164ddebf2/js/src/vm/JSFunction.cpp#1143-1210\">with step</a> <a href=\"https://searchfox.org/mozilla-central/rev/3434a9df60373a997263107e6f124fb164ddebf2/js/src/builtin/Array.js#7-47\">numbers for comments</a>. Following the exact letter of the specification can also be helpful where the specification is highly complex and small divergences can lead to compatibility risks.</p>\n<p>Sometimes however, there are good reasons to diverge from the specification language. JavaScript implementations have been honed for high performance for years, and there are many implementation tricks that have been applied to make that happen. Sometimes recasting a part of the specification in terms of code already written is the right thing to do, because that means the new code is also able to have the performance characteristics of the already written code.</p>\n<h4 id=\"implementing-private-names\">Implementing Private Names</h4>\n<p>The specification language for Private Names already almost matches the semantics around <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Symbol\"><code>Symbols</code></a>, which already exist in SpiderMonkey. So adding <code>PrivateNames</code> as a special kind of <code>Symbol</code> is a fairly easy choice.</p>\n<h4 id=\"implementing-private-fields\">Implementing Private Fields</h4>\n<p>Looking at the specification for private fields, the specification implementation would be to add an extra hidden slot to every object in SpiderMonkey, which contains a reference to a list of <code>{PrivateName, Value}</code> pairs. However, implementing this directly has a number of clear downsides:</p>\n<ul>\n<li>It adds memory usage to objects without private fields</li>\n<li>It requires invasive addition of either new bytecodes or complexity to performance sensitive property access paths.</li>\n</ul>\n<p>An alternative option is to diverge from the specification language, and implement only the semantics, not the actual specification algorithms. In the majority of cases, you really <em>can</em> think of private fields as special properties on objects that are hidden from reflection or introspection outside a class.</p>\n<p>If we model private fields as properties, rather than a special side-list that is maintained with an object, we are able to take advantage of the fact that property manipulation is already extremely optimized in a JavaScript engine.</p>\n<p>However, properties are subject to reflection. So if we model private fields as object properties, we need to ensure that reflection APIs don’t reveal them, and that you can’t get access to them via Proxies.</p>\n<p>In SpiderMonkey, we elected to implement private fields as hidden properties in order to take advantage of all the optimized machinery that already exists for properties in the engine. When I started implementing this feature André Bargull – a SpiderMonkey contributor for many years – actually handed me a series of patches that had a good chunk of the private fields implementation already done, for which I was hugely grateful.</p>\n<p>Using our special PrivateName symbols, we effectively desuagar</p>\n<pre class=\"js\"><code>class A {\n  #x = 10;\n  x() {\n    return this.#x;\n  }\n}</code></pre>\n<p>to something that looks closer to</p>\n<pre class=\"js\"><code>class A_desugared {\n  constructor() {\n    this[PrivateSymbol(#x)] = 10;\n  }\n  x() {\n    return this[PrivateSymbol(#x)];\n  }\n}</code></pre>\n<p>Private fields have slightly different semantics than properties however. They are designed to issue errors on patterns expected to be programming mistakes, rather than silently accepting it. For example:</p>\n<ol type=\"1\">\n<li>Accessing an a property on an object that doesn’t have it returns <code>undefined</code>. Private fields are specified to throw a <code>TypeError</code>, as a result of the <a href=\"https://tc39.es/proposal-class-fields/#sec-privatefieldget\"><code>PrivateFieldGet</code> algorithm</a>.</li>\n<li>Setting a property on an object that doesn’t have it simply adds the property. Private fields will throw a <code>TypeError</code> in <a href=\"https://tc39.es/proposal-class-fields/#sec-privatefieldset\"><code>PrivateFieldSet</code></a>.</li>\n<li>Adding a private field to an object that already has that field also throws a <code>TypeError</code> in <a href=\"https://tc39.es/proposal-class-fields/#sec-privatefieldadd\"><code>PrivateFieldAdd</code></a>. See “The Constructor Override Trick” above for how this can happen.</li>\n</ol>\n<p>To handle the different semantics, we modified the bytecode emission for private field accesses. We added a new bytecode op, <code>CheckPrivateField</code> which verifies an object has the correct state for a given private field. This means throwing an exception if the property is missing or present, as appropriate for Get/Set or Add. <code>CheckPrivateField</code> is emitted just before using the regular ‘computed property name’ path (the one used for <code>A[someKey]</code>).</p>\n<p><code>CheckPrivateField</code> is designed such that we can easily implement an <a href=\"https://www.mgaudet.ca/technical/2018/6/5/an-inline-cache-isnt-just-a-cache\">inline cache</a> using <a href=\"https://jandemooij.nl/blog/2017/01/25/cacheir/\">CacheIR</a>. Since we are storing private fields as properties, we can use the Shape of an object as a guard, and simply return the appropriate boolean value. The Shape of an object in SpiderMonkey determines what properties it has, and where they are located in the storage for that object. Objects that have the same shape are guaranteed to have the same properties, and it’s a perfect check for an IC for <code>CheckPrivateField</code>.</p>\n<p>Other modifications we made to make to the engine include omitting private fields from the property enumeration protocol, and allowing the extension of sealed objects if we are adding private field.</p>\n<h2 id=\"proxies\">Proxies</h2>\n<p>Proxies presented us a bit of a new challenge. Concretely, using the <code>Stamper</code> class above, you can add a private field directly to a Proxy:</p>\n<pre><code>let obj3 = {};\nlet proxy = new Proxy(obj3, handler);\nnew Stamper(proxy)\n\nStamper.getX(proxy) // =&gt; \"stamped\"\nStamper.getX(obj3)  // TypeError, private field is stamped\n                    // onto the Proxy Not the target!</code></pre>\n<p>I definitely found this surprising initially. The reason I found this surprising was I had expected that, like other operations, the addition of a private field would tunnel through the proxy to the target. However, once I was able to internalize the WeakMap mental model, I was able to understand this example much better. The trick is that in the WeakMap model, it is the <code>Proxy</code>, not the target object, used as the key in the <code>#x</code> WeakMap.</p>\n<p>These semantics presented a challenge to our implementation choice to model private fields as hidden properties however, as SpiderMonkey’s Proxies are highly specialized objects that do not have room for arbitrary properties. In order to support this case, we added a new reserved slot for an ‘expando’ object. The expando is an object allocated lazily that acts as the holder for dynamically added properties on the proxy. This pattern is used already for DOM objects, which are typically implemented as C++ objects with no room for extra properties. So if you write <code>document.foo = \"hi\"</code>, this allocates an expando object for <code>document</code>, and puts the <code>foo</code> property and value in there instead. Returning to private fields, when <code>#x</code> is accessed on a Proxy, the proxy code knows to go and look in the expando object for that property.</p>\n<h2 id=\"in-conclusion\">In Conclusion</h2>\n<p>Private Fields is an instance of implementing a JavaScript language feature where directly implementing the specification as written would be less performant than re-casting the specification in terms of already optimized engine primitives. Yet, that recasting itself can require some problem solving not present in the specification.</p>\n<p>At the end, I am fairly happy with the choices made for our implementation of Private Fields, and am excited to see it finally enter the world!</p>\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n<p>I have to thank, again, André Bargull, who provided the first set of patches and laid down an excellent trail for me to follow. His work made finishing private fields much easier, as he’d already put a lot of thought into decision making.</p>\n<p>Jason Orendorff has been an excellent and patient mentor as I have worked through this implementation, including two separate implementations of the private field bytecode, as well as two separate implementations of proxy support.</p>\n<p>Thanks to Caroline Cullen, and Iain Ireland for helping to read drafts of this post, and to Steve Fink for fixing many typos.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/06/implementing-private-fields-for-javascript/\">Implementing Private Fields for JavaScript</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "This post is cross-posted from Matthew Gaudet’s blog\nWhen implementing a language feature for JavaScript, an implementer must make decisions about how the language in the specification maps to the implementation. Sometimes this is fairly simple, where the specification and implementation can share much of the same terminology and algorithms. Other times, pressures in the implementation make it more challenging, requiring or pressuring the implementation strategy diverge to diverge from the language specification.\nPrivate fields is an example of where the specification language and implementation reality diverge, at least in SpiderMonkey– the JavaScript engine which powers Firefox. To understand more, I’ll explain what private fields are, a couple of models for thinking about them, and explain why our implementation diverges from the specification language.\nPrivate Fields\nPrivate fields are a language feature being added to the JavaScript language through the TC39 proposal process, as part of the class fields proposal, which is at Stage 4 in the TC39 process. We will ship private fields and private methods in Firefox 90.\nThe private fields proposal adds a strict notion of ‘private state’ to the language. In the following example, #x may only be accessed by instances of class A:\nclass A {\n  #x = 10;\n}\nThis means that outside of the class, it is impossible to access that field. Unlike public fields for example, as the following example shows:\nclass A {\n  #x = 10; // Private field\n  y = 12; // Public Field\n}\n\nvar a = new A();\na.y; // Accessing public field y: OK\na.#x; // Syntax error: reference to undeclared private field\nEven various other tools that JavaScript gives you for interrogating objects are prevented from accessing private fields (e.g. Object.getOwnProperty{Symbols,Names} don’t list private fields; there’s no way to use Reflect.get to access them).\nA Feature Three Ways\nWhen talking about a feature in JavaScript, there are often three different aspects in play: the mental model, the specification, and the implementation.\nThe mental model provides the high-level thinking that we expect programmers to use mostly. The specification in turn provides the detail of the semantics required by the feature. The implementation can look wildly different from the specification text, so long as the specification semantics are maintained.\nThese three aspects shouldn’t produce different results for people reasoning through things (though, sometimes a ‘mental model’ is shorthand, and doesn’t accurately capture semantics in edge case scenarios).\nWe can look at private fields using these three aspects:\nMental Model\nThe most basic mental model one can have for private fields is what it says on the tin: fields, but private. Now, JS fields become properties on objects, so the mental model is perhaps ‘properties that can’t be accessed from outside the class’.\nHowever, when we encounter proxies, this mental model breaks down a bit; trying to specify the semantics for ‘hidden properties’ and proxies is challenging (what happens when a Proxy is trying to provide access control to properties, if you aren’t supposed to be able see private fields with Proxies? Can subclasses access private fields? Do private fields participate in prototype inheritance?) . In order to preserve the desired privacy properties an alternative mental model became the way the committee thinks about private fields.\nThis alternative model is called the ‘WeakMap’ model. In this mental model you imagine that each class has a hidden weak map associated with each private field, such that you could hypothetically ‘desugar’\nclass A {\n  #x = 15;\n  g() {\n    return this.#x;\n  }\n}\ninto something like\nclass A_desugared {\n  static InaccessibleWeakMap_x = new WeakMap();\n  constructor() {\n    A_desugared.InaccessibleWeakMap_x.set(this, 15);\n  }\n\n  g() {\n    return A_desugared.InaccessibleWeakMap_x.get(this);\n  }\n}\nThe WeakMap model is, surprisingly, not how the feature is written in the specification, but is an important part of the design intention is behind them. I will cover a bit later how this mental model shows up in places later.\nSpecification\nThe actual specification changes are provided by the class fields proposal, specifically the changes to the specification text. I won’t cover every piece of this specification text, but I’ll call out specific aspects to help elucidate the differences between specification text and implementation.\nFirst, the specification adds the notion of [[PrivateName]], which is a globally unique field identifier. This global uniqueness is to ensure that two classes cannot access each other’s fields merely by having the same name.\nfunction createClass() {\n  return class {\n    #x = 1;\n    static getX(o) {\n      return o.#x;\n    }\n  };\n}\n\nlet [A, B] = [0, 1].map(createClass);\nlet a = new A();\nlet b = new B();\n\nA.getX(a); // Allowed: Same class\nA.getX(b); // Type Error, because different class.\nThe specification also adds a new ‘internal slot’, which is a specification level piece of internal state associated with an object in the spec, called [[PrivateFieldValues]] to all objects. [[PrivateFieldValues]] is a list of records of the form:\n{\n  [[PrivateName]]: Private Name,\n  [[PrivateFieldValue]]: ECMAScript value\n}\nTo manipulate this list, the specification adds four new algorithms:\n\nPrivateFieldFind\nPrivateFieldAdd\nPrivateFieldGet\nPrivateFieldSet\n\nThese algorithms largely work as you would expect: PrivateFieldAdd appends an entry to the list (though, in the interest of trying to provide errors eagerly, if a matching Private Name already exists in the list, it will throw a TypeError. I’ll show how that can happen later). PrivateFieldGet retrieves a value stored in the list, keyed by a given Private name, etc.\nThe Constructor Override Trick\nWhen I first started to read the specification, I was surprised to see that PrivateFieldAdd could throw. Given that it was only called from a constructor on the object being constructed, I had fully expected that the object would be freshly created, and therefore you’d not need to worry about a field already being there.\nThis turns out to be possible, a side effect of some of the specification’s handling of constructor return values. To be more concrete, the following is an example provided to me by André Bargull, which shows this in action.\nclass Base {\n  constructor(o) {\n    return o; // Note: We are returning the argument!\n  }\n}\n\nclass Stamper extends Base {\n  #x = \"stamped\";\n  static getX(o) {\n    return o.#x;\n  }\n}\nStamper is a class which can ‘stamp’ its private field onto any object:\nlet obj = {};\nnew Stamper(obj); // obj now has private field #x\nStamper.getX(obj); // => \"stamped\"\nThis means that when we add private fields to an object we cannot assume it doesn’t have them already. This is where the pre-existence check in PrivateFieldAdd comes into play:\nlet obj2 = {};\nnew Stamper(obj2);\nnew Stamper(obj2); // Throws 'TypeError' due to pre-existence of private field\nThis ability to stamp private fields into arbitrary objects interacts with the WeakMap model a bit here as well. For example, given that you can stamp private fields onto any object, that means you could also stamp a private field onto a sealed object:\nvar obj3 = {};\nObject.seal(obj3);\nnew Stamper(obj3);\nStamper.getX(obj3); // => \"stamped\"\nIf you imagine private fields as properties, this is uncomfortable, because it means you’re modifying an object that was sealed by a programmer to future modification. However, using the weak map model, it is totally acceptable, as you’re only using the sealed object as a key in the weak map.\nPS: Just because you can stamp private fields into arbitrary objects, doesn’t mean you should: Please don’t do this.\nImplementing the Specification\nWhen faced with implementing the specification, there is a tension between following the letter of the specification, and doing something different to improve the implementation on some dimension.\nWhere it is possible to implement the steps of the specification directly, we prefer to do that, as it makes maintenance of features easier as specification changes are made. SpiderMonkey does this in many places. You will see sections of code that are transcriptions of specification algorithms, with step numbers for comments. Following the exact letter of the specification can also be helpful where the specification is highly complex and small divergences can lead to compatibility risks.\nSometimes however, there are good reasons to diverge from the specification language. JavaScript implementations have been honed for high performance for years, and there are many implementation tricks that have been applied to make that happen. Sometimes recasting a part of the specification in terms of code already written is the right thing to do, because that means the new code is also able to have the performance characteristics of the already written code.\nImplementing Private Names\nThe specification language for Private Names already almost matches the semantics around Symbols, which already exist in SpiderMonkey. So adding PrivateNames as a special kind of Symbol is a fairly easy choice.\nImplementing Private Fields\nLooking at the specification for private fields, the specification implementation would be to add an extra hidden slot to every object in SpiderMonkey, which contains a reference to a list of {PrivateName, Value} pairs. However, implementing this directly has a number of clear downsides:\n\nIt adds memory usage to objects without private fields\nIt requires invasive addition of either new bytecodes or complexity to performance sensitive property access paths.\n\nAn alternative option is to diverge from the specification language, and implement only the semantics, not the actual specification algorithms. In the majority of cases, you really can think of private fields as special properties on objects that are hidden from reflection or introspection outside a class.\nIf we model private fields as properties, rather than a special side-list that is maintained with an object, we are able to take advantage of the fact that property manipulation is already extremely optimized in a JavaScript engine.\nHowever, properties are subject to reflection. So if we model private fields as object properties, we need to ensure that reflection APIs don’t reveal them, and that you can’t get access to them via Proxies.\nIn SpiderMonkey, we elected to implement private fields as hidden properties in order to take advantage of all the optimized machinery that already exists for properties in the engine. When I started implementing this feature André Bargull – a SpiderMonkey contributor for many years – actually handed me a series of patches that had a good chunk of the private fields implementation already done, for which I was hugely grateful.\nUsing our special PrivateName symbols, we effectively desuagar\nclass A {\n  #x = 10;\n  x() {\n    return this.#x;\n  }\n}\nto something that looks closer to\nclass A_desugared {\n  constructor() {\n    this[PrivateSymbol(#x)] = 10;\n  }\n  x() {\n    return this[PrivateSymbol(#x)];\n  }\n}\nPrivate fields have slightly different semantics than properties however. They are designed to issue errors on patterns expected to be programming mistakes, rather than silently accepting it. For example:\n\nAccessing an a property on an object that doesn’t have it returns undefined. Private fields are specified to throw a TypeError, as a result of the PrivateFieldGet algorithm.\nSetting a property on an object that doesn’t have it simply adds the property. Private fields will throw a TypeError in PrivateFieldSet.\nAdding a private field to an object that already has that field also throws a TypeError in PrivateFieldAdd. See “The Constructor Override Trick” above for how this can happen.\n\nTo handle the different semantics, we modified the bytecode emission for private field accesses. We added a new bytecode op, CheckPrivateField which verifies an object has the correct state for a given private field. This means throwing an exception if the property is missing or present, as appropriate for Get/Set or Add. CheckPrivateField is emitted just before using the regular ‘computed property name’ path (the one used for A[someKey]).\nCheckPrivateField is designed such that we can easily implement an inline cache using CacheIR. Since we are storing private fields as properties, we can use the Shape of an object as a guard, and simply return the appropriate boolean value. The Shape of an object in SpiderMonkey determines what properties it has, and where they are located in the storage for that object. Objects that have the same shape are guaranteed to have the same properties, and it’s a perfect check for an IC for CheckPrivateField.\nOther modifications we made to make to the engine include omitting private fields from the property enumeration protocol, and allowing the extension of sealed objects if we are adding private field.\nProxies\nProxies presented us a bit of a new challenge. Concretely, using the Stamper class above, you can add a private field directly to a Proxy:\nlet obj3 = {};\nlet proxy = new Proxy(obj3, handler);\nnew Stamper(proxy)\n\nStamper.getX(proxy) // => \"stamped\"\nStamper.getX(obj3)  // TypeError, private field is stamped\n                    // onto the Proxy Not the target!\nI definitely found this surprising initially. The reason I found this surprising was I had expected that, like other operations, the addition of a private field would tunnel through the proxy to the target. However, once I was able to internalize the WeakMap mental model, I was able to understand this example much better. The trick is that in the WeakMap model, it is the Proxy, not the target object, used as the key in the #x WeakMap.\nThese semantics presented a challenge to our implementation choice to model private fields as hidden properties however, as SpiderMonkey’s Proxies are highly specialized objects that do not have room for arbitrary properties. In order to support this case, we added a new reserved slot for an ‘expando’ object. The expando is an object allocated lazily that acts as the holder for dynamically added properties on the proxy. This pattern is used already for DOM objects, which are typically implemented as C++ objects with no room for extra properties. So if you write document.foo = \"hi\", this allocates an expando object for document, and puts the foo property and value in there instead. Returning to private fields, when #x is accessed on a Proxy, the proxy code knows to go and look in the expando object for that property.\nIn Conclusion\nPrivate Fields is an instance of implementing a JavaScript language feature where directly implementing the specification as written would be less performant than re-casting the specification in terms of already optimized engine primitives. Yet, that recasting itself can require some problem solving not present in the specification.\nAt the end, I am fairly happy with the choices made for our implementation of Private Fields, and am excited to see it finally enter the world!\nAcknowledgements\nI have to thank, again, André Bargull, who provided the first set of patches and laid down an excellent trail for me to follow. His work made finishing private fields much easier, as he’d already put a lot of thought into decision making.\nJason Orendorff has been an excellent and patient mentor as I have worked through this implementation, including two separate implementations of the private field bytecode, as well as two separate implementations of proxy support.\nThanks to Caroline Cullen, and Iain Ireland for helping to read drafts of this post, and to Steve Fink for fixing many typos.\nThe post Implementing Private Fields for JavaScript appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-06-08T15:26:19.000Z",
      "date_modified": "2021-06-08T15:26:19.000Z",
      "_plugin": {
        "pageFilename": "bc0b1ff95dcae95f8874bfbca0ab50dc8c3aa863d4b048c5e854eef02233d13c.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47303",
      "url": "https://hacks.mozilla.org/2021/06/looking-fine-with-firefox-89/",
      "title": "Looking fine with Firefox 89",
      "summary": "Firefox 89 has smartened up and brings with it a slimmed-down, slightly more minimalist interface.\nAlong with this new look, we get some great styling features including a force-colours feature for media queries and better control over how fonts are displayed. The long-awaited top-level await keyword for JavaScript modules is now enabled, as well as the PerformanceEventTiming interface, which is another addition to the performance suite of APIs: 89 really has been working out!\nThe post Looking fine with Firefox 89 appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>While we&#8217;re sitting here feeling a bit frumpy after a year with reduced activity, Firefox 89 has smartened up and brings with it a slimmed down, slightly more minimalist interface.</p>\n<p>Along with this new look, we get some great styling features including a <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/@media/forced-colors\"><code>force-colors</code></a> feature for media queries and better control over how fonts are displayed. The long awaited <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/await#top-level-await\">top-level <code>await</code></a> keyword for JavaScript modules is now enabled, as well as the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/PerformanceEventTiming\"><code>PerformanceEventTiming</code></a> interface, which is another addition to the performance suite of APIs: 89 really has been working out!</p>\n<p>This blog post provides merely a set of highlights; for all the details, check out the following:</p>\n<ul>\n<li><a href=\"https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Releases/89\">Firefox 89 for developers on MDN</a></li>\n<li><a href=\"https://www.mozilla.org/en-US/firefox/89.0/releasenotes/\">Firefox 89 end-user release notes</a></li>\n</ul>\n<h2>forced-colors media feature</h2>\n<p>The <code>forced-colors</code> CSS media feature detects if a user agent restricts the color palette used on a web page. For instance Windows has a <em>High Contrast</em> mode. If it&#8217;s turned on, using <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/@media/forced-colors\"><code>forced-colors: active</code></a> within a CSS media query would apply the styles nested inside.</p>\n<p>In this example we have a <code>.button</code> class that declares a <code>box-shadow</code> property, giving any HTML element using that class a nice drop-shadow.</p>\n<p>If <code>forced-colors</code> mode is active, this shadow would not be rendered, so instead we&#8217;re declaring a border to make up for the shadow loss:</p>\n<pre><code class=\"language-css\">.button {\n  border: 0;\n  padding: 10px;\n  box-shadow: -2px -2px 5px gray, 2px 2px 5px gray;\n}\n\n@media (forced-colors: active) {\n  .button {\n    /* Use a border instead, since box-shadow is forced to 'none' in forced-colors mode */\n    border: 2px ButtonText solid;\n  }\n}</code></pre>\n<h2>Better control for displayed fonts</h2>\n<p>Firefox 89 brings with it the <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/@font-face/line-gap-override\"><code>line-gap-override</code></a>, <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/@font-face/ascent-override\"><code>ascent-override</code></a> and <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/@font-face/descent-override\"><code>descent-override</code></a> CSS properties. These allow developers more control over how fonts are displayed. The following snippet shows just how useful these properties are when using a local fallback font:</p>\n<pre><code class=\"language-css\">@font-face {\n  font-family: web-font;\n  src: url(\"&lt;https://example.com/font.woff&gt;\");\n}\n\n@font-face {\n  font-family: local-font;\n  src: local(Local Font);\n  ascent-override: 90%;\n  descent-override: 110%;\n  line-gap-override: 120%;\n}</code></pre>\n<p>These new properties help to reduce layout shift when fonts are loading, as developers can better match the intricacies of a local font with a web font. They work alongside the <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/@font-face/size-adjust\"><code>size-adjust</code></a> property which is currently behind a <a href=\"https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Experimental_features#descriptor_size_adjust\">preference in Firefox 89</a>.</p>\n<h2>Top-level await</h2>\n<p>If you&#8217;ve been writing JavaScript over the past few years you&#8217;ve more than likely become familiar with <code>async</code> functions. Now the <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/await#top-level-await\"><code>await</code> keyword</a>, usually confined to use within an <code>async</code> function, has been given independence and allowed to go it alone. As long as it stays within <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules#top_level_await\">modules</a> that is.</p>\n<p>In short, this means JavaScript modules that have child modules using top level await wait for that child to execute before they themselves run. All while not blocking other child modules from loading.</p>\n<p>Here is a very small example of a module using the &gt;a href=&#8221;https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API&#8221;&gt;Fetch API and specifying <code>await</code> within the <code>export</code> statement. Any modules that include this will wait for the fetch to resolve before running any code.</p>\n<pre><code class=\"language-js\">// fetch request\nconst colors = fetch('../data/colors.json')\n  .then(response =&gt; response.json());\n\nexport default await colors;</code></pre>\n<h2>PerformanceEventTiming</h2>\n<p>A new look can&#8217;t go unnoticed without mentioning performance. There&#8217;s a plethora of Performance APIs, which give developers granular power over their own bespoke performance tests. The <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/PerformanceEventTiming\"><code>PerformanceEventTiming</code></a> interface is now available in Firefox 89 and provides timing information for a whole array of events. It adds yet another extremely useful feature for developers by cleverly giving information about when a user-triggered event starts and when it ends. A very welcome addition to the new release.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/06/looking-fine-with-firefox-89/\">Looking fine with Firefox 89</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "While we’re sitting here feeling a bit frumpy after a year with reduced activity, Firefox 89 has smartened up and brings with it a slimmed down, slightly more minimalist interface.\nAlong with this new look, we get some great styling features including a force-colors feature for media queries and better control over how fonts are displayed. The long awaited top-level await keyword for JavaScript modules is now enabled, as well as the PerformanceEventTiming interface, which is another addition to the performance suite of APIs: 89 really has been working out!\nThis blog post provides merely a set of highlights; for all the details, check out the following:\n\nFirefox 89 for developers on MDN\nFirefox 89 end-user release notes\n\nforced-colors media feature\nThe forced-colors CSS media feature detects if a user agent restricts the color palette used on a web page. For instance Windows has a High Contrast mode. If it’s turned on, using forced-colors: active within a CSS media query would apply the styles nested inside.\nIn this example we have a .button class that declares a box-shadow property, giving any HTML element using that class a nice drop-shadow.\nIf forced-colors mode is active, this shadow would not be rendered, so instead we’re declaring a border to make up for the shadow loss:\n.button {\n  border: 0;\n  padding: 10px;\n  box-shadow: -2px -2px 5px gray, 2px 2px 5px gray;\n}\n\n@media (forced-colors: active) {\n  .button {\n    /* Use a border instead, since box-shadow is forced to 'none' in forced-colors mode */\n    border: 2px ButtonText solid;\n  }\n}\nBetter control for displayed fonts\nFirefox 89 brings with it the line-gap-override, ascent-override and descent-override CSS properties. These allow developers more control over how fonts are displayed. The following snippet shows just how useful these properties are when using a local fallback font:\n@font-face {\n  font-family: web-font;\n  src: url(\"<https://example.com/font.woff>\");\n}\n\n@font-face {\n  font-family: local-font;\n  src: local(Local Font);\n  ascent-override: 90%;\n  descent-override: 110%;\n  line-gap-override: 120%;\n}\nThese new properties help to reduce layout shift when fonts are loading, as developers can better match the intricacies of a local font with a web font. They work alongside the size-adjust property which is currently behind a preference in Firefox 89.\nTop-level await\nIf you’ve been writing JavaScript over the past few years you’ve more than likely become familiar with async functions. Now the await keyword, usually confined to use within an async function, has been given independence and allowed to go it alone. As long as it stays within modules that is.\nIn short, this means JavaScript modules that have child modules using top level await wait for that child to execute before they themselves run. All while not blocking other child modules from loading.\nHere is a very small example of a module using the >a href=”https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API”>Fetch API and specifying await within the export statement. Any modules that include this will wait for the fetch to resolve before running any code.\n// fetch request\nconst colors = fetch('../data/colors.json')\n  .then(response => response.json());\n\nexport default await colors;\nPerformanceEventTiming\nA new look can’t go unnoticed without mentioning performance. There’s a plethora of Performance APIs, which give developers granular power over their own bespoke performance tests. The PerformanceEventTiming interface is now available in Firefox 89 and provides timing information for a whole array of events. It adds yet another extremely useful feature for developers by cleverly giving information about when a user-triggered event starts and when it ends. A very welcome addition to the new release.\nThe post Looking fine with Firefox 89 appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-06-01T15:19:01.000Z",
      "date_modified": "2021-06-01T15:19:01.000Z",
      "_plugin": {
        "pageFilename": "7e0445d75617d7170f103149e946c2ce9d83b6f9c16c6bf24d83c6ccaddd558b.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47175",
      "url": "https://hacks.mozilla.org/2021/05/improving-firefox-stability-on-linux/",
      "title": "Improving Firefox stability on Linux",
      "summary": "Roughly a year ago at Mozilla we started an effort to improve Firefox stability on Linux. This effort quickly became an example of good synergies between FOSS projects.\nThe post Improving Firefox stability on Linux appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Roughly a year ago at Mozilla we started an effort to improve Firefox stability on Linux. This effort quickly became an example of good synergies between FOSS projects.</p>\n<p>Every time Firefox crashes, the user can send us a crash report which we use to analyze the problem and hopefully fix it:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47180\" src=\"https://hacks.mozilla.org/files/2021/04/image2.png\" alt=\"A screenshot of a tab that justc crashed\" width=\"893\" height=\"812\" srcset=\"https://hacks.mozilla.org/files/2021/04/image2.png 893w, https://hacks.mozilla.org/files/2021/04/image2-250x227.png 250w, https://hacks.mozilla.org/files/2021/04/image2-500x455.png 500w, https://hacks.mozilla.org/files/2021/04/image2-768x698.png 768w\" sizes=\"(max-width: 893px) 100vw, 893px\" /></p>\n<p>This report contains, among other things, a minidump: a small snapshot of the process memory at the time it crashed. This includes the contents of the processor&#8217;s registers as well as data from the stacks of every thread.</p>\n<p>Here’s what this usually looks like:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47188\" src=\"https://hacks.mozilla.org/files/2021/04/image4.png\" alt=\"\" width=\"1253\" height=\"596\" srcset=\"https://hacks.mozilla.org/files/2021/04/image4.png 1253w, https://hacks.mozilla.org/files/2021/04/image4-250x119.png 250w, https://hacks.mozilla.org/files/2021/04/image4-500x238.png 500w, https://hacks.mozilla.org/files/2021/04/image4-768x365.png 768w\" sizes=\"(max-width: 1253px) 100vw, 1253px\" /></p>\n<p>If you&#8217;re familiar with core dumps, then minidumps are essentially a smaller version of them. The minidump format was originally designed at Microsoft and Windows has a native way of writing out minidumps. On Linux, we use Breakpad for this task. Breakpad originated at Google for their software (Picasa, Google Earth, etc&#8230;) but we have forked, heavily modified for our purposes and recently partly rewrote it in Rust.</p>\n<p>Once the user submits a crash report, we have a server-side component &#8211; called Socorro &#8211; that processes it and extracts a stack trace from the minidump. The reports are then clustered based on the top method name of the stack trace of the crashing thread. When a new crash is spotted we assign it a bug and start working on it. See the picture below for an example of how crashes are grouped:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47184\" src=\"https://hacks.mozilla.org/files/2021/04/image3.png\" alt=\"The snapshot of a stack trace as displayed on crash-stats.mozilla.com\" width=\"1827\" height=\"250\" srcset=\"https://hacks.mozilla.org/files/2021/04/image3.png 1827w, https://hacks.mozilla.org/files/2021/04/image3-250x34.png 250w, https://hacks.mozilla.org/files/2021/04/image3-500x68.png 500w, https://hacks.mozilla.org/files/2021/04/image3-768x105.png 768w, https://hacks.mozilla.org/files/2021/04/image3-1536x210.png 1536w\" sizes=\"(max-width: 1827px) 100vw, 1827px\" /></p>\n<p>To extract a meaningful stack trace from a minidump two more things are needed: unwinding information and symbols. The unwinding information is a set of instructions that describe how to find the various frames in the stack given an instruction pointer. Symbol information contains the names of the functions corresponding to a given range of addresses as well as the source files they come from and the line numbers a given instruction corresponds to.</p>\n<p>In regular Firefox releases, we extract this information from the build files and store it into symbol files in Breakpad standard format. Equipped with this information Socorro can produce a human-readable stack trace. The whole flow can be seen below:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47200\" src=\"https://hacks.mozilla.org/files/2021/04/image7.png\" alt=\"A graphicsl representation of our crash reporting flow, from the capture on the client to processing on the server\" width=\"708\" height=\"774\" srcset=\"https://hacks.mozilla.org/files/2021/04/image7.png 708w, https://hacks.mozilla.org/files/2021/04/image7-250x273.png 250w, https://hacks.mozilla.org/files/2021/04/image7-500x547.png 500w\" sizes=\"(max-width: 708px) 100vw, 708px\" /></p>\n<p>Here’s an example of a proper stack trace:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47176 size-full\" src=\"https://hacks.mozilla.org/files/2021/04/image1.png\" alt=\"A fully symbolicated stack trace\" width=\"1776\" height=\"620\" srcset=\"https://hacks.mozilla.org/files/2021/04/image1.png 1776w, https://hacks.mozilla.org/files/2021/04/image1-250x87.png 250w, https://hacks.mozilla.org/files/2021/04/image1-500x175.png 500w, https://hacks.mozilla.org/files/2021/04/image1-768x268.png 768w, https://hacks.mozilla.org/files/2021/04/image1-1536x536.png 1536w\" sizes=\"(max-width: 1776px) 100vw, 1776px\" /></p>\n<p>If Socorro doesn’t have access to the appropriate symbol files for a crash the resulting trace contains only addresses and isn’t very helpful:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47196\" src=\"https://hacks.mozilla.org/files/2021/04/image6.png\" alt=\"A stack trace showing raw addresses instead of symbols\" width=\"859\" height=\"334\" srcset=\"https://hacks.mozilla.org/files/2021/04/image6.png 859w, https://hacks.mozilla.org/files/2021/04/image6-250x97.png 250w, https://hacks.mozilla.org/files/2021/04/image6-500x194.png 500w, https://hacks.mozilla.org/files/2021/04/image6-768x299.png 768w\" sizes=\"(max-width: 859px) 100vw, 859px\" /></p>\n<p>When it comes to Linux things work differently than on other platforms: most of our users do not install our builds, they install the Firefox version that comes packaged for their favourite distribution.</p>\n<p>This posed a significant problem when dealing with stability issues on Linux: for the majority of our crash reports, we couldn&#8217;t produce high-quality stack traces because we didn&#8217;t have the required symbol information. The Firefox builds that submitted the reports weren’t done by us. To make matters worse, Firefox depends on a number of third-party packages (such as GTK, Mesa, FFmpeg, SQLite, etc.). We wouldn&#8217;t get good stack traces if a crash occurred in one of these packages instead of Firefox itself because we didn&#8217;t have symbols for them either.</p>\n<p>To address this issue, we started scraping debug information for Firefox builds and their dependencies from the package repositories of multiple distributions: Arch, Debian, Fedora, OpenSUSE and Ubuntu. Since every distribution does things a little bit differently, we had to write distro-specific scripts that would go through the list of packages in their repositories and find the associated debug information (the scripts are available <a href=\"https://github.com/gabrielesvelto/symbol-scrapers/\">here</a>). This data is then fed into a tool that extracts symbol files from the debug information and uploads it to our symbol server.</p>\n<p>With that information now available, we were able to analyze &gt;99% of the crash reports we received from Linux users, up from less than 20%. Here’s an example of a high-quality trace extracted from a distro-packaged version of Firefox. We haven’t built any of the libraries involved yet the function names are present and so are the file and line numbers of the affected code:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47192\" src=\"https://hacks.mozilla.org/files/2021/04/image5.png\" alt=\"A fully symbolicated stack trace including external code\" width=\"1452\" height=\"799\" srcset=\"https://hacks.mozilla.org/files/2021/04/image5.png 1452w, https://hacks.mozilla.org/files/2021/04/image5-250x138.png 250w, https://hacks.mozilla.org/files/2021/04/image5-500x275.png 500w, https://hacks.mozilla.org/files/2021/04/image5-768x423.png 768w\" sizes=\"(max-width: 1452px) 100vw, 1452px\" /></p>\n<p>The importance of this cannot be overestimated: Linux users tend to be more tech-savvy and are more likely to help us solve issues, so all those reports were a treasure trove for improving stability even for other operating systems (Windows, Mac, Android, etc). In particular, we often identified <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1633459\">Fission bugs</a> on Linux first.</p>\n<p>The first effect of this newfound ability to inspect Linux crashes is that it greatly sped up our response time to Linux-specific issues, and often allowed us to identify problems in the Nightly and Beta versions of Firefox before they reached users on the release channel.</p>\n<p>We could also quickly identify issues in bleeding-edge components such as <a href=\"https://github.com/servo/webrender\">WebRender</a>, <a href=\"https://hacks.mozilla.org/2020/04/experimental-webgpu-in-firefox/\">WebGPU</a>, <a href=\"https://fedoraproject.org/wiki/Changes/Firefox_Wayland_By_Default_On_Gnome\">Wayland</a> and VA-API video acceleration; oftentimes providing a fix within days of the change that triggered the issue.</p>\n<p>We didn&#8217;t stop there: we could now identify distro-specific issues and regressions. This allowed us to inform package maintainers of the problems and have them resolved quickly. For example, we were able to identify a Debian-specific issue only two weeks after it was introduced and fixed it right away. The crash was caused by a modification made by Debian to one of Firefox dependencies that could cause a crash on startup, it’s filed under bug <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1679430\">1679430</a> if you’re curious about the details.</p>\n<p>Another good example comes from Fedora: they had been using their own crash reporting system (ABRT) to catch Firefox crashes in their Firefox builds, but given the improvements on our side <a href=\"https://src.fedoraproject.org/rpms/firefox/c/de27f20acc7bdf391ccb1b571a9cb2061fc2dc3c?branch=master\">they started sending Firefox crashes our way instead</a>.</p>\n<p>We could also finally identify regressions and issues in our dependencies. This allowed us to communicate the issues upstream and sometimes even contributed fixes, benefiting both our users and theirs.</p>\n<p>For example, at some point, Debian updated the fontconfig package by backporting an upstream fix for a memory leak. Unfortunately, the fix contained a bug that <a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1633467\">would crash Firefox</a> and possibly other software too. We spotted the new crash only six days after the change landed in Debian sources and only a couple of weeks afterwards the issue had been fixed both upstream and in Debian. We sent reports and fixes to other projects too including <a href=\"https://gitlab.freedesktop.org/mesa/mesa/-/issues/3066\">Mesa</a>, GTK, <a href=\"https://gitlab.gnome.org/GNOME/glib/-/issues/954\">glib</a>, <a href=\"https://github.com/LudovicRousseau/PCSC/issues/51\">PCSC</a>, SQLite and more.</p>\n<p>Nightly versions of Firefox also include a tool to spot security-sensitive issues: <a href=\"https://groups.google.com/g/mozilla.dev.platform/c/AyECjDNsqUE/m/Jd7Jr4cXAgAJ\">the probabilistic heap checker</a>. This tool randomly pads a handful of memory allocations in order to detect buffer overflows and use-after-free accesses. When it detects one of these, it sends us a very detailed crash report. Given Firefox&#8217;s large user-base on Linux, this allowed us to spot some elusive issues in upstream projects and report them.</p>\n<p>This also exposed some limitations in the tools we use for crash analysis, so we decided to rewrite them in Rust largely relying on the excellent crates developed by Sentry. The resulting tools were dramatically faster than our old ones, used a fraction of the memory and produced more accurate results. Code flowed both ways: we contributed improvements to their crates (and their dependencies) while they expanded their APIs to address our new use-cases and fixed the issues we discovered.</p>\n<p>Another pleasant side-effect of this work is that Thunderbird now also benefits from the improvement we made for Firefox.</p>\n<p>This goes on to show how collaboration between FOSS projects not only benefits their users but ultimately improves the whole ecosystem and the broader community that relies on it.</p>\n<p>Special thanks to Calixte Denizet, Nicholas Nethercote, Jan Auer and all the others that contributed to this effort!</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/05/improving-firefox-stability-on-linux/\">Improving Firefox stability on Linux</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Roughly a year ago at Mozilla we started an effort to improve Firefox stability on Linux. This effort quickly became an example of good synergies between FOSS projects.\nEvery time Firefox crashes, the user can send us a crash report which we use to analyze the problem and hopefully fix it:\n\nThis report contains, among other things, a minidump: a small snapshot of the process memory at the time it crashed. This includes the contents of the processor’s registers as well as data from the stacks of every thread.\nHere’s what this usually looks like:\n\nIf you’re familiar with core dumps, then minidumps are essentially a smaller version of them. The minidump format was originally designed at Microsoft and Windows has a native way of writing out minidumps. On Linux, we use Breakpad for this task. Breakpad originated at Google for their software (Picasa, Google Earth, etc…) but we have forked, heavily modified for our purposes and recently partly rewrote it in Rust.\nOnce the user submits a crash report, we have a server-side component – called Socorro – that processes it and extracts a stack trace from the minidump. The reports are then clustered based on the top method name of the stack trace of the crashing thread. When a new crash is spotted we assign it a bug and start working on it. See the picture below for an example of how crashes are grouped:\n\nTo extract a meaningful stack trace from a minidump two more things are needed: unwinding information and symbols. The unwinding information is a set of instructions that describe how to find the various frames in the stack given an instruction pointer. Symbol information contains the names of the functions corresponding to a given range of addresses as well as the source files they come from and the line numbers a given instruction corresponds to.\nIn regular Firefox releases, we extract this information from the build files and store it into symbol files in Breakpad standard format. Equipped with this information Socorro can produce a human-readable stack trace. The whole flow can be seen below:\n\nHere’s an example of a proper stack trace:\n\nIf Socorro doesn’t have access to the appropriate symbol files for a crash the resulting trace contains only addresses and isn’t very helpful:\n\nWhen it comes to Linux things work differently than on other platforms: most of our users do not install our builds, they install the Firefox version that comes packaged for their favourite distribution.\nThis posed a significant problem when dealing with stability issues on Linux: for the majority of our crash reports, we couldn’t produce high-quality stack traces because we didn’t have the required symbol information. The Firefox builds that submitted the reports weren’t done by us. To make matters worse, Firefox depends on a number of third-party packages (such as GTK, Mesa, FFmpeg, SQLite, etc.). We wouldn’t get good stack traces if a crash occurred in one of these packages instead of Firefox itself because we didn’t have symbols for them either.\nTo address this issue, we started scraping debug information for Firefox builds and their dependencies from the package repositories of multiple distributions: Arch, Debian, Fedora, OpenSUSE and Ubuntu. Since every distribution does things a little bit differently, we had to write distro-specific scripts that would go through the list of packages in their repositories and find the associated debug information (the scripts are available here). This data is then fed into a tool that extracts symbol files from the debug information and uploads it to our symbol server.\nWith that information now available, we were able to analyze >99% of the crash reports we received from Linux users, up from less than 20%. Here’s an example of a high-quality trace extracted from a distro-packaged version of Firefox. We haven’t built any of the libraries involved yet the function names are present and so are the file and line numbers of the affected code:\n\nThe importance of this cannot be overestimated: Linux users tend to be more tech-savvy and are more likely to help us solve issues, so all those reports were a treasure trove for improving stability even for other operating systems (Windows, Mac, Android, etc). In particular, we often identified Fission bugs on Linux first.\nThe first effect of this newfound ability to inspect Linux crashes is that it greatly sped up our response time to Linux-specific issues, and often allowed us to identify problems in the Nightly and Beta versions of Firefox before they reached users on the release channel.\nWe could also quickly identify issues in bleeding-edge components such as WebRender, WebGPU, Wayland and VA-API video acceleration; oftentimes providing a fix within days of the change that triggered the issue.\nWe didn’t stop there: we could now identify distro-specific issues and regressions. This allowed us to inform package maintainers of the problems and have them resolved quickly. For example, we were able to identify a Debian-specific issue only two weeks after it was introduced and fixed it right away. The crash was caused by a modification made by Debian to one of Firefox dependencies that could cause a crash on startup, it’s filed under bug 1679430 if you’re curious about the details.\nAnother good example comes from Fedora: they had been using their own crash reporting system (ABRT) to catch Firefox crashes in their Firefox builds, but given the improvements on our side they started sending Firefox crashes our way instead.\nWe could also finally identify regressions and issues in our dependencies. This allowed us to communicate the issues upstream and sometimes even contributed fixes, benefiting both our users and theirs.\nFor example, at some point, Debian updated the fontconfig package by backporting an upstream fix for a memory leak. Unfortunately, the fix contained a bug that would crash Firefox and possibly other software too. We spotted the new crash only six days after the change landed in Debian sources and only a couple of weeks afterwards the issue had been fixed both upstream and in Debian. We sent reports and fixes to other projects too including Mesa, GTK, glib, PCSC, SQLite and more.\nNightly versions of Firefox also include a tool to spot security-sensitive issues: the probabilistic heap checker. This tool randomly pads a handful of memory allocations in order to detect buffer overflows and use-after-free accesses. When it detects one of these, it sends us a very detailed crash report. Given Firefox’s large user-base on Linux, this allowed us to spot some elusive issues in upstream projects and report them.\nThis also exposed some limitations in the tools we use for crash analysis, so we decided to rewrite them in Rust largely relying on the excellent crates developed by Sentry. The resulting tools were dramatically faster than our old ones, used a fraction of the memory and produced more accurate results. Code flowed both ways: we contributed improvements to their crates (and their dependencies) while they expanded their APIs to address our new use-cases and fixed the issues we discovered.\nAnother pleasant side-effect of this work is that Thunderbird now also benefits from the improvement we made for Firefox.\nThis goes on to show how collaboration between FOSS projects not only benefits their users but ultimately improves the whole ecosystem and the broader community that relies on it.\nSpecial thanks to Calixte Denizet, Nicholas Nethercote, Jan Auer and all the others that contributed to this effort!\nThe post Improving Firefox stability on Linux appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-05-19T14:28:53.000Z",
      "date_modified": "2021-05-19T14:28:53.000Z",
      "_plugin": {
        "pageFilename": "8749790a7818f80db8eafc803d83fc03f48dfe99dd398af4ada4f91ffdf33df7.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47259",
      "url": "https://hacks.mozilla.org/2021/05/introducing-firefox-new-site-isolation-security-architecture/",
      "title": "Introducing Firefox’s new Site Isolation Security Architecture",
      "summary": "Like any web browser, Firefox loads code from untrusted and potentially hostile websites and runs it on your computer. To protect you against new types of attacks from malicious sites and to meet the security principles of Mozilla, we set out to redesign Firefox on desktop.\nThe post Introducing Firefox’s new Site Isolation Security Architecture appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>Like any web browser, Firefox loads code from untrusted and potentially hostile websites and runs it on your computer. To protect you against new types of attacks from malicious sites and to meet the security principles of Mozilla, we set out to redesign Firefox on desktop.</p>\n<p>Site Isolation builds upon a new security architecture that extends current protection mechanisms by separating (web) content and loading each site in its own operating system process.</p>\n<p>This new security architecture allows Firefox to completely separate code originating from different sites and, in turn, defend against malicious sites trying to access sensitive information from other sites you are visiting.</p>\n<p>In more detail, whenever you open a website and enter a password, a credit card number, or any other sensitive information, you want to be sure that this information is kept secure and inaccessible to malicious actors.</p>\n<p>As a first line of defence Firefox enforces a variety of security mechanisms, e.g. the <a href=\"https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy\">same-origin policy</a> which prevents adversaries from accessing such information when loaded into the same application.</p>\n<p>Unfortunately, the web evolves and so do the techniques of malicious actors. To fully protect your private information, a modern web browser not only needs to provide protections on the application layer but also needs to entirely separate the memory space of different sites &#8211; the new Site Isolation security architecture in Firefox provides those security guarantees.</p>\n<h2>Why separating memory space is crucial</h2>\n<p>In early 2018, security researchers disclosed two major vulnerabilities, known as <a href=\"https://en.wikipedia.org/wiki/Meltdown_(security_vulnerability)\">Meltdown</a> and <a href=\"https://en.wikipedia.org/wiki/Spectre_(security_vulnerability)\">Spectre</a>. The researchers exploited fundamental assumptions about modern hardware execution, and were able to demonstrate how untrusted code can access and read memory anywhere within a process’ address space, even in a language as high level as JavaScript (which powers almost every single website).</p>\n<p>While band-aid countermeasures deployed by OS, CPU and major web browser vendors quickly neutralized the attacks, they came with a performance cost and were designed to be temporary. Back when the attacks were announced publicly, Firefox teams promptly <a href=\"https://blog.mozilla.org/security/2018/01/03/mitigations-landing-new-class-timing-attack/\">reduced the precision of high-precision timers and disabled APIs that allowed such timers to be implemented</a> to keep our users safe.</p>\n<p>Going forward, it was clear that we needed to fundamentally re-architecture the security design of Firefox to mitigate future variations of such vulnerabilities.</p>\n<p>Let&#8217;s take a closer look at the following example which demonstrates how an attacker can access your private data when executing a Spectre-like attack.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47264\" src=\"https://hacks.mozilla.org/files/2021/05/figure1-500x707.jpg\" alt=\"Two hand-drawn diagrams, with the first labeled “Without Site Isolation, we might load both of these sites in the same process :( ”. Two browser windows with partially visible sites “attacker.com” and “my-bank” partial site, are loaded in the same process - process 16. On top of the banking window, there is a cartoon face that looks happy, personifying the browser. The attacker site window contains a face that is looking at the banking window, with a mischievous smile. In the second diagram, labeled “Attacker.com executes a sophisticated attack”, we see the same two browser windows loaded in process 16 and a 1 column table labelled “memory where my-bank’s data is stored in process 16” underneath the banking window. It has two entries: “credit card info” and “login password”. A hand extending from the malicious site reaches toward the table (aka memory of the second window), signifying that the malicious site is able to access sensitive data belonging to the banking window because it is in the same process. The personified browser character is looking towards the malicious site, and exhibits feelings of concern and worry, with exclamation marks floating around the face.\" width=\"700\" height=\"990\" srcset=\"https://hacks.mozilla.org/files/2021/05/figure1-500x707.jpg 500w, https://hacks.mozilla.org/files/2021/05/figure1-250x354.jpg 250w, https://hacks.mozilla.org/files/2021/05/figure1-768x1086.jpg 768w, https://hacks.mozilla.org/files/2021/05/figure1-1086x1536.jpg 1086w, https://hacks.mozilla.org/files/2021/05/figure1-1448x2048.jpg 1448w, https://hacks.mozilla.org/files/2021/05/figure1-scaled.jpg 1810w\" sizes=\"(max-width: 700px) 100vw, 700px\" /></p>\n<p>Without Site Isolation, Firefox might load a malicious site in the same process as a site that is handling sensitive information. In the worst case scenario, a malicious site might execute a Spectre-like attack to gain access to memory of the other site.</p>\n<p>Suppose you have two websites open &#8211; www.my-bank.com and www.attacker.com. As illustrated in the diagram above, with current web browser architecture it’s possible that web content from both sites ends up being loaded into the same operating system process. To make things worse, using a Spectre-like attack would allow attacker.com to query and access data from the my-bank.com website.</p>\n<p>Despite existing security mitigations, the only way to provide memory protections necessary to defend against Spectre-like attacks is to rely on the security guarantees that come with isolating content from different sites using the operating system’s process separation.</p>\n<h2>Background on Current Browser Architecture</h2>\n<p>Upon being launched, the Firefox web browser internally spawns one privileged process (also known as the parent process) which then launches and coordinates activities of multiple (web) content processes &#8211; the parent process is the most privileged one, as it is allowed to perform any action that the end-user can.</p>\n<p>This multi-process architecture allows Firefox to separate more complicated or less trustworthy code into processes, most of which have reduced access to operating system resources or user files. As a consequence, less privileged code will need to ask more privileged code to perform operations which it itself cannot.</p>\n<p>For example, a content process will have to ask the parent process to save a download because it does not have the permissions to write to disk. Put differently, if an attacker manages to compromise the content process it must additionally (ab)use one of the APIs to convince the parent process to act on its behalf.</p>\n<p>In great detail, (as of April 2021) Firefox’s parent process launches a fixed number of processes: eight web content processes, up to two additional semi-privileged web content processes, and four utility processes for web extensions, GPU operations, networking, and media decoding.</p>\n<p>While separating content into currently eight web content processes already provides a solid foundation, it does not meet the security standards of Mozilla because it allows two completely different sites to end up in the same operating system process and, therefore, share process memory. To counter this, we are targeting a Site Isolation architecture that loads every single site into its own process.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47268\" src=\"https://hacks.mozilla.org/files/2021/05/figure2-500x707.jpg\" alt=\"A hand drawn diagram titled “Loading Sites with Current Browser Architecture”. On the left hand-side, from top to bottom, there are four browser windows with different sites loaded. The first window, www.my-bank.com, is loaded in process 3. The second window is loaded in process 4. The third window is loaded in process 5. The last window with a url - “www.attacker.com” - is loaded in process 3, same as the first window. On the right hand-side of the drawing, there is a table titled “List of Content Processes”. The table contains two columns: “site” and “pid”, which stands for process id. In the table, the first window, my-bank.com, and the last attacker.com window have the same PID.\" width=\"700\" height=\"990\" srcset=\"https://hacks.mozilla.org/files/2021/05/figure2-500x707.jpg 500w, https://hacks.mozilla.org/files/2021/05/figure2-250x354.jpg 250w, https://hacks.mozilla.org/files/2021/05/figure2-768x1086.jpg 768w, https://hacks.mozilla.org/files/2021/05/figure2-1086x1536.jpg 1086w, https://hacks.mozilla.org/files/2021/05/figure2-1448x2048.jpg 1448w, https://hacks.mozilla.org/files/2021/05/figure2-scaled.jpg 1810w\" sizes=\"(max-width: 700px) 100vw, 700px\" /></p>\n<p>Without Site Isolation, Firefox does not separate web content into different processes and it’s possible for different sites to be loaded in the same process.</p>\n<p>Imagine you open some websites in different tabs: www.my-bank.com, www.getpocket.com, www.mozilla.org and www.attacker.com. As illustrated in the diagram above, it&#8217;s entirely possible that my-bank.com and attacker.com end up being loaded in the same operating system process, which would result in them sharing process memory. As we saw in the previous example, with this separation model, an attacker could perform a Spectre-like attack to access my-bank.com’s data.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47272\" src=\"https://hacks.mozilla.org/files/2021/05/figure3-500x366.jpg\" alt=\"A hand drawn diagram titled “Loading Subframes With Current Browser Architecture”. There is one browser window drawn. The window, www.attacker.com, embeds a page from a different site, www.my-bank.com. The top level page and the subframe are loaded in the same process - process 3.\" width=\"700\" height=\"513\" srcset=\"https://hacks.mozilla.org/files/2021/05/figure3-500x366.jpg 500w, https://hacks.mozilla.org/files/2021/05/figure3-250x183.jpg 250w, https://hacks.mozilla.org/files/2021/05/figure3-768x562.jpg 768w, https://hacks.mozilla.org/files/2021/05/figure3-1536x1125.jpg 1536w, https://hacks.mozilla.org/files/2021/05/figure3.jpg 1830w\" sizes=\"(max-width: 700px) 100vw, 700px\" /></p>\n<p>Without Site Isolation, the browser will load embedded pages, such as a bank page or an ad, in the same process as the top level document.</p>\n<p>While straightforward to understand sites being loaded into different tabs, it’s also possible that sites are embedded into other sites through so-called subframes &#8211; if you ever visited a website that had ads on it, those are probably subframes. If you ever had a personal website and you embedded a YouTube video with your favourite song within it, the YouTube video was embedded in a subframe.</p>\n<p>In a more dangerous scenario, a malicious site could embed a legitimate site within a subframe and try to trick you into entering sensitive information. With the current architecture, if a page contains any subframes from a different site, they will generally be in the same process as the outer tab.</p>\n<p>This results in both the page and all of its subframes sharing process memory, even if the subframes originate from different sites. In the case of a successful Spectre-like attack, a top-level site might access sensitive information it should not have access to from a subframe it embeds (and vice-versa) &#8211; the new Site Isolation security architecture within Firefox will effectively make it even harder for malicious sites to execute such attacks.</p>\n<h2>How Site Isolation Works in Firefox</h2>\n<p>When enabling Site Isolation in Firefox for desktop, each unique site is loaded in a separate process. In more detail, loading “https://mozilla.org” and also loading &#8220;http://getpocket.com&#8221; will cause Site Isolation to separate the two sites into their own operating system process because they are not considered “same-site”.</p>\n<p>Similarly, &#8220;https://getpocket.com&#8221; (note the difference between http and https) will also be loaded into a separate process &#8211; so ultimately all three sites will load in different processes.</p>\n<p>For the sake of completeness, there are some domains such as “.github.io” or “.blogspot.com” that would be too general to identify a “site”. This is why we use a <a href=\"https://github.com/publicsuffix/list\">community-maintained</a> list of effective top level domains (eTLDs) to aid in differentiating between sites.</p>\n<p>Since “github.io” is listed as an eTLD, “a.github.io”  and “b.github.io” would load in different processes. In our running examples, websites “www.my-bank.com” and “www.attacker.com” are not considered “same-site” with each other and will be isolated in separate processes.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47276\" src=\"https://hacks.mozilla.org/files/2021/05/figure4-500x707.jpg\" alt=\"Two hand-drawn diagrams, with the first labeled “With Site Isolation, we will load these sites in different processes”. It shows two browser windows, one www.attacker.com , loaded in process 5, and www.my-bank.com loaded in process 16. On top of the banking window, there is a cartoon face that looks happy, personifying the browser. In contrast, the webpage area of the www.attacker.com window, contains a face that is looking at the banking window, with a mischievous smile. In the second diagram, labeled “Attacker.com tries to execute a sophisticated attack”, we see the same two browser windows. There is a 1 column table labelled “memory where my-bank’s data is stored in process 16” underneath the banking window . It has two entries: “credit card info” and “login password”. A hand extending from the malicious site tries to reach towards the table (aka memory of the banking window), but is unable to reach it, due to the process boundary. The face of the malicious site is frowning and looks unhappy, while the face, representing the browser, continues to look happy and carefree. The second window’s data is safe from the malicious site.\" width=\"700\" height=\"990\" srcset=\"https://hacks.mozilla.org/files/2021/05/figure4-500x707.jpg 500w, https://hacks.mozilla.org/files/2021/05/figure4-250x354.jpg 250w, https://hacks.mozilla.org/files/2021/05/figure4-768x1086.jpg 768w, https://hacks.mozilla.org/files/2021/05/figure4-1086x1536.jpg 1086w, https://hacks.mozilla.org/files/2021/05/figure4-1448x2048.jpg 1448w, https://hacks.mozilla.org/files/2021/05/figure4-scaled.jpg 1810w\" sizes=\"(max-width: 700px) 100vw, 700px\" /></p>\n<p>With Site Isolation, Firefox loads each site in its own process, thereby isolating their memory from each other, and relies on security guarantees of the operating system.</p>\n<p>Suppose now, you open the same two websites: www.attacker.com and www.my-bank.com, as seen in the diagram above. Site isolation recognizes that the two sites are not “same-site” and hence the site isolation architecture will completely separate content from attacker.com and my-bank.com into separate operating system processes.</p>\n<p>This process separation of content from different sites provides the memory protections required to allow for a secure browsing experience, making it even harder for sites to execute Spectre-like attacks, and, ultimately, provide a secure browsing experience for our users.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47280\" src=\"https://hacks.mozilla.org/files/2021/05/figure5-500x323.jpg\" alt=\"The window, www.attacker.com, embeds a page from a different site, www.my-bank.com. The top level page is loaded in process 3 and the subframe corresponding to the bank site is loaded in process 5. The two sites are, thus, isolated from each other in different operating system processes.\" width=\"700\" height=\"452\" srcset=\"https://hacks.mozilla.org/files/2021/05/figure5-500x323.jpg 500w, https://hacks.mozilla.org/files/2021/05/figure5-250x161.jpg 250w, https://hacks.mozilla.org/files/2021/05/figure5-768x496.jpg 768w, https://hacks.mozilla.org/files/2021/05/figure5-1536x991.jpg 1536w, https://hacks.mozilla.org/files/2021/05/figure5.jpg 1830w\" sizes=\"(max-width: 700px) 100vw, 700px\" /></p>\n<p>With Site Isolation, Firefox loads subframes from different sites in their own processes.</p>\n<p>Identical to loading sites into two different tabs is the separation of two different sites when loaded into subframes. Let’s revisit an earlier example where pages contained subframes, with Site Isolation, subframes that are not “same-site” with the top level page will load in a different process.</p>\n<p>In the diagram above, we see that the page www.attacker.com embeds a page from www.my-bank.com and loads in a different process. Having a top level document and subframes from different sites loaded in their own processes ensures their memory is isolated from each other, yielding profound security guarantees.</p>\n<h2>Additional Benefits of Site Isolation</h2>\n<p>With Site Isolation architecture in place, we are able to bring additional security hardening to Firefox to keep you and your data safe. Besides providing an extra layer of defence against possible security threats, Site Isolation brings other wins:</p>\n<ul>\n<li aria-level=\"1\">By placing more pages into separate processes, we can ensure that doing heavy computation or garbage collection on one page will not degrade the responsiveness of pages in other processes.</li>\n<li aria-level=\"1\">Using more processes to load websites allows us to spread work across many CPU cores and use the underlying hardware more efficiently.</li>\n<li aria-level=\"1\">Due to the finer-grained separation of sites, a subframe or a tab crashing will not affect websites loaded in different processes, resulting in an improved application stability and better user experience.</li>\n</ul>\n<h2>Going Forward</h2>\n<p>We are currently testing Site Isolation on desktop browsers Nightly and Beta with a subset of users and will be rolling out to more desktop users soon. However, if you already want to benefit from the improved security architecture now, you can enable it by downloading the Nightly or Beta browser from <a href=\"https://www.mozilla.org/firefox/channel/desktop/\">here</a> and following these steps:</p>\n<p><b>To enable Site Isolation on Firefox Nightly:</b></p>\n<ol>\n<li aria-level=\"1\">Navigate to about:preferences#experimental</li>\n<li aria-level=\"1\">Check the &#8220;Fission (Site Isolation)&#8221; checkbox to enable.</li>\n<li aria-level=\"1\">Restart Firefox.</li>\n</ol>\n<p><b>To enable Site Isolation on Firefox Beta or Release:</b></p>\n<ol>\n<li aria-level=\"1\">Navigate to about:config.</li>\n<li aria-level=\"1\">Set `fission.autostart` pref to `true`.</li>\n<li aria-level=\"1\">Restart Firefox.</li>\n</ol>\n<p>For technical details on how we group sites and subframes together, you can check out our new process manager tool at “about:processes” (type it into the address bar) and follow the project at  <a href=\"https://wiki.mozilla.org/Project_Fission\">https://wiki.mozilla.org/Project_Fission</a>.</p>\n<p>With Site Isolation enabled on Firefox for Desktop, Mozilla takes its security guarantees to the next level and protects you against a new class of malicious attacks by relying on memory protections of OS-level process separation for each site. If you are interested in contributing to Mozilla’s open-source projects, you can help us by filing bugs <a href=\"https://bugzilla.mozilla.org/enter_bug.cgi?product=Core&amp;bug_type=defect&amp;short_desc=%5bFission%5d&amp;blocked=fission-dogfooding\">here</a> if you run into any problems with Site Isolation enabled.</p>\n<h2>Acknowledgements</h2>\n<p>Site Isolation (Project Fission), has been a massive multi-year project. Thank you to all of the talented and awesome colleagues who contributed to this work! It’s a privilege to work with people who are passionate about building the web we want: free, inclusive, independent and secure! In particular, I would like to thank Neha Kochar, Nika Layzell, Mike Conley, Melissa Thermidor, Chris Peterson, Kashav Madan, Andrew McCreight, Peter Van der Beken, Tantek Çelik and Christoph Kerschbaumer for their insightful comments and discussions.  Finally, thank you to Morgan Rae Reschenberg for helping me craft alt-text to meet the high standards of our web accessibility principles and allow everyone on the internet to easily gather the benefits provided by Site Isolation.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/05/introducing-firefox-new-site-isolation-security-architecture/\">Introducing Firefox&#8217;s new Site Isolation Security Architecture</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "Like any web browser, Firefox loads code from untrusted and potentially hostile websites and runs it on your computer. To protect you against new types of attacks from malicious sites and to meet the security principles of Mozilla, we set out to redesign Firefox on desktop.\nSite Isolation builds upon a new security architecture that extends current protection mechanisms by separating (web) content and loading each site in its own operating system process.\nThis new security architecture allows Firefox to completely separate code originating from different sites and, in turn, defend against malicious sites trying to access sensitive information from other sites you are visiting.\nIn more detail, whenever you open a website and enter a password, a credit card number, or any other sensitive information, you want to be sure that this information is kept secure and inaccessible to malicious actors.\nAs a first line of defence Firefox enforces a variety of security mechanisms, e.g. the same-origin policy which prevents adversaries from accessing such information when loaded into the same application.\nUnfortunately, the web evolves and so do the techniques of malicious actors. To fully protect your private information, a modern web browser not only needs to provide protections on the application layer but also needs to entirely separate the memory space of different sites – the new Site Isolation security architecture in Firefox provides those security guarantees.\nWhy separating memory space is crucial\nIn early 2018, security researchers disclosed two major vulnerabilities, known as Meltdown and Spectre. The researchers exploited fundamental assumptions about modern hardware execution, and were able to demonstrate how untrusted code can access and read memory anywhere within a process’ address space, even in a language as high level as JavaScript (which powers almost every single website).\nWhile band-aid countermeasures deployed by OS, CPU and major web browser vendors quickly neutralized the attacks, they came with a performance cost and were designed to be temporary. Back when the attacks were announced publicly, Firefox teams promptly reduced the precision of high-precision timers and disabled APIs that allowed such timers to be implemented to keep our users safe.\nGoing forward, it was clear that we needed to fundamentally re-architecture the security design of Firefox to mitigate future variations of such vulnerabilities.\nLet’s take a closer look at the following example which demonstrates how an attacker can access your private data when executing a Spectre-like attack.\n\nWithout Site Isolation, Firefox might load a malicious site in the same process as a site that is handling sensitive information. In the worst case scenario, a malicious site might execute a Spectre-like attack to gain access to memory of the other site.\nSuppose you have two websites open – www.my-bank.com and www.attacker.com. As illustrated in the diagram above, with current web browser architecture it’s possible that web content from both sites ends up being loaded into the same operating system process. To make things worse, using a Spectre-like attack would allow attacker.com to query and access data from the my-bank.com website.\nDespite existing security mitigations, the only way to provide memory protections necessary to defend against Spectre-like attacks is to rely on the security guarantees that come with isolating content from different sites using the operating system’s process separation.\nBackground on Current Browser Architecture\nUpon being launched, the Firefox web browser internally spawns one privileged process (also known as the parent process) which then launches and coordinates activities of multiple (web) content processes – the parent process is the most privileged one, as it is allowed to perform any action that the end-user can.\nThis multi-process architecture allows Firefox to separate more complicated or less trustworthy code into processes, most of which have reduced access to operating system resources or user files. As a consequence, less privileged code will need to ask more privileged code to perform operations which it itself cannot.\nFor example, a content process will have to ask the parent process to save a download because it does not have the permissions to write to disk. Put differently, if an attacker manages to compromise the content process it must additionally (ab)use one of the APIs to convince the parent process to act on its behalf.\nIn great detail, (as of April 2021) Firefox’s parent process launches a fixed number of processes: eight web content processes, up to two additional semi-privileged web content processes, and four utility processes for web extensions, GPU operations, networking, and media decoding.\nWhile separating content into currently eight web content processes already provides a solid foundation, it does not meet the security standards of Mozilla because it allows two completely different sites to end up in the same operating system process and, therefore, share process memory. To counter this, we are targeting a Site Isolation architecture that loads every single site into its own process.\n\nWithout Site Isolation, Firefox does not separate web content into different processes and it’s possible for different sites to be loaded in the same process.\nImagine you open some websites in different tabs: www.my-bank.com, www.getpocket.com, www.mozilla.org and www.attacker.com. As illustrated in the diagram above, it’s entirely possible that my-bank.com and attacker.com end up being loaded in the same operating system process, which would result in them sharing process memory. As we saw in the previous example, with this separation model, an attacker could perform a Spectre-like attack to access my-bank.com’s data.\n\nWithout Site Isolation, the browser will load embedded pages, such as a bank page or an ad, in the same process as the top level document.\nWhile straightforward to understand sites being loaded into different tabs, it’s also possible that sites are embedded into other sites through so-called subframes – if you ever visited a website that had ads on it, those are probably subframes. If you ever had a personal website and you embedded a YouTube video with your favourite song within it, the YouTube video was embedded in a subframe.\nIn a more dangerous scenario, a malicious site could embed a legitimate site within a subframe and try to trick you into entering sensitive information. With the current architecture, if a page contains any subframes from a different site, they will generally be in the same process as the outer tab.\nThis results in both the page and all of its subframes sharing process memory, even if the subframes originate from different sites. In the case of a successful Spectre-like attack, a top-level site might access sensitive information it should not have access to from a subframe it embeds (and vice-versa) – the new Site Isolation security architecture within Firefox will effectively make it even harder for malicious sites to execute such attacks.\nHow Site Isolation Works in Firefox\nWhen enabling Site Isolation in Firefox for desktop, each unique site is loaded in a separate process. In more detail, loading “https://mozilla.org” and also loading “http://getpocket.com” will cause Site Isolation to separate the two sites into their own operating system process because they are not considered “same-site”.\nSimilarly, “https://getpocket.com” (note the difference between http and https) will also be loaded into a separate process – so ultimately all three sites will load in different processes.\nFor the sake of completeness, there are some domains such as “.github.io” or “.blogspot.com” that would be too general to identify a “site”. This is why we use a community-maintained list of effective top level domains (eTLDs) to aid in differentiating between sites.\nSince “github.io” is listed as an eTLD, “a.github.io”  and “b.github.io” would load in different processes. In our running examples, websites “www.my-bank.com” and “www.attacker.com” are not considered “same-site” with each other and will be isolated in separate processes.\n\nWith Site Isolation, Firefox loads each site in its own process, thereby isolating their memory from each other, and relies on security guarantees of the operating system.\nSuppose now, you open the same two websites: www.attacker.com and www.my-bank.com, as seen in the diagram above. Site isolation recognizes that the two sites are not “same-site” and hence the site isolation architecture will completely separate content from attacker.com and my-bank.com into separate operating system processes.\nThis process separation of content from different sites provides the memory protections required to allow for a secure browsing experience, making it even harder for sites to execute Spectre-like attacks, and, ultimately, provide a secure browsing experience for our users.\n\nWith Site Isolation, Firefox loads subframes from different sites in their own processes.\nIdentical to loading sites into two different tabs is the separation of two different sites when loaded into subframes. Let’s revisit an earlier example where pages contained subframes, with Site Isolation, subframes that are not “same-site” with the top level page will load in a different process.\nIn the diagram above, we see that the page www.attacker.com embeds a page from www.my-bank.com and loads in a different process. Having a top level document and subframes from different sites loaded in their own processes ensures their memory is isolated from each other, yielding profound security guarantees.\nAdditional Benefits of Site Isolation\nWith Site Isolation architecture in place, we are able to bring additional security hardening to Firefox to keep you and your data safe. Besides providing an extra layer of defence against possible security threats, Site Isolation brings other wins:\n\nBy placing more pages into separate processes, we can ensure that doing heavy computation or garbage collection on one page will not degrade the responsiveness of pages in other processes.\nUsing more processes to load websites allows us to spread work across many CPU cores and use the underlying hardware more efficiently.\nDue to the finer-grained separation of sites, a subframe or a tab crashing will not affect websites loaded in different processes, resulting in an improved application stability and better user experience.\n\nGoing Forward\nWe are currently testing Site Isolation on desktop browsers Nightly and Beta with a subset of users and will be rolling out to more desktop users soon. However, if you already want to benefit from the improved security architecture now, you can enable it by downloading the Nightly or Beta browser from here and following these steps:\nTo enable Site Isolation on Firefox Nightly:\n\nNavigate to about:preferences#experimental\nCheck the “Fission (Site Isolation)” checkbox to enable.\nRestart Firefox.\n\nTo enable Site Isolation on Firefox Beta or Release:\n\nNavigate to about:config.\nSet `fission.autostart` pref to `true`.\nRestart Firefox.\n\nFor technical details on how we group sites and subframes together, you can check out our new process manager tool at “about:processes” (type it into the address bar) and follow the project at  https://wiki.mozilla.org/Project_Fission.\nWith Site Isolation enabled on Firefox for Desktop, Mozilla takes its security guarantees to the next level and protects you against a new class of malicious attacks by relying on memory protections of OS-level process separation for each site. If you are interested in contributing to Mozilla’s open-source projects, you can help us by filing bugs here if you run into any problems with Site Isolation enabled.\nAcknowledgements\nSite Isolation (Project Fission), has been a massive multi-year project. Thank you to all of the talented and awesome colleagues who contributed to this work! It’s a privilege to work with people who are passionate about building the web we want: free, inclusive, independent and secure! In particular, I would like to thank Neha Kochar, Nika Layzell, Mike Conley, Melissa Thermidor, Chris Peterson, Kashav Madan, Andrew McCreight, Peter Van der Beken, Tantek Çelik and Christoph Kerschbaumer for their insightful comments and discussions.  Finally, thank you to Morgan Rae Reschenberg for helping me craft alt-text to meet the high standards of our web accessibility principles and allow everyone on the internet to easily gather the benefits provided by Site Isolation.\nThe post Introducing Firefox’s new Site Isolation Security Architecture appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-05-18T15:45:15.000Z",
      "date_modified": "2021-05-18T15:45:15.000Z",
      "_plugin": {
        "pageFilename": "be6d54f00d796c380e2f81e522e757976542ade7d8b57370f8deba97009912b9.html"
      }
    },
    {
      "id": "https://hacks.mozilla.org/?p=47230",
      "url": "https://hacks.mozilla.org/2021/04/pyodide-spin-out-and-0-17-release/",
      "title": "Pyodide Spin Out and 0.17 Release",
      "summary": "We are happy to announce that Pyodide has become an independent and community-driven project. We are also pleased to announce the 0.17 release for Pyodide with many new features and improvements. Pyodide consists of the CPython 3.8 interpreter compiled to WebAssembly which allows Python to run in the browser.\nThe post Pyodide Spin Out and 0.17 Release appeared first on Mozilla Hacks - the Web developer blog.",
      "content_html": "<p>We are happy to announce that Pyodide has become an independent and community-driven project. We are also pleased to announce the 0.17 release for Pyodide with many new features and improvements.</p>\n<p>Pyodide consists of the CPython 3.8 interpreter compiled to WebAssembly which allows Python to run in the browser. Many popular scientific Python packages have also been compiled and made available. In addition, Pyodide can install any Python package with a pure Python wheel from the Python Package Index (PyPi). Pyodide also includes a comprehensive foreign function interface which exposes the ecosystem of Python packages to Javascript and the browser user interface, including the DOM, to Python.</p>\n<p>You can try out the latest version of Pyodide in a <a href=\"https://pyodide.org/en/0.17.0/console.html\">REPL</a> directly in your browser.</p>\n<h2>Pyodide is now an independent project</h2>\n<p>We are happy to announce that Pyodide now has a new home in a separate GitHub organisation (<a href=\"https://github.com/pyodide\">github.com/pyodide</a>) and is maintained by a volunteer team of contributors. The project documentation is available on <a href=\"https://pyodide.org\">pyodide.org</a>.</p>\n<p>Pyodide was originally developed inside Mozilla to allow the use of Python in <a href=\"https://hacks.mozilla.org/2019/03/iodide-an-experimental-tool-for-scientific-communicatiodide-for-scientific-communication-exploration-on-the-web/\">Iodide</a>, an experimental effort to build an interactive scientific computing environment for the web.  Since <a href=\"https://hacks.mozilla.org/2019/04/pyodide-bringing-the-scientific-python-stack-to-the-browser/\">its initial release and announcement</a>, Pyodide has attracted a large amount of interest from the community, remains actively developed, and is used in many projects outside of Mozilla.</p>\n<p>The core team has approved a transparent <a href=\"https://pyodide.org/en/0.17.0/project/governance.html\">governance document </a> and has a <a href=\"https://pyodide.org/en/0.17.0/project/roadmap.html\">roadmap for future developments</a>. Pyodide also has a<a href=\"https://pyodide.org/en/0.17.0/project/code-of-conduct.html#code-of-conduct\"> Code of Conduct</a> which we expect all contributors and core members to adhere to.</p>\n<p>New contributors are welcome to participate in the project development on Github. There are many ways to contribute, including code contributions, documentation improvements, adding packages, and using Pyodide for your applications and providing feedback.</p>\n<h2>The Pyodide 0.17 release</h2>\n<p>Pyodide 0.17.0 is a major step forward from previous versions. It includes:</p>\n<ul>\n<li>major maintenance improvements,</li>\n<li>a thorough redesign of the central APIs, and</li>\n<li>careful elimination of error leaks and memory leaks</li>\n</ul>\n<h3>Type translation improvements</h3>\n<p>The type translations module was significantly reworked in v0.17 with the goal that round trip translations of objects between Python and Javascript produces an identical object.</p>\n<p>In other words, Python -&gt; JS -&gt; Python translation and JS -&gt; Python -&gt; JS translation now produce objects that are  equal to the original object. (A couple of exceptions to this remain due to unavoidable design tradeoffs.)</p>\n<p>One of Pyodide’s strengths is the foreign function interface between Python and Javascript, which at its best can practically erase the mental overhead of working with two different languages. All I/O must pass through the usual web APIs, so in order for Python code to take advantage of the browser’s strengths , we need to be able to support use cases like generating image data in Python and rendering the data to an HTML5 Canvas, or implementing event handlers in Python.</p>\n<p>In the past we found that one of the major pain points in using Pyodide occurs when an object makes a round trip from Python to Javascript and back to Python and comes back different. This violated the expectations of the user and forced inelegant workarounds.</p>\n<p>The issues with round trip translations were primarily caused by implicit conversion of Python types to Javascript. The implicit conversions were intended to be convenient, but the system was inflexible and surprising to users. We still implicitly convert strings, numbers, booleans, and None. Most other objects are shared between languages using proxies that allow methods and some operations to be called on the object from the other language. The proxies can be converted to native types with new explicit converter methods called <code>.toJs</code> and <code>to_py</code>.</p>\n<p>For instance, given an Array in JavaScript,</p>\n<pre><code class=\"language-js\">window.x = [\"a\", \"b\", \"c\"];\n</code></pre>\n<p>We can access it in Python as,</p>\n<pre>&gt;&gt;&gt; from js import x # import x from global Javascript scope\n>&gt;&gt; type(x)\n&lt;class 'JsProxy'&gt;\n>&gt;&gt; x[0]    # can index x directly\n'a'\n>&gt;&gt; x[1] = 'c' # modify x\n>&gt;&gt; x.to_py()   # convert x to a Python list\n['a', 'c']\n</pre>\n<p>Several other conversion methods have been added for more complicated use cases. This gives the user much finer control over type conversions than was previously possible.</p>\n<p>For example, suppose we have a Python list and want to use it as an argument to a Javascript function that expects an Array.  Either the caller or the callee needs to take care of the conversion. This allows us to directly call functions that are unaware of Pyodide.</p>\n<p>Here is an example of calling a Javascript function from Python with argument conversion on the Python side:</p>\n<pre><code class=\"js\">\nfunction jsfunc(array) {\n  array.push(2);\n  return array.length;\n}\n\npyodide.runPython(`\nfrom js import jsfunc\nfrom pyodide import to_js\n\ndef pyfunc():\n  mylist = [1,2,3]\n  jslist = to_js(mylist)\n  return jsfunc(jslist) # returns 4\n`)\n</code></pre>\n<p>This would work well in the case that <code>jsfunc</code> is a Javascript built-in and <code>pyfunc</code> is part of our codebase. If <code>pyfunc</code> is part of a Python package, we can handle the conversion in Javascript instead:</p>\n<pre><code class=\"js\">\nfunction jsfunc(pylist) {\n  let array = pylist.toJs();\n  array.push(2);\n  return array.length;\n}\n</code></pre>\n<p>See the <a href=\"https://pyodide.org/en/0.17.0/usage/type-conversions.html\">type translation documentation</a> for more information.</p>\n<h3>Asyncio support</h3>\n<p>Another major new feature is the implementation of a Python event loop that schedules coroutines to run on the browser event loop. This makes it possible to use asyncio in Pyodide.</p>\n<p>Additionally, it is now possible to await Javascript Promises in Python and to await Python awaitables in Javascript. This allows for seamless interoperability between asyncio in Python and Javascript (though memory management issues may arise in complex use cases).</p>\n<p>Here is an example where we define a Python async function that awaits the Javascript async function “fetch” and then we await the Python async function from Javascript.</p>\n<pre><code class=\"js\">\npyodide.runPython(`\nasync def test():\n    from js import fetch\n    # Fetch the Pyodide packages list\n    r = await fetch(\"packages.json\")\n    data = await r.json()\n    # return all available packages\n    return data.dependencies.object_keys()\n`);\n\nlet test = pyodide.globals.get(\"test\");\n\n// we can await the test() coroutine from Javascript\nresult = await test();\nconsole.log(result);\n// logs [\"asciitree\", \"parso\", \"scikit-learn\", ...]\n</code></pre>\n<h3>Error Handling</h3>\n<p>Errors can now be thrown in Python and caught in Javascript or thrown in Javascript and caught in Python. Support for this is integrated at the lowest level, so calls between Javascript and C functions behave as expected. The error translation code is generated by C macros which makes implementing and debugging new logic dramatically simpler.</p>\n<p>For example:</p>\n<pre><code class=\"js\">\nfunction jserror() {\n  throw new Error(\"ooops!\");\n}\n\npyodide.runPython(`\nfrom js import jserror\nfrom pyodide import JsException\n\ntry:\n  jserror()\nexcept JsException as e:\n  print(str(e)) # prints \"TypeError: ooops!\"\n`);\n</code></pre>\n<h3>Emscripten update</h3>\n<p>Pyodide uses the <a href=\"https://emscripten.org/\">Emscripten</a> compiler toolchain to compile the CPython 3.8 interpreter and Python packages with C extensions to WebAssembly. In this release we finally completed the migration to the latest version of Emscripten that uses the upstream LLVM backend. This allows us to take advantage of recent improvements to the toolchain, including significant reductions in package size and execution time.</p>\n<p>For instance, the SciPy package shrank dramatically from 92 MB to 15 MB so Scipy is now cached by browsers. This greatly improves the usability of scientific Python packages that depend on scipy, such as scikit-image and scikit-learn. The size of the base Pyodide environment with only the CPython standard library shrank from 8.1 MB to 6.4 MB.</p>\n<p>On the performance side, the latest toolchain comes with a 25% to 30% run time improvement:<br />\n<img loading=\"lazy\" src=\"https://hacks.mozilla.org/files/2021/04/pyodide-benchmarks.png\" alt=\"\" width=\"853\" height=\"538\" class=\"alignnone size-full wp-image-47245\" srcset=\"https://hacks.mozilla.org/files/2021/04/pyodide-benchmarks.png 853w, https://hacks.mozilla.org/files/2021/04/pyodide-benchmarks-250x158.png 250w, https://hacks.mozilla.org/files/2021/04/pyodide-benchmarks-500x315.png 500w, https://hacks.mozilla.org/files/2021/04/pyodide-benchmarks-768x484.png 768w\" sizes=\"(max-width: 853px) 100vw, 853px\" /><br />\nPerformance ranges between near native to up to 3 to 5 times slower, depending on the benchmark.  The above benchmarks were created with Firefox 87.</p>\n<h3>Other changes</h3>\n<p>Other notable features include:</p>\n<ul>\n<li aria-level=\"1\">Fixed package loading for Safari v14+ and other Webkit-based browsers</li>\n<li aria-level=\"1\">Added support for relative URLs in micropip and loadPackage, and improved interaction between micropip and loadPackage</li>\n<li aria-level=\"1\">Support for implementing Python modules in Javascript</li>\n</ul>\n<p>We also did a large amount of maintenance work and code quality improvements:</p>\n<ul>\n<li aria-level=\"1\">Lots of bug fixes</li>\n<li aria-level=\"1\">Upstreamed a number of patches to the emscripten compiler toolchain</li>\n<li aria-level=\"1\">Added systematic error handling to the C code, including automatic adaptors between Javascript errors and CPython errors</li>\n<li aria-level=\"1\">Added internal consistency checks to detect memory leaks, detect fatal errors, and improve ease of debugging</li>\n</ul>\n<p>See <a href=\"https://pyodide.org/en/0.17.0/project/changelog.html#version-0-17-0\">the changelog</a> for more details.</p>\n<h2>Winding down Iodide</h2>\n<p>Mozilla has made the difficult decision to wind down the Iodide project. While <a href=\"https://alpha.iodide.io\">alpha.iodide.io</a> will continue to be available for now (in part to provide a demonstration of Pyodide’s capabilities), we do not recommend using it for important work as it may shut down in the future. Since iodide’s release, there have been <a href=\"https://pyodide.org/en/0.17.0/project/related-projects.html#notebook-environements-ides-repls\">many efforts</a> at creating interactive notebook environments based on Pyodide which are in active development and offer a similar environment for creating interactive visualizations in the browser using python.</p>\n<h2>Next steps for Pyodide</h2>\n<p>While many issues were addressed in this release, a number of other major steps remain on the roadmap. We can mention</p>\n<ul>\n<li aria-level=\"1\">Reducing download sizes and initialization times</li>\n<li aria-level=\"1\">Improve performance of Python code in Pyodide</li>\n<li aria-level=\"1\">Simplification of package loading system</li>\n<li aria-level=\"1\">Update scipy to a more recent version</li>\n<li aria-level=\"1\">Better project sustainability, for instance, by seeking synergies with the conda-forge project and its tooling.</li>\n<li aria-level=\"1\">Better support for web workers</li>\n<li aria-level=\"1\">Better support for synchronous IO (popular for programming education)</li>\n</ul>\n<p>For additional information see the <a href=\"https://pyodide.org/en/latest/project/roadmap.html\">project roadmap</a>.</p>\n<h2>Acknowledgements</h2>\n<p>Lots of thanks to:</p>\n<ul>\n<li aria-level=\"1\"><a href=\"https://github.com/dalcde\">Dexter Chua</a> and <a href=\"https://github.com/joemarshall\">Joe Marshall</a> for improving the build setup and making Emscripten migration possible.</li>\n<li aria-level=\"1\"><a href=\"https://github.com/hoodmane\">Hood Chatham</a> for in-depth improvement of the type translation module and adding asyncio support</li>\n<li aria-level=\"1\">and <a href=\"https://github.com/casatir\">Romain Casati</a> for improving the Pyodide REPL console.</li>\n</ul>\n<p>We are also grateful to all <a href=\"https://github.com/pyodide/pyodide/graphs/contributors\">Pyodide contributors</a>.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/04/pyodide-spin-out-and-0-17-release/\">Pyodide Spin Out and 0.17 Release</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>",
      "content_text": "We are happy to announce that Pyodide has become an independent and community-driven project. We are also pleased to announce the 0.17 release for Pyodide with many new features and improvements.\nPyodide consists of the CPython 3.8 interpreter compiled to WebAssembly which allows Python to run in the browser. Many popular scientific Python packages have also been compiled and made available. In addition, Pyodide can install any Python package with a pure Python wheel from the Python Package Index (PyPi). Pyodide also includes a comprehensive foreign function interface which exposes the ecosystem of Python packages to Javascript and the browser user interface, including the DOM, to Python.\nYou can try out the latest version of Pyodide in a REPL directly in your browser.\nPyodide is now an independent project\nWe are happy to announce that Pyodide now has a new home in a separate GitHub organisation (github.com/pyodide) and is maintained by a volunteer team of contributors. The project documentation is available on pyodide.org.\nPyodide was originally developed inside Mozilla to allow the use of Python in Iodide, an experimental effort to build an interactive scientific computing environment for the web.  Since its initial release and announcement, Pyodide has attracted a large amount of interest from the community, remains actively developed, and is used in many projects outside of Mozilla.\nThe core team has approved a transparent governance document  and has a roadmap for future developments. Pyodide also has a Code of Conduct which we expect all contributors and core members to adhere to.\nNew contributors are welcome to participate in the project development on Github. There are many ways to contribute, including code contributions, documentation improvements, adding packages, and using Pyodide for your applications and providing feedback.\nThe Pyodide 0.17 release\nPyodide 0.17.0 is a major step forward from previous versions. It includes:\n\nmajor maintenance improvements,\na thorough redesign of the central APIs, and\ncareful elimination of error leaks and memory leaks\n\nType translation improvements\nThe type translations module was significantly reworked in v0.17 with the goal that round trip translations of objects between Python and Javascript produces an identical object.\nIn other words, Python -> JS -> Python translation and JS -> Python -> JS translation now produce objects that are  equal to the original object. (A couple of exceptions to this remain due to unavoidable design tradeoffs.)\nOne of Pyodide’s strengths is the foreign function interface between Python and Javascript, which at its best can practically erase the mental overhead of working with two different languages. All I/O must pass through the usual web APIs, so in order for Python code to take advantage of the browser’s strengths , we need to be able to support use cases like generating image data in Python and rendering the data to an HTML5 Canvas, or implementing event handlers in Python.\nIn the past we found that one of the major pain points in using Pyodide occurs when an object makes a round trip from Python to Javascript and back to Python and comes back different. This violated the expectations of the user and forced inelegant workarounds.\nThe issues with round trip translations were primarily caused by implicit conversion of Python types to Javascript. The implicit conversions were intended to be convenient, but the system was inflexible and surprising to users. We still implicitly convert strings, numbers, booleans, and None. Most other objects are shared between languages using proxies that allow methods and some operations to be called on the object from the other language. The proxies can be converted to native types with new explicit converter methods called .toJs and to_py.\nFor instance, given an Array in JavaScript,\nwindow.x = [\"a\", \"b\", \"c\"];\n\nWe can access it in Python as,\n>>> from js import x # import x from global Javascript scope\n>>> type(x)\n<class 'JsProxy'>\n>>> x[0]    # can index x directly\n'a'\n>>> x[1] = 'c' # modify x\n>>> x.to_py()   # convert x to a Python list\n['a', 'c']\n\nSeveral other conversion methods have been added for more complicated use cases. This gives the user much finer control over type conversions than was previously possible.\nFor example, suppose we have a Python list and want to use it as an argument to a Javascript function that expects an Array.  Either the caller or the callee needs to take care of the conversion. This allows us to directly call functions that are unaware of Pyodide.\nHere is an example of calling a Javascript function from Python with argument conversion on the Python side:\n\nfunction jsfunc(array) {\n  array.push(2);\n  return array.length;\n}\n\npyodide.runPython(`\nfrom js import jsfunc\nfrom pyodide import to_js\n\ndef pyfunc():\n  mylist = [1,2,3]\n  jslist = to_js(mylist)\n  return jsfunc(jslist) # returns 4\n`)\n\nThis would work well in the case that jsfunc is a Javascript built-in and pyfunc is part of our codebase. If pyfunc is part of a Python package, we can handle the conversion in Javascript instead:\n\nfunction jsfunc(pylist) {\n  let array = pylist.toJs();\n  array.push(2);\n  return array.length;\n}\n\nSee the type translation documentation for more information.\nAsyncio support\nAnother major new feature is the implementation of a Python event loop that schedules coroutines to run on the browser event loop. This makes it possible to use asyncio in Pyodide.\nAdditionally, it is now possible to await Javascript Promises in Python and to await Python awaitables in Javascript. This allows for seamless interoperability between asyncio in Python and Javascript (though memory management issues may arise in complex use cases).\nHere is an example where we define a Python async function that awaits the Javascript async function “fetch” and then we await the Python async function from Javascript.\n\npyodide.runPython(`\nasync def test():\n    from js import fetch\n    # Fetch the Pyodide packages list\n    r = await fetch(\"packages.json\")\n    data = await r.json()\n    # return all available packages\n    return data.dependencies.object_keys()\n`);\n\nlet test = pyodide.globals.get(\"test\");\n\n// we can await the test() coroutine from Javascript\nresult = await test();\nconsole.log(result);\n// logs [\"asciitree\", \"parso\", \"scikit-learn\", ...]\n\nError Handling\nErrors can now be thrown in Python and caught in Javascript or thrown in Javascript and caught in Python. Support for this is integrated at the lowest level, so calls between Javascript and C functions behave as expected. The error translation code is generated by C macros which makes implementing and debugging new logic dramatically simpler.\nFor example:\n\nfunction jserror() {\n  throw new Error(\"ooops!\");\n}\n\npyodide.runPython(`\nfrom js import jserror\nfrom pyodide import JsException\n\ntry:\n  jserror()\nexcept JsException as e:\n  print(str(e)) # prints \"TypeError: ooops!\"\n`);\n\nEmscripten update\nPyodide uses the Emscripten compiler toolchain to compile the CPython 3.8 interpreter and Python packages with C extensions to WebAssembly. In this release we finally completed the migration to the latest version of Emscripten that uses the upstream LLVM backend. This allows us to take advantage of recent improvements to the toolchain, including significant reductions in package size and execution time.\nFor instance, the SciPy package shrank dramatically from 92 MB to 15 MB so Scipy is now cached by browsers. This greatly improves the usability of scientific Python packages that depend on scipy, such as scikit-image and scikit-learn. The size of the base Pyodide environment with only the CPython standard library shrank from 8.1 MB to 6.4 MB.\nOn the performance side, the latest toolchain comes with a 25% to 30% run time improvement:\n\nPerformance ranges between near native to up to 3 to 5 times slower, depending on the benchmark.  The above benchmarks were created with Firefox 87.\nOther changes\nOther notable features include:\n\nFixed package loading for Safari v14+ and other Webkit-based browsers\nAdded support for relative URLs in micropip and loadPackage, and improved interaction between micropip and loadPackage\nSupport for implementing Python modules in Javascript\n\nWe also did a large amount of maintenance work and code quality improvements:\n\nLots of bug fixes\nUpstreamed a number of patches to the emscripten compiler toolchain\nAdded systematic error handling to the C code, including automatic adaptors between Javascript errors and CPython errors\nAdded internal consistency checks to detect memory leaks, detect fatal errors, and improve ease of debugging\n\nSee the changelog for more details.\nWinding down Iodide\nMozilla has made the difficult decision to wind down the Iodide project. While alpha.iodide.io will continue to be available for now (in part to provide a demonstration of Pyodide’s capabilities), we do not recommend using it for important work as it may shut down in the future. Since iodide’s release, there have been many efforts at creating interactive notebook environments based on Pyodide which are in active development and offer a similar environment for creating interactive visualizations in the browser using python.\nNext steps for Pyodide\nWhile many issues were addressed in this release, a number of other major steps remain on the roadmap. We can mention\n\nReducing download sizes and initialization times\nImprove performance of Python code in Pyodide\nSimplification of package loading system\nUpdate scipy to a more recent version\nBetter project sustainability, for instance, by seeking synergies with the conda-forge project and its tooling.\nBetter support for web workers\nBetter support for synchronous IO (popular for programming education)\n\nFor additional information see the project roadmap.\nAcknowledgements\nLots of thanks to:\n\nDexter Chua and Joe Marshall for improving the build setup and making Emscripten migration possible.\nHood Chatham for in-depth improvement of the type translation module and adding asyncio support\nand Romain Casati for improving the Pyodide REPL console.\n\nWe are also grateful to all Pyodide contributors.\nThe post Pyodide Spin Out and 0.17 Release appeared first on Mozilla Hacks - the Web developer blog.",
      "date_published": "2021-04-22T15:17:11.000Z",
      "date_modified": "2021-04-22T15:17:11.000Z",
      "_plugin": {
        "pageFilename": "26beaf9d7c83aa598090ae7192c1b905281101f0b96f99ed17106e65c2586469.html"
      }
    }
  ],
  "_plugin": {
    "rawFeed": "<?xml version=\"1.0\" encoding=\"UTF-8\"?><rss version=\"2.0\"\n\txmlns:content=\"http://purl.org/rss/1.0/modules/content/\"\n\txmlns:wfw=\"http://wellformedweb.org/CommentAPI/\"\n\txmlns:dc=\"http://purl.org/dc/elements/1.1/\"\n\txmlns:atom=\"http://www.w3.org/2005/Atom\"\n\txmlns:sy=\"http://purl.org/rss/1.0/modules/syndication/\"\n\txmlns:slash=\"http://purl.org/rss/1.0/modules/slash/\"\n\t>\n\n<channel>\n\t<title>Mozilla Hacks &#8211; the Web developer blog</title>\n\t<atom:link href=\"https://hacks.mozilla.org/feed/\" rel=\"self\" type=\"application/rss+xml\" />\n\t<link>https://hacks.mozilla.org</link>\n\t<description>hacks.mozilla.org</description>\n\t<lastBuildDate>Tue, 14 Jun 2022 15:49:04 +0000</lastBuildDate>\n\t<language>en-US</language>\n\t<sy:updatePeriod>\n\thourly\t</sy:updatePeriod>\n\t<sy:updateFrequency>\n\t1\t</sy:updateFrequency>\n\t<generator>https://wordpress.org/?v=5.9.3</generator>\n\t<item>\n\t\t<title>Everything Is Broken: Shipping rust-minidump at Mozilla &#8211; Part 1</title>\n\t\t<link>https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/</link>\n\t\t\t\t\t<comments>https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/#respond</comments>\n\t\t\n\t\t<dc:creator><![CDATA[Aria Beingessner]]></dc:creator>\n\t\t<pubDate>Tue, 14 Jun 2022 15:05:06 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Developer Tools]]></category>\n\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[breakpad]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<category><![CDATA[google]]></category>\n\t\t<category><![CDATA[macos]]></category>\n\t\t<category><![CDATA[minidump]]></category>\n\t\t<category><![CDATA[rust]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47842</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>For the last year, we've been working on the development of rust-minidump, a pure-Rust replacement for the minidump-processing half of google-breakpad. The first in this two-part series explains what minidumps are, and how we made rust-minidump.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/\">Everything Is Broken: Shipping rust-minidump at Mozilla &#8211; Part 1</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<h1><strong>Everything Is Broken: Shipping rust-minidump at Mozilla</strong></h1>\n<p>For the last year I&#8217;ve been leading the development of <a href=\"https://github.com/luser/rust-minidump/\">rust-minidump</a>, a pure-Rust replacement for the minidump-processing half of <a href=\"https://chromium.googlesource.com/breakpad/breakpad/\">google-breakpad</a>.</p>\n<p>Well actually in some sense I <em>finished</em> that work, because Mozilla already <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">deployed it</a> as <a href=\"https://crash-stats.mozilla.org/\">the crash processing backend for Firefox</a> 6 months ago, it runs in half the time, and seems to be more reliable. (And you know, <em>isn&#8217;t</em> a terrifying ball of C++ that parses and evaluates arbitrary input from the internet. We did our best to isolate Breakpad, but still… <em>yikes</em>.)</p>\n<p>This is a pretty fantastic result, but there&#8217;s always more work to do because <em>Minidumps are an inky abyss that grows deeper the further you delve…</em> wait no I&#8217;m getting ahead of myself. First the light, then the abyss. Yes. Light first.</p>\n<p>What I <em>can</em> say is that we have a very solid implementation of the core functionality of minidump parsing+analysis for the biggest platforms (x86, x64, ARM, ARM64; Windows, MacOS, Linux, Android). But if you want to read minidumps generated on a <em>PlayStation 3</em> or process a <em>Full Memory</em> dump, you won&#8217;t be served quite as well.</p>\n<p>We&#8217;ve put a lot of effort into documenting and testing this thing, so I&#8217;m pretty confident in it!</p>\n<p><strong>Unfortunately! Confidence! Is! Worth! Nothing!</strong></p>\n<p>Which is why this is the story of how we did our best to make this nightmare as robust as we could and still got 360 dunked on from space by the sudden and <em>incredible</em> fuzzing efforts of <a href=\"https://github.com/5225225\">@5225225</a>.</p>\n<p>This article is broken into two parts:</p>\n<ol>\n<li>what minidumps are, and how we made rust-minidump</li>\n<li>how we got absolutely owned by simple fuzzing</li>\n</ol>\n<p>You are reading part 1, wherein we build up our hubris.</p>\n<h1><strong>Background: What&#8217;s A Minidump, and Why Write rust-minidump?</strong></h1>\n<p>Your program crashes. You want to know why your program crashed, but it happened on a user&#8217;s machine on the other side of the world. A full coredump (all memory allocated by the program) is enormous &#8212; we can&#8217;t have users sending us 4GB files! Ok let&#8217;s just collect up the most important regions of memory like the stacks and where the program crashed. Oh and I guess if we&#8217;re taking the time, let&#8217;s stuff some metadata about the system and process in there too.</p>\n<p>Congratulations you have invented <a href=\"https://docs.microsoft.com/en-us/windows/win32/debug/minidump-files\">Minidumps</a>. Now you can turn a 100-thread coredump that would otherwise be 4GB into a nice little 2MB file that you can send over the internet and do postmortem analysis on.</p>\n<p>Or more specifically, Microsoft did. So long ago that their docs don&#8217;t even discuss platform support. MiniDumpWriteDump&#8217;s supported versions are simply &#8220;Windows&#8221;. Microsoft Research has presumably developed a time machine to guarantee this.</p>\n<p>Then Google came along (circa 2006-2007) and said &#8220;wouldn&#8217;t it be nice if we could make minidumps on <em>any</em> platform&#8221;? Thankfully Microsoft had actually built the format pretty extensibly, so it wasn&#8217;t too bad to extend the format for Linux, MacOS, BSD, Solaris, and so on. Those extensions became <a href=\"https://chromium.googlesource.com/breakpad/breakpad/\">google-breakpad</a> (or just Breakpad) which included a ton of different tools for generating, parsing, and analyzing their extended minidump format (and native Microsoft ones).</p>\n<p>Mozilla helped out with this a lot because apparently, our crash reporting infrastructure (&#8220;Talkback&#8221;) was <em>miserable</em> circa 2007, and this seemed like a nice improvement. Needless to say, we&#8217;re pretty invested in breakpad&#8217;s minidumps at this point.</p>\n<p>Fast forward to the present day and in a hilarious twist of fate, products like VSCode mean that Microsoft now supports applications that run on Linux and MacOS so it runs breakpad in production and has to handle non-Microsoft minidumps somewhere in its crash reporting infra, so someone else&#8217;s extension of their own format is somehow their problem now!</p>\n<p>Meanwhile, Google has kind-of moved on to <a href=\"https://chromium.googlesource.com/crashpad/crashpad\">Crashpad</a>. I say kind-of because there&#8217;s still a lot of Breakpad in there, but they&#8217;re more interested in building out tooling on top of it than improving Breakpad itself. Having made a few changes to Breakpad: <strong>honestly fair</strong>, I don&#8217;t want to work on it either. Still, this was a bit of a problem for us, because it meant the project became increasingly under-staffed.</p>\n<p>By the time I started working on crash reporting, Mozilla had basically given up on upstreaming fixes/improvements to Breakpad, and was just using its own patched fork. But even <em>without</em> the need for upstreaming patches, every change to Breakpad filled us with dread: many proposed improvements to our crash reporting infrastructure stalled out at &#8220;time to implement this in Breakpad&#8221;.</p>\n<p>Why is working on Breakpad so miserable, you ask?</p>\n<p>Parsing and analyzing minidumps is basically an exercise in writing a fractal parser of platform-specific formats nested in formats nested in formats. For many operating systems. For many hardware architectures. And all the inputs you&#8217;re parsing and analyzing are terrible and buggy so you <em>have</em> to write a really permissive parser and crawl forward however you can.</p>\n<p>Some specific MSVC toolchain that was part of Windows XP had a bug in its debuginfo format? <strong>Too bad, symbolicate that stack frame anyway!</strong></p>\n<p>The program crashed because it horribly corrupted its own stack? <strong>Too bad, produce a backtrace anyway!</strong></p>\n<p>The minidump writer itself completely freaked out and wrote a bunch of garbage to one stream? <strong>Too bad, produce whatever output you can anyway!</strong></p>\n<p>Hey, you know who has a lot of experience dealing with really complicated permissive parsers written in C++? Mozilla! That&#8217;s like <em>the core functionality</em> of a web browser.</p>\n<p>Do you know Mozilla&#8217;s secret solution to writing really complicated permissive parsers in C++?</p>\n<p><strong>We stopped doing it.</strong></p>\n<p>We developed Rust and ported our nastiest parsers to it.</p>\n<p>We&#8217;ve done it a lot, and <a href=\"https://hacks.mozilla.org/2017/08/inside-a-super-fast-css-engine-quantum-css-aka-stylo/\">when we do</a> we&#8217;re always like <a href=\"https://www.joshmatthews.net/rbr17/\">&#8220;wow this is so much more reliable and easy to maintain and it&#8217;s even faster now&#8221;</a>. Rust is a really good language for writing parsers. C++ really isn&#8217;t.</p>\n<p>So we Rewrote It In Rust (or as the kids call it, &#8220;Oxidized It&#8221;). Breakpad is big, so we haven&#8217;t actually covered all of its features. We&#8217;ve specifically written and deployed:</p>\n<ul>\n<li><a href=\"https://github.com/mozilla/dump_syms\">dump_syms</a> which processes native build artifacts into symbol files.</li>\n<li><a href=\"https://github.com/luser/rust-minidump/\">rust-minidump</a> which is a collection of crates that parse and analyze minidumps. Or more specifically, we deployed <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">minidump-stackwalk</a>, which is the high-level cli interface to all of rust-minidump.</li>\n</ul>\n<p>Notably missing from this picture is <em>minidump writing</em>, or what google-breakpad calls a <em>client</em> (because it runs on the client&#8217;s machine). We <em>are </em>working <a href=\"https://github.com/rust-minidump/minidump-writer\">on a rust-based minidump writer</a>, but it&#8217;s not something we can recommend using quite yet (although it has sped up a lot thanks to help from <a href=\"https://embark.dev/\">Embark Studios</a>).</p>\n<p>This is arguably the messiest and hardest work because it has a horrible job: use a bunch of native system APIs to gather up a bunch of OS-specific and Hardware-specific information about the crash AND do it for a program that just crashed, on a machine that <em>caused </em>the program to crash.</p>\n<p>We have a long road ahead but every time we get to the other side of one of these projects it&#8217;s <em>wonderful</em>.</p>\n<p>&nbsp;</p>\n<h1><strong>Background: Stackwalking and Calling Conventions</strong></h1>\n<p>One of rust-minidump&#8217;s (<a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">minidump-stackwalk&#8217;s</a>) most important jobs is to take the state for a thread (general purpose registers and stack memory) and create a backtrace for that thread (unwind/stackwalk). This is a surprisingly complicated and messy job, made only more complicated by the fact that <em>we are trying to analyze the memory of a process that got messed up enough to crash</em>.</p>\n<p>This means our stackwalkers are inherently working with dubious data, and all of our stackwalking techniques are based on heuristics that can go wrong and we can very easily find ourselves in situations where the stackwalk goes backwards or sideways or infinite and we just have to try to deal with it!</p>\n<p>It&#8217;s also pretty common to see a stackwalker start <em>hallucinating</em>, which is my term for &#8220;the stackwalker found something that looked plausible enough and went on a wacky adventure through the stack and made up a whole pile of useless garbage frames&#8221;. Hallucination is most common near the bottom of the stack where it&#8217;s also least offensive. This is because each frame you walk is another chance for something to go wrong, but also increasingly uninteresting because you&#8217;re rarely interested in confirming that a thread started in The Same Function All Threads Start In.</p>\n<p>All of these problems would basically go away if everyone agreed to properly preserve their cpu&#8217;s <a href=\"https://gankra.github.io/blah/compact-unwinding/#frame-pointer-unwinding-standard-prologues\">PERFECTLY GOOD DEDICATED FRAME POINTER REGISTER</a>. Just kidding, turning on frame pointers doesn&#8217;t really work either because Microsoft <a href=\"https://github.com/rust-lang/rust/issues/82333\">invented chaos frame pointers</a> that can&#8217;t be used for unwinding! I assume this happened because they accidentally stepped on the wrong butterfly while they were traveling back in time to invent minidumps. (I&#8217;m sure it was a decision that made more sense 20 years ago, but it has not aged well.)</p>\n<p>If you would like to learn more about the different techniques for unwinding, <a href=\"https://gankra.github.io/blah/compact-unwinding/#background-unwinding-and-debug-info\">I wrote about them over here</a> in my <a href=\"https://gankra.github.io/blah/compact-unwinding\">article on Apple&#8217;s Compact Unwind Info</a>. I&#8217;ve also attempted to <a href=\"https://docs.rs/breakpad-symbols/latest/breakpad_symbols/walker/index.html\">document breakpad&#8217;s STACK WIN and STACK CFI unwind info formats here</a>, which are more similar to the  DWARF and PE32 unwind tables (which are basically tiny programming languages).</p>\n<p>If you would like to learn more about ABIs in general, <a href=\"https://gankra.github.io/blah/rust-layouts-and-abis/#calling-conventions\">I wrote an entire article about them here</a>. The end of that article also includes an <a href=\"https://gankra.github.io/blah/rust-layouts-and-abis/#calling-conventions\">introduction to how calling conventions work</a>. Understanding calling conventions is key to implementing unwinders.</p>\n<p>&nbsp;</p>\n<p><strong>How Hard Did You Really Test Things?</strong></p>\n<p>Hopefully you now have a bit of a glimpse into why analyzing minidumps is an enormous headache. And of course you know how the story ends: that fuzzer kicks our butts! But of course to really savor our defeat, you have to see how hard we tried to do a good job! It&#8217;s time to build up our hubris and pat ourselves on the back.</p>\n<p>So how much work <em>actually</em> went into making rust-minidump robust before the fuzzer went to work on it?</p>\n<p>Quite a bit!</p>\n<p>I&#8217;ll never argue all the work we did was <em>perfect</em> but we definitely did some good work here, both for synthetic inputs and real world ones. Probably the biggest &#8220;flaw&#8221; in our methodology was the fact that we were only focused on getting Firefox&#8217;s usecase to work. Firefox runs on a lot of platforms and sees a lot of messed up stuff, but it&#8217;s still a fairly coherent product that only uses so many features of minidumps.</p>\n<p>This is one of the nice benefits of our recent work with <a href=\"https://sentry.io/\">Sentry</a>, which is basically a Crash Reporting As A Service company. They are <em>way</em> more liable to stress test all kinds of weird corners of the format that Firefox doesn&#8217;t, and they have definitely found (and fixed!) some places where something is wrong or missing! (And they recently deployed it into production too! <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f389.png\" alt=\"🎉\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" />)</p>\n<p>But hey don&#8217;t take my word for it, check out all the different testing we did:</p>\n<h2><strong>Synthetic Minidumps for Unit Tests</strong></h2>\n<p>rust-minidump includes a <a href=\"https://github.com/rust-minidump/rust-minidump/tree/553735e2624dcc6af82167f502cf92ae9a9fdc87/minidump-synth\">synthetic minidump generator</a> which lets you come up with a high-level description of the contents of a minidump, and then produces an actual minidump binary that we can feed it into the full parser:</p>\n<p>// Let&#8217;s make a synth minidump with this particular Crashpad Info&#8230;</p>\n<pre>let module = ModuleCrashpadInfo::new(42, Endian::Little)\r\n    .add_list_annotation(\"annotation\")\r\n    .add_simple_annotation(\"simple\", \"module\")\r\n    .add_annotation_object(\"string\", AnnotationValue::String(\"value\".to_owned()))\r\n    .add_annotation_object(\"invalid\", AnnotationValue::Invalid)\r\n    .add_annotation_object(\"custom\", AnnotationValue::Custom(0x8001, vec![42]));\r\n\r\nlet crashpad_info = CrashpadInfo::new(Endian::Little)\r\n    .add_module(module)\r\n    .add_simple_annotation(\"simple\", \"info\");\r\n\r\nlet dump = SynthMinidump::with_endian(Endian::Little).add_crashpad_info(crashpad_info);\r\n\r\n// convert the synth minidump to binary and read it like a normal minidump\r\nlet dump = read_synth_dump(dump).unwrap();</pre>\n<p>// Now check that the minidump reports the values we expect…</p>\n<p>minidump-synth intentionally avoids sharing layout code with the actual implementation so that incorrect changes to layouts won&#8217;t &#8220;accidentally&#8221; pass tests.</p>\n<p><em>A brief aside for some history</em>: this testing framework was started by the original lead on this project, <a href=\"https://twitter.com/TedMielczarek\">Ted Mielczarek</a>. He started rust-minidump as a side project to learn Rust when 1.0 was released and just never had the time to finish it. Back then he was working at Mozilla and also a major contributor to Breakpad, which is why rust-minidump has a lot of similar design choices and terminology.</p>\n<p>This case is no exception: our minidump-synth is a shameless copy of the <a href=\"https://chromium.googlesource.com/breakpad/breakpad/+/refs/heads/main/src/processor/synth_minidump.cc\">synth-minidump utility in breakpad&#8217;s code</a>, which was originally written by our <em>other</em> coworker <a href=\"https://www.red-bean.com/~jimb/\">Jim Blandy</a>. Jim is one of the only people in the world that I will actually admit writes really good tests and docs, so I am totally happy to blatantly copy his work here.</p>\n<p>Since this was all a learning experiment, Ted was understandably less rigorous about testing than usual. This meant a lot of minidump-synth was unimplemented when I came along, which also meant lots of minidump features were completely untested. (He built an absolutely great skeleton, just hadn&#8217;t had the time to fill it all in!)</p>\n<p>We spent <em>a lot</em> of time filling in more of minidump-synth&#8217;s implementation so we could write more tests and catch more issues, but this is <em>definitely</em> the weakest part of our tests. Some stuff was implemented before I got here, so I don&#8217;t even <em>know</em> what tests are missing!</p>\n<p>This is a good argument for some code coverage checks, but it would probably come back with &#8220;wow you should write a lot more tests&#8221; and we would all look at it and go &#8220;wow we sure should&#8221; and then we would probably never get around to it, because there are <em>many</em> things we <em>should</em> do.</p>\n<p>On the other hand, Sentry has been very useful in this regard because they already <em>have</em> a mature suite of tests full of weird corner cases they&#8217;ve built up over time, so they can easily identify things that really matter, know what the fix should roughly be, and can contribute pre-existing test cases!</p>\n<h2><strong>Integration and Snapshot Tests</strong></h2>\n<p>We tried our best to shore up coverage issues in our unit tests by adding more holistic tests. There&#8217;s a few checked in Real Minidumps that we have <a href=\"https://github.com/luser/rust-minidump/blob/40c3390f5705890f932f78b7db4fc02866e012b8/minidump-processor/tests/test_processor.rs\">some integration tests for</a> to make sure we handle Real Inputs properly.</p>\n<p>We even wrote a bunch of <a href=\"https://github.com/luser/rust-minidump/blob/40c3390f5705890f932f78b7db4fc02866e012b8/minidump-stackwalk/tests/test-minidump-stackwalk.rs\">integration tests for the CLI application that snapshot its output</a> to confirm that we never <em>accidentally</em> change the results.</p>\n<p>Part of the motivation for this is to ensure we don&#8217;t break the JSON output, which we also wrote a <a href=\"https://github.com/luser/rust-minidump/blob/40c3390f5705890f932f78b7db4fc02866e012b8/minidump-processor/json-schema.md\">very detailed schema document for</a> and are trying to keep stable so people can actually rely on it while the actual implementation details are still in flux.</p>\n<p>Yes, <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk\">minidump-stackwalk</a> is supposed to be stable and reasonable to use in production!</p>\n<p>For our snapshot tests we use <a href=\"https://github.com/mitsuhiko/insta\">insta</a>, which I think is fantastic and more people should use. All you need to do is assert_snapshot! any output you want to keep track of and it will magically take care of the storing, loading, and diffing.</p>\n<p>Here&#8217;s one of the snapshot tests where we invoke the CLI interface and snapshot stdout:</p>\n<pre>#[test]\r\nfn test_evil_json() {\r\n    // For a while this didn't parse right\r\n    let bin = env!(\"CARGO_BIN_EXE_minidump-stackwalk\");\r\n    let output = Command::new(bin)\r\n        .arg(\"--json\")\r\n        .arg(\"--pretty\")\r\n        .arg(\"--raw-json\")\r\n        .arg(\"../testdata/evil.json\")\r\n        .arg(\"../testdata/test.dmp\")\r\n        .arg(\"../testdata/symbols/\")\r\n        .stdout(Stdio::piped())\r\n        .stderr(Stdio::piped())\r\n        .output()\r\n        .unwrap();\r\n\r\n    let stdout = String::from_utf8(output.stdout).unwrap();\r\n    let stderr = String::from_utf8(output.stderr).unwrap();\r\n\r\n    assert!(output.status.success());\r\n    insta::assert_snapshot!(\"json-pretty-evil-symbols\", stdout);\r\n    assert_eq!(stderr, \"\");\r\n}\r\n\r\n</pre>\n<h2><b>Stackwalker Unit Testing</b></h2>\n<p>The stackwalker is easily the most complicated and subtle part of the new implementation, because every platform can have <em>slight</em> quirks and you need to implement several different unwinding strategies and carefully tune everything to work well <em>in practice</em>.</p>\n<p>The scariest part of this was the call frame information (CFI) unwinders, because they are basically little virtual machines we need to parse and execute at runtime. Thankfully breakpad had long ago smoothed over this issue by defining a simplified and unified CFI format, STACK CFI (well, nearly unified, x86 Windows was still a special case as STACK WIN). So even if DWARF CFI has a ton of complex features, we mostly need to implement a <a href=\"https://en.wikipedia.org/wiki/Reverse_Polish_notation\">Reverse Polish Notation Calculator</a> except it can read registers and load memory from addresses it computes (and for STACK WIN it has access to named variables it can declare and mutate).</p>\n<p>Unfortunately, <a href=\"https://chromium.googlesource.com/breakpad/breakpad/+/master/docs/symbol_files.md\">Breakpad&#8217;s description for this format is pretty underspecified</a> so I had to basically pick some semantics I thought made sense and go with that. This made me <em>extremely</em> paranoid about the implementation. (And yes I will be more first-person for this part, because this part was genuinely where I personally spent most of my time and did a lot of stuff from scratch. All the blame belongs to me here!)</p>\n<p>The<a href=\"https://docs.rs/breakpad-symbols/latest/breakpad_symbols/walker/index.html\"> STACK WIN / STACK CFI parser+evaluator</a> is 1700 lines. 500 of those lines are a detailed documentation and discussion of the format, and 700 of those lines are an enormous pile of ~80 test cases where I tried to come up with every corner case I could think of.</p>\n<p>I even checked in two tests I <em>knew</em> were failing just to be honest that there were a couple cases to fix! One of them is a corner case involving dividing by a negative number that almost certainly just doesn&#8217;t matter. The other is a buggy input that old x86 Microsoft toolchains actually produce and parsers need to deal with. The latter was fixed before the fuzzing started.</p>\n<p>And 5225225 <em>still</em> found an integer overflow in the STACK WIN preprocessing step! (Not actually that surprising, it&#8217;s a hacky mess that tries to cover up for how messed up x86 Windows unwinding tables were.)</p>\n<p>(The code isn&#8217;t terribly interesting here, it&#8217;s just a ton of assertions that a given input string produces a given output/error.)</p>\n<p>Of course, I wasn&#8217;t satisfied with just coming up with my own semantics and testing them: I also <a href=\"https://github.com/luser/rust-minidump/blob/master/minidump-processor/src/stackwalker/x86_unittest.rs\">ported most of breakpad&#8217;s own stackwalker tests to rust-minidump</a>! This definitely found a bunch of bugs I had, but also taught me some weird quirks in Breakpad&#8217;s stackwalkers that I&#8217;m not sure I <em>actually</em> agree with. But in this case I was flying so blind that even being bug-compatible with Breakpad was some kind of relief.</p>\n<p>Those tests also included several tests for the non-CFI paths, which were similarly wobbly and quirky. I still really hate a lot of the weird platform-specific rules they have for stack scanning, but I&#8217;m forced to work on the assumption that they might be load-bearing. (I definitely had several cases where I disabled a breakpad test because it was &#8220;obviously nonsense&#8221; and then hit it in the wild while testing. I quickly learned to accept that <strong>Nonsense Happens And Cannot Be Ignored</strong>.)</p>\n<p>One major thing I <em>didn&#8217;t</em> replicate was some of the really hairy hacks for STACK WIN. Like there are several places where they introduce extra stack-scanning to try to deal with the fact that stack frames can have mysterious extra alignment that the windows unwinding tables just don&#8217;t tell you about? I guess?</p>\n<p>There&#8217;s almost certainly some exotic situations that rust-minidump does worse on because of this, but it probably also means we do better in some random other situations too. I never got the two to perfectly agree, but at some point the divergences were all in weird enough situations, and as far as I was concerned both stackwalkers were producing equally bad results in a bad situation. Absent any reason to prefer one over the other, divergence seemed acceptable to keep the implementation cleaner.</p>\n<p>Here&#8217;s a simplified version of one of the ported breakpad tests, if you&#8217;re curious (thankfully minidump-synth is based off of the same binary data mocking framework these tests use):</p>\n<pre>#[test]\r\nfn test_x86_frame_pointer() {\r\n    let mut f = TestFixture::new();\r\n    let frame0_ebp = Label::new();\r\n    let frame1_ebp = Label::new();\r\n    let mut stack = Section::new();\r\n\r\n    // Setup the stack and registers so frame pointers will work\r\n    stack.start().set_const(0x80000000);\r\n    stack = stack\r\n        .append_repeated(12, 0) // frame 0: space\r\n        .mark(&amp;frame0_ebp)      // frame 0 %ebp points here\r\n        .D32(&amp;frame1_ebp)       // frame 0: saved %ebp\r\n        .D32(0x40008679)        // frame 0: return address\r\n        .append_repeated(8, 0)  // frame 1: space\r\n        .mark(&amp;frame1_ebp)      // frame 1 %ebp points here\r\n        .D32(0)                 // frame 1: saved %ebp (stack end)\r\n        .D32(0);                // frame 1: return address (stack end)\r\n    f.raw.eip = 0x4000c7a5;\r\n    f.raw.esp = stack.start().value().unwrap() as u32;\r\n    f.raw.ebp = frame0_ebp.value().unwrap() as u32;\r\n\r\n    // Check the stackwalker's output:\r\n    let s = f.walk_stack(stack).await;\r\n    assert_eq!(s.frames.len(), 2);\r\n    {\r\n        let f0 = &amp;s.frames[0];\r\n        assert_eq!(f0.trust, FrameTrust::Context);\r\n        assert_eq!(f0.context.valid, MinidumpContextValidity::All);\r\n        assert_eq!(f0.instruction, 0x4000c7a5);\r\n    }\r\n    {\r\n        let f1 = &amp;s.frames[1];\r\n        assert_eq!(f1.trust, FrameTrust::FramePointer);\r\n        assert_eq!(f1.instruction, 0x40008678);\r\n    }\r\n}</pre>\n<h2>A Dedicated Production Diffing, Simulating, and Debugging Tool</h2>\n<p>Because minidumps are so horribly fractal and corner-casey, I spent <em>a lot</em> of time terrified of subtle issues that would become huge disasters if we ever actually tried to deploy to production. So I also spent a bunch of time building <a href=\"https://github.com/Gankra/socc-pair/\">socc-pair</a>, which takes the id of a crash report from Mozilla&#8217;s <a href=\"https://crash-stats.mozilla.org/\">crash reporting system</a> and pulls down the minidump, the old breakpad-based implementation&#8217;s output, and extra metadata.</p>\n<p>It then runs a local rust-minidump (minidump-stackwalk) implementation on the minidump and does a domain-specific diff over the two inputs. The most substantial part of this is a fuzzy diff on the stackwalks that tries to better handle situations like when one implementation adds an extra frame but the two otherwise agree. It also uses the reported techniques each implementation used to try to identify whose output is more trustworthy when they totally diverge.</p>\n<p>I also ended up adding a bunch of mocking and benchmarking functionality to it as well, as I found more and more places where I just wanted to simulate a production environment.</p>\n<p>Oh also I added <a href=\"https://github.com/luser/rust-minidump/tree/master/minidump-stackwalk#debugging-stackwalking\">really detailed trace-logging for the stackwalker</a> so that I could easily post-mortem debug why it made the decisions it made.</p>\n<p>This tool found so many issues and more importantly has helped me quickly isolate their causes. I am so happy I made it. Because of it, we know we actually <em>fixed</em> several issues that happened with the old breakpad implementation, which is great!</p>\n<p>Here&#8217;s a trimmed down version of the kind of report socc-pair would produce (yeah I abused diff syntax to get error highlighting. It&#8217;s a great hack, and I love it like a child):</p>\n<pre>comparing json...\r\n\r\n: {\r\n    crash_info: {\r\n        address: 0x7fff1760aca0\r\n        crashing_thread: 8\r\n        type: EXCEPTION_BREAKPOINT\r\n    }\r\n    crashing_thread: {\r\n        frames: [\r\n            0: {\r\n                file: wrappers.cpp:1750da2d7f9db490b9d15b3ee696e89e6aa68cb7\r\n                frame: 0\r\n                function: RustMozCrash(char const*, int, char const*)\r\n                function_offset: 0x00000010\r\n-               did not match\r\n+               line: 17\r\n-               line: 20\r\n                module: xul.dll\r\n\r\n.....\r\n\r\n    unloaded_modules: [\r\n        0: {\r\n            base_addr: 0x7fff48290000\r\n-           local val was null instead of:\r\n            code_id: 68798D2F9000\r\n            end_addr: 0x7fff48299000\r\n            filename: KBDUS.DLL\r\n        }\r\n        1: {\r\n            base_addr: 0x7fff56020000\r\n            code_id: DFD6E84B14000\r\n            end_addr: 0x7fff56034000\r\n            filename: resourcepolicyclient.dll\r\n        }\r\n    ]\r\n~   ignoring field write_combine_size: \"0\"\r\n}\r\n\r\n- Total errors: 288, warnings: 39\r\n\r\nbenchmark results (ms):\r\n    2388, 1986, 2268, 1989, 2353, \r\n    average runtime: 00m:02s:196ms (2196ms)\r\n    median runtime: 00m:02s:268ms (2268ms)\r\n    min runtime: 00m:01s:986ms (1986ms)\r\n    max runtime: 00m:02s:388ms (2388ms)\r\n\r\nmax memory (rss) results (bytes):\r\n    267755520, 261152768, 272441344, 276131840, 279134208, \r\n    average max-memory: 258MB (271323136 bytes)\r\n    median max-memory: 259MB (272441344 bytes)\r\n    min max-memory: 249MB (261152768 bytes)\r\n    max max-memory: 266MB (279134208 bytes)\r\n\r\nOutput Files: \r\n    * (download) Minidump: b4f58e9f-49be-4ba5-a203-8ef160211027.dmp\r\n    * (download) Socorro Processed Crash: b4f58e9f-49be-4ba5-a203-8ef160211027.json\r\n    * (download) Raw JSON: b4f58e9f-49be-4ba5-a203-8ef160211027.raw.json\r\n    * Local minidump-stackwalk Output: b4f58e9f-49be-4ba5-a203-8ef160211027.local.json\r\n    * Local minidump-stackwalk Logs: b4f58e9f-49be-4ba5-a203-8ef160211027.log.txt</pre>\n<h2><b>Staging and Deploying to Production</b></h2>\n<p>Once we were confident enough in the implementation, a lot of the remaining testing was taken over by Will Kahn-Greene, who&#8217;s responsible for a lot of the server-side details of our crash-reporting infrastructure.</p>\n<p>Will spent a bunch of time getting a bunch of machinery setup to manage the deployment and monitoring of rust-minidump. He also did a lot of the hard work of cleaning up all our server-side configuration scripts to handle any differences between the two implementations. (Although I spent a lot of time on compatibility, we both agreed this was a good opportunity to clean up old cruft and mistakes.)</p>\n<p>Once all of this was set up, he turned it on in staging and we got our first look at how rust-minidump actually worked in ~production:</p>\n<p><strong>Terribly!</strong></p>\n<p>Our staging servers take in about 10% of the inputs that also go to our production servers, but even at that reduced scale we very quickly found several new corner cases and we were getting <em>tons</em> of crashes, which is mildly embarrassing for<em> the thing that handles other people&#8217;s crashes</em>.</p>\n<p>Will did a great job here in monitoring and reporting the issues. Thankfully they were all fairly easy for us to fix. Eventually, everything smoothed out and things seemed to be working just as reliably as the old implementation on the production server. The only places where we were completely failing to produce any output were for horribly truncated minidumps that may as well have been empty files.</p>\n<p>We originally <em>did</em> have some grand ambitions of running socc-pair on everything the staging servers processed or something to get <em>really</em> confident in the results. But by the time we got to that point, we were completely exhausted and feeling pretty confident in the new implementation.</p>\n<p>Eventually Will just said &#8220;let&#8217;s turn it on in production&#8221; and I said &#8220;AAAAAAAAAAAAAAA&#8221;.</p>\n<p>This moment was pure terror. There had always been <em>more</em> corner cases. There&#8217;s no way we could just be <em>done</em>. This will probably set all of Mozilla on fire and delete Firefox from the internet!</p>\n<p>But Will convinced me. We wrote up some docs detailing all the subtle differences and sent them to everyone we could. Then the moment of truth finally came: Will turned it on in production, and I got to really see how well it worked in production:</p>\n<p><em>*dramatic drum roll*</em></p>\n<p>It worked fine.</p>\n<p>After all that stress and anxiety, we turned it on and it was <em>fine</em>.</p>\n<p>Heck, I&#8217;ll say it: it ran <em>well</em>.</p>\n<p>It was faster, it crashed less, and we even knew it fixed some issues.</p>\n<p>I was in a bit of a stupor for the rest of that week, because I kept waiting for the other shoe to drop. I kept waiting for someone to emerge from the mist and explain that I had somehow bricked <em>Thunderbird</em> or something. But no, it just worked.</p>\n<p>So we left for the holidays, and I kept waiting for it to break, but it was <em>still fine</em>.</p>\n<p>I am honestly still shocked about this!</p>\n<p>But hey, as it turns out we really did put a <em>lot</em> of careful work into testing the implementation. At every step we found new problems but that was <em>good</em>, because once we got to the final step there were no more problems to surprise us.</p>\n<p><strong>And the fuzzer still kicked our butts afterwards.</strong></p>\n<p>But that&#8217;s part 2! Thanks for reading!</p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/\">Everything Is Broken: Shipping rust-minidump at Mozilla &#8211; Part 1</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\t\t\t<wfw:commentRss>https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/feed/</wfw:commentRss>\n\t\t\t<slash:comments>0</slash:comments>\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Training efficient neural network models for Firefox Translations</title>\n\t\t<link>https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/</link>\n\t\t\t\t\t<comments>https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/#respond</comments>\n\t\t\n\t\t<dc:creator><![CDATA[Evgeny Pavlov]]></dc:creator>\n\t\t<pubDate>Tue, 07 Jun 2022 15:25:47 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[Machine Translation]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<category><![CDATA[nmt]]></category>\n\t\t<category><![CDATA[Translations]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47809</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>The Bergamot project is a collaboration between Mozilla, University of Edinburgh, Charles University in Prague, the University of Sheffield, and University of Tartu with funding from the European Union’s Horizon 2020 research and innovation programme. It brings MT to the local environment, providing small, high-quality, CPU optimized NMT models. The Firefox Translations web extension utilizes proceedings of project Bergamot and brings local translations to Firefox. In this article, we will discuss the components used to train our efficient NMT models.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/\">Training efficient neural network models for Firefox Translations</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p>Machine Translation is an important tool for expanding the accessibility of web content. Usually, people use cloud providers to translate web pages. State-of-the-art Neural Machine Translation (NMT) models are large and often require specialized hardware like GPUs to run inference in real-time.</p>\n<p>If people were able to run a compact Machine Translation (MT) model on their local machine CPU without sacrificing translation accuracy it would help to preserve privacy and reduce costs.</p>\n<p>The Bergamot <a href=\"https://browser.mt/\" target=\"_blank\" rel=\"noopener\">project</a> is a collaboration between Mozilla, the University of Edinburgh, Charles University in Prague, the University of Sheffield, and the University of Tartu with funding from the European Union’s Horizon 2020 research and innovation programme. It brings MT to the local environment, providing small, high-quality, CPU optimized NMT models. <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">The Firefox Translations web extension</a> utilizes proceedings of project Bergamot and brings local translations to Firefox.</p>\n<p>In this article, we will discuss the components used to train our efficient NMT models. The project is open-source, so you can give it a try and train your model too!</p>\n<h2><b>Architecture</b></h2>\n<p>NMT models are trained as language pairs, translating from language A to language B. The <a href=\"https://github.com/mozilla/firefox-translations-training\" target=\"_blank\" rel=\"noopener\">training pipeline</a> was designed to train translation models for a language pair end-to-end, from environment configuration to exporting the ready-to-use models. The pipeline run is completely reproducible given the same code, hardware and configuration files.</p>\n<p>The complexity of the pipeline comes from the requirement to produce an efficient model. We use Teacher-Student distillation to compress a high-quality but resource-intensive teacher model into an efficient CPU-optimized student model that still has good translation quality. We explain this further in the Compression section.</p>\n<p>The pipeline includes many steps: compiling of components, downloading and cleaning datasets, training teacher, student and backward models, decoding, quantization, evaluation etc (more details below). The pipeline can be represented as a Directly Acyclic Graph (DAG).</p>\n<p>&nbsp;</p>\n<p><img class=\"aligncenter wp-image-47810\" src=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-500x246.png\" alt=\"Firfox Translation training pipeline DAG\" width=\"591\" height=\"291\" srcset=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-500x246.png 500w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-250x123.png 250w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-768x379.png 768w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-1536x757.png 1536w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-5.20.04-PM-2048x1009.png 2048w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></p>\n<p>The workflow is file-based and employs self-sufficient scripts that use data on disk as input, and write intermediate and output results back to disk.</p>\n<p>We use the Marian Neural Machine Translation engine. It is written in C++ and designed to be fast. The engine is open-sourced and used by many universities and companies, including Microsoft.</p>\n<h2><b>Training a quality model</b></h2>\n<p>The first task of the pipeline is to train a high-quality model that will be compressed later. The main challenge at this stage is to find a good parallel corpus that contains translations of the same sentences in both source and target languages and then apply appropriate cleaning procedures.</p>\n<h3><b>Datasets</b></h3>\n<p>It turned out there are many open-source parallel datasets for machine translation available on the internet. The most interesting project that aggregates such datasets is <a href=\"https://opus.nlpl.eu/\" target=\"_blank\" rel=\"noopener\">OPUS</a>. The Annual Conference on Machine Translation also collects and distributes some datasets for competitions, for example, <a href=\"https://www.statmt.org/wmt21/translation-task.html#download\" target=\"_blank\" rel=\"noopener\">WMT21 Machine Translation of News</a>. Another great source of MT corpus is the <a href=\"https://paracrawl.eu/\" target=\"_blank\" rel=\"noopener\">Paracrawl</a> project.</p>\n<p>OPUS dataset search interface:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47814\" src=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-500x403.png\" alt=\"OPUS dataset search interface\" width=\"591\" height=\"476\" srcset=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-500x403.png 500w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-250x202.png 250w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-768x619.png 768w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM-1536x1238.png 1536w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-20-at-6.04.13-PM.png 1992w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></p>\n<p>It is possible to use any dataset on disk, but automating dataset downloading from Open source resources makes adding new language pairs easy, and whenever the data set is expanded we can then easily retrain the model to take advantage of the additional data. Make sure to check the licenses of the open-source datasets before usage.</p>\n<h3><b>Data cleaning</b></h3>\n<p>Most open-source datasets are somewhat noisy. Good examples are crawled websites and translation of subtitles. Texts from websites can be poor-quality automatic translations or contain unexpected HTML, and subtitles are often free-form translations that change the meaning of the text.</p>\n<p>It is well known in the world of Machine Learning (ML) that if we feed garbage into the model we get garbage as a result. Dataset cleaning is probably the most crucial step in the pipeline to achieving good quality.</p>\n<p>We employ some basic cleaning techniques that work for most datasets like removing too short or too long sentences and filtering the ones with an unrealistic source to target length ratio. We also use <a href=\"https://github.com/bitextor/bicleaner\" target=\"_blank\" rel=\"noopener\">bicleaner</a>, a pre-trained ML classifier that attempts to indicate whether the training example in a dataset is a reversible translation. We can then remove low-scoring translation pairs that may be incorrect or otherwise add unwanted noise.</p>\n<p>Automation is necessary when your training set is large. However, it is always recommended to look at your data manually in order to tune the cleaning thresholds and add dataset-specific fixes to get the best quality.</p>\n<h3><b>Data augmentation</b></h3>\n<p>There are more than 7000 languages spoken in the world and most of them are classified as low-resource for our purposes, meaning there is little parallel corpus data available for training. In these cases, we use a popular data augmentation strategy called back-translation.</p>\n<p>Back-translation is a technique to increase the amount of training data available by adding synthetic translations. We get these synthetic examples by training a translation model from the target language to the source language. Then we use it to translate monolingual data from the target language into the source language, creating synthetic examples that are added to the training data for the model we actually want, from the source language to the target language.</p>\n<h3><b>The model</b></h3>\n<p>Finally, when we have a clean parallel corpus we train a big transformer model to reach the best quality we can.</p>\n<p>Once the model converges on the augmented dataset, we fine-tune it on the original parallel corpus that doesn’t include synthetic examples from back-translation to further improve quality.</p>\n<h2><b>Compression</b></h2>\n<p>The trained model can be 800Mb or more in size depending on configuration and requires significant computing power to perform translation (decoding). At this point, it’s generally executed on GPUs and not practical to run on most consumer laptops. In the next steps we will prepare a model that works efficiently on consumer CPUs.</p>\n<h3><b>Knowledge distillation</b></h3>\n<p>The main technique we use for compression is Teacher-Student Knowledge Distillation. The idea is to decode a lot of text from the source language into the target language using the heavy model we trained (Teacher) and then train a much smaller model with fewer parameters (Student) on these synthetic translations. The student is supposed to imitate the teacher’s behavior and demonstrate similar translation quality despite being significantly faster and more compact.</p>\n<p>We also augment the parallel corpus data with monolingual data in the source language for decoding. This improves the student by providing additional training examples of the teacher&#8217;s behavior.</p>\n<h3><b>Ensemble</b></h3>\n<p>Another trick is to use not just one teacher but an ensemble of 2-4 teachers independently trained on the same parallel corpus. It can boost quality a little bit at the cost of having to train more teachers. The pipeline supports training and decoding with an ensemble of teachers.</p>\n<h3><b>Quantization</b></h3>\n<p>One more popular technique for model compression is quantization. We use 8-bit quantization which essentially means that we store weights of the neural net as int8 instead of float32. It saves space and speeds up matrix multiplication on inference.</p>\n<h3><b>Other tricks</b></h3>\n<p>Other features worth mentioning but beyond the scope of this already lengthy article are the specialized Neural Network architecture of the student model, half-precision decoding by the teacher model to speed it up, lexical shortlists, training of word alignments, and finetuning of the quantized student.</p>\n<p>Yes, it’s a lot! Now you can see why we wanted to have an end-to-end pipeline.</p>\n<h2><b>How to learn more</b></h2>\n<p>This work is based on a lot of research. If you are interested in the science behind the training pipeline, check out reference publications listed <a href=\"https://github.com/mozilla/firefox-translations-training#references\" target=\"_blank\" rel=\"noopener\">in the training pipeline repository README</a> and <a href=\"https://browser.mt/publications\" target=\"_blank\" rel=\"noopener\">across the wider Bergamot project</a>. <a href=\"https://aclanthology.org/2020.ngt-1.26/\" target=\"_blank\" rel=\"noopener\">Edinburgh&#8217;s Submissions to the 2020 Machine Translation Efficiency Task</a> is a good academic starting article. Check <a href=\"https://nbogoychev.com/efficient-machine-translation/\" target=\"_blank\" rel=\"noopener\">this tutorial</a> by Nikolay Bogoychev for a more practical and operational explanation of the steps.</p>\n<h2><b>Results</b></h2>\n<p>The final student model is 47 times smaller and 37 times faster than the original teacher model and has only a small quality decrease!</p>\n<p>Benchmarks for en-pt model and Flores dataset:</p>\n<table style=\"table-layout: fixed; width: 100%;\">\n<tbody>\n<tr>\n<td><b>Model</b></td>\n<td><b>Size</b></td>\n<td><b>Total number of parameters</b></td>\n<td><b>Dataset decoding time on 1 CPU core</b></td>\n<td><b>Quality, BLEU</b></td>\n</tr>\n<tr>\n<td>Teacher</td>\n<td>798Mb</td>\n<td>192.75M</td>\n<td>631s</td>\n<td>52.5</td>\n</tr>\n<tr>\n<td>Student quantized</td>\n<td>17Mb</td>\n<td>15.7M</td>\n<td>17.9s</td>\n<td>50.7</td>\n</tr>\n</tbody>\n</table>\n<p>We evaluate results using MT standard <a href=\"https://en.wikipedia.org/wiki/BLEU\" target=\"_blank\" rel=\"noopener\">BLEU scores</a> that essentially represent how similar translated and reference texts are. This method is not perfect but it has been shown that BLEU scores correlate well with human judgment of translation quality.</p>\n<p>We have a <a href=\"https://github.com/mozilla/firefox-translations-models\" target=\"_blank\" rel=\"noopener\">GitHub repository</a> with all the trained models and <a href=\"https://github.com/mozilla/firefox-translations-models/blob/main/evaluation/prod/results.md\" target=\"_blank\" rel=\"noopener\">evaluation results</a> where we compare the accuracy of our models to popular APIs of cloud providers. We can see that some models perform similarly, or even outperform, the cloud providers which is a great result taking into account our model’s efficiency, reproducibility and open-source nature.</p>\n<p>For example, here you can see evaluation results for the English to Portuguese model trained by Mozilla using open-source data only.</p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47818\" src=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM.png\" alt=\"Evaluation results en-pt\" width=\"591\" height=\"476\" srcset=\"https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM.png 2066w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-250x201.png 250w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-500x403.png 500w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-768x619.png 768w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-1536x1237.png 1536w, https://hacks.mozilla.org/files/2022/04/Screen-Shot-2022-04-22-at-1.28.56-PM-2048x1650.png 2048w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></p>\n<p>Anyone can train models and contribute them to our repo. Those contributions can be used in the <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">Firefox Translations web extension</a> and other places (see below).</p>\n<h2><b>Scaling</b></h2>\n<p>It is of course possible to run the whole pipeline on one machine, though it may take a while. Some steps of the pipeline are CPU bound and difficult to parallelize, while other steps can be offloaded to multiple GPUs. Most of the official models in the repository were trained on machines with 8 GPUs. A few steps, like teacher decoding during knowledge distillation, can take days even on well-resourced single machines. So to speed things up, we added cluster support to be able to spread different steps of the pipeline over multiple nodes.</p>\n<h3><b>Workflow manager</b></h3>\n<p>To manage this complexity we chose <a href=\"https://snakemake.github.io/\" target=\"_blank\" rel=\"noopener\">Snakemake</a> which is very popular in the bioinformatics community. It uses file-based workflows, allows specifying step dependencies in Python, supports containerization and integration with different cluster software. We considered alternative solutions that focus on job scheduling, but ultimately chose Snakemake because it was more ergonomic for one-run experimentation workflows.</p>\n<p>Example of a Snakemake rule (dependencies between rules are inferred implicitly):</p>\n<pre><code class=\"js\">rule train_teacher:\r\n    message: \"Training teacher on all data\"\r\n    log: f\"{log_dir}/train_teacher{{ens}}.log\"\r\n    conda: \"envs/base.yml\"\r\n    threads: gpus_num*2\r\n    resources: gpu=gpus_num\r\n    input:\r\n        rules.merge_devset.output, \r\n        train_src=f'{teacher_corpus}.{src}.gz',\r\n        train_trg=f'{teacher_corpus}.{trg}.gz',\r\n        bin=ancient(trainer), \r\n        vocab=vocab_path\r\n    output: model=f'{teacher_base_dir}{{ens}}/{best_model}'\r\n    params: \r\n        prefix_train=teacher_corpus, \r\n        prefix_test=f\"{original}/devset\", \r\n        dir=directory(f'{teacher_base_dir}{{ens}}'),\r\n        args=get_args(\"training-teacher-base\")\r\n    shell: '''bash pipeline/train/train.sh \\\r\n                teacher train {src} {trg} \"{params.prefix_train}\" \\\r\n                \"{params.prefix_test}\" \"{params.dir}\" \\\r\n                \"{input.vocab}\" {params.args} &gt;&gt; {log} 2&gt;&amp;1'''</code></pre>\n<h3><b>Cluster support</b></h3>\n<p>To parallelize workflow steps across cluster nodes we use <a href=\"https://slurm.schedmd.com/\">Slurm</a> resource manager. It is relatively simple to operate, fits well for high-performance experimentation workflows, and supports Singularity containers for easier reproducibility. Slurm is also the most popular cluster manager for High-Performance Computers (HPC) used for model training in academia, and most of the consortium partners were already using or familiar with it.</p>\n<h2><b>How to start training</b></h2>\n<p>The workflow is quite resource-intensive, so you’ll need a pretty good server machine or even a cluster. We recommend using 4-8 Nvidia 2080-equivalent or better GPUs per machine.</p>\n<p>Clone <a href=\"https://github.com/mozilla/firefox-translations-training\" target=\"_blank\" rel=\"noopener\">https://github.com/mozilla/firefox-translations-training</a> and follow the instructions in the <a href=\"https://github.com/mozilla/firefox-translations-training/blob/main/README.md\" target=\"_blank\" rel=\"noopener\">readme</a> for configuration.</p>\n<p>The most important part is to find parallel datasets and properly configure settings based on your available data and hardware. You can learn more about this in the readme.</p>\n<h2><b>How to use the existing models</b></h2>\n<p>The existing models are shipped with the <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">Firefox Translations web extension</a>, enabling users to translate web pages in Firefox. The models are downloaded to a local machine on demand. The web extension uses these models with the<a href=\"https://github.com/browsermt/bergamot-translator\" target=\"_blank\" rel=\"noopener\"> bergamot-translator</a> Marian wrapper compiled to Web Assembly.</p>\n<p>Also, there is a playground website at <a href=\"https://mozilla.github.io/translate\" target=\"_blank\" rel=\"noopener\">https://mozilla.github.io/translate</a> where you can input text and translate it right away, also locally but served as a static website instead of a browser extension.</p>\n<p>If you are interested in an efficient NMT inference on the server, you can try a prototype <a href=\"https://github.com/mozilla/translation-service\" target=\"_blank\" rel=\"noopener\">HTTP service</a> that uses bergamot-translator natively compiled, instead of compiled to WASM.</p>\n<p>Or follow the build instructions in the <a href=\"https://github.com/browsermt/bergamot-translator#build-instructions\" target=\"_blank\" rel=\"noopener\">bergamot-translator readme</a> to directly use the C++, JavaScript WASM, or Python bindings.</p>\n<h2><b>Conclusion</b></h2>\n<p>It is fascinating how far Machine Translation research has come in recent years. Local high-quality translations are the future and it’s becoming more and more practical for companies and researchers to train such models even without access to proprietary data or large-scale computing power.</p>\n<p>We hope that <a href=\"https://github.com/mozilla/firefox-translations\" target=\"_blank\" rel=\"noopener\">Firefox Translations</a> will set a new standard of privacy-preserving, efficient, open-source machine translation accessible for all.</p>\n<h2><b>Acknowledgements</b></h2>\n<p>I would like to thank all the participants of <a href=\"https://browser.mt/\" target=\"_blank\" rel=\"noopener\">the Bergamot Project</a> for making this technology possible, my teammates Andre Natal and Abhishek Aggarwal for the incredible work they have done bringing Firefox Translations to life, Lonnen for managing the project and editing this blog post and of course awesome Mozilla community for helping with localization of the web-extension and testing its early builds.</p>\n<p><i>This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 825303 <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f1ea-1f1fa.png\" alt=\"🇪🇺\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></i></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/\">Training efficient neural network models for Firefox Translations</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\t\t\t<wfw:commentRss>https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/feed/</wfw:commentRss>\n\t\t\t<slash:comments>0</slash:comments>\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Improved Process Isolation in Firefox 100</title>\n\t\t<link>https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/</link>\n\t\t\t\t\t<comments>https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/#comments</comments>\n\t\t\n\t\t<dc:creator><![CDATA[Gian-Carlo Pascutto]]></dc:creator>\n\t\t<pubDate>Thu, 12 May 2022 15:09:10 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[Firefox OS]]></category>\n\t\t<category><![CDATA[HTML]]></category>\n\t\t<category><![CDATA[JavaScript]]></category>\n\t\t<category><![CDATA[api]]></category>\n\t\t<category><![CDATA[css]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<category><![CDATA[site isolation]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47829</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>Firefox uses a multi-process model for additional security and stability while browsing: Web Content (such as HTML/CSS and Javascript) is rendered in separate processes that are isolated from the rest of the operating system and managed by a privileged parent process. This way, the amount of control gained by an attacker that exploits a bug in a content process is limited. In this article, we would like to dive a bit further into the latest major milestone we have reached: Win32k Lockdown, which greatly reduces the capabilities of the content process when running on Windows.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/\">Improved Process Isolation in Firefox 100</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<h2><strong>Introduction</strong></h2>\n<p><span style=\"font-weight: 400;\">Firefox uses a </span><a href=\"https://hacks.mozilla.org/2021/05/introducing-firefox-new-site-isolation-security-architecture/\"><span style=\"font-weight: 400;\">multi-process model</span></a><span style=\"font-weight: 400;\"> for additional security and stability while browsing: Web Content (such as HTML/CSS and Javascript) is rendered in separate processes that are isolated from the rest of the operating system and managed by a privileged parent process. This way, the amount of control gained by an attacker that exploits a bug in a content process is limited. </span></p>\n<p><span style=\"font-weight: 400;\">Ever since we deployed this model, we have been working on improving the isolation of the content processes to further limit the attack surface. This is a challenging task since content processes need access to some operating system APIs to properly function: for example, they still need to be able to talk to the parent process. </span></p>\n<p><span style=\"font-weight: 400;\">In this article, we would like to dive a bit further into the latest major milestone we have reached: </span><i><span style=\"font-weight: 400;\">Win32k Lockdown,</span></i><span style=\"font-weight: 400;\"> which greatly reduces the capabilities of the content process when running on Windows. Together with two major earlier efforts (</span><a href=\"https://hacks.mozilla.org/2021/05/introducing-firefox-new-site-isolation-security-architecture/\"><span style=\"font-weight: 400;\">Fission</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://hacks.mozilla.org/2021/12/webassembly-and-back-again-fine-grained-sandboxing-in-firefox-95/\"><span style=\"font-weight: 400;\">RLBox</span></a><span style=\"font-weight: 400;\">) that shipped before, this completes a sequence of large leaps forward that will significantly improve Firefox&#8217;s security.</span></p>\n<p><span style=\"font-weight: 400;\">Although </span><i><span style=\"font-weight: 400;\">Win32k Lockdown</span></i><span style=\"font-weight: 400;\"> is a Windows-specific technique, it became possible because of a significant re-architecting of the Firefox security boundaries that Mozilla has been working on for around four years, which allowed similar security advances to be made on other operating systems.</span></p>\n<h2><strong>The Goal: Win32k Lockdown</strong></h2>\n<p><span style=\"font-weight: 400;\">Firefox runs the processes that render web content with quite a few restrictions on what they are allowed to do when running on Windows. Unfortunately, by default they still have access to the entire Windows API, which opens up a large attack surface: the Windows API consists of many parts, for example, a core part dealing with threads, processes, and memory management, but also networking and socket libraries, printing and multimedia APIs, and so on.</span></p>\n<p><span style=\"font-weight: 400;\">Of particular interest for us is the </span><i><span style=\"font-weight: 400;\">win32k.sys API,</span></i><span style=\"font-weight: 400;\"> which includes many graphical and widget related system calls that have a history of being exploitable. Going back further in Windows&#8217; origins, this situation is likely the result of Microsoft moving many operations that were originally running in user mode into the kernel in order to improve performance around the Windows 95 and NT4 timeframe. </span></p>\n<p><span style=\"font-weight: 400;\">Having likely never been originally designed to run in this sensitive context, these APIs have been a traditional target for hackers to break out of application sandboxes and into the kernel.</span></p>\n<p><span style=\"font-weight: 400;\">In Windows 8, Microsoft introduced a new mitigation named </span><a href=\"https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-process_mitigation_system_call_disable_policy\"><span style=\"font-weight: 400;\">PROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY</span></a><span style=\"font-weight: 400;\"> that an application can use to disable access to win32k.sys system calls. That is a long name to keep repeating, so we&#8217;ll refer to it hereafter by our internal designation: &#8220;</span><i><span style=\"font-weight: 400;\">Win32k Lockdown</span></i><span style=\"font-weight: 400;\">&#8220;.</span></p>\n<h2><strong>The Work Required</strong></h2>\n<p><span style=\"font-weight: 400;\">Flipping the Win32k Lockdown flag on the Web Content processes &#8211; the processes most vulnerable to potentially hostile web pages and JavaScript &#8211; means that those processes can no longer perform any graphical, window management, input processing, etc. operations themselves. </span></p>\n<p><span style=\"font-weight: 400;\">To accomplish these tasks, such operations must be remoted to a process that has the necessary permissions, typically the process that has access to the GPU and handles compositing and drawing (hereafter called the GPU Process), or the privileged parent process. </span></p>\n<h3><span style=\"font-weight: 400;\">Drawing web pages: WebRender</span></h3>\n<p><span style=\"font-weight: 400;\">For painting the web pages&#8217; contents, Firefox historically used various methods for interacting with the Windows APIs, ranging from using modern Direct3D based textures, to falling back to GDI surfaces, and eventually dropping into pure software mode. </span></p>\n<p><span style=\"font-weight: 400;\">These different options would have taken quite some work to remote, as most of the graphics API is off limits in Win32k Lockdown. The good news is that as of Firefox 92, our rendering stack has switched to </span><a href=\"https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/\"><span style=\"font-weight: 400;\">WebRender</span></a><span style=\"font-weight: 400;\">, which moves all the actual drawing from the content processes to WebRender in the GPU Process.</span></p>\n<p><span style=\"font-weight: 400;\">Because with WebRender the content process no longer has a need to directly interact with the platform drawing APIs, this avoids any Win32k Lockdown related problems. WebRender itself has been designed partially to be </span><a href=\"https://github.com/servo/webrender/wiki/\"><span style=\"font-weight: 400;\">more similar to game engines, and thus, be less susceptible to driver bugs</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">For the remaining drivers that are just too broken to be of any use, it still has a </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1601053\"><span style=\"font-weight: 400;\">fully software-based mode</span></a><span style=\"font-weight: 400;\">, which means we have no further fallbacks to consider.</span></p>\n<h3><span style=\"font-weight: 400;\">Webpages drawing: Canvas 2D and WebGL 3D</span></h3>\n<p><span style=\"font-weight: 400;\">The </span><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API\"><span style=\"font-weight: 400;\">Canvas API</span></a><span style=\"font-weight: 400;\"> provides web pages with the ability to draw 2D graphics. In the original Firefox implementation, these JavaScript APIs were executed in the Web Content processes and the calls to the Windows drawing APIs were made directly from the same processes. </span></p>\n<p><span style=\"font-weight: 400;\">In a Win32k Lockdown scenario, this is no longer possible, so </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1464032\"><span style=\"font-weight: 400;\">all drawing commands are remoted</span></a><span style=\"font-weight: 400;\"> by recording and playing them back in the GPU process over IPC.</span></p>\n<p><span style=\"font-weight: 400;\">Although the initial implementation had good performance, there were nevertheless reports from some sites that experienced performance regressions (the web sites that became faster generally didn&#8217;t complain!). A particular pain point are applications that call </span><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/getImageData\"><span style=\"font-weight: 400;\">getImageData()</span></a><span style=\"font-weight: 400;\"> repeatedly: having the Canvas remoted means that GPU textures must now be obtained from another process and sent over IPC. </span></p>\n<p><span style=\"font-weight: 400;\">We compensated for this in the scenario where getImageData is called at the start of a frame, by detecting this and preparing the right surfaces proactively to make the copying from the GPU faster.</span></p>\n<p><span style=\"font-weight: 400;\">Besides the Canvas API to draw 2D graphics, the web platform also exposes an </span><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API\"><span style=\"font-weight: 400;\">API to do 3D drawing, called WebGL</span></a><span style=\"font-weight: 400;\">. WebGL is a state-heavy API, so properly and efficiently synchronizing child and parent (as well as parent and driver) takes </span><a href=\"https://phabricator.services.mozilla.com/D54019\"><span style=\"font-weight: 400;\">great</span></a> <a href=\"https://phabricator.services.mozilla.com/D54019\"><span style=\"font-weight: 400;\">care</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">WebGL originally handled all validation in Content, but with access to the GPU and the associated attack surface removed from there, we needed to craft a robust validating API between child and parent as well to get the full security benefit.</span></p>\n<h3><span style=\"font-weight: 400;\">(Non-)Native Theming for Forms</span></h3>\n<p><span style=\"font-weight: 400;\">HTML web pages have the ability to display form controls. While the overwhelming majority of websites provide a </span><a href=\"https://developer.mozilla.org/en-US/docs/Learn/Forms/Advanced_form_styling\"><span style=\"font-weight: 400;\">custom look and styling for those form controls</span></a><span style=\"font-weight: 400;\">, not all of them do, and if they do not you get an input GUI widget that is styled like (and originally was!) a </span><a href=\"http://stephenhorlander.com/form-controls.html\"><span style=\"font-weight: 400;\">native element of the operating system</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\"> Historically, these were drawn by calling the appropriate OS widget APIs from within the content process, but those are not available under Win32k Lockdown. </span></p>\n<p><span style=\"font-weight: 400;\">This cannot easily be fixed by remoting the calls, as the widgets themselves come in an infinite amount of sizes, shapes, and styles can be interacted with, and need to be responsive to user input and dispatch messages. We settled on having Firefox </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1381938\"><span style=\"font-weight: 400;\">draw the form controls itself</span></a><span style=\"font-weight: 400;\">, in a cross-platform style. </span></p>\n<p><span style=\"font-weight: 400;\">While changing the look of form controls has web compatibility implications, and some people prefer the more native look &#8211; on the few pages that don&#8217;t apply their own styles to controls &#8211; Firefox’s approach is consistent with that taken by other browsers, probably because of very similar considerations.</span></p>\n<p><span style=\"font-weight: 400;\">Scrollbars were a particular pain point: we didn&#8217;t want to draw the main scrollbar of the content window in a different manner as the rest of the UX, since nested scrollbars would show up with different styles which would look awkward. But, unlike the rather rare non-styled form widgets, the main scrollbar is visible on most web pages, and because it conceptually belongs to the browser UX we really wanted it to look native. </span></p>\n<p><span style=\"font-weight: 400;\">We, therefore, decided to draw all scrollbars to match the system theme, although it&#8217;s a bit of an open question though how things should look if even </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1719427#c3\"><span style=\"font-weight: 400;\">the vendor of the operating system can&#8217;t seem to decide what the &#8220;native&#8221; look is</span></a><span style=\"font-weight: 400;\">.</span></p>\n<h2><strong>Final Hurdles</strong></h2>\n<h3><span style=\"font-weight: 400;\">Line Breaking</span></h3>\n<p><span style=\"font-weight: 400;\">With the above changes, we thought we had all the usual suspects that would access graphics and widget APIs in win32k.sys wrapped up, so we started running the full Firefox test suite with win32k syscalls disabled. This caused at least one unexpected failure: Firefox was </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1713973\"><span style=\"font-weight: 400;\">crashing when trying to find line breaks</span></a><span style=\"font-weight: 400;\"> for some languages with complex scripts. </span></p>\n<p><span style=\"font-weight: 400;\">While Firefox is able to correctly determine word endings in multibyte character streams for most languages by itself, the support for Thai, Lao, Tibetan and Khmer is known to be imperfect, and </span><a href=\"https://searchfox.org/mozilla-central/rev/80f11ac5d938f6fce255c56279f46f13a49ea5c3/intl/lwbrk/LineBreaker.h#65\"><span style=\"font-weight: 400;\">in these cases, Firefox can ask the operating system to handle the line breaking</span></a><span style=\"font-weight: 400;\"> for it. But at least on Windows, the functions to do so are covered by the Win32k Lockdown switch. Oops!</span></p>\n<p><span style=\"font-weight: 400;\">There are </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1684927\"><span style=\"font-weight: 400;\">efforts underway to incorporate ICU4X</span></a><span style=\"font-weight: 400;\"> and base all i18n related functionality on that, meaning that Firefox will be able to handle all scripts perfectly without involving the OS, but this is a major effort and it was not clear if it would end up delaying the rollout of win32k lockdown. </span></p>\n<p><span style=\"font-weight: 400;\">We did some experimentation with trying to forward the line breaking over IPC. Initially, this had bad performance, but when we </span><a href=\"https://phabricator.services.mozilla.com/D129125\"><span style=\"font-weight: 400;\">added caching</span></a><span style=\"font-weight: 400;\"> performance was satisfactory or sometimes even improved, since OS calls could be avoided in many cases now.</span></p>\n<h3><span style=\"font-weight: 400;\">DLL Loading &amp; Third Party Interactions</span></h3>\n<p><span style=\"font-weight: 400;\">Another complexity of disabling win32k.sys access is that so much Windows functionality assumes it is available by default, and specific effort must be taken to ensure the relevant DLLs do not get loaded on startup. Firefox itself for example won&#8217;t load the user32 DLL containing some win32k APIs, but </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1719212\"><span style=\"font-weight: 400;\">injected third party DLLs sometimes do</span></a><span style=\"font-weight: 400;\">. This causes problems because </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1751367\"><span style=\"font-weight: 400;\">COM initialization in particular uses win32k calls to get the Window Station and Desktop</span></a><span style=\"font-weight: 400;\"> if the DLL is present. Those calls will fail with Win32k Lockdown enabled, silently breaking COM and features that depend on it such as our accessibility support. </span></p>\n<p><span style=\"font-weight: 400;\">On Windows 10 Fall Creators Update and later we have a fix that blocks these calls and forces a fallback, which keeps everything working nicely. We measured that </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1750742#c9\"><span style=\"font-weight: 400;\">not loading the DLLs causes about a 15% performance gain</span></a><span style=\"font-weight: 400;\"> when opening new tabs, adding a nice performance bonus on top of the security benefit.</span></p>\n<h3><span style=\"font-weight: 400;\">Remaining Work</span></h3>\n<p><span style=\"font-weight: 400;\">As hinted in the previous section, Win32k Lockdown will initially roll out on Windows 10 Fall Creators Update and later. On Windows 8, and unpatched Windows 10 (which unfortunately seems to be in use!), we are still testing a fix for the case where third party DLLs interfere, so support for those will come in a </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1759167#c7\"><span style=\"font-weight: 400;\">future release</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">For Canvas 2D support, we&#8217;re still </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1766402\"><span style=\"font-weight: 400;\">looking into improving the performance of applications</span></a><span style=\"font-weight: 400;\"> that regressed when the processes were switched around. Simultaneously, there is experimentation underway to see if hardware acceleration for </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1739448\"><span style=\"font-weight: 400;\">Canvas 2D can be implemented through WebGL</span></a><span style=\"font-weight: 400;\">, which would increase code sharing between the 2D and 3D implementations and take advantage of modern video drivers being better optimized for the 3D case.</span></p>\n<h2><strong>Conclusion</strong></h2>\n<p><span style=\"font-weight: 400;\">Retrofitting a significant change in the separation of responsibilities in a large application like Firefox presents a large, multi-year engineering challenge, but it is absolutely required in order to advance browser security and to continue keeping our users safe. We&#8217;re pleased to have made it through and present you with the result in Firefox 100.</span></p>\n<h3><span style=\"font-weight: 400;\">Other Platforms</span></h3>\n<p><span style=\"font-weight: 400;\">If you&#8217;re a Mac user, you might wonder if there’s anything similar to Win32k Lockdown that can be done for macOS. You&#8217;d be right, and I have good news for you: we already quietly </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1467758\"><span style=\"font-weight: 400;\">shipped the changes that block access to the WindowServer</span></a><span style=\"font-weight: 400;\"> in Firefox 95, improving security and speeding process startup by about 30-70%. This too became possible because of the Remote WebGL and Non-Native Theming work described above.</span></p>\n<p><span style=\"font-weight: 400;\">For Linux users, </span><a href=\"https://bugzilla.mozilla.org/show_bug.cgi?id=1129492\"><span style=\"font-weight: 400;\">we removed the connection from content processes to the X11 Server</span></a><span style=\"font-weight: 400;\">, which stops attackers from exploiting the unsecured X11 protocol. Although Linux distributions have been moving towards the more secure Wayland protocol as the default, we still see a lot of users that are using X11 or XWayland configurations, so this is definitely a nice-to-have, which shipped in Firefox 99.</span></p>\n<h2><strong>We&#8217;re Hiring</strong></h2>\n<p><span style=\"font-weight: 400;\">If you found the technical background story above fascinating, I&#8217;d like to point out that our OS Integration &amp; Hardening team is going to be hiring soon. We&#8217;re especially looking for experienced C++ programmers with some interest in Rust and in-depth knowledge of Windows programming. </span></p>\n<p><span style=\"font-weight: 400;\">If you fit this description and are interested in taking the next leap in Firefox security together with us, </span><a href=\"https://www.mozilla.org/en-US/careers/\"><span style=\"font-weight: 400;\">we&#8217;d encourage you to keep an eye on our careers page</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><i>Thanks to Bob Owen, Chris Martin, and Stephen Pohl for their technical input to this article, and for all the heavy lifting they did together with Kelsey Gilbert and Jed Davis to make these security improvements ship.<br />\n</i></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/\">Improved Process Isolation in Firefox 100</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\t\t\t<wfw:commentRss>https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/feed/</wfw:commentRss>\n\t\t\t<slash:comments>5</slash:comments>\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Common Voice dataset tops 20,000 hours</title>\n\t\t<link>https://hacks.mozilla.org/2022/04/common-voice-dataset-tops-20000-hours/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Melissa Thermidor]]></dc:creator>\n\t\t<pubDate>Thu, 28 Apr 2022 15:23:57 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Accessibility]]></category>\n\t\t<category><![CDATA[Developer Tools]]></category>\n\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Mozilla]]></category>\n\t\t<category><![CDATA[open source]]></category>\n\t\t<category><![CDATA[common voice]]></category>\n\t\t<category><![CDATA[data]]></category>\n\t\t<category><![CDATA[data sets]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47798</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>The latest Common Voice dataset, released today, has achieved a major milestone: More than 20,000 hours of open-source speech data that anyone, anywhere can use. The dataset has nearly doubled in the past year. Mozilla’s Common Voice seeks to change the language technology ecosystem by supporting communities to collect voice data for the creation of voice-enabled applications for their own languages. </p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/common-voice-dataset-tops-20000-hours/\">Common Voice dataset tops 20,000 hours</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><span style=\"font-weight: 400;\">The latest Common Voice dataset, released today, has achieved a major milestone: More than 20,000 hours of open-source speech data that anyone, anywhere can use. The dataset has nearly doubled in the past year.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47799 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/common-voice.png\" alt=\"\" width=\"512\" height=\"269\" srcset=\"https://hacks.mozilla.org/files/2022/04/common-voice.png 512w, https://hacks.mozilla.org/files/2022/04/common-voice-250x131.png 250w, https://hacks.mozilla.org/files/2022/04/common-voice-500x263.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<h2><b>Why should you care about Common Voice?</b></h2>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Do you have to change your accent to be understood by a virtual assistant? </span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Are you worried that so many voice-operated devices are collecting your voice data for proprietary Big Tech datasets?</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Are automatic subtitles unavailable for you in your language?</span></li>\n</ul>\n<p><span style=\"font-weight: 400;\">Automatic Speech Recognition plays an important role in the way we can access information, however, of the 7,000 languages spoken globally today only a handful are supported by most products.</span></p>\n<p><a href=\"https://commonvoice.mozilla.org/\"><span style=\"font-weight: 400;\">Mozilla’s Common Voice</span></a><span style=\"font-weight: 400;\"> seeks to change the language technology ecosystem by supporting communities to collect voice data for the creation of voice-enabled applications for their own languages. </span></p>\n<h2><b>Common Voice Dataset Release </b></h2>\n<p><span style=\"font-weight: 400;\">This release wouldn’t be possible without our contributors — from voice donations to initiating their language in our project, to opening new opportunities for people to build voice technology tools that can support every language spoken across the world.</span></p>\n<p><span style=\"font-weight: 400;\">Access the dataset:</span><a href=\"https://commonvoice.mozilla.org/datasets\"><span style=\"font-weight: 400;\"> https://commonvoice.mozilla.org/datasets</span></a></p>\n<p><span style=\"font-weight: 400;\">Access the metadata: </span><a href=\"https://github.com/common-voice/cv-dataset\"><span style=\"font-weight: 400;\">https://github.com/common-voice/cv-dataset</span></a><span style=\"font-weight: 400;\"> </span></p>\n<h3><b>Highlights from the latest dataset:</b><b></b></h3>\n<ul>\n<li aria-level=\"1\"><b>The new release also features six new languages:</b><span style=\"font-weight: 400;\"> Tigre, Taiwanese (Minnan), Meadow Mari, Bengali, Toki Pona and Cantonese.</span></li>\n<li aria-level=\"1\"><b>Twenty-seven languages now have at least 100 hours of speech data. </b><span style=\"font-weight: 400;\">They include Bengali, Thai, Basque, and Frisian.</span></li>\n<li aria-level=\"1\"><b>Nine languages now have at least 500 hours of speech data. </b><span style=\"font-weight: 400;\">They include Kinyarwanda (2,383 hours), Catalan (2,045 hours), and Swahili (719 hours).</span></li>\n<li aria-level=\"1\"><b>Nine languages now all have at least 45% of their gender tags as female. </b><span style=\"font-weight: 400;\">They include Marathi, Dhivehi, and Luganda.</span></li>\n<li aria-level=\"1\"><b>The Catalan community fueled major growth.</b><span style=\"font-weight: 400;\"> The Catalan community&#8217;s</span><a href=\"https://www.projecteaina.cat/\"> <span style=\"font-weight: 400;\">Project AINA</span></a> <span style=\"font-weight: 400;\">— a collaboration between Barcelona Supercomputing Center and the Catalan Government — mobilized Catalan speakers to contribute to Common Voice. </span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><b>Supporting community participation in decision making yet.</b><span style=\"font-weight: 400;\"> The </span><a href=\"https://foundation.mozilla.org/en/blog/introducing-cvlr-20212022/\"><span style=\"font-weight: 400;\">Common Voice language Rep Cohort</span></a><span style=\"font-weight: 400;\"> has contributed feedback and learnings about optimal sentence collection, the inclusion of language variants, and more. </span></li>\n</ul>\n<h2><b> Create with the Dataset </b></h2>\n<p><span style=\"font-weight: 400;\">How will you create with the Common Voice Dataset?</span></p>\n<p><span style=\"font-weight: 400;\">Take some inspiration from technologists who are creating conversational chatbots, spoken language identifiers, research papers and virtual assistants with the Common Voice Dataset by watching this talk: </span></p>\n<p><a href=\"https://mozilla.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6492f3ae-3a0d-4363-99f6-adc00111b706\"><span style=\"font-weight: 400;\">https://mozilla.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6492f3ae-3a0d-4363-99f6-adc00111b706</span></a><span style=\"font-weight: 400;\"> </span></p>\n<p><span style=\"font-weight: 400;\">Share with us how you are using the dataset on social media using #CommonVoice or sharing on </span><a href=\"https://discourse.mozilla.org/c/voice/using/661\"><span style=\"font-weight: 400;\">our Community discourse.</span></a><span style=\"font-weight: 400;\"> </span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/common-voice-dataset-tops-20000-hours/\">Common Voice dataset tops 20,000 hours</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>MDN Plus now available in more countries</title>\n\t\t<link>https://hacks.mozilla.org/2022/04/mdn-plus-now-available-in-more-markets/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Hermina]]></dc:creator>\n\t\t<pubDate>Thu, 28 Apr 2022 10:05:35 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[MDN]]></category>\n\t\t<category><![CDATA[mdn]]></category>\n\t\t<category><![CDATA[mdnplus]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47805</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>Almost a month ago, we announced MDN Plus, a new premium service on MDN that allows users to customize their experience on the website.</p>\n<p>We are very glad to announce today that it is now possible for MDN users around the globe to create an MDN Plus free account, no matter where they are.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/mdn-plus-now-available-in-more-markets/\">MDN Plus now available in more countries</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p>Almost a month ago, we announced MDN Plus, a new premium service on MDN that allows users to customize their experience on the website.</p>\n<p>We are very glad to announce today that it is now possible for MDN users around the globe to create an MDN Plus free account, no matter where they are.</p>\n<p>Click <a href=\"https://developer.mozilla.org/en-US/plus#subscribe\">here</a> to create an MDN Plus free account*.</p>\n<p>Also starting today, the premium version of the service will be available in 16 more countries: Austria, Belgium, Finland, France, United Kingdom, Germany, Ireland, Italy, Malaysia, the Netherlands, New Zealand, Puerto Rico, Sweden, Singapore, Switzerland, and Spain. We continue to work towards expanding this list even further.</p>\n<p>Click<a href=\"https://developer.mozilla.org/en-US/plus#subscribe\"> here</a> to create an MDN Plus premium account**.</p>\n<p><span style=\"font-weight: 400;\">* Now available to everyone</span></p>\n<p>** You will need to subscribe from one of the countries mentioned above to be able to have an MDN Plus premium account at this time</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/mdn-plus-now-available-in-more-markets/\">MDN Plus now available in more countries</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Adopting users&#8217; design feedback</title>\n\t\t<link>https://hacks.mozilla.org/2022/04/adopting-users-design-feedback/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Hermina]]></dc:creator>\n\t\t<pubDate>Thu, 21 Apr 2022 15:04:05 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[MDN]]></category>\n\t\t<category><![CDATA[Accessibility]]></category>\n\t\t<category><![CDATA[github]]></category>\n\t\t<category><![CDATA[mdn]]></category>\n\t\t<category><![CDATA[mdn plus]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47784</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>On March 1st, 2022, MDN Web Docs released a new design and a new brand identity. Overall, the community responded to the redesign enthusiastically and we received many positive messages and kudos. We also received valuable feedback on some of the things we didn’t get quite right, like the browser compatibility table changes as well as some accessibility and readability issues.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/adopting-users-design-feedback/\">Adopting users&#8217; design feedback</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><span style=\"font-weight: 400;\">On March 1st, 2022, MDN Web Docs released a new design and a new brand identity. Overall, the community responded to the </span><a href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\"><span style=\"font-weight: 400;\">redesign</span></a><span style=\"font-weight: 400;\"> enthusiastically and we received many positive messages and kudos. We also received valuable feedback on some of the things we didn’t get quite right, like the browser compatibility table changes as well as some accessibility and readability issues.</span></p>\n<p><span style=\"font-weight: 400;\">For us, MDN Web Docs has always been synonymous with the term Ubuntu, </span><em>“I am because we are.”</em><span style=\"font-weight: 400;\"> Translated in this context, “MDN Web Docs is the amazing resource it is because of our community’s support, feedback, and contributions.”</span></p>\n<p><span style=\"font-weight: 400;\"> Since the initial launch of the </span><a href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\"><span style=\"font-weight: 400;\">redesign</span></a><span style=\"font-weight: 400;\"> and of </span><a href=\"https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/\"><span style=\"font-weight: 400;\">MDN Plus</span></a><span style=\"font-weight: 400;\"> afterwards, we have been humbled and overwhelmed by the level of support we received from our community of readers. We do our best to listen to what you have to say and to act on suggestions so that together, we make MDN better. </span></p>\n<p><span style=\"font-weight: 400;\">Here is a summary of how we went about addressing the feedback we received.</span></p>\n<p><span style=\"font-weight: 400;\">Eight days after the redesign launch, we started the </span><i><span style=\"font-weight: 400;\">MDN Web Docs Readability Project</span></i><span style=\"font-weight: 400;\">. Our first task was to triage all issues submitted by the community that related to readability and accessibility on MDN Web Docs. Next up, we identified common themes and </span><a href=\"https://github.com/mdn/yari/issues/5546\"><span style=\"font-weight: 400;\">collected them in this meta issue</span></a><span style=\"font-weight: 400;\">. Over time, this grew into 27 unique issues and several related discussions and </span><span style=\"font-weight: 400;\">comments. We collected feedback on GitHub and also from our communities on</span> <a href=\"https://twitter.com/MozDevNet\"><span style=\"font-weight: 400;\">Twitter</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://matrix.to/#/%23mdn:mozilla.org\"><span style=\"font-weight: 400;\">Matrix</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">With the main pain points identified, we </span><a href=\"https://github.com/mdn/yari/discussions/5715\"><span style=\"font-weight: 400;\">opened a discussion on GitHub</span></a><span style=\"font-weight: 400;\">, inviting our readers to follow along and provide feedback on the changes as they were rolled out to a staging instance of the website. Today, roughly six weeks later, we are pleased to announce that all these changes are in production. This was not the effort of any one person but is made up of the work and contributions of people across staff and community.</span></p>\n<p><span style=\"font-weight: 400;\">Below are some of the highlights from this work.</span></p>\n<h2><strong>Dark mode</strong></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47789 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/mdn.png\" alt=\"\" width=\"512\" height=\"171\" srcset=\"https://hacks.mozilla.org/files/2022/04/mdn.png 512w, https://hacks.mozilla.org/files/2022/04/mdn-250x83.png 250w, https://hacks.mozilla.org/files/2022/04/mdn-500x167.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<p><span style=\"font-weight: 400;\">We updated the color palette used in dark mode in particular.</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reworked the initial color palette to use colors that are slightly</span><a href=\"https://github.com/mdn/yari/issues/5378\"><span style=\"font-weight: 400;\"> more subtle in dark mode</span></a><span style=\"font-weight: 400;\"> while ensuring that we still meet AA accessibility guidelines for color contrast.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reconsidered the darkness of the primary background color in dark mode and settled on a compromise that improved the experience for the majority of readers.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We </span><a href=\"https://github.com/mdn/yari/discussions/5715#discussioncomment-2457075\"><span style=\"font-weight: 400;\">cleaned up the notecards</span></a><span style=\"font-weight: 400;\"> that indicate notices such as warnings, experimental features, items not on the standards track, etc.</span></li>\n</ul>\n<h2><strong>Readability</strong></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47793 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/readibility.png\" alt=\"\" width=\"512\" height=\"171\" srcset=\"https://hacks.mozilla.org/files/2022/04/readibility.png 512w, https://hacks.mozilla.org/files/2022/04/readibility-250x83.png 250w, https://hacks.mozilla.org/files/2022/04/readibility-500x167.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<p><span style=\"font-weight: 400;\">We got a clear sense from some of our community folks that readers found it more difficult to skim content and find sections of interest after the redesign. To address these issues, we made the following improvements:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We implemented a clearly defined type-scale </span><a href=\"https://github.com/mdn/yari/issues/5546#issuecomment-1095192000\"><span style=\"font-weight: 400;\">adjusted for mobile</span></a><span style=\"font-weight: 400;\"> to optimize legibility and to effectively use space, especially on smaller screens.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We made the distinction</span><a href=\"https://github.com/mdn/yari/issues/5485\"><span style=\"font-weight: 400;\"> between the different heading levels</span></a><span style=\"font-weight: 400;\"> clearer.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We moved away from changing </span><a href=\"https://github.com/mdn/yari/issues/5755\"><span style=\"font-weight: 400;\">link colors across different areas</span></a><span style=\"font-weight: 400;\"> of MDN Web Docs. We still retain some of the intent of this design decision, but this is now more subtle.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">The font size was bumped up across all pages, </span><a href=\"https://github.com/mdn/yari/issues/5372\"><span style=\"font-weight: 400;\">including the home page</span></a><span style=\"font-weight: 400;\">. We have also optimized letter and line spacing for a more effortless reading experience. This has also improved the reading experience for our Asian readers, for whom </span><a href=\"https://github.com/mdn/yari/issues/5415\"><span style=\"font-weight: 400;\">line heights were much too tight</span></a><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Missing links are </span><a href=\"https://github.com/mdn/yari/issues/5906#issuecomment-1090400185\"><span style=\"font-weight: 400;\">clearly distinguishable</span></a><span style=\"font-weight: 400;\"> from other links and content.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We improved the layout and </span><a href=\"https://github.com/mdn/yari/issues/5632\"><span style=\"font-weight: 400;\">readability of specifications pages</span></a><span style=\"font-weight: 400;\"> across desktop and small screen devices.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We addressed a bug in how the highlighting in the table of contents worked and </span><a href=\"https://github.com/mdn/yari/pull/5852\"><span style=\"font-weight: 400;\">moved to use an IntersectionObserver</span></a><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We made styling for tables consistent across all pages.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">To ensure readers are always oriented regarding which page they are currently viewing, we have </span><a href=\"https://github.com/mdn/yari/issues/5521\"><span style=\"font-weight: 400;\">made the header (which includes the breadcrumbs) sticky</span></a><span style=\"font-weight: 400;\"> on desktop and mobile.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We fixed our </span><a href=\"https://github.com/mdn/yari/issues/5919\"><span style=\"font-weight: 400;\">accessibility skip navigation</span></a><span style=\"font-weight: 400;\"> to now offer skip to content, skip to search, and skip to language selectors.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We </span><a href=\"https://github.com/mdn/yari/pull/5977\"><span style=\"font-weight: 400;\">fixed a tricky issue</span></a><span style=\"font-weight: 400;\"> that caused some elements to flicker when interacting with the page, especially in dark mode. Many thanks to Daniel Holbert for his assistance in diagnosing the problem.</span></li>\n</ul>\n<h2><strong>Browser compatibility tables</strong></h2>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47785 size-full\" src=\"https://hacks.mozilla.org/files/2022/04/browser-compat.png\" alt=\"\" width=\"512\" height=\"171\" srcset=\"https://hacks.mozilla.org/files/2022/04/browser-compat.png 512w, https://hacks.mozilla.org/files/2022/04/browser-compat-250x83.png 250w, https://hacks.mozilla.org/files/2022/04/browser-compat-500x167.png 500w\" sizes=\"(max-width: 512px) 100vw, 512px\" /></p>\n<p><span style=\"font-weight: 400;\">Another area of the site for which we received feedback after the redesign launch was the browser compatibility tables. Almost its own project inside the larger readability effort, the work we invested here resulted, we believe, in a much-improved user experience. All of the changes listed below are now in production:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We restored version numbers in the overview, which are now color-coded across desktop and mobile.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">The font size has been bumped up for easier reading and skimming.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">The line height of rows has been increased for readability.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reduced the table cells to </span><a href=\"https://github.com/mdn/yari/pull/5648\"><span style=\"font-weight: 400;\">one focusable button element</span></a><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://github.com/mdn/yari/pull/5749\"><span style=\"font-weight: 400;\">Browser icons</span></a><span style=\"font-weight: 400;\"> have been restored in the overview header.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">We reordered </span><a href=\"https://github.com/mdn/yari/pull/5649\"><span style=\"font-weight: 400;\">support history chronologically</span></a><span style=\"font-weight: 400;\"> to make the version range that the support notes refer to visually unambiguous.</span></li>\n</ul>\n<p><span style=\"font-weight: 400;\">We also fixed the following </span><a href=\"https://github.com/mdn/yari/pulls?q=is%3Apr+is%3Aclosed+label%3Abrowser-compat\"><span style=\"font-weight: 400;\">bugs</span></a><span style=\"font-weight: 400;\">:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Color-coded pre-release versions in the overview</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Showing consistent mouseover titles with release dates</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Added the missing </span><a href=\"https://github.com/mdn/yari/pull/5557\"><span style=\"font-weight: 400;\">footnote icon</span></a><span style=\"font-weight: 400;\"> in the overview</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Showing </span><a href=\"https://github.com/mdn/yari/pull/5614\"><span style=\"font-weight: 400;\">correct</span></a><span style=\"font-weight: 400;\"> support status for </span><a href=\"https://github.com/mdn/yari/pull/5616\"><span style=\"font-weight: 400;\">edge cases</span></a><span style=\"font-weight: 400;\"> (e.g., omit prefix symbol if prefixed and unprefixed support)</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Streamlined mobile </span><a href=\"https://github.com/mdn/yari/pull/5933\"><span style=\"font-weight: 400;\">dark mode</span></a></li>\n</ul>\n<p><span style=\"font-weight: 400;\">We believe this is a big step in the right direction but we are not done. We can, and will, continue to </span><span style=\"font-weight: 400;\">improve site-wide readability and functionality of page areas,</span><span style=\"font-weight: 400;\"> such as the sidebars and general accessibility. As with the current improvements, we invite you to </span><a href=\"https://github.com/mdn/yari/issues/new/choose\"><span style=\"font-weight: 400;\">provide us with your feedback</span></a><span style=\"font-weight: 400;\"> and always welcome your pull requests to address </span><a href=\"https://github.com/mdn/yari/issues\"><span style=\"font-weight: 400;\">known issues</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">This was a collective effort, but we&#8217;d like to mention folks who went above and beyond. </span><a href=\"https://github.com/schalkneethling\"><span style=\"font-weight: 400;\">Schalk Neethling</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://github.com/caugner\"><span style=\"font-weight: 400;\">Claas Augner</span></a><span style=\"font-weight: 400;\"> from the MDN Team were responsible for most of the updates. From the community, we’d like to especially thank </span><a href=\"https://github.com/OnkarRuikar\"><span style=\"font-weight: 400;\">Onkar Ruikar</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://github.com/danielhjacobs\"><span style=\"font-weight: 400;\">Daniel Jacobs</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://github.com/iDave2\"><span style=\"font-weight: 400;\">Dave King</span></a><span style=\"font-weight: 400;\">, and </span><a href=\"https://github.com/queengooborg\"><span style=\"font-weight: 400;\">Queen Vinyl Da.i’gyu-Kazotetsu</span></a><span style=\"font-weight: 400;\">.</span></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/adopting-users-design-feedback/\">Adopting users&#8217; design feedback</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Mozilla partners with the Center for Humane Technology</title>\n\t\t<link>https://hacks.mozilla.org/2022/04/mozilla-partners-with-the-center-for-humane-technology/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Melissa Thermidor]]></dc:creator>\n\t\t<pubDate>Wed, 13 Apr 2022 15:02:02 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[center for humane tech]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<category><![CDATA[mdn]]></category>\n\t\t<category><![CDATA[partnership]]></category>\n\t\t<category><![CDATA[pocket]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47775</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>We’re pleased to announce that we have partnered with Center for Humane Tech, a nonprofit organization that radically reimagines the digital infrastructure. Its mission is to drive a comprehensive shift toward humane technology that supports the collective well-being, democracy and shared information environment.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/mozilla-partners-with-the-center-for-humane-technology/\">Mozilla partners with the Center for Humane Technology</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><span style=\"font-weight: 400;\">We’re pleased to announce that we have partnered with the </span><a href=\"https://www.humanetech.com/\"><span style=\"font-weight: 400;\">Center for Humane Technology</span></a><span style=\"font-weight: 400;\">, a nonprofit organization that radically reimagines the digital infrastructure. Its mission is to drive a comprehensive shift toward humane technology that supports the collective well-being, democracy and shared information environment. Many of you may remember the Center for Humane Tech from the Netflix documentary </span><a href=\"https://www.youtube.com/watch?v=uaaC57tcci0\"><span style=\"font-weight: 400;\">‘Social Dilemma’,</span></a><span style=\"font-weight: 400;\"> solidifying the saying “If you&#8217;re not paying for the product, then you are the product”. The Social Dilemma, is all about the dark side of technology, focusing on the individual and societal impact of algorithms. </span></p>\n<p><span style=\"font-weight: 400;\">It’s no surprise that this decision to partner was a no brainer and supports our efforts for a safe and open web that is </span>accessible and joyful for all.<span style=\"font-weight: 400;\"> Many people do not understand how AI and algorithms regularly touch our lives and feel powerless in the face of these systems. We are dedicated to making sure the public understands that we can and must have a say in when machines are used to make important decisions – and shape how those decisions are made. </span></p>\n<p><span style=\"font-weight: 400;\">Over the last few years, our work has been increasingly focused on building more trustworthy AI and safe online spaces. From challenging YouTube’s algorithms, where Mozilla </span><a href=\"https://foundation.mozilla.org/en/blog/mozilla-investigation-youtube-algorithm-recommends-videos-that-violate-the-platforms-very-own-policies/\"><span style=\"font-weight: 400;\">research</span></a><span style=\"font-weight: 400;\"> shows that the platform keeps pushing harmful videos and its algorithm is recommending videos with misinformation, violent content, hate speech and scams to its over two billion users to developing Enhanced Tracking Protection in Firefox that automatically protects your privacy while you browse, and </span><a href=\"https://www.mozilla.org/en-US/firefox/pocket/\"><span style=\"font-weight: 400;\">Pocket</span></a><span style=\"font-weight: 400;\"> which recommends high-quality, human-curated articles without collecting your browsing history or sharing your personal information with advertisers.</span></p>\n<p><span style=\"font-weight: 400;\">Let’s face it, most, if not all people, would probably prefer to use social media platforms that are safer and technologists should design products that reflect all users and without bias. As we collectively continue to think about our role in these areas &#8212; now and in the future, this course from the Center for Humane Tech is a great addition to the many tools necessary for change to take place. </span></p>\n<p><span style=\"font-weight: 400;\">The course rightly titled ‘</span><a href=\"https://www.humanetech.com/course\"><i><span style=\"font-weight: 400;\">Foundations of Humane Technology</span></i></a><i><span style=\"font-weight: 400;\">’ </span></i><span style=\"font-weight: 400;\">launched out of beta in March of this year, after rave reviews from hundreds of beta testers! </span></p>\n<p><span style=\"font-weight: 400;\">It explores the personal, societal, and practical challenges of being a humane technologist. Participants will leave the course with a strong conceptual framework, hands-on tools, and an ecosystem of support from peers and experts. Topics range from respecting human nature to minimizing harm to designing technology that deliberately avoids reinforcing inequitable dynamics of the past. </span></p>\n<p><span style=\"font-weight: 400;\">The course is completely free of charge and is centered towards building awareness and self-education through an online, at-your-own pace or binge-worthy set of eight modules. The course is marketed to professionals, with or without a technical background involved in shaping tomorrow&#8217;s technology. </span></p>\n<p><span style=\"font-weight: 400;\">It includes interactive exercises and reflections to help you internalize what you’re learning and regular optional Zoom sessions to discuss course content, connect with like-minded people, learn from experts in the field and even rewards a credential upon completion that can be shared with colleagues and prospective employers.</span></p>\n<p><span style=\"font-weight: 400;\">The problem with tech is not a new one, but this course is a stepping stone in the right direction.</span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/04/mozilla-partners-with-the-center-for-humane-technology/\">Mozilla partners with the Center for Humane Technology</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Performance Tool in Firefox DevTools Reloaded</title>\n\t\t<link>https://hacks.mozilla.org/2022/03/performance-tool-in-firefox-devtools-reloaded/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Jan Honza Odvarko]]></dc:creator>\n\t\t<pubDate>Wed, 30 Mar 2022 14:59:00 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Developer Tools]]></category>\n\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[Firefox Releases]]></category>\n\t\t<category><![CDATA[Tools]]></category>\n\t\t<category><![CDATA[developers]]></category>\n\t\t<category><![CDATA[devtools]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<category><![CDATA[firefox 98]]></category>\n\t\t<category><![CDATA[Firefox Profiler]]></category>\n\t\t<category><![CDATA[performance]]></category>\n\t\t<category><![CDATA[performance data]]></category>\n\t\t<category><![CDATA[web developer]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47723</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>In Firefox 98, we’re shipping a new version of the existing Performance panel. This panel is now based on the Firefox profiler tool that can be used to capture a performance profile for a web page, inspect visualized performance data and analyze it to identify slow areas.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/performance-tool-in-firefox-devtools-reloaded/\">Performance Tool in Firefox DevTools Reloaded</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p>In Firefox 98, we’re shipping a new version of the existing Performance panel. This panel is now based on the <a href=\"https://profiler.firefox.com/docs/#/\">Firefox profiler</a> tool that can be used to capture a performance profile for a web page, inspect visualized performance data and analyze it to identify slow areas.</p>\n<p>The icing on the cake of this already extremely powerful tool is that you can upload collected profile data with a single click and share the resulting link with your teammates (or anyone really). This makes it easier to collaborate on performance issues, especially in a distributed work environment.</p>\n<p>The new Performance panel is available in Firefox DevTools Toolbox by default and can be opened by <strong>Shift+F5</strong> key shortcut.</p>\n<h2>Usage</h2>\n<p>The only thing the user needs to do to start profiling is clicking on the big blue button &#8211; <strong>Start recording</strong>. Check out the screenshot below.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47724\" src=\"https://hacks.mozilla.org/files/2022/03/perf-panel.png\" alt=\"\" width=\"1037\" height=\"619\" srcset=\"https://hacks.mozilla.org/files/2022/03/perf-panel.png 1037w, https://hacks.mozilla.org/files/2022/03/perf-panel-250x149.png 250w, https://hacks.mozilla.org/files/2022/03/perf-panel-500x298.png 500w, https://hacks.mozilla.org/files/2022/03/perf-panel-768x458.png 768w\" sizes=\"(max-width: 1037px) 100vw, 1037px\" /></p>\n<p>As indicated by the onboarding message at the top of the new panel the previous profiler will be available for some time and eventually removed entirely.</p>\n<p>When profiling is started (i.e. the profiler is gathering performance data) the user can see two more buttons:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47728\" src=\"https://hacks.mozilla.org/files/2022/03/perf-panel-buttons.png\" alt=\"\" width=\"1035\" height=\"97\" srcset=\"https://hacks.mozilla.org/files/2022/03/perf-panel-buttons.png 1035w, https://hacks.mozilla.org/files/2022/03/perf-panel-buttons-250x23.png 250w, https://hacks.mozilla.org/files/2022/03/perf-panel-buttons-500x47.png 500w, https://hacks.mozilla.org/files/2022/03/perf-panel-buttons-768x72.png 768w\" sizes=\"(max-width: 1035px) 100vw, 1035px\" /></p>\n<ul>\n<li><strong>Capture recording</strong> &#8211; Stop recording, get what’s been collected so far and visualize it</li>\n<li><strong>Cancel recording</strong> &#8211; Stop recording and throw away all collected data</li>\n</ul>\n<p>When the user clicks on Capture recording all collected data are visualized in a new tab. You should see something like the following:</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47740\" src=\"https://hacks.mozilla.org/files/2022/03/perf-data.png\" alt=\"\" width=\"4256\" height=\"2264\" srcset=\"https://hacks.mozilla.org/files/2022/03/perf-data.png 4256w, https://hacks.mozilla.org/files/2022/03/perf-data-250x133.png 250w, https://hacks.mozilla.org/files/2022/03/perf-data-500x266.png 500w, https://hacks.mozilla.org/files/2022/03/perf-data-768x409.png 768w, https://hacks.mozilla.org/files/2022/03/perf-data-1536x817.png 1536w, https://hacks.mozilla.org/files/2022/03/perf-data-2048x1089.png 2048w\" sizes=\"(max-width: 4256px) 100vw, 4256px\" /></p>\n<p>The inspection capabilities of the UI are powerful and let the user inspect every bit of the performance data. You might want to follow this detailed <a href=\"https://profiler.firefox.com/docs/#/./guide-ui-tour\">UI Tour</a> presentation created by the Performance team at Mozilla to learn more about all available features.</p>\n<h2>Customization</h2>\n<p>There are many options that can be used to customize how and what performance data should be collected to optimize specific use cases (see also the <strong>Edit Settings…</strong> link at the bottom of the panel).</p>\n<p>To make customization easier some presets are available and the <strong>Web Developer</strong> preset is selected by default. The profiler can be also used for profiling Firefox itself and Mozilla is extensively using it to make Firefox fast for millions of its users. The WebDeveloper preset is intended for profiling standard web pages and the rest is for profiling Firefox.</p>\n<p>The Profiler can be also used directly from the Firefox toolbar without the DevTools Toolbox being opened. The Profiler button isn’t visible in the toolbar by default, but you can enable it by loading <a href=\"https://profiler.firefox.com/\">https://profiler.firefox.com/</a> and clicking on the “Enable Firefox Profiler Menu Button” on the page.</p>\n<p>This is what the button looks like in the Firefox toolbar.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-large wp-image-47732\" src=\"https://hacks.mozilla.org/files/2022/03/toolbar-500x459.png\" alt=\"\" width=\"500\" height=\"459\" srcset=\"https://hacks.mozilla.org/files/2022/03/toolbar-500x459.png 500w, https://hacks.mozilla.org/files/2022/03/toolbar-250x230.png 250w, https://hacks.mozilla.org/files/2022/03/toolbar.png 611w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p>As you can see from the screenshot above the UI is almost exactly the same (compared to the DevTools Performance panel).</p>\n<h2>Sharing Data</h2>\n<p>Collected performance data can be shared publicly. This is one of the most powerful features of the profiler since it allows the user to upload data to the Firefox Profiler online storage. Before uploading a profile, you can select the data that you want to include, and what you don’t want to include to avoid leaking personal data. The profile link can then be shared in online chats, emails, and bug reports so other people can see and investigate a specific case.</p>\n<p>This is great for team collaboration and that’s something Firefox developers have been doing for years to work on performance. The profile can also be saved as a file on a local machine and imported later from <a href=\"https://profiler.firefox.com/\">https://profiler.firefox.com/</a></p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-47736\" src=\"https://hacks.mozilla.org/files/2022/03/share-perf-data.png\" alt=\"\" width=\"930\" height=\"617\" srcset=\"https://hacks.mozilla.org/files/2022/03/share-perf-data.png 930w, https://hacks.mozilla.org/files/2022/03/share-perf-data-250x166.png 250w, https://hacks.mozilla.org/files/2022/03/share-perf-data-500x332.png 500w, https://hacks.mozilla.org/files/2022/03/share-perf-data-768x510.png 768w\" sizes=\"(max-width: 930px) 100vw, 930px\" /></p>\n<p>There are many more powerful features available and you can learn more about them in the extensive <a href=\"https://profiler.firefox.com/docs/#/\">documentation</a>. And of course, just like Firefox itself, the profiler tool is an open source project and you might want to <a href=\"https://github.com/firefox-devtools/profiler\">contribute</a> to it.</p>\n<p>There is also a great <a href=\"https://profiler.firefox.com/docs/#/./bunny\">case study</a> on using the profiler to identify performance issues.</p>\n<p>More is coming to DevTools, so stay tuned!</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/performance-tool-in-firefox-devtools-reloaded/\">Performance Tool in Firefox DevTools Reloaded</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Introducing MDN Plus: Make MDN your own</title>\n\t\t<link>https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Hermina]]></dc:creator>\n\t\t<pubDate>Thu, 24 Mar 2022 16:00:29 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[MDN]]></category>\n\t\t<category><![CDATA[mdn]]></category>\n\t\t<category><![CDATA[mozilla]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47744</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>MDN is one of the most trusted resources for information about web standards, code samples, tools, and everything you need as a developer to create websites. Today, we are launching MDN Plus, our first step to providing a personalized and more powerful experience while continuing to invest in our always free and open webdocs.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/\">Introducing MDN Plus: Make MDN your own</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[\r\n<p>MDN is one of the most trusted resources for information about web standards, code samples, tools, and everything you need as a developer to create websites. In 2015, we explored how we could expand beyond documentation to provide a structured learning experience. Our first foray was the <a href=\"https://developer.mozilla.org/en-US/docs/Learn\">Learning Area</a>, with the goal of providing a useful addition to the regular MDN reference and guide material. In 2020, we added the first <a href=\"https://developer.mozilla.org/en-US/docs/Learn/Front-end_web_developer\">Front-end developer learning pathway</a>. We saw a lot of interest and engagement from users, and the learning area contributed to about 10% of MDN’s monthly web traffic. These two initiatives were the start of our exploration into how we could offer more learning resources to our community. Today, we are launching MDN Plus, our first step to providing a personalized and more powerful experience while continuing to invest in our always free and open webdocs.</p>\r\n\r\n<p><iframe loading=\"lazy\" title=\"YouTube video player\" src=\"https://www.youtube.com/embed/OBv9qnCesaQ\" width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe></p>\r\n<h2>Build your own MDN Experience with MDN Plus</h2>\r\n<p>In 2020 and 2021 we surveyed over 60,000 MDN users and learned that many of the respondents  wanted a customized MDN experience. They wanted to organize MDN’s vast library in a way that worked for them. For today’s premium subscription service, MDN Plus, we are releasing three new features that begin to address this need: Notifications, Collections and MDN Offline. More details about the features are listed below:</p>\r\n<ul>\r\n<li><b><i>Notifications: </i></b>Technology is ever changing, and we know how important it is to stay on top of the latest updates and developments. From tutorial pages to API references, you can now get notifications for the latest developments on MDN. When you follow a page, you’ll get notified when the documentation changes, CSS features launch, and APIs ship. Now, you can get a notification for significant events relating to the pages you want to follow. <a href=\"https://developer.mozilla.org/en-US/plus/docs/features/notifications\">Read more about it here</a>.</li>\r\n</ul>\r\n<p><img loading=\"lazy\" class=\"alignnone size-large wp-image-47750\" src=\"https://hacks.mozilla.org/files/2022/03/image-14-500x292.png\" alt=\"Screenshot of a list of notifications on mdn plus\" width=\"500\" height=\"292\" srcset=\"https://hacks.mozilla.org/files/2022/03/image-14-500x292.png 500w, https://hacks.mozilla.org/files/2022/03/image-14-250x146.png 250w, https://hacks.mozilla.org/files/2022/03/image-14-768x448.png 768w, https://hacks.mozilla.org/files/2022/03/image-14-1536x896.png 1536w, https://hacks.mozilla.org/files/2022/03/image-14-2048x1195.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\r\n<ul>\r\n<li><b><i>Collections:</i></b> Find what you need fast with our new collections feature. Not only can you pick the MDN articles you want to save, we also automatically save the pages you visit frequently. Collections help you quickly access the articles that matter the most to you and your work. <a href=\"https://developer.mozilla.org/en-US/plus/docs/features/collections\">Read more about it here</a>.</li>\r\n</ul>\r\n<p><img loading=\"lazy\" class=\"alignnone size-large wp-image-47746\" src=\"https://hacks.mozilla.org/files/2022/03/image-13-500x292.png\" alt=\"Screenshot of a collections list on mdn plus\" width=\"500\" height=\"292\" srcset=\"https://hacks.mozilla.org/files/2022/03/image-13-500x292.png 500w, https://hacks.mozilla.org/files/2022/03/image-13-250x146.png 250w, https://hacks.mozilla.org/files/2022/03/image-13-768x448.png 768w, https://hacks.mozilla.org/files/2022/03/image-13-1536x896.png 1536w, https://hacks.mozilla.org/files/2022/03/image-13-2048x1195.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\r\n<ul>\r\n<li><b><i>MDN offline</i></b>: Sometimes you need to access MDN but don’t have an internet connection. MDN offline leverages a Progressive Web Application (PWA) to give you access to MDN Web Docs even when you lack internet access so you can continue your work without any interruptions. Plus, with MDN offline you can have a faster experience while saving data. <a href=\"https://developer.mozilla.org/en-US/plus/docs/features/offline\">Read more about it here</a>.</li>\r\n</ul>\r\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47760 size-large\" src=\"https://hacks.mozilla.org/files/2022/03/image1-500x299.png\" alt=\"Screenshot of offline settings on mdn plus\" width=\"500\" height=\"299\" srcset=\"https://hacks.mozilla.org/files/2022/03/image1-500x299.png 500w, https://hacks.mozilla.org/files/2022/03/image1-250x150.png 250w, https://hacks.mozilla.org/files/2022/03/image1-768x460.png 768w, https://hacks.mozilla.org/files/2022/03/image1.png 1507w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\r\n<p id=\"countries\">Today, MDN Plus is available in the US and Canada. In the coming months, we will expand to other countries including France, Germany, Italy, Spain, Belgium, Austria, the Netherlands, Ireland, United Kingdom, Switzerland, Malaysia, New Zealand and Singapore. </p>\r\n<h2>Find the right MDN Plus plan for you</h2>\r\n<p>MDN is part of the daily life of millions of web developers. For many of us MDN helped with getting that first job or helped land a promotion. During our research we found many of these users, users who felt so much value from MDN that they wanted to contribute financially. We were both delighted and humbled by this feedback. To provide folks with a few options, we are launching MDN Plus with three plans including a supporter plan for those that want to spend a little extra. Here are the details of those plans:</p>\r\n<ul>\r\n<li aria-level=\"1\"><b><i>MDN Core</i></b>: For those who want to do a test drive before purchasing a plan, we created an option that lets you try a limited version for free.  </li>\r\n<li aria-level=\"1\"><b><i>MDN Plus 5</i></b>:  Offers unlimited access to notifications, collections, and MDN offline with new features added all the time. $5 a month or an annual subscription of $50.</li>\r\n<li aria-level=\"1\"><b><i>MDN Supporter 10</i></b>:  For MDN’s loyal supporters the supporter plan gives you everything under MDN Plus 5 plus early access to new features and a direct feedback channel to  the MDN team. It’s $10 a month or $100 for an annual subscription.  </li>\r\n</ul>\r\n<p>Additionally, we will offer a 20% discount if you subscribe to one of the annual subscription plans.</p>\r\n<p>We invite you to try the <a style=\"color:#e80840;\" href=\"https://developer.mozilla.org/en-US/plus#subscribe\">free trial version or sign up</a> today for a subscription plan that’s right for you. MDN Plus is only <a href=\"#countries\">available in selected countries</a> at this time.</p>\r\n<p>&nbsp;</p><p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/\">Introducing MDN Plus: Make MDN your own</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Mozilla and Open Web Docs working together on MDN</title>\n\t\t<link>https://hacks.mozilla.org/2022/03/mozilla-and-open-web-docs-working-together-on-mdn/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Hermina]]></dc:creator>\n\t\t<pubDate>Thu, 17 Mar 2022 14:07:34 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Docs]]></category>\n\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[MDN]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<category><![CDATA[mdn]]></category>\n\t\t<category><![CDATA[mdn plus]]></category>\n\t\t<category><![CDATA[mozilla]]></category>\n\t\t<category><![CDATA[open docs]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47707</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>For both MDN and Open Web Docs (OWD), transparency is paramount to our missions. With the upcoming launch of MDN Plus, we believe it’s a good time to talk about how our two organizations work together, and if there is a financial relationship between us. Here is an overview of how our missions overlap and how they differ, and how a premium subscription service fits all this.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/mozilla-and-open-web-docs-working-together-on-mdn/\">Mozilla and Open Web Docs working together on MDN</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><span style=\"font-weight: 400;\">For both MDN and Open Web Docs (OWD), transparency is paramount to our missions. With the upcoming launch of MDN Plus, we believe it’s a good time to talk about how our two organizations work together and if there is a financial relationship between us. Here is an overview of how our missions overlap, how they differ, and how a premium subscription service fits all this.</span></p>\n<h2><strong>History of our collaboration</strong></h2>\n<p><span style=\"font-weight: 400;\">MDN and Open Web Docs began working together after the creation of Open Web Docs in 2021. Our organizations were born out of the same ethos, and we constantly collaborate on MDN content, contributing to different parts of MDN and even teaming up for shared projects like the conversion to Markdown. We meet on a weekly basis to discuss content strategies and maintain an open dialogue on our respective roadmaps.</span></p>\n<p><span style=\"font-weight: 400;\">MDN and Open Web Docs are different organizations; while our missions and goals frequently overlap, our work is not identical. Open Web Docs is an open collective, with a mission to contribute content to open source projects that are considered important for the future of the Web. MDN is currently the most significant project that Open Web Docs contributes to.</span></p>\n<h2><strong>Separate funding streams, division of labor</strong></h2>\n<p><span style=\"font-weight: 400;\">Mozilla and Open Web Docs collaborate closely on sustaining the Web Docs part of MDN. The Web Docs part is and will remain free and accessible to all. Each organization shoulders part of the costs of this labor, from our distinct budgets and revenue sources.</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Mozilla covers the cost of infrastructure, development and maintenance of the MDN platform including a team of engineers and its own team of dedicated writers.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Open Web Docs receives donations from companies like Google, Microsoft, Meta, Coil and others, and from private individuals. These donations pay for Technical Writing staff and help finance Open Web Docs projects. None of the donations that Open Web Docs receive go to MDN or Mozilla; rather they pay for a team of writers to contribute to MDN. </span></li>\n</ul>\n<h2><strong>Transparency and dialogue but independent decision-making</strong></h2>\n<p><span style=\"font-weight: 400;\">Mozilla and OWD have an open dialogue on content related to MDN. Mozilla sits on the Open Web Docs&#8217; Steering Committee, sharing expertise and experience but does not currently sit on the Open Web Docs’ Governing Committee. Mozilla does not provide direct financial support to Open Web Docs and does not participate in making decisions about Open Web Docs&#8217; overall direction, objectives, hiring and budgeting.</span></p>\n<h2><strong>MDN Plus: How does it fit into the big picture?</strong></h2>\n<p><span style=\"font-weight: 400;\">MDN Plus is a new premium subscription service by Mozilla that allows users to customize their MDN experience. </span></p>\n<p><span style=\"font-weight: 400;\">As with so much of our work, our organizations engaged in a transparent dialogue regarding MDN Plus. When requested, Open Web Docs has provided Mozilla with feedback, but it has not been a part of the development of MDN Plus. The resources Open Web Docs has are used only to improve the free offering of MDN. </span></p>\n<p><span style=\"font-weight: 400;\">The existence of a new subscription model will not detract from MDN&#8217;s current free Web Docs offering in any way. The current experience of accessing web documentation will not change for users who do not wish to sign up for a premium subscription. </span></p>\n<p><span style=\"font-weight: 400;\">Mozilla’s goal with MDN Plus is to help ensure that MDN&#8217;s open source content continues to be supported into the future. While Mozilla has incorporated its partners’ feedback into their vision for the product, MDN Plus has been built only with Mozilla resources. Any revenue generated by MDN Plus will stay within Mozilla. Mozilla is looking into ways to reinvest some of these additional funds into open source projects contributing to MDN but it is still in early stages.</span></p>\n<p><span style=\"font-weight: 400;\">A subscription to MDN Plus gives paying subscribers extra MDN features provided by Mozilla while a donation to Open Web Docs goes to funding writers creating content on MDN Web Docs, and potentially elsewhere. Work produced via OWD will always be publicly available and accessible to all. </span></p>\n<p><span style=\"font-weight: 400;\">Open Web Docs and Mozilla will continue to work closely together on MDN for the best possible web platform documentation for everyone!</span></p>\n<p><span style=\"font-weight: 400;\">Thanks for your continuing feedback and support.</span></p>\n<p>&nbsp;</p>\n<p><img loading=\"lazy\" class=\"wp-image-47717 alignleft\" src=\"https://hacks.mozilla.org/files/2022/03/mdn_owd-250x63.png\" alt=\"\" width=\"365\" height=\"92\" srcset=\"https://hacks.mozilla.org/files/2022/03/mdn_owd-250x63.png 250w, https://hacks.mozilla.org/files/2022/03/mdn_owd-500x125.png 500w, https://hacks.mozilla.org/files/2022/03/mdn_owd.png 600w\" sizes=\"(max-width: 365px) 100vw, 365px\" /></p>\n<p>&nbsp;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/mozilla-and-open-web-docs-working-together-on-mdn/\">Mozilla and Open Web Docs working together on MDN</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Announcing Interop 2022</title>\n\t\t<link>https://hacks.mozilla.org/2022/03/interop-2022/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Anne van Kesteren]]></dc:creator>\n\t\t<pubDate>Thu, 03 Mar 2022 17:00:02 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[Standards]]></category>\n\t\t<category><![CDATA[Testing]]></category>\n\t\t<category><![CDATA[browsers]]></category>\n\t\t<category><![CDATA[interop-2022]]></category>\n\t\t<category><![CDATA[interoperability]]></category>\n\t\t<category><![CDATA[web-platform-tests]]></category>\n\t\t<category><![CDATA[wpt]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47622</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>Writing high quality standards is a necessary first step to an interoperable web platform, but ensuring that browsers are consistent in their behavior requires an ongoing process. Browsers must work to ensure that they have a shared understanding of web standards, and that their implementation matches that understanding.</p>\n<p>Interop 2022 is a cross-browser initiative to find and address the most important interoperability pain points on the web platform. The end result is a public metric that will assess progress toward fixing these interoperability issues.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/interop-2022/\">Announcing Interop 2022</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p>A key benefit of the web platform is that it&#8217;s defined by standards, rather than by the code of a single implementation. This creates a shared platform that isn&#8217;t tied to specific hardware, a company, or a business model.</p>\n<p>Writing high quality standards is a necessary first step to an interoperable web platform, but ensuring that browsers are consistent in their behavior requires an ongoing process. Browsers must work to ensure that they have a shared understanding of web standards, and that their implementation matches that understanding.</p>\n<h2>Interop 2022</h2>\n<p>Interop 2022 is a cross-browser initiative to find and address the most important interoperability pain points on the web platform. The end result is a public metric that will assess progress toward fixing these interoperability issues.</p>\n<p><a href=\"https://wpt.fyi/interop-2022\"><img loading=\"lazy\" class=\"alignnone\" src=\"https://hacks.mozilla.org/files/2022/02/interop-2022-dashboard.png\" alt=\"Interop 2022 scores. Chrome/Edge 71, Firefox 74, and Safari 73.\" width=\"1200\" height=\"580\" /></a></p>\n<p>In order to identify the areas to include, we looked at two primary sources of data:</p>\n<ul>\n<li>Web developer feedback (e.g., through developer facing surveys including <a href=\"https://insights.developer.mozilla.org/\">MDN’s Web DNA Report</a>) on the most common pain points they experience.</li>\n<li>End user bug reports (e.g., via <a href=\"https://webcompat.com/\">webcompat.com</a>) that could be traced back to implementation differences between browsers.</li>\n</ul>\n<p>During the process of collecting this data, it became clear there are two principal kinds of interoperability problems which affect end users and developers:</p>\n<ul>\n<li>Problems where there&#8217;s a relatively clear and widely accepted standard, but where implementations are incomplete or buggy.</li>\n<li>Problems where the standard is missing, unclear, or doesn&#8217;t match the behavior sites depend on.</li>\n</ul>\n<p>Problems of the first kind have been termed &#8220;focus areas&#8221;. For these we use <a href=\"https://web-platform-tests.org/\">web-platform-tests</a>: a large, shared testsuite that aims to ensure web standards are implemented consistently across browsers. It accepts contributions from anyone, and browsers, including Firefox, contribute tests as part of their process for fixing bugs and shipping new features.</p>\n<p>The path to improvement for these areas is clear: identify or write tests in web-platform-tests that measure conformance to the relevant standard, and update implementations so that they pass those tests.</p>\n<p>Problems of the second kind have been termed “investigate areas”. For these it’s not possible to simply write tests as we&#8217;re not really sure what&#8217;s necessary to reach interoperability. Such unknown unknowns turn out to be extremely common sources of developer and user frustration!</p>\n<p>We’ll make progress here through investigation. And we’ll measure progress with more qualitative goals, e.g., working out what exact behavior sites depend on, and what can be implemented in practice without breaking the web.</p>\n<p>In all cases, the hope is that we can move toward a future in which we know how to make these areas interoperable, update the relevant web standards for them, and measure them with tests as we do with focus areas.</p>\n<h3>Focus areas</h3>\n<p>Interop 2022 has ten new focus areas:</p>\n<ul>\n<li>Cascade Layers</li>\n<li>Color Spaces and Functions</li>\n<li>Containment</li>\n<li>Dialog Element</li>\n<li>Forms</li>\n<li>Scrolling</li>\n<li>Subgrid</li>\n<li>Typography and Encodings</li>\n<li>Viewport Units</li>\n<li>Web Compat</li>\n</ul>\n<p>Unlike the others the Web Compat area doesn&#8217;t represent a specific technology, but is a group of specific known problems with already shipped features, where we see bugs and deviations from standards cause frequent site breakage for end users.</p>\n<p>There are also five additional areas that have been adopted from Google and Microsoft&#8217;s “Compat 2021” effort:</p>\n<ul>\n<li>Aspect Ratio</li>\n<li>Flexbox</li>\n<li>Grid</li>\n<li>Sticky Positioning</li>\n<li>Transforms</li>\n</ul>\n<p>A browser&#8217;s test pass rate in each area contributes 6% — totaling at 90% for fifteen areas — of their score of Interop 2022.</p>\n<p>We believe these are areas where the standards are in good shape for implementation, and where improving interoperability will directly improve the lives of developers and end users.</p>\n<h3>Investigate areas</h3>\n<p>Interop 2022 has three investigate areas:</p>\n<ul>\n<li>Editing, contentEditable, and execCommand</li>\n<li>Pointer and Mouse Events</li>\n<li>Viewport Measurement</li>\n</ul>\n<p>These are areas in which we often see complaints from end users, or reports of site breakage, but where the path toward solving the issues isn&#8217;t clear. Collaboration between vendors is essential to working out how to fix these problem areas, and we believe that Interop 2022 is a unique opportunity to make progress on historically neglected areas of the web platform.</p>\n<p>The overall progress in this area will contribute 10% to the overall score of Interop 2022. This score will be the same across all browsers. This reflects the fact that progress on the web platform requires browsers to collaborate on new or updated web standards and accompanying tests, to achieve the best outcomes for end users and developers.</p>\n<h2>Contributions welcome!</h2>\n<p>Whilst the focus and investigate areas for 2022 are now set, there is still much to do. For the investigate areas, the detailed targets need to be set, and the complex work of understanding the current state of the art, and assessing the options to advance it, are just starting. Additional tests for the focus areas might be needed as well to address particular edge cases.</p>\n<p>If this sounds like something you&#8217;d like to get involved with, follow the instructions on the <a href=\"https://wpt.fyi/interop-2022\">Interop 2022 Dashboard</a>.</p>\n<p>Finally, it&#8217;s also possible that Interop 2022 is missing an area you consider to be a significant pain point. It won&#8217;t be possible to add areas this year, but, if the effort is a success we may end up running further iterations. Feedback on browser differences that are making your life hard as developer or end user are always welcome and will be helpful for identifying the correct focus and investigate areas for any future edition.</p>\n<h2>Partner announcements</h2>\n<p>Bringing Interop 2022 to fruition was a collaborative effort and you might be interested in the other announcements:</p>\n<ul>\n<li>Apple&#8217;s <a href=\"https://webkit.org/blog/12288/working-together-on-interop-2022/\">Working together on Interop 2022</a></li>\n<li><a href=\"https://bocoup.com/blog/interop-2022\">Bocoup and Interop 2022</a></li>\n<li>Google&#8217;s <a href=\"https://web.dev/interop-2022\">Interop 2022: browsers working together to improve the web for developers</a></li>\n<li><a href=\"https://www.igalia.com/news/interop2022.html\">Igalia and Interop 2022</a></li>\n<li><a href=\"https://aka.ms/microsoft-interop2022\">Microsoft and Interop 2022</a></li>\n</ul>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/interop-2022/\">Announcing Interop 2022</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>A new year, a new MDN</title>\n\t\t<link>https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Hermina]]></dc:creator>\n\t\t<pubDate>Tue, 01 Mar 2022 14:00:24 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Developer Tools]]></category>\n\t\t<category><![CDATA[Docs]]></category>\n\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[MDN]]></category>\n\t\t<category><![CDATA[developer communities]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<category><![CDATA[mdn]]></category>\n\t\t<category><![CDATA[mdn plus]]></category>\n\t\t<category><![CDATA[open source]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47643</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>If you’ve accessed the MDN website today, you probably noticed that it looks quite different. We hope it’s a good different. Let us explain!</p>\n<p>In mid-2021 we started to think about modernizing MDN’s design, to create a clean and inviting website that makes navigating our 44,000 articles as easy as possible. We wanted to create a more holistic experience for our users, with an emphasis on improved navigability and a universal look and feel across all our pages. </p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\">A new year, a new MDN</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><span style=\"font-weight: 400;\">If you’ve accessed the MDN website today, you probably noticed that it looks quite different. We hope it’s a good different. Let us explain!</span></p>\n<p><span style=\"font-weight: 400;\">MDN has undergone many changes in its sixteen-year history from its early beginning as a wiki to the recent migration of a static site backed by GitHub. During that time MDN grew organically, with over 45,000 contributors and numerous developers and designers. It’s no surprise that the user experience became somewhat inconsistent throughout the website. </span></p>\n<p><span style=\"font-weight: 400;\">In mid-2021 we started to think about modernizing MDN’s design, to create a clean and inviting website that makes navigating our 44,000 articles as easy as possible. We wanted to create a more holistic experience for our users, with an emphasis on improved navigability and a universal look and feel across all our pages. </span></p>\n<h2><b>A new Homepage, focused on community</b></h2>\n<p><span style=\"font-weight: 400;\">The MDN community is the reason our content can be counted on to be both high quality and trustworthy. MDN content is scrutinized, discussed, and yes, in some cases argued about. Anyone can contribute to MDN, either by writing content, suggesting changes or fixing bugs.</span></p>\n<p><span style=\"font-weight: 400;\">We wanted to acknowledge and celebrate our awesome community and our homepage is the perfect place to do so.</span></p>\n<p><span style=\"font-weight: 400;\">The new homepage was built with a focus on the core concepts of community and simplicity. We made an improved search a central element on the page, while also showing users a selection of the newest and most-read articles. </span></p>\n<p><span style=\"font-weight: 400;\">We will also show the most recent contributions to our GitHub content repo and added a contributor spotlight where we will highlight MDN contributors.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47690 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_mdn_dark-mode.gif\" alt=\"\" width=\"1080\" height=\"675\" /></p>\n<h2><b>Redesigned article pages for improved navigation</b></h2>\n<p><span style=\"font-weight: 400;\">It’s been years—</span><a href=\"https://hacks.mozilla.org/2017/07/the-mdn-redesign-behind-the-scenes/\"><span style=\"font-weight: 400;\">five of them, in fact</span></a><span style=\"font-weight: 400;\">—since MDN’s core content presentation has received a comprehensive design review. In those years, MDN’s content has evolved and changed, with new </span><a href=\"https://hacks.mozilla.org/2018/02/mdn-browser-compatibility-data/\"><span style=\"font-weight: 400;\">ways of structuring content</span></a><span style=\"font-weight: 400;\">, new ways to </span><a href=\"https://hacks.mozilla.org/2020/12/welcome-yari-mdn-web-docs-has-a-new-platform/\"><span style=\"font-weight: 400;\">build</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://github.com/mdn/content/pull/7092\"><span style=\"font-weight: 400;\">write docs</span></a><span style=\"font-weight: 400;\">, and new </span><a href=\"https://github.com/mdn/content/pulse\"><span style=\"font-weight: 400;\">contributors</span></a><span style=\"font-weight: 400;\">. Over time, the documentation’s look and feel had become increasingly disconnected from the way it’s read and written.</span></p>\n<p><span style=\"font-weight: 400;\">While you won’t see a dizzying reinvention of what documentation is, you’ll find that most visual elements on MDN did get love and attention, creating a more coherent view of our docs. This redesign gives MDN content its due, featuring:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">More consistent colors and theming</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Better signposting of major sections, such as HTML, CSS, and JavaScript</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Improved accessibility, such as increased contrast</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span style=\"font-weight: 400;\">Added dark mode toggle for easy switching between modes</span></li>\n</ul>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47694 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_mdn_navigation.2022-02-24-13_14_00.gif\" alt=\"\" width=\"1080\" height=\"675\" /></p>\n<p>&nbsp;</p>\n<p><span style=\"font-weight: 400;\">We’re especially proud of some subtle improvements and conveniences. For example, in-page navigation is always in view to show you where you are in the page as you scroll:</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47698 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_mdn_scrolling.gif\" alt=\"\" width=\"1080\" height=\"675\" /></p>\n<p><span style=\"font-weight: 400;\">We’re also revisiting the way browser compatibility data appears, with better at-a-glance browser support. So you don’t have to keep version numbers in your head, we’ve put more emphasis on </span><i><span style=\"font-weight: 400;\">yes</span></i><span style=\"font-weight: 400;\"> and </span><i><span style=\"font-weight: 400;\">no</span></i><span style=\"font-weight: 400;\"> iconography for browser capabilities, with the option to view the detailed information you’ve come to expect from </span><a href=\"https://github.com/mdn/browser-compat-data\"><span style=\"font-weight: 400;\">our browser compatibility data</span></a><span style=\"font-weight: 400;\">. We think you should check it out. </span></p>\n<p><span style=\"font-weight: 400;\">And we’re not stopping there. The work we’ve done is far-reaching and there are still many opportunities to polish and improve on the design we’re shipping.</span></p>\n<h2><b>A new logo, chosen by our community</b></h2>\n<p><span style=\"font-weight: 400;\">As we began working on both the redesign and expanding MDN beyond WebDocs we realized it was also time for a new logo. We wanted a modern and easily customizable logo that would represent what MDN is today while also strengthening its identity and making it consistent with Mozilla’s current brand.</span></p>\n<p><span style=\"font-weight: 400;\">We worked closely with branding specialist </span><a href=\"https://lucdoucedame.com/\"><span style=\"font-weight: 400;\">Luc Doucedame</span></a><span style=\"font-weight: 400;\">, narrowed down our options to eight potential logos and put out a call to our community of users to help us choose and invited folks to vote on their favorite. We received over 10,000 votes in just three days and are happy to share with you “the MDN people&#8217;s choice.”</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47673 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21.png\" alt=\"\" width=\"643\" height=\"277\" srcset=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21.png 643w, https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21-250x108.png 250w, https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.36.21-500x215.png 500w\" sizes=\"(max-width: 643px) 100vw, 643px\" /></p>\n<p><span style=\"font-weight: 400;\">The winner was Option 4, an M monogram using underscore to convey the process of writing code. Many thanks to everyone who voted!</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47669 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/moz_blog_header_MDN-Intro.png\" alt=\"\" width=\"1920\" height=\"1080\" /></p>\n<h2><b>What you can expect next with MDN</b></h2>\n<h3></h3>\n<h3><b>Bringing content to the places where you need it most</b></h3>\n<p><span style=\"font-weight: 400;\">In recent years, MDN content has grown more sophisticated for authors, such as moving from a wiki to Git and converting from HTML to Markdown. This has been a boon to contributors, who can use more powerful and familiar tools to create more structured and consistent content.</span></p>\n<p><span style=\"font-weight: 400;\">With better tools in place, we’re finally in a position to build more visible and systematic benefits to readers. For example, many of you probably navigate MDN via your favorite search engine, rather than MDN’s own site navigation. We get it. Historically, a wiki made large content architecture efforts impractical. But we’re now closer than ever to making site-wide improvements to structure and navigation.</span></p>\n<p><span style=\"font-weight: 400;\">Looking forward, we have ambitious plans to take advantage of our new tools to explore improved navigation, generated standardization and support summarizes, and embedding MDN documentation in the places where developers need it most: in their IDE, browser tools, and more.</span></p>\n<h2><b>Coming soon: MDN Plus</b></h2>\n<p><span style=\"font-weight: 400;\">MDN has built a reputation as a trusted and central resource for information about standards, codes, tools, and everything you need as a developer to create websites. In 2015, we explored ways to be more than a central resource through creating a </span><a href=\"https://developer.mozilla.org/en-US/docs/Learn\"><span style=\"font-weight: 400;\">Learning Area</span></a><span style=\"font-weight: 400;\">, with the aim of providing a useful counterpart to the regular MDN reference and guide material. </span></p>\n<p><span style=\"font-weight: 400;\">In 2020, we added the first </span><a href=\"https://developer.mozilla.org/en-US/docs/Learn/Front-end_web_developer\"><span style=\"font-weight: 400;\">Front-end developer learning pathway</span></a><span style=\"font-weight: 400;\"> to it.  We saw a lot of interest and engagement from users, the learning area currently being responsible for 10% of MDN’s monthly web traffic. This started us on a path to see what more we can do in this area for our community.</span></p>\n<p><span style=\"font-weight: 400;\">Last year we surveyed users and asked them what they wanted out of their MDN experience. The top requested features included notifications, article collections and an offline experience on MDN. The overall theme we saw was that users wanted to be able to organize MDN’s vast library in a way that worked for them. </span></p>\n<p><span style=\"font-weight: 400;\">We are always looking for ways to meet our users&#8217; needs whether it&#8217;s through MDN’s free web documentation or personalized features. In the coming months, we’ll be expanding MDN to include a premium subscription service based on the feedback we received from web developers who want to customize their MDN experience. Stay tuned for more information on MDN Plus.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47677 size-full\" src=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.45.53.png\" alt=\"\" width=\"372\" height=\"430\" srcset=\"https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.45.53.png 372w, https://hacks.mozilla.org/files/2022/02/Screenshot-2022-02-24-at-10.45.53-250x289.png 250w\" sizes=\"(max-width: 372px) 100vw, 372px\" /></p>\n<h2><b>Thank you, MDN community</b></h2>\n<p><span style=\"font-weight: 400;\">We appreciate the thousands of people who voted for the new logo as well as everyone who participated in the early beta testing phase since we started this journey. Also, many thanks to our partners from the </span><a href=\"https://openwebdocs.org\"><span style=\"font-weight: 400;\">Open Web Docs</span></a><span style=\"font-weight: 400;\">, who gave us valuable feedback on the redesign and continue to make daily contributions to MDN content. Thanks to you all we could make this a reality and we will continue to invest in improving even further the experience on MDN.</span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/\">A new year, a new MDN</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Version 100 in Chrome and Firefox</title>\n\t\t<link>https://hacks.mozilla.org/2022/02/version-100-in-chrome-and-firefox/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Karl Dubost]]></dc:creator>\n\t\t<pubDate>Tue, 15 Feb 2022 18:05:20 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Debugging]]></category>\n\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[Testing]]></category>\n\t\t<category><![CDATA[Web Developers]]></category>\n\t\t<category><![CDATA[HTTP]]></category>\n\t\t<category><![CDATA[JavaScript]]></category>\n\t\t<category><![CDATA[web compatibility]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47607</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>Chrome and Firefox will reach version 100 in a couple of months. This has the potential to cause breakage on sites that rely on identifying the browser version to perform business logic.  This post covers the timeline of events, the strategies that Chrome and Firefox are taking to mitigate the impact, and how you can help.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/version-100-in-chrome-and-firefox/\">Version 100 in Chrome and Firefox</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p>Chrome and Firefox will reach version 100 in a <a href=\"https://developer.chrome.com/blog/force-major-version-to-100/\">couple</a> of <a href=\"https://www.otsukare.info/2021/04/20/ua-three-digits-get-ready\">months</a>. This has the potential to cause breakage on sites that rely on identifying the browser version to perform business logic.  This post covers the timeline of events, the strategies that Chrome and Firefox are taking to mitigate the impact, and how you can help.</p>\n<h2>User-Agent string</h2>\n<p><a href=\"https://www.rfc-editor.org/rfc/rfc7231.html#section-5.5.3\">User-Agent (UA)</a> is a string that browsers send in HTTP headers, so servers can identify the browser.  The string is also accessible through JavaScript with <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/userAgent\">navigator.userAgent</a>. It’s usually formatted as follows:</p>\n<p><code>browserName/majorVersion.minorVersion</code></p>\n<p>For example, the latest release versions of browsers at the time of publishing this post are:</p>\n<ul>\n<li aria-level=\"1\"><code>Chrome: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36</code></li>\n<li aria-level=\"1\"><code>Firefox: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0</code></li>\n<li aria-level=\"1\"><code>Safari: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15</code></li>\n</ul>\n<h2>Major version 100—three-digit version number</h2>\n<p>Major version 100 is a big milestone for both Chrome and Firefox. It also has the potential to cause breakage on websites as we move from a two-digit to a <b>three-digit version number</b>.  Web developers use all kinds of techniques for parsing these strings, from custom code to using User-Agent parsing libraries, which can then be used to determine the corresponding processing logic. The User-Agent and any other version reporting mechanisms will soon report a three-digit version number.</p>\n<h3>Version 100 timelines</h3>\n<p>Version 100 browsers will be first released in experimental versions (Chrome Canary, Firefox Nightly), then beta versions, and then finally on the stable channel.</p>\n<table>\n<tbody>\n<tr>\n<td>Chrome (<a href=\"https://chromiumdash.appspot.com/schedule\">Release Schedule</a>)</td>\n<td>March 29, 2022</td>\n</tr>\n<tr>\n<td>Firefox (<a href=\"https://wiki.mozilla.org/Release_Management/Calendar\">Release Schedule</a>)</td>\n<td>May 3, 2022</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"three-digit\">Why can a three-digit version number be problematic?</h2>\n<p>When browsers first reached version 10 a little over 12 years ago, <a href=\"https://maqentaer.com/devopera-static-backup/http/dev.opera.com/articles/view/opera-ua-string-changes/index.html\">many issues were discovered</a> with User-Agent parsing libraries as the major version number went from one digit to two.</p>\n<p>Without a single specification to follow, <a href=\"https://developer.mozilla.org/docs/Web/HTTP/Headers/User-Agent\">different browsers have different formats</a> for the User-Agent string, and site-specific User-Agent parsing. It’s possible that some parsing libraries may have hard-coded assumptions or bugs that don’t take into account three-digit major version numbers.  Many libraries improved the parsing logic when browsers moved to two-digit version numbers, so hitting the three-digit milestone is expected to cause fewer problems. Mike Taylor, an engineer on the Chrome team, has done a survey of common UA parsing libraries which didn&#8217;t uncover any issues. Running Chrome experiments in the field has surfaced some issues, which are being worked on.</p>\n<h2>What are browsers doing about it?</h2>\n<p>Both Firefox and Chrome have been running experiments where current versions of the browser report being at major version 100 in order to detect possible website breakage. This has led to a few <a href=\"https://github.com/webcompat/web-bugs/labels/version100\">reported</a> <a href=\"https://bugs.chromium.org/p/chromium/issues/detail?id=1273958\">issues</a>, some of which have already been fixed. These experiments will continue to run until the release of version 100.</p>\n<p>There are also backup mitigation strategies in place, in case version 100 release to stable channels causes more damage to websites than anticipated.</p>\n<h2 id=\"firefox-mitigation\">Firefox mitigation</h2>\n<p>In Firefox, the strategy will depend on how important the breakage is. Firefox has a <a href=\"https://wiki.mozilla.org/Compatibility/Interventions_Releases\">site interventions mechanism</a>. Mozilla webcompat team can hot fix broken websites in Firefox using this mechanism. If you type <code>about:compat</code> in the Firefox URL bar, you can see what is currently being fixed. If a site breaks with the major version being 100 on a specific domain, it is possible to fix it by sending version 99 instead.</p>\n<p>If the breakage is widespread and individual site interventions become unmanageable, Mozilla can temporarily freeze Firefox&#8217;s major version at 99 and then test other options.</p>\n<h2 id=\"chrome-mitigation\">Chrome mitigation</h2>\n<p>In Chrome, the backup plan is to use a flag to freeze the major version at 99 and report the real major version number in the minor version part of the User-Agent string (the code has already <a href=\"https://chromium-review.googlesource.com/c/chromium/src/+/3341658\">landed</a>).</p>\n<p>The Chrome version as reported in the User-Agent string follows the pattern &lt;major_version&gt;.&lt;minor_version&gt;.&lt;build_number&gt;.&lt;patch_number&gt;.</p>\n<p>If the backup plan is employed, then the User-Agent string would look like this:</p>\n<p><code>99.101.4988.0</code></p>\n<p>Chrome is also running experiments to ensure that reporting a three-digit value in the minor version part of the string does not result in breakage, since the minor version in the Chrome User-Agent string has reported 0 for a very long time. The Chrome team will decide on whether to resort to the backup option based on the number and severity of the issues reported.</p>\n<h2>What can you do to help?</h2>\n<p>Every strategy that adds complexity to the User-Agent string has a strong impact on the ecosystem. Let’s work together to avoid yet another quirky behavior. In Chrome and Firefox Nightly, you can configure the browser to report the version as 100 right now and report any issues you come across.</p>\n<h3 id=\"firefox-config\">Configure Firefox Nightly to report the major version as 100</h3>\n<ol>\n<li aria-level=\"1\">Open Firefox Nightly’s Settings menu.</li>\n<li aria-level=\"1\">Search for “Firefox 100” and then check the “Firefox 100 User-Agent String” option.</li>\n</ol>\n<h3 id=\"chrome-config\">Configure Chrome to report the major version as 100</h3>\n<ol>\n<li aria-level=\"1\">Go to chrome://flags/#force-major-version-to-100</li>\n<li aria-level=\"1\">Set the option to `Enabled`.</li>\n</ol>\n<h3 id=\"test-report\">Test and file reports</h3>\n<ul>\n<li aria-level=\"1\"><b>If you are a website maintainer</b>, test your website with Chrome and Firefox 100. Review your User-Agent parsing code and libraries, and ensure they are able to handle three-digit version numbers. We have compiled some of the <a href=\"https://www.otsukare.info/2022/01/14/broken-ua-detection\">patterns that are currently breaking</a>.</li>\n<li aria-level=\"1\"><b>If you develop a User-Agent parsing library</b>, add tests to parse versions greater than and equal to 100. Our early tests show that recent versions of libraries can handle it correctly. But the Web is a legacy machine, so if you have old versions of parsing libraries, it’s probably time to check and eventually upgrade.</li>\n<li aria-level=\"1\"><b>If you are browsing the web</b> and notice any issues with the major version 100, <a href=\"https://webcompat.com/issues/new?label=version100\">file a report on webcompat.com</a>.</li>\n</ul>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/version-100-in-chrome-and-firefox/\">Version 100 in Chrome and Firefox</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Improving the Storage Access API in Firefox</title>\n\t\t<link>https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/</link>\n\t\t\t\t\t<comments>https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/#comments</comments>\n\t\t\n\t\t<dc:creator><![CDATA[Benjamin VanderSloot]]></dc:creator>\n\t\t<pubDate>Tue, 08 Feb 2022 16:59:21 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[Privacy]]></category>\n\t\t<category><![CDATA[partitioning]]></category>\n\t\t<category><![CDATA[privacy]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47599</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>Before we roll out State Partitioning for all Firefox users, we intend to make a few privacy and ergonomic improvements to the Storage Access API. In this blog post, we’ll detail a few of the new changes we made. </p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/\">Improving the Storage Access API in Firefox</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Before we roll out </span><a class=\"editor-rtfLink\" href=\"https://hacks.mozilla.org/2021/02/introducing-state-partitioning/\" target=\"_blank\" rel=\"noopener\"><span data-preserver-spaces=\"true\">State Partitioning</span></a><span data-preserver-spaces=\"true\"> for all Firefox users, we intend to make a few privacy and ergonomic improvements to the </span><a class=\"editor-rtfLink\" href=\"https://privacycg.github.io/storage-access/\" target=\"_blank\" rel=\"noopener\"><span data-preserver-spaces=\"true\">Storage Access API</span></a><span data-preserver-spaces=\"true\">. In this blog post, we’ll detail a few of the new changes we made.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">With State Partitioning, third parties can’t access the same cookie jar when they’re embedded in different sites. Instead, they get a fresh cookie jar for each site they’re embedded in. This isn&#8217;t just limited to cookies either—all storage is partitioned in this way.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">In an ideal world, this would stop trackers from keeping tabs on you wherever they’re embedded because they can&#8217;t keep a unique identifier for you across all of these sites. Unfortunately, the world isn&#8217;t so simple—trackers aren&#8217;t the only third parties that use storage. If you&#8217;ve ever used an authentication provider that requires an embedded resource, you know how important third-party storage can be.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Enter the Storage Access API. This API lets third parties request storage access as if they were a first party. This is called “unpartitioning” and it gives browsers and users control over which third parties can maintain state across first-party origins as well as determine which origins they can access that state from. This is the preferred way for third parties to keep sharing storage across sites.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">The Storage Access API leaves a lot of room for the browser to decide when to allow a third party unrestricted storage access. This is a feature that gives the browser freedom to make decisions it feels are best for the user and decide when to present choices about storage permissions to users directly. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">On the other hand, this means the Storage Access API can vary from browser to browser and version to version. As a result, the developer experience will suffer unless we do two things: 1) Design with the developer experience in mind; and 2) communicate what we’re doing. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">So let’s dive in! Here are four changes we’re making to the Storage Access API that will improve user privacy and maintain a strong developer experience…</span></p>\n<h2 style=\"text-align: left;\"><span data-preserver-spaces=\"true\">Requiring User Consent for Third-Parties the User Never Interacted With</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">With Storage API, the browser determines whether to involve the user in the decision to grant storage access to a third party. Previously, Firefox didn’t involve users until a third party already had access to its storage on five different sites. At that point, the third party&#8217;s storage access requests were presented to users to make a decision. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We’re allowing third parties some leeway to unpartition their storage on a few sites because we’re worried about overwhelming users with popup permission requests. We feel that allowing only a few permission grants per third party would keep the permission frequency down while still preventing any one party from tracking the user on many sites.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We also wanted to improve user privacy in our Storage Access API implementation by reducing the number of times third parties can automatically unpartition themselves without overwhelming the user with storage access requests. The improvement we settled on was requiring the user to have interacted with the third party recently to give them storage access without explicitly asking the user whether or not to allow it. We believe that removing automatic storage access grants for sites the user has never seen before captures the spirit of State Partitioning without having to bother the user too much more.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Careful readers may now be concerned that any embed-only pages, like some authentication services, will be heavily impacted by this. To tip the scales even further toward low user touch, we expanded the definition of “interacting with a site” to support embed-only contexts. Now, whenever a user grants storage access via permission popups or interacts with an iframe with storage access, these both count as user interactions. This change is the result of a lot of careful balancing between preserving legitimate use cases, protecting user privacy, and not annoying users with endless permission prompts. We think we found the sweet spot.</span></p>\n<h2><span data-preserver-spaces=\"true\">Changing the Scope of First-Party Storage Access to Site</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">While rolling out State Partitioning, we’ve seen the emergence of a fair number of use cases for the Storage Access API. One common use is to enable authentication using a third party.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We found on occasion the login portal that gave first-party storage access to the authentication service was a subdomain, like </span><strong><span data-preserver-spaces=\"true\">https://login.example.com</span></strong><span data-preserver-spaces=\"true\">. This caused problems when the user navigated to </span><strong><span data-preserver-spaces=\"true\">https://example.com</span></strong><span data-preserver-spaces=\"true\"> after logging in… they were no longer logged in! This is because the storage access permission was only granted to the login subdomain and not the rest of the site. The authentication provider had access to its cookies on </span><strong><span data-preserver-spaces=\"true\">https://login.example.com</span></strong><span data-preserver-spaces=\"true\">, but not on </span><strong><span data-preserver-spaces=\"true\">https://example.com</span></strong><span data-preserver-spaces=\"true\">. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We fixed this by moving the storage access permission to the Site-scope. This means that when a third party gets storage access on a page, it has access to unpartitioned storage on all pages on that same Site. So in the example above, the authenticating third party would have access to the user&#8217;s login cookie on </span><strong><span data-preserver-spaces=\"true\">https://login.example.com</span></strong><span data-preserver-spaces=\"true\">, </span><strong><span data-preserver-spaces=\"true\">https://example.com</span></strong><span data-preserver-spaces=\"true\">, and </span><strong><span data-preserver-spaces=\"true\">https://any.different.subdomain.example.com</span></strong><span data-preserver-spaces=\"true\">! Yet they still wouldn&#8217;t have access to that login cookie on </span><strong><span data-preserver-spaces=\"true\">http://example.com</span></strong><span data-preserver-spaces=\"true\"> or </span><strong><span data-preserver-spaces=\"true\">https://different-example.com</span></strong><span data-preserver-spaces=\"true\">.</span></p>\n<h2><span data-preserver-spaces=\"true\">Cleaning Up User Interaction Requirements</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Requiring user interaction when requesting storage access was one rough edge of the Storage Access API definition. Let’s talk about that requirement.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">If a third party calls requestStorageAccess as soon as a page loads, it should not get that storage access. It needs to wait until the user interacts with their iframe. Scrolling or clicking are good ways to get this user interaction and it will expire a few seconds after it’s granted. Unfortunately, there were some corner cases in this requirement that we needed to clean up. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">One corner case concerns what to do with the user’s interaction state when they click Accept or Deny on a permission prompt. We decided that when a user clicks Deny on a storage access permission prompt, the third party should lose their user interaction. This prevents the third party from immediately requesting storage access again, bothering the user until they accept. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Conversely, we decided to reset the timer for user interaction if the user clicks Accept to reflect that the user did interact with the third party. This will allow the third party to use APIs that require both storage access and user interaction with only one user interaction in their iframe.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">Another corner case concerned how strict to be when requiring user interaction for storage access requests. As we’ve iterated on the Storage Access API, minor changes have been introduced. One of the changes has to do with the case of giving a third party storage access on a page, but then the page is reloaded. Does the third party have to get a user interaction before requesting storage access again? Initially, the answer was no, but now it is yes. We updated our implementation to reflect that change and align with other browsers.</span><span data-preserver-spaces=\"true\"> </span></p>\n<h2><span data-preserver-spaces=\"true\">Integrating User Cookie Preferences</span></h2>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">In the settings for Firefox </span><a class=\"editor-rtfLink\" href=\"https://support.mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop\" target=\"_blank\" rel=\"noopener\"><span data-preserver-spaces=\"true\">Enhanced Tracking Protection</span></a><span data-preserver-spaces=\"true\">, users can specify how they want the browser to handle cookies. By default, Firefox blocks cookies from known trackers. But we have a few other possible selections, such as allowing all cookies or blocking all third-party cookies. Users can alter this preference to their liking.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">We have always respected this user choice when implementing the Storage Access API. However, this wasn&#8217;t clear to developers. For example, users that set Firefox to block all third-party cookies will be relieved to know the Storage Access API in no way weakens their protection; even a storage access permission doesn&#8217;t give a third party any access to storage. But this wasn’t clear to the third party&#8217;s developers. </span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\">The returned promise from requestStorageAccess would resolve, indicating that the third party had access to its unpartitioned storage. We endeavored to fix this. In Firefox 98, when the user has disabled third-party cookies via the preferences, the function requestStorageAccess will always return a rejecting promise and hasStorageAccess will always return false.</span></p>\n<p style=\"text-align: justify;\"><span data-preserver-spaces=\"true\"> </span></p>\n<p><span data-preserver-spaces=\"true\"> </span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/\">Improving the Storage Access API in Firefox</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\t\t\t<wfw:commentRss>https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/feed/</wfw:commentRss>\n\t\t\t<slash:comments>1</slash:comments>\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Retrospective and Technical Details on the recent Firefox Outage</title>\n\t\t<link>https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/</link>\n\t\t\t\t\t<comments>https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/#comments</comments>\n\t\t\n\t\t<dc:creator><![CDATA[Christian Holler]]></dc:creator>\n\t\t<pubDate>Wed, 02 Feb 2022 09:00:50 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[Firefox OS]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47574</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>On January 13th 2022, Firefox became unusable for close to two hours for users worldwide. This incident interrupted many people’s workflow. This post highlights the complex series of events and circumstances that, together, triggered a bug deep in the networking code of Firefox. What Happened? Firefox has a number of servers and related infrastructure that [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/\">Retrospective and Technical Details on the recent Firefox Outage</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p>On January 13th 2022, Firefox became unusable for close to two hours for users worldwide. This incident interrupted many people’s workflow. This post highlights the complex series of events and circumstances that, together, triggered a bug deep in the networking code of Firefox.<span id=\"more-47574\"></span></p>\n<h2>What Happened?</h2>\n<p align=\"justify\">Firefox has a number of servers and related infrastructure that handle several internal services. These include updates, telemetry, certificate management, crash reporting and other similar functionality. This infrastructure is hosted by different cloud service providers that use load balancers to distribute the load evenly across servers. For those services hosted on Google Cloud Platform (GCP) these load balancers have settings related to the HTTP protocol they should advertise and one of these settings is HTTP/3 support with three states: “Enabled”, “Disabled” or “Automatic (default)”. Our load balancers were set to the “Automatic (default)” setting and on January 13, 2022 at 07:28 UTC, GCP deployed an unannounced change to make HTTP/3 the default. As Firefox uses HTTP/3 when supported, from that point forward, some connections that Firefox makes to the services infrastructure would use HTTP/3 instead of the previously used HTTP/2 protocol.<a id=\"footnote1\"></a>¹</p>\n<p align=\"justify\">Shortly after, we noticed a spike in crashes being reported through our crash reporter and also received several reports from inside and outside of Mozilla describing a hang of the browser.</p>\n<p><div id=\"attachment_47575\" style=\"width: 510px\" class=\"wp-caption aligncenter\"><a href=\"https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2.png\"><img aria-describedby=\"caption-attachment-47575\" loading=\"lazy\" class=\"wp-image-47575 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-500x231.png\" alt=\"A graph showing the curve of unprocessed crash reports quickly growing.\" width=\"500\" height=\"231\" srcset=\"https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-500x231.png 500w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-250x115.png 250w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-768x355.png 768w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-1536x709.png 1536w, https://hacks.mozilla.org/files/2022/01/crashes-foxstuck2-2048x946.png 2048w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a><p id=\"caption-attachment-47575\" class=\"wp-caption-text\">Backlog of pending crash reports building up and reaching close to 300K unprocessed reports.</p></div></p>\n<p align=\"justify\">As part of the incident response process, we quickly discovered that the client was hanging inside a network request to one of the Firefox internal services. However, at this point we neither had an explanation for why this would trigger just now, nor what the scope of the problem was. We continued to look for the “trigger” — some change that must have occurred to start the problem. We found that we had not shipped updates or configuration changes that could have caused this problem. At the same time, we were keeping in mind that HTTP/3 had been enabled since Firefox 88 and was actively used by some popular websites.</p>\n<p align=\"justify\">Although we couldn’t see it, we suspected that there had been some kind of “invisible” change rolled out by one of our cloud providers that somehow modified load balancer behavior. On closer inspection, none of our settings were changed. We then discovered through logs that for some reason, the load balancers for our Telemetry service were serving HTTP/3 connections while they hadn’t done that before. We disabled HTTP/3 explicitly on GCP at 09:12 UTC. This unblocked our users, but we were not yet certain about the root cause and without knowing that, it was impossible for us to tell if this would affect additional HTTP/3 connections.</p>\n<p align=\"justify\"><small><a href=\"#footnote1\">¹</a> <i>Some highly critical services such as updates use a special <code>beConservative</code> flag that prevents the use of any experimental technology for their connections (e.g. HTTP/3).</i></small></p>\n<h2>A Special Mix of Ingredients</h2>\n<p align=\"justify\">It quickly became clear to us that there must be some combination of special circumstances for the hang to occur. We performed a number of tests with various tools and remote services and were not able to reproduce the problem, not even with a regular connection to the Telemetry staging server (a server only used for testing deployments, which we had left in its original configuration for testing purposes). With Firefox itself, however, we were able to reproduce the issue with the staging server.</p>\n<p align=\"justify\">After further debugging, we found the “special ingredient” required for this bug to happen. All HTTP/3 connections go through Necko, our networking stack. However, Rust components that need direct network access are not using Necko directly, but are calling into it through an intermediate library called <a href=\"https://github.com/mozilla/application-services/tree/main/components/viaduct\"><i><code>viaduct</code></i></a>.</p>\n<p align=\"justify\">In order to understand why this mattered, we first need to understand some things about the internals of Necko, in particular about HTTP/3 upload requests. For such requests, the higher-level Necko APIs<a id=\"footnote2\"></a>² check if the <code>Content-Length</code> header is present and if it isn&#8217;t, it will automatically be added. The lower-level HTTP/3 code later relies on this header to determine the request size. This works fine for web content and other requests in our code.</p>\n<p align=\"justify\">When requests pass through <code>viaduct</code> first, however, <code>viaduct</code> will lower-case each header and pass it on to Necko. And here is the problem: the API checks in Necko are case-<b>insensitive</b> while the lower-level HTTP/3 code is case-<b>sensitive</b>. So if any code was to add a <code>Content-Length</code> header and pass the request through <code>viaduct</code>, it would pass the Necko API checks but the HTTP/3 code would not find the header.</p>\n<p align=\"justify\">It just so happens that Telemetry is currently the only Rust-based component in Firefox Desktop that uses the network stack and adds a <code>Content-Length</code> header. This is why users who disabled Telemetry would see this problem resolved even though the problem is not related to Telemetry functionality itself and could have been triggered otherwise.</p>\n<p><div id=\"attachment_47579\" style=\"width: 510px\" class=\"wp-caption aligncenter\"><a href=\"https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4.png\"><img aria-describedby=\"caption-attachment-47579\" loading=\"lazy\" class=\"wp-image-47579 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-500x266.png\" alt=\"A diagram showing the different network components in Firefox.\" width=\"500\" height=\"266\" srcset=\"https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-500x266.png 500w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-250x133.png 250w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-768x409.png 768w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4-1536x818.png 1536w, https://hacks.mozilla.org/files/2022/01/foxstuck-diagram4.png 1826w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a><p id=\"caption-attachment-47579\" class=\"wp-caption-text\">A specific code path was required to trigger the problem in the HTTP/3 protocol implementation.</p></div></p>\n<p align=\"justify\"><small><a href=\"#footnote2\">²</a> <i>These are internal APIs, not accessible to web content.</i></small></p>\n<h2>The Infinite Loop</h2>\n<p align=\"justify\">With the load balancer change in place, and a special code path in a new Rust service now active, the necessary final ingredient to trigger the problem for users was deep in Necko HTTP/3 code.</p>\n<p align=\"justify\">When handling a request, the code <a href=\"https://searchfox.org/mozilla-central/rev/435a77f1a1aaf1a78d30a2aaa81c6158a2f83dba/netwerk/protocol/http/Http3Stream.cpp#71,79-83\">looked up the field in a case-sensitive way</a> and failed to find the header as it had been lower-cased by <code>viaduct</code>. Without the header, the request was determined by the Necko code to be complete, leaving the real request body unsent. However, this code would only terminate when there was no additional content to send. This <a href=\"https://searchfox.org/mozilla-central/rev/435a77f1a1aaf1a78d30a2aaa81c6158a2f83dba/netwerk/protocol/http/Http3Stream.cpp#223,228,272-274\">unexpected state caused the code to loop indefinitely rather than returning an error</a>. Because all network requests go through one <i>socket thread</i>, this loop blocked any further network communication and made Firefox unresponsive, unable to load web content.</p>\n<h2>Lessons Learned</h2>\n<p align=\"justify\">As so often is the case, the issue was a lot more complex than it appeared at first glance and there were many contributing factors working together. Some of the key factors we have identified include:</p>\n<ul>\n<li aria-level=\"1\">\n<p align=\"justify\">GCP’s deployment of HTTP/3 as default was unannounced. We are actively working with them to improve the situation. We realize that an announcement (as is usually sent) might not have entirely mitigated the risk of an incident, but it would likely have triggered more controlled experiments (e.g. in a staging environment) and deployment.</p>\n</li>\n<li aria-level=\"1\">\n<p align=\"justify\">Our setting of “Automatic (default)” on the load balancers instead of a more explicit choice allowed the deployment to take place automatically. We are reviewing all service configurations to avoid similar mistakes in the future.</p>\n</li>\n<li aria-level=\"1\">\n<p align=\"justify\">The particular combination of HTTP/3 and <code>viaduct</code> on Firefox Desktop was not covered in our continuous integration system. While we cannot test every possible combination of configurations and components, the choice of HTTP version is a fairly major change that should have been tested, as well as the use of an additional networking layer like <code>viaduct</code>. Current HTTP/3 tests cover the low-level protocol behavior and the Necko layer as it is used by web content. We should run more system tests with different HTTP versions and doing so could have revealed this problem.</p>\n</li>\n</ul>\n<p align=\"justify\">We are also investigating action points both to make the browser more resilient towards such problems and to make incident response even faster. Learning as much as possible from this incident will help us improve the quality of our products. We’re grateful to all the users who have sent crash reports, worked with us in Bugzilla or helped others to work around the problem.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/\">Retrospective and Technical Details on the recent Firefox Outage</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\t\t\t<wfw:commentRss>https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/feed/</wfw:commentRss>\n\t\t\t<slash:comments>5</slash:comments>\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Hacks Decoded: Adewale Adetona</title>\n\t\t<link>https://hacks.mozilla.org/2022/01/hacks-decoded-adewale-adetona/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Xavier Harding]]></dc:creator>\n\t\t<pubDate>Mon, 31 Jan 2022 17:44:10 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Apps]]></category>\n\t\t<category><![CDATA[Developer Tools]]></category>\n\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Hacks Decoded]]></category>\n\t\t<category><![CDATA[africa]]></category>\n\t\t<category><![CDATA[apps]]></category>\n\t\t<category><![CDATA[nigeria]]></category>\n\t\t<category><![CDATA[startup]]></category>\n\t\t<category><![CDATA[tech]]></category>\n\t\t<category><![CDATA[technology]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47585</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>Adetona Adewale Akeem, more popularly known as iSlimfit, is a Nigeria-born revered digital technologist and marketing expert. He is the co-founder of Menopays, a fintech startup offering another Buy Now Pay Later (BNPL) option across Africa. We chatted with him about founding Menopays and the impact of tech solutions developed in Nigeria. </p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/01/hacks-decoded-adewale-adetona/\">Hacks Decoded: Adewale Adetona</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><i>Welcome to our Hacks: Decoded Interview series!</i></p>\n<p><i>Once a month, </i><a href=\"https://foundation.mozilla.org/\" target=\"_blank\" rel=\"noopener\"><i>Mozilla Foundation</i></a><i>’s </i><a href=\"https://www.xavierharding.com/\" target=\"_blank\" rel=\"noopener\"><i>Xavier Harding</i></a><i> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </i><a href=\"https://hacks.mozilla.org/\"><i>Hacks</i></a><i> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.</i></p>\n<p><strong>Meet Adetona Adewale Akeem!</strong></p>\n<p><strong><img loading=\"lazy\" class=\"aligncenter\" src=\"https://cdn.vanguardngr.com/wp-content/uploads/2021/07/WA-683x1024.jpeg\" alt=\"Adewale Adetona\" width=\"395\" height=\"593\" /></strong></p>\n<p><span style=\"font-weight: 400;\">Adetona Adewale Akeem, more popularly known as iSlimfit, is a Nigeria-born revered digital technologist and marketing expert. He is the co-founder of <a href=\"https://menopays.com/\">Menopays,</a> a fintech startup offering another Buy Now Pay Later (BNPL) option across Africa. </span></p>\n<p><b>So, I’ve got to ask — where does the name iSlimfit come from?</b></p>\n<p><span style=\"font-weight: 400;\">“Slimfit” is a nickname from my University days. But when I wanted to join social media, Twitter, in particular, I figured out the username Slimfit was already taken. All efforts to reach and plead with the user — who even up until now has never posted anything on the account — to release the username for me proved abortive. Then I came up with another username by adding “i” (which signifies referring to myself) to the front of Slimfit. </span><span style=\"font-weight: 400;\"><br />\n</span><span style=\"font-weight: 400;\"><br />\n</span><b>How did you get started in the tech industry, iSlimfit?</b><b></b></p>\n<p><span style=\"font-weight: 400;\">My journey into tech started as far back as 2014, when I made the switch from working at a Media &amp; Advertising Agency in Lagos Nigeria to working as a Digital Marketing Executive in a Fintech Company called SystemSpecs in Nigeria. Being someone that loved combining data with tech, I have always had a knack for growth marketing. So the opportunity to work in a fintech company in that capacity wasn’t something I could let slide.</span></p>\n<p><b>Where are you based currently? And where are you from originally? How does where you&#8217;re from affect how you move through the tech industry?</b></p>\n<p><span style=\"font-weight: 400;\">I am currently based in Leeds, United Kingdom after recently getting a <a href=\"https://technation.io/visa/\">Tech Nation Global Talent</a> endorsement by the UK government. I am from Ogun State, Nigeria. </span></p>\n<p><span style=\"font-weight: 400;\">There is actually no negative impact from my background or where I am from as regards my work in tech. The Nigerian tech space is huge and the opportunities are enormous. Strategic positioning and working with a goal in mind has helped me in navigating my career in tech so far.</span></p>\n<p><b>What brought about the idea of your new vlog Tech Chat with iSlimfit?</b></p>\n<p><b></b><b></b><span style=\"font-weight: 400;\">My desire to make an impact and contribute to the growth of upcoming tech professionals birthed </span><a href=\"https://www.youtube.com/c/AdetonaAdewaleSlimfit\"><span style=\"font-weight: 400;\">the vlog</span></a><span style=\"font-weight: 400;\">. Also, I wanted to replicate what I do offline with Lagos Digital Summit, in an online manner. The vlog is basically a series of YouTube chat series where I bring various people in tech — growth marketers, UI/UX designers, product managers, startup founders, mobile app developers, etc. — to share their career journey, background, transitioning, their career journey, learnings, and general questions about their day-to-day job so that Tech enthusiasts can learn from their expertise.</span></p>\n<p><b>I have to bring up the fact that in 2021, you were endorsed by Tech Nation as an Exceptional agent in Digital Tech. What’s it feel like to achieve something like that?</b><b><br />\n</b><b><br />\n</b><span style=\"font-weight: 400;\">The Tech Nation endorsement by the UK government is one of my biggest achievements. It made me realize how important my impact on the Nigerian tech industry over the years has been. The endorsement was granted based on my significant contribution to the Nigerian Digital Tech sector, my mentorship &amp; leadership capabilities, and also the potential contribution my talent &amp; expertise would add to the UK digital economy. I am particularly grateful for the opportunity to positively make an impact to the digital economy of the United Kingdom.</span></p>\n<p><b>What&#8217;s something folks may not immediately realize about the tech sector in Nigeria if they’re not from there?</b><b></b></p>\n<p><span style=\"font-weight: 400;\">Easy: the fact that the tech sector in Nigeria is the biggest in Africa, and the impact of tech solutions developed in Nigeria is felt all over Africa. Also, as we can see from a recent </span><a href=\"https://www.linkedin.com/posts/islimfit_startups-activity-6871734350942085120-Jh0V\"><b>report</b></a><span style=\"font-weight: 400;\">, Nigerian startups lead the list of African Startups that received funding in 2021.</span></p>\n<p><b>What digital policy or policies do you think Nigeria (your home country) should pursue in order to accelerate digital development in the country?</b></p>\n<p><span style=\"font-weight: 400;\">The Nigerian government need to come to terms with the fact that digital technology is the bedrock for the development of the Nation. They need to develop policies that will shape the Nation’s digital economy and design a roadmap for grassroots digital Tech empowerment of Nigeria’s agile population. </span></p>\n<p><span style=\"font-weight: 400;\">We also need more people to champion and improve on our quest for digital entrepreneurship development through various platforms.</span></p>\n<p><b>You helped co-found a company called Menopays. What were some of the hurdles when it comes to getting a tech company off the ground over there? What about the opposite? What are the ways those in tech benefit from founding and working in Nigeria?</b></p>\n<p><span style=\"font-weight: 400;\">Some hurdles in starting a tech company is putting together the right team for the job. This cuts across legal, product, marketing, and the tech itself. The idea could be great but without the right team, execution is challenging. </span></p>\n<p><span style=\"font-weight: 400;\">A great benefit is that the continent of Africa is gaining in popularity and the world is watching, so a genuine team founding a business will get the benefits of foreign investments which is great in terms of dollar value.</span></p>\n<p><b>Some take issue with </b><a href=\"https://www.nerdwallet.com/article/loans/personal-loans/buy-now-pay-later-apps\"><b>Buy Now Pay Later apps</b></a><b> and services like Menopays in how they may profit off of buyers who may have less. How is Menopays different? How does the company make money? What measures are in place to make sure you aren’t taking advantage of people?</b></p>\n<p><span style=\"font-weight: 400;\">Menopays is different because our focus goes beyond the profitability of the industry. We tailored a minimum spendable amount with a decent repayment period for the minimum wage in Nigeria. Our vision stands in the middle of every decision we make both business-wise and/or product development-wise. </span></p>\n<p><span style=\"font-weight: 400;\">The measure in place is that decisions are guided by why we started Menopays, which is “to fight poverty”. We don’t charge customers exorbitant interest as it goes against what we are preaching as a brand. So our Vision is imprinted in the heart of all the team members working towards making Menopays a family brand.</span></p>\n<p><b>You’ve </b><a href=\"https://technext.ng/2021/12/03/lagos-digital-summit-has-inspired-the-birth-of-similar-digital-gatherings-across-nigeria-adewale-adetona/\"><b>mentioned</b></a><b> Menopays is fighting poverty in Nigeria and eventually all of Africa, how so?</b><b><br />\n</b><b><br />\n</b><span style=\"font-weight: 400;\">Thinking about one of the incidents that happened to one of our co-founders, Reuben Olawale Odumosu, about eight years back. He lost his best friend because of a substandard malaria medication. His best friend in high school died because his parents couldn’t afford NGN2,500 malaria medication at the time and point of need which led to them going for a cheaper drug that eventually led to his death. Menopays exists to prevent such situations by making basic needs like healthcare, groceries and clothing available to our customers even when they don’t have the money to pay at that moment.</span></p>\n<p><span style=\"font-weight: 400;\">So in light of this, at Menopays, we believe that if some particular things are taken care of, individuals stand a lot more chances of survival. Take for instance, someone earns NGN18,000, spends NGN5,000 on transport, NGN7,000 on food and rent and some other miscellaneous of NGN6,000; with Menopays, we take out the cost of transportation and food (by providing you access to our merchants) and we give them more time to pay over the next three months. Which means each month the customer is positive cash flow of NGN6,000. We turn a negative cash flow into a positive cash flow and savings, thereby fighting poverty.</span></p>\n<p><b>If you didn’t help found Menopays, what would you be doing now instead?</b></p>\n<p><span style=\"font-weight: 400;\">I would probably be working on founding another tech startup doing something for the greater good of the world and helping brands achieve their desired marketing objectives.</span></p>\n<p><b>How can the African tech diaspora help startups similar to Menopays?</b></p>\n<p><span style=\"font-weight: 400;\">One way African tech diaspora can help startups similar to Menopays is by promoting their services, sharing with potential users, and also by investing in it.</span></p>\n<p><b>How did you come up with the idea for Lagos Digital Summit?</b><b></b></p>\n<p><span style=\"font-weight: 400;\">Lagos Digital Summit started in 2017 with just an idea in my small shared apartment back then in Lagos with my friend who is now in Canada. The goal back then was simply to facilitate a platform for the convergence of 50 to 60 digital marketing professionals and business thought leaders for the advancement of SMEs and Digital Media enthusiasts within our network.</span><span style=\"font-weight: 400;\"><br />\n</span><span style=\"font-weight: 400;\"><br />\n</span><span style=\"font-weight: 400;\">Five years down the line, despite being faced with plenty of challenges, it&#8217;s been a big success story. We have had the privilege of empowering over 5,000 businesses and individuals with diverse digital marketing skills. </span></p>\n<p><b>What’s it been like arranging that sort of summit in the midst of a pandemic?</b></p>\n<p><span style=\"font-weight: 400;\">Lagos Digital Summit 2020 has been the only edition that we’ve had to do full virtual because it was in the peak of the COVID-19 pandemic. Every other edition before then had been physical with fully packed attendees of an average of 1,000. For the 2021 edition, it was hybrid because Covid-19 restrictions were relaxed, where we had just 300 people attend physically and every other people watched online.</span></p>\n<p><b>What&#8217;s something you see everywhere in tech that you wish more people would talk about?</b></p>\n<p><span style=\"font-weight: 400;\">I wish more people would talk about the struggle, the disappointments, the challenges and the numerous sacrifices that comes with building a tech startup. A lot of times, the media only portray the success stories, especially when a startup raises funds; the headlines are always very inspiring and rosy. </span></p>\n<p><b>What’s been the most impactful thing you’ve done since working in tech? What’s been the most memorable?</b><b><br />\n</b><b><br />\n</b><span style=\"font-weight: 400;\">That should be founding Lagos Digital Summit; the kind of sponsors, corporate organisations, high-profiled speakers, volunteers and attendees that the Summit has been able to attract has been a memorable and proud feeling.</span></p>\n<p><b>What sort of lasting impact do you want to have on the industry and the world? What keeps you going?</b></p>\n<p><span style=\"font-weight: 400;\">Waking up every day, knowing that a lot of people would have a smile on their faces because I have chosen to impact lives and make the world a better place through relevant tech solutions and platforms is the best feeling for me. The fact that I can read through reports and data and see the number of people using Menopays as a Buy Now Pay Later (BNPL) payment option to ease their lifestyle is a big motivation for me. </span></p>\n<p><b>What’s some advice you’d give to others hoping to enter the tech world or hoping to start up a company?</b></p>\n<p><span style=\"font-weight: 400;\">Venturing into Tech or building a Startup takes a whole lot of concerted effort and determination. Getting the right set of partner(s) would however make the journey easier for you. Just have partners or cofounders with similar vision and complementing skills.</span></p>\n<p>—</p>\n<p><em>You can keep up with Adewale’s work by following him <a href=\"https://twitter.com/iSlimfit\" target=\"_blank\" rel=\"noopener\">here.</a> Stay tuned for more Hacks Decoded Q&amp;A’s!</em></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/01/hacks-decoded-adewale-adetona/\">Hacks Decoded: Adewale Adetona</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Contributing to MDN: Meet the Contributors</title>\n\t\t<link>https://hacks.mozilla.org/2022/01/contributing-to-mdn-meet-the-contributors/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Francesca Minelli]]></dc:creator>\n\t\t<pubDate>Tue, 18 Jan 2022 16:07:59 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Developer Tools]]></category>\n\t\t<category><![CDATA[Docs]]></category>\n\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[MDN]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<category><![CDATA[github]]></category>\n\t\t<category><![CDATA[mdn]]></category>\n\t\t<category><![CDATA[open source]]></category>\n\t\t<category><![CDATA[open web docs]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47555</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>If you’ve ever built anything with web technologies, you’re probably familiar with MDN Web Docs. With about 13,000 pages documenting how to use programming languages such as HTML, CSS and JavaScript, the site has about 8,000 people using it at any given moment. MDN relies on contributors to help maintain its ever-expanding and up to date documentation. We reached out to 4 long-time community contributors to talk about how and why they started contributing, why they kept going, and ask what advice they have for new contributors.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/01/contributing-to-mdn-meet-the-contributors/\">Contributing to MDN: Meet the Contributors</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><span style=\"font-weight: 400;\">If you’ve ever built anything with web technologies, you’re probably familiar with MDN Web Docs. With about 13,000 pages documenting how to use programming languages such as HTML, CSS and JavaScript, the site has about 8,000 people using it at any given moment.</span></p>\n<p><span style=\"font-weight: 400;\">MDN relies on contributors to help maintain its ever-expanding and up to date documentation. Supported by companies such as Open Web Docs, Google, w3c, Microsoft, Samsung and Igalia (to name a few), contributions also come from community members. These contributions take many different forms, from fixing issues to contributing code to helping newcomers and localizing content.</span></p>\n<p><span style=\"font-weight: 400;\">We reached out to 4 long-time community contributors to talk about how and why they started contributing, why they kept going, and ask what advice they have for new contributors.</span></p>\n<h2><b>Meet the contributors</b></h2>\n<p><span style=\"font-weight: 400;\">MDN contributors come from all over the world, have different backgrounds, and contribute in different ways. </span></p>\n<p><span style=\"font-weight: 400;\">Irvin and Julien&#8217;s main area of contribution is localizations. They are part of a diverse team of volunteers that ensure that MDN is translated in seven different languages (Discover </span><a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute/Localize\"><span style=\"font-weight: 400;\">here how translations of MDN content happens</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">Since the end of 2020, the translation of MDN articles happen on the new GitHub based platform.</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47557 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-500x334.jpg\" alt=\"Irvin\" width=\"500\" height=\"334\" srcset=\"https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-500x334.jpg 500w, https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-250x167.jpg 250w, https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c-768x513.jpg 768w, https://hacks.mozilla.org/files/2022/01/40057461685_97ac7c0447_c.jpg 799w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p style=\"text-align: center;\"><b><i>Irvin, @irvinfly, volunteer from Mozilla Taiwan Community</i></b></p>\n<p><i><span style=\"font-weight: 400;\">I had been a front-end engineer for more than a decade. I had been a leisure contributor on MDN for a long time. I check MDN all the time when writing websites, but only made some simple contributions, like fixing typos.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">In early 2020, the MDN team asked us if zh (Chinese) locale would like to join the early stage of the localization system on </span></i><a href=\"https://github.com/mdn/yari)\"><i><span style=\"font-weight: 400;\">Yari</span></i></a><i><span style=\"font-weight: 400;\">, the new Github-based platform. We accepted the invitation and formed the </span></i><a href=\"https://github.com/mdn/translated-content/blob/main/PEERS_GUIDELINES.md#review-teams\"><i><span style=\"font-weight: 400;\">zh-review-tea</span></i></a><i><span style=\"font-weight: 400;\">m. Since then, I have begun to contribute to MDN every week.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">My primary work is collaboration with other zh reviewers to check and review the open </span></i><a href=\"https://github.com/mdn/translated-content/pulls?q=is%3Apr+label%3Al10n-zh+)\"><i><span style=\"font-weight: 400;\">pull requests</span></i></a><i><span style=\"font-weight: 400;\"> on both Traditional Chinese and Simplified Chinese locales. Our goal is to ensure that all the changes to the zh docs are well done, both regarding the file format and translations. </span></i></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47569 size-full\" src=\"https://hacks.mozilla.org/files/2022/01/ensemble.png\" alt=\"Julien\" width=\"354\" height=\"286\" srcset=\"https://hacks.mozilla.org/files/2022/01/ensemble.png 354w, https://hacks.mozilla.org/files/2022/01/ensemble-250x202.png 250w\" sizes=\"(max-width: 354px) 100vw, 354px\" /></p>\n<p style=\"text-align: center;\"><b><i>Sphinx  (Julien) (he / him), @</i></b><a href=\"https://twitter.com/Sphinx_Twitt\"><b><i>Sphinx_Twitt</i></b></a><b><i> </i></b></p>\n<p><i><span style=\"font-weight: 400;\">Most of my contributions revolve around localizing MDN content in French (translating new articles and also maintaining existing pages). Since MDN moved to GitHub, contributing also encompasses reviewing other&#8217;s contributions. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">I started to contribute when, having time as a student, I joined a collaborative translation project led by Framasoft. After a few discussions, I joined a mailing list and IRC. One of the first contribution proposals I saw was about improving the translation of the MDN Glossary in French to help newcomers. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">I started helping and was welcomed by the team and community at that time. One thing led to another, and I started helping to translate other areas of MDN in French.</span></i></p>\n<p><span style=\"font-weight: 400;\">Tanner and Kenrick are also longtime volunteers. Their main areas of activity are contributing code, solving issues in MDN repositories, as well as reviewing and assisting the submissions of other contributors.</span></p>\n<p><span style=\"font-weight: 400;\">In MDN, all users can </span><a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute\"><span style=\"font-weight: 400;\">add issues to the issue tracker, as well as contributing fixes, and reviewing other people fixes</span></a><span style=\"font-weight: 400;\">. </span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47561 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/tanner-casual-med-500x666.jpg\" alt=\"Tanner\" width=\"500\" height=\"666\" srcset=\"https://hacks.mozilla.org/files/2022/01/tanner-casual-med-500x666.jpg 500w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-250x333.jpg 250w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-768x1022.jpg 768w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-1154x1536.jpg 1154w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-1539x2048.jpg 1539w, https://hacks.mozilla.org/files/2022/01/tanner-casual-med-scaled.jpg 1923w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p style=\"text-align: center;\"><b><i>Tanner Dolby, @tannerdolby </i></b></p>\n<p><i><span style=\"font-weight: 400;\"> I contribute to MDN by being active in the issue tracker of MDN repositories. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">I tend to look through the issues and search for one I understand, then I read the conversation in the issue thread for context. If I have any questions or notice that the conversation wasn’t resolved, I comment in the thread to get clarification before moving forward. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">From there, I test my proposed changes locally and then submit a pull request to fix the issue on GitHub. The changes I submit are then reviewed by project maintainers. After the review, I implement recommended changes. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Outside of this, I contribute to MDN by spotting bugs and creating new issues, fixing existing issues, making feature requests for things I’d like to see on the site, assisting in the completion of a feature request, participating in code review and interacting with other contributors on existing issues.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">I started contributing to MDN by creating an issue in the mdn/yari repository. I was referencing documentation and wanted to clarify a bit of information that could be a typo. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">The MDN Web Docs team was welcoming of me resolving the issue, so I opened and reviewed/merged a PR I submitted, which fixed things. The Yari project maintainers explained things in detail, helping me to understand that the content for MDN Web Docs lived in mdn/content and not directly in mdn/yari source. The issue I originally opened was transferred to mdn/content and the corresponding fix was merged. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">My first OSS experience with MDN was really fun. It helped me to branch out and explore other issues/pull requests in MDN repositories to better understand how MDN Web Docs worked, so I could contribute again in the future.</span></i></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47565 size-large\" src=\"https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040-500x500.jpeg\" alt=\"Kenrick\" width=\"500\" height=\"500\" srcset=\"https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040-500x500.jpeg 500w, https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040-250x250.jpeg 250w, https://hacks.mozilla.org/files/2022/01/88f91a2ed263afb8b69b08f8351b8040.jpeg 512w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p style=\"text-align: center;\"><b><i>Kenrick, @kenrick95</i></b></p>\n<p><i><span style=\"font-weight: 400;\">I’ve edited content and contributed codes to MDN repositories: browser-compat-data, interactive-examples, and yari.</span></i></p>\n<p><i><span style=\"font-weight: 400;\">My first contribution to content was a long time ago, when we could directly edit on MDN. I can no longer recall what it was, probably fixing a typo. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">My first code contribution was to the “interactive-examples” repo. I noticed that the editor had some bugs, and I found the GitHub issue. After I read the codes, it seemed to me that the bug could be easily fixed, so I went ahead and sent a pull request</span></i></p>\n<h2><b>Why contribute?</b></h2>\n<p><span style=\"font-weight: 400;\">Contributions are essential to the MDN project. When talking about why they deem contribution to MDN a critical task, contributors underlined different facets, stressing its importance as an open, reliable and easily accessible resource to programmers, web developers and learners. </span></p>\n<p><span style=\"font-weight: 400;\">Contributions to MDN documentation and infrastructure help insure the constant improvement of this resource. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">Contributions to MDN are important because it helps to provide a reliable and accessible </span></i><i><span style=\"font-weight: 400;\">source of information on the Web for developers. MDN Web Docs being open source allows for bugs to quickly be spotted by contributors and for feature requests to be readily prototyped. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Building in the open creates an environment that allows for contributors from all over the world to help make MDN a better resource for everyone and that is incredible. </span></i><span style=\"font-weight: 400;\">(Tanner)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">Contributions to the platform and tools that powers MDN are important to enhance users experience (</span></i><span style=\"font-weight: 400;\">Kenrick)</span></p></blockquote>\n<p><span style=\"font-weight: 400;\">Small and big contributions are all significant and have a real impact. A common misconception about contributing to MDN is that you can only contribute code, but that is not the case! </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">MDN is the primary place for people to check any references on web-dev tech. As small as fixing one typo, any contribution to MDN can always help thousands of programmers and learners. (Irvin)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">Contribution to localization allows learners and developers to access this resource in languages other than English, making it more accessible. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">Especially for those who are struggling with reading English docs, localization can enable them to access the latest and solid knowledge (Irvin)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">Contributing to localization help beginners on the Web finding quality documentation and explanations so that they can build sites, apps and so on without having to know English. MDN is a technical reference, but also a fantastic learning ground to educate newcomers. From basic concepts to complex techniques, language should not be a barrier to build something on the Web. (Julien)</span></i></p></blockquote>\n<h2></h2>\n<h2><b>Contributing is a rewarding experience</b></h2>\n<p><span style=\"font-weight: 400;\">We asked contributors why they find contributing to MDN a rewarding experience. </span><span style=\"font-weight: 400;\">They told us that contribution is a way to help others, but also to learn new things. They spoke about the relationship that volunteers build with other people while contributing, and the possibility to learn from and help others. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">The part of contributing that I enjoy most is providing a fix for something that positively </span></i><i><span style=\"font-weight: 400;\">impacts the experience for users browsing MDN Web Docs. This could be an update to </span></i><i><span style=\"font-weight: 400;\">documentation to help provide developers with accurate docs, or helping to land a new feature on the site that will provide users new or improved functionality. Before I started contributing to MDN, I referenced MDN Web Docs very often and really appreciated the hard work that was put into the site. To this day, I’m motivated to continue help making MDN Web Docs the best resource it can be through open source contributions. (</span></i><span style=\"font-weight: 400;\">Tanner)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">I enjoy finding different points of view on how to achieve the same things. This is natural, since the people I interact comes from different part of the world and we all are influenced by our local cultures (Kenrick)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">The part of contributing I most enjoy is definitely the part when I&#8217;m learning and discovering from what I&#8217;m translating (&#8230;). </span></i><i><span style=\"font-weight: 400;\">My best memory to contribute to MDN is that </span></i><i><span style=\"font-weight: 400;\">I had the great privilege of spending an evening watching a sunset of lava and sea with people related to MDN for whom I have the deepest esteem. (Julien)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">The journey of contribution itself is important. The support of MDN maintainers and the exchange of ideas is essential. Contribution does not happen in a silo but is a collaborative effort between volunteers and the MDN team.</span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">My best memory of contributing to MDN would have to be the journey of creating the </span></i><i><span style=\"font-weight: 400;\">copy-to-clipboard functionality for code snippets on MDN Web Docs. I remember prototyping the feature in mdn/yari locally and then beginning to see it come to life really quickly, which was wonderful to see. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">The code review process for this feature was such a joy and incredibly motivating. Each step of the feature was tested thoroughly and every win was celebrated. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Each morning, I would wake up and eagerly check my email and see if any “Re: [mdn/yari]” labelled emails were there because it meant I could get back to collaborating with the MDN Web Docs team. This contribution really opened my eyes to how incredibly fun and rewarding open source software can be. (Tanner)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">My best memory of contributing to MDN was working on </span></i><a href=\"https://github.com/mdn/yari/pull/172\"><i><span style=\"font-weight: 400;\">https://github.com/mdn/yari/pull/172</span></i></a><i><span style=\"font-weight: 400;\">. The change in itself wasn’t big, but the solution changed several times after lengthy discussion. I’m amazed on how open the maintainers are in accepting different point of views for achieving the end goal (Kenrick</span></i><i><span style=\"font-weight: 400;\">)</span></i></p></blockquote>\n<h2><b>Contributions to be proud of</b></h2>\n<p><span style=\"font-weight: 400;\">All contributions are important, but some hold a special place with each volunteer.</span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">The contribution that I’m most proud of is adding copy-to-clipboard functionality to all code snippets for documentation pages on MDN Web Docs. I use this utility very often while browsing pages on MDN Web Docs and seeing a feature I helped build live on the site for other people to use is an amazing feeling. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">This contribution was something I wanted to see on the site and after discussing the feature with the Yari team, I began prototyping and participating in code review until the feature was merged into the live site. This utility was one of the first “large” feature requests that I contributed to mdn/yari and is something I’m very proud of.</span></i><span style=\"font-weight: 400;\"> (Tanner)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">The contribution I am most proud of is having the HTML, CSS, and JavaScript section complete and up-to-date in French in 2017 after being told this would be impossible :) . More recently, helping rebuilding tools for localizers on the new MDN platform with a tracking dashboard</span></i> <span style=\"font-weight: 400;\">(Julien)</span></p></blockquote>\n<p><span style=\"font-weight: 400;\">Kenrick was most proud of adding a feature that marks the page you are looking at in the sidebar. This change makes a significant difference for visual learners. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">It was a simple change, but I felt that this UX improvement is important because it serves as a guide to the reader to check what are the documents related to the one they are reading. </span></i></p></blockquote>\n<h2></h2>\n<h2><b>Getting started </b></h2>\n<p><span style=\"font-weight: 400;\">There are many ways to contribute to MDN! Our seasoned contributors suggest starting with reporting issues and trying to fix them, follow the issue trackers and getting familiarized with GitHub. Don’t be afraid to ask questions, and to make mistakes, there are people that will help you and review your work.</span></p>\n<blockquote><p><span style=\"font-weight: 400;\"> G</span><i><span style=\"font-weight: 400;\">o at your own pace, don&#8217;t hesitate to ask questions. If you can, try to hack things to fix the issues you encounter on a project. If you are eager to learn things about the Web, check MDN as a way to contribute to open source</span></i><span style=\"font-weight: 400;\"> (Julien)</span></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">Suppose you become aware of a bug in any MDN doc (such as a typo), you are welcome to fix them directly by clicking the &#8220;Edit on Github&#8221; button. The review team will ensure it&#8217;s good, so you don&#8217;t need to worry about making any mistakes. (Irvin)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">From taking the first steps, contributors can then progress to more difficult issues and contributions. </span></p>\n<blockquote><p><i><span style=\"font-weight: 400;\">Don’t be afraid of reading code. Pick up any issue from GitHub, and you can easily start contributing code! (Kenrick)</span></i></p></blockquote>\n<blockquote><p><i><span style=\"font-weight: 400;\">My advice for new contributors or those getting started with open source is to get familiarized with the project that they wish to contribute in and then begin staying up-to-date with the issue tracker. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Start being active in the project by looking through issues and reading through the comments, this is a sure-fire way to learn about the project. If there is something that you aren’t ready to contribute but want to have a conversation about, drop a comment in the issue thread or create a discussion in the repository for a great way to inspire conversation about a topic. </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Lastly, understanding a version control software like Git is recommended for those that are considering starting to contribute to open source software. Be open to help in any way you can when first getting started in open source, I started small with documentation fixes on MDN Web Docs and then gradually worked my way into more complex contributions as I became more familiar with the project. (Tanner)</span></i></p></blockquote>\n<p><span style=\"font-weight: 400;\">If you want to start contributing, please check out these resources:</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://developer.mozilla.org/en-US/docs/MDN/Contribute\"><span style=\"font-weight: 400;\">Contributing to MDN</span></a></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://community.mozilla.org/en/activities/contribute-to-mdn-web-docs/\"><span style=\"font-weight: 400;\">MDN activity</span></a></li>\n</ul>\n<p><span style=\"font-weight: 400;\">If you have any questions, join the</span><a href=\"https://chat.mozilla.org/#/room/#mdn:mozilla.org\"> <span style=\"font-weight: 400;\">matrix chat room</span></a><span style=\"font-weight: 400;\"> for MDN.</span></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2022/01/contributing-to-mdn-meet-the-contributors/\">Contributing to MDN: Meet the Contributors</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author</title>\n\t\t<link>https://hacks.mozilla.org/2021/12/hacks-decoded-sara-soueidan-award-winning-ui-design-engineer-and-author/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Xavier Harding]]></dc:creator>\n\t\t<pubDate>Thu, 30 Dec 2021 15:13:51 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Accessibility]]></category>\n\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[Hacks Decoded]]></category>\n\t\t<category><![CDATA[Mozilla]]></category>\n\t\t<category><![CDATA[code]]></category>\n\t\t<category><![CDATA[coding]]></category>\n\t\t<category><![CDATA[css]]></category>\n\t\t<category><![CDATA[decoded]]></category>\n\t\t<category><![CDATA[design]]></category>\n\t\t<category><![CDATA[engineer]]></category>\n\t\t<category><![CDATA[HTML]]></category>\n\t\t<category><![CDATA[JavaScript]]></category>\n\t\t<category><![CDATA[open source]]></category>\n\t\t<category><![CDATA[web development]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47506</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>Sara Soueidan is an independent Web UI and design engineer, author, speaker, and trainer from Lebanon. Currently, she’s working on a new course, \"Practical Accessibility,\" meant to teach devs and designers ways to make their products accessible. We chatted with Sara about front-end web development, the importance of design and her appreciation of birds.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/12/hacks-decoded-sara-soueidan-award-winning-ui-design-engineer-and-author/\">Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><i>Welcome to our Hacks: Decoded Interview series! </i></p>\n<p><i>Once a month, </i><a href=\"https://foundation.mozilla.org/\"><i>Mozilla Foundation</i></a><i>’s </i><a href=\"https://www.xavierharding.com/\"><i>Xavier Harding</i></a><i> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </i><a href=\"https://hacks.mozilla.org/\"><i>Hacks</i></a><i> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.</i></p>\n<p>&nbsp;</p>\n<p><strong>Meet Sara Soueidan!</strong></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47511 size-large\" src=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-Mozilla-Hacks-Decoded-interview-QA-500x466.jpg\" alt=\"\" width=\"500\" height=\"466\" /></p>\n<div class=\"block docBlock Block_container__39pfw Block_readOnly__aAcdv rootBlock rect-definition-node\" data-block-id=\"5c056e39-ea06-4755-a174-53f031ca6d3e\" data-full-hit=\"false\" data-text=\"false\" data-frame=\"false\" data-positioned=\"false\" data-editing=\"false\" data-indent=\"0\">\n<div class=\"ContextMenuHandler_wrapper__2D0Q8\">\n<div class=\"BlockType_block__Szmra\">\n<p>Sara Soueidan is an independent Web UI and design engineer, author, speaker, and trainer from Lebanon.</p>\n<p>Sara has worked with companies around the world, building web user interfaces, designing systems, and creating digital products that focus on responsive design and accessibility. She’s worked with companies like SuperFriendly, Herman Miller, Khan Academy, and has given workshops within companies like Netflix and Telus that focus on building scalable, resilient design.</p>\n<p>When Sara isn’t offering keynote speeches at conferences (she’s done so a dozen times) she’s writing books like “Codrops CSS Reference” and “Smashing Book 5.” Currently, she’s working on a new course, &#8220;Practical Accessibility,&#8221; meant to teach devs and designers ways to make their products accessible.</p>\n<p>In 2015, Sara was voted Developer of the Year in the net awards, and shortlisted for the Outstanding Contribution of the Year award. She also won an O’Reilly Web Platform Award for “exceptional leadership, creativity, and collaboration in the development of JavaScript, HTML, CSS, and the supporting Web ecosystem.”</p>\n<p>We chatted with Sara about front-end web development, the importance of design and her appreciation of birds.</p>\n</div>\n</div>\n</div>\n<p><b>Where did you get your start? How did you end up working in tech?</b></p>\n<p>I took my first HTML class in eighth grade. I instantly fell in love with it. It just made sense; and it felt like a second language that I found myself speaking fluently. But back then, it was just another class. As I continued my journey through high school, I considered architecture as a major. I never thought I&#8217;d major in anything even remotely related to tech. I always thought I’d choose a career that had nothing to do with computers. In fact, before choosing computer science as a major, I was preparing to study architecture in the Faculty of Arts.</p>\n<p>Then, life happened. A series of events had me choosing CS as a major. And even after I did, I didn’t really think I’d make a career in tech. I spent 18 months after college pondering what I could do for a living with a CS major in Lebanon, but I didn’t find my calling anywhere.</p>\n<p>My love for the web was rekindled when someone suggested I learn web development and try making websites for a living. The appeal of that was two-fold: I&#8217;d get to work remotely from the comfort of my home, and I&#8217;d get to be my own boss, and have full control over my time and the work that I choose.</p>\n<p>After a few weeks of learning modern HTML and CSS, and dipping my feet into JavaScript, I was hooked. I found myself spending more time learning and practicing. <a href=\"https://codepen.io/\">Codepen</a> was new back then, and it was a great place to do quick code exercises and experiments. I also created a one-page Web site — because if you&#8217;re going to work freelance and accept work requests, you gotta have that!</p>\n<p>As I continued learning and experimenting for a few months, I started sharing what I learned as articles on a blog that I started in 2013. A few weeks after I published my first article, I got my first client request to create the UI for a Facebook-like Web application. And over the course of the first year, I got one small client project after another.</p>\n<p>My career really kicked off though in 2014. By then, I was writing more, getting more client work, and writing a CSS reference for Codrops. Conference speaking invitations started flooding in after I delivered my first talk at CSSConf in 2014. I gave my first workshop in LA in 2015. And I have been doing what I do now since.</p>\n<p>I am grateful things didn’t work out the way I wanted them to after high school.</p>\n<p><b>You’ve been programming for a while now, you’ve co-authored a book about the craft, you’ve created guides like the Codrops CSS Reference — what drives you?</b></p>\n<p>A thirst for knowledge and a craving for variety in work. I don’t think I’d be inspired enough to do <i>any</i> kind of work that doesn’t satisfy both. I also need to feel like I’m doing something meaningful, like helping others. And I&#8217;ve been able to fulfill all of these needs in this field. That&#8217;s why I fell in love with it.</p>\n<p>Being independent, I have full control over my time and the type of work I spend it on. While building websites is my main work and source of income, I do spend a large portion of my time switching between writing, editing, giving talks, running workshops (in-house and at events), making courses (this one’s new!) and working on personal projects.</p>\n<p>Everything I do complements one another: I learn, to write, to teach; I code, to write, to speak; I code, to learn, to share. It’s a wonderful circle of creative work! This variety helps keep the spark alive, and helps me rekindle my passion for the web even after frequent burnouts.</p>\n<p>I like that I must keep learning for a living! And that I get to also teach (another passion and — dare I say — talent of mine) as part of my job. I teach through writing, through speaking, through running workshops, and even through direct collaboration with designers and engineers on client projects.</p>\n<p>I always think that even if I end up changing careers, I would still make some time to fiddle with code and make web projects on the side of whatever else I&#8217;d be doing for a living.</p>\n<p><b>When it comes to front-end versus back-end versus full stack, you seem to be #TeamFrontEnd. What is it about front-end web and app development that calls your name (more so than back-end)?</b></p>\n<p>I love working at the intersection of design and engineering! This is the area of the front end typically referred to as “the front of the front end.” It is the perfect sweet spot between design and engineering. It stimulates both parts of my brain, and keeps me inspired and challenged — a combination my brain needs to stay creative.</p>\n<p>I find building interfaces fascinating. I love the fact that the interfaces I build are the bridge between people and the information they access online.</p>\n<p>That comes with great responsibility, of course. Building for people is not easy because people are so diverse and so are the ways they access the Web. And it&#8217;s the interfaces they use that determine whether they can!</p>\n<p>It is <i>our</i> responsibility as front-end developers and designers to ensure that what we create is inclusive of as many people as possible.</p>\n<p>While this may sound intimidating and maybe even scary, I find it inspiring. It is what gives more meaning to what I do, and what pushes me to keep learning and trying to do better. The front of the front end is where I found my sweet spot: a place where I can be challenged and inspired.</p>\n<p>A couple of years ago, I was feeling this so much that <a href=\"https://twitter.com/DNABeast/status/1150326370007842816\">I shared that moment on Twitter</a>. Among the many replies I got, this quote by Douglas Adams stuck with me:</p>\n<p>“<i>We all like to congregate, at boundary conditions. Where land meets water. Where earth meets air. Where body meets mind. Where space meets time.”</i></p>\n<p><b>What do you love about coding? What’s your least favorite part?</b></p>\n<p>My favorite part is the satisfaction of seeing my code “come to life”. The idea that I can write a few lines of code that computers understand, and that so many people can consume and interact with it using various technologies — present and in the future.</p>\n<p>I also appreciate the short feedback loop in modern code environments: you write code or make changes to existing one, and see the results immediately in the browser. It is almost magical. And who doesn’t like a little bit of magic in their lives?</p>\n<p>My least favorite part, however, is that it requires so little movement. There is life in movement! One of my favorite yoga teachers once said: &#8220;Once you stop moving, you start dying.&#8221; And I felt that. Spending so much time in front of a screen is very taxing.</p>\n<p>Regular exercise is crucial for my ability to continue doing what I do. But I still sometimes feel like I need more movement <i>during</i> my work sessions. So I got a standing desk a couple of years ago.</p>\n<p>Switching between standing and sitting gives my body short &#8220;breathers&#8221; throughout the day and allows for better blood flow. A balanced lifestyle is crucial to maintaining a good health when you spend as much time in front of a screen. Try to move, drink lots of water, and go outside more.</p>\n<p><b>You’re based out of Lebanon. What’s something many folks may not realize about the tech scene there?</b></p>\n<p>I know this isn&#8217;t the answer you&#8217;re expecting, but I think what many people don&#8217;t realize about the tech scene here is how challenging it is! In Lebanon, we live in a country that has a massive, serious, and ongoing power crisis.</p>\n<p>This crisis, as you can imagine, affects almost every facet of our lives, including the digital. You need power to do work. And you need an internet connection to do work. We’ve always had problems with internet speed. And with the fuel shortage, full power outages, and reception problems, having a <i>reliable</i> connection is less likely than before.</p>\n<p>But there are some incredibly talented designers and developers still making it work through this all. Living in Lebanon brings daily challenges, but being challenged in life is inevitable.</p>\n<p>I try to look on the bright side of everything. Working on a slow connection has its upsides, you know. You learn to appreciate performance more and strive to make better, faster Web sites. You appreciate tech like Service Worker more, and learn to use it to <a href=\"https://www.sarasoueidan.com/blog/going-offline/\">make content available offline</a>. If anything, living here has made many of us more resilient to change, and more creative with our solutions in the face of crisis.</p>\n<p><b>How do you find (tech) supporting communities in Lebanon, if not where does your community live?</b></p>\n<p>I don’t. But that’s mainly because I live in an area with no active tech community. And I live far from where any tech meetups happen. I also don’t know any front-end focused developers in Lebanon. I’m sure they exist; it’s just that, being the introvert that I am, I don’t happen to know any. So my community is mainly online — on Twitter, and in a couple of not-very-busy Slack channels.</p>\n<p><b>Ok, random question. We’ve gotta know about the birds. You’ve raised at least a dozen. What’s the story there?</b></p>\n<p>It all started back in 2009, I think. A close friend had, for whatever reason, decided that I might enjoy taking care of baby birds. So, he got me a baby <a href=\"https://www.flickr.com/photos/raed_shorrosh/30669614184/\">White-spectacled Bulbul</a> (my favorite bird species currently), with all the bird food I needed to start. He taught me what I needed to know to take care of it. And he told me that, when it grows up, it won’t need to live in a cage because <i>I</i> would be its home. I had no idea back then how much I’d fall in love with that bird.</p>\n<p>I&#8217;ve raised 10+ birds since. Not a single one of them was kept in a cage. I would raise them and train them so that, when they grew up, they would fly out in the morning — making friends, living like they were meant to, and return home before the end of the day.</p>\n<p>They would drink from my tea cup, share my sandwiches, eat out of my plate (mainly rice) and spend most of the day either sitting on my shoulder and head, or napping on my arm. Friends have always told me that I was like a Disney princess with my birds. I&#8217;m not sure about that, but it did sometimes <i>feel</i> that way. x)</p>\n<p>Here’s a photo of my last two baby birds from a couple of years ago. I took them out in a car drive to &#8220;explore the outside world&#8221; for the first time.</p>\n<p>They just sat there chilling on my arm, as they watched the world (cars, mainly) pass by.</p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47515 size-large\" src=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397-500x754.jpg\" alt=\"\" width=\"500\" height=\"754\" srcset=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397-500x754.jpg 500w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397-250x377.jpg 250w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-IMG_7397.jpg 679w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p>Years after my friend got me my first bird, I asked him why he did, and whether he knew about the connection that was going to happen. His answer was short. He said: &#8220;<i>You have the heart of a bird. I knew you&#8217;d love creatures that are like you.</i>&#8221;</p>\n<p><b>Another random question: In an interview, you mentioned mainly working in the morning (6am-10am), and slowing down after lunch. You’re like me! How important is a flexible work day to your workflow? (And how do we convince more people that 9-to-5 work isn’t realistic for everyone? How do we normalize hard work in the morning, meetings and calls in the afternoon?) </b></p>\n<p>I can’t imagine myself working on a 9-to-5 schedule! That’s actually one of the few reasons I never took a full-time job. As I mentioned earlier, flexibility was a key factor in choosing a freelance career.</p>\n<p>I am an early bird. On <a href=\"https://www.sarasoueidan.com/desk/typical-day/\">a typical day</a>, I wake up no later than 5:30 in the morning. So my day starts very early. My brain’s information retention powers are at their highest early in the morning. So I get my best work done during that time. With my brain firing on all cylinders, I make quite a bit of headway with the day’s tasks. What makes this time even more productive is the fact that there are no expectations, nor interruptions: no emails, no client communication, not even any IRL interruptions.</p>\n<p>The earlier you start in the day, and knowing that most people are only really productive for about 4.5 hours a day, I believe it makes a lot of sense to slow down after lunch.</p>\n<p>I realize this is easier said than done, though. Being freelance gives me this flexibility but I realize others may not have that working full time. But with more companies going fully or partially remote now, I think more people will hopefully get to choose when they work during the day.</p>\n<p><b>You’re working on an accessibility course, can you talk a bit about why you decided to develop this course and the importance of creating more accessible web interfaces?</b></p>\n<p>Before COVID-19 hit, I traveled to run workshops at conferences and in-house at companies. The lockdown had us all, well, locked down, so that was put on temporary hold.</p>\n<p>Over the years, I collected some amazing feedback to my accessibility workshop from former attendees. I knew I had useful content that many others would find helpful.</p>\n<p>As many events went online, running the workshop online was the sensible plan B. But the fact that my Internet was unreliable made that a little risky — I wouldn’t want my internet connection to fail in the middle of an online workshop! So that plan was put on hold too.</p>\n<p>On the other hand, working with designers and engineers on client projects made me realize that there was a big accessibility knowledge gap in most companies I’ve worked with. I love to teach teams I work with about accessibility at every chance I get, but there’s only so much you can share in Zoom meetings and Slack channels. In-house workshops were not always an option, and online training was not feasible at the time.</p>\n<p>And last but not least, I noticed that there is quite a bit of misinformation and bad advice circulating the web community around accessibility. You can cover a good amount of information in articles, but I already had a good bunch of content I could start with from the accessibility workshop that I can use as a foundation for a more comprehensive series of teaching materials — sort of like a mini curriculum.</p>\n<p>By developing this course I am scratching my own itch. All the reasons mentioned above had me wishing I had created a course that I could share around, especially with client teams, and then with members of the community. So with the time I have in between client projects and speaking, I started working on it!</p>\n<p>The course is called <a href=\"https://practical-accessibility.today\">Practical Accessibility</a>, and is <b>still under active development</b>, coming in 2022. The content of the course is going to be much more comprehensive than that of the workshop, and it will cover much more ground, and hopefully be a great foundation for anyone wanting to learn how to create more accessible websites.</p>\n<p><b>Of everything you worked on, what’s your favorite?</b></p>\n<p>Out of all the projects I’ve worked on, probably the one that stood out for me is a project for <a href=\"http://hermanmiller.com\">Herman Miller</a> that I collaborated with <a href=\"https://superfriendlydesign.systems\">SuperFriendly</a> on. The project was under NDA, and was discontinued a few weeks after COVID-19 hit and the world realized it was going to change moving forward; so I, unfortunately, don’t have any details to share about the project itself.</p>\n<p>But what made this opportunity so special is that this was the first and only project that I was involved in from the very start— from early kick-off meetings and ideation, through research and user testing, UX and UI design, and development. I learned so much working with an amazing group of SuperFriends. The trip to the Herman Miller showroom in Atlanta, where we ran a workshop with the team at Herman Miller, was the last trip most of us took before the big lockdown.</p>\n<p>Herman Miller is a furniture company. And what many people don’t know about me is how much I <i>love</i> interior design. I even took an interior design course last year! So, on this project, I got to (1) work with an amazing team (who I get to call my friends now <img src=\"https://s.w.org/images/core/emoji/13.1.0/72x72/1f495.png\" alt=\"💕\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" />), (2) on a creative project, (3) for a company specializing in making modern furniture, (4) in the field of interior design! How could I not love that?!</p>\n<p>The cherry on top of the cake was that I got a generous discount which I used to upgrade my office chair and desk to an ergonomic Herman Miller chair and standing desk. So even my body and health were thankful for this opportunity!</p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-47519 size-large\" src=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-500x667.jpg\" alt=\"Sara Soueidan desk - Sara' favorite project was working with SuperFriendly and Herman Miller. &quot;The cherry on top of the cake was that I got a generous discount which I used to upgrade my office chair and desk to an ergonomic Herman Miller chair and standing desk. So even my body and health were thankful for this opportunity!&quot;\" width=\"500\" height=\"667\" srcset=\"https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-500x667.jpg 500w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-250x333.jpg 250w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-768x1024.jpg 768w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-1152x1536.jpg 1152w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-1536x2048.jpg 1536w, https://hacks.mozilla.org/files/2021/12/Sara-Soueidan-desk-2021-scaled.jpg 1920w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p><strong>Final question:</strong> <b>W</b><b>hat would you tell folks learning a programming language or aspiring to be a front end developer, or any sort of developer. What advice would you give them?</b></p>\n<p>Learn the fundamentals — HTML, accessibility, CSS, and just enough vanilla JavaScript to get started. Build upon those skills with tools and frameworks as your work needs.</p>\n<p>Don‘t get intimidated or overwhelmed by what everybody else is doing. Learn what you need when you need it. And practice as much as you can. Practice won’t make you perfect because there is no Perfect in this field, but it will make you better!</p>\n<p>This probably should have been the first piece of advice though: <b>Put the user first.</b> User experience should trump developer convenience. Once you let that guide your work, you’re already halfway through to being a better developer than many others.</p>\n<p>Oh and last but certainly not least: Create a personal website! Own your content. And share your work with the world!</p>\n<p>&#8212;</p>\n<p><em>You can keep up with Sara&#8217;s work by following her blog on her personal site <a href=\"https://www.sarasoueidan.com/blog/\">here.</a> Stay tuned for more Hacks Decoded Q&amp;A&#8217;s!</em></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/12/hacks-decoded-sara-soueidan-award-winning-ui-design-engineer-and-author/\">Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>WebAssembly and Back Again: Fine-Grained Sandboxing in Firefox 95</title>\n\t\t<link>https://hacks.mozilla.org/2021/12/webassembly-and-back-again-fine-grained-sandboxing-in-firefox-95/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Bobby Holley]]></dc:creator>\n\t\t<pubDate>Mon, 06 Dec 2021 13:05:44 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[JavaScript]]></category>\n\t\t<category><![CDATA[firefox]]></category>\n\t\t<category><![CDATA[rlbox]]></category>\n\t\t<category><![CDATA[wasm]]></category>\n\t\t<category><![CDATA[WebAssembly]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47492</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>In Firefox 95, we're shipping a novel sandboxing technology called RLBox — developed in collaboration with researchers at the University of California San Diego and the University of Texas — that makes it easy and efficient to isolate subcomponents to make the browser more secure. This technology opens up new opportunities beyond what's been possible with traditional process-based sandboxing, and we look forward to expanding its usage and (hopefully) seeing it adopted in other browsers and software projects.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/12/webassembly-and-back-again-fine-grained-sandboxing-in-firefox-95/\">WebAssembly and Back Again: Fine-Grained Sandboxing in Firefox 95</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><span style=\"font-weight: 400;\">In Firefox 95, we&#8217;re shipping a novel sandboxing technology called </span><a href=\"https://plsyssec.github.io/rlbox_sandboxing_api/sphinx/\"><span style=\"font-weight: 400;\">RLBox</span></a><span style=\"font-weight: 400;\"> — developed in collaboration with researchers at the University of California San Diego and the University of Texas — that makes it easy and efficient to isolate subcomponents to make the browser more secure. </span><span style=\"font-weight: 400;\">This technology opens up new opportunities beyond what&#8217;s been possible with traditional process-based sandboxing, and we look forward to expanding its usage and (hopefully) seeing it adopted in other browsers and software projects.</span></p>\n<p><span style=\"font-weight: 400;\">This technique, which uses WebAssembly to isolate potentially-buggy code, builds on the </span><a href=\"https://hacks.mozilla.org/2020/02/securing-firefox-with-webassembly/\"><span style=\"font-weight: 400;\">prototype</span></a><span style=\"font-weight: 400;\"> we shipped last year to Mac and Linux users. Now, we’re bringing that technology to all supported Firefox platforms (desktop and mobile), and isolating five different modules: </span><a href=\"https://scripts.sil.org/cms/scripts/page.php?site_id=projects&amp;item_id=graphite_home\"><span style=\"font-weight: 400;\">Graphite</span></a><span style=\"font-weight: 400;\">, </span><a href=\"http://hunspell.github.io/\"><span style=\"font-weight: 400;\">Hunspell</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://xiph.org/ogg/\"><span style=\"font-weight: 400;\">Ogg</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://libexpat.github.io/\"><span style=\"font-weight: 400;\">Expat</span></a><span style=\"font-weight: 400;\"> and </span><a href=\"https://github.com/google/woff2\"><span style=\"font-weight: 400;\">Woff2</span></a> [1]<span style=\"font-weight: 400;\">. </span></p>\n<p><span style=\"font-weight: 400;\">Going forward, we can treat these modules as untrusted code, and — assuming we did it right — even a zero-day vulnerability in any of them should pose no threat to Firefox. Accordingly, we’ve updated our </span><a href=\"https://www.mozilla.org/en-US/security/client-bug-bounty/#exploit-mitigation-bounty\"><span style=\"font-weight: 400;\">bug bounty program</span></a><span style=\"font-weight: 400;\"> to pay researchers for bypassing the sandbox even without a vulnerability in the isolated library.</span></p>\n<h2><strong>The Limits of Process Sandboxing</strong></h2>\n<p><span style=\"font-weight: 400;\">All major browsers run Web content in its own sandboxed process, in theory preventing it from exploiting a browser vulnerability to compromise your computer. On desktop operating systems, Firefox also isolates each site in its own process in order to protect sites from each other. </span></p>\n<p><span style=\"font-weight: 400;\">Unfortunately, threat actors routinely attack users by chaining together two vulnerabilities — one to compromise the sandboxed process containing the malicious site, and another to escape the sandbox [2]</span><span style=\"font-weight: 400;\">. To keep our users secure against the most well-funded adversaries, we need multiple layers of protection.</span></p>\n<p><span style=\"font-weight: 400;\">Having already isolated things along trust boundaries, the next logical step is to isolate across functional boundaries. Historically, this has meant hoisting a subcomponent into its own process. For example, Firefox runs audio and video codecs in a dedicated, locked-down process with a limited interface to the rest of the system. However, there are some serious limitations to this approach. </span><span style=\"font-weight: 400;\">First, it requires decoupling the code and making it asynchronous, which is usually time-consuming and may impose a performance cost. Second, processes have a fixed memory overhead, and adding more of them increases the memory footprint of the application. </span></p>\n<p><span style=\"font-weight: 400;\">For all of these reasons, nobody would seriously consider hoisting something like the XML parser into its own process. To isolate at that level of granularity, we need a different approach.</span></p>\n<h2><strong>Isolating with RLBox</strong></h2>\n<p><span style=\"font-weight: 400;\">This is where RLBox comes in. Rather than hoisting the code into a separate process, we instead compile it into WebAssembly and then compile that WebAssembly into native code. This doesn’t result in us shipping any .wasm files in Firefox, since the WebAssembly step is only an intermediate representation in our build process. </span></p>\n<p><span style=\"font-weight: 400;\">However, the transformation places two key restrictions on the target code: it can’t jump to unexpected parts of the rest of the program, and it can’t access memory outside of a specified region. Together, these restrictions </span><a href=\"http://www.cse.psu.edu/~gxt29/papers/sfi-final.pdf\"><span style=\"font-weight: 400;\">make it safe to share an address space</span></a><span style=\"font-weight: 400;\"> (</span><a href=\"https://arxiv.org/abs/2105.00033\"><span style=\"font-weight: 400;\">including the stack</span></a><span style=\"font-weight: 400;\">) between trusted and untrusted code, allowing us to run them in the same process largely as we were doing before. </span><span style=\"font-weight: 400;\">This, in turn, makes it easy to apply without major refactoring: the programmer only needs to sanitize any values that come from the sandbox (since they could be maliciously-crafted), a task which RLBox makes easy with a <a href=\"https://hacks.mozilla.org/2020/02/securing-firefox-with-webassembly/\">tainting layer</a></span><span style=\"font-weight: 400;\">.</span></p>\n<p><span style=\"font-weight: 400;\">The first step in this transformation is straightforward: we use </span><a href=\"https://clang.llvm.org/\"><span style=\"font-weight: 400;\">Clang</span></a><span style=\"font-weight: 400;\"> to compile Firefox, and Clang knows how to emit WebAssembly, so we simply need to switch the output format for the given module from native code to wasm. For the second step, our prototype implementation used </span><a href=\"https://github.com/bytecodealliance/wasmtime/tree/main/cranelift\"><span style=\"font-weight: 400;\">Cranelift</span></a><span style=\"font-weight: 400;\">. Cranelift is excellent, but a second native code generator added complexity — and we realized that it would be simpler to just map the WebAssembly back into something that our existing build system could ingest. </span></p>\n<p><span style=\"font-weight: 400;\">We accomplished this with </span><a href=\"https://github.com/WebAssembly/wabt/tree/main/wasm2c\"><span style=\"font-weight: 400;\">wasm2c</span></a><span style=\"font-weight: 400;\">, which performs a straightforward translation of WebAssembly into equivalent C code, which we can then feed back into Clang along with the rest of the Firefox source code. This approach is very simple, and automatically enables a number of important features that we support for regular Firefox code: profile-guided optimization, inlining across sandbox boundaries, crash reporting, debugger support, source-code indexing, and likely other things that we have yet to appreciate.</span></p>\n<h2><strong>Next Steps</strong></h2>\n<p><span style=\"font-weight: 400;\">RLBox is a big win for us on several fronts: it protects our users from accidental defects as well as supply-chain attacks, and it reduces the need for us to scramble when such issues are disclosed upstream. </span><span style=\"font-weight: 400;\">As such, we intend to continue applying to more components going forward. Some components are not a good fit for this approach — either because they depend too much on sharing memory with the rest of the program, or because they’re too performance-sensitive to accept the modest overhead incurred — but we’ve identified a number of other good candidates. </span></p>\n<p><span style=\"font-weight: 400;\">Moreover, we hope to see this technology make its way into other browsers and software projects to make the ecosystem safer. </span><a href=\"https://github.com/PLSysSec/rlbox_sandboxing_api\"><span style=\"font-weight: 400;\">RLBox</span></a><span style=\"font-weight: 400;\"> is a standalone project that’s designed to be very modular and easy-to-use, and the team behind it would welcome other use-cases.</span></p>\n<p><span style=\"font-weight: 400;\">Speaking of the team: I’d like to thank </span><a href=\"https://shravanrn.com/\"><span style=\"font-weight: 400;\">Shravan Narayan</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://cseweb.ucsd.edu/~dstefan/\"><span style=\"font-weight: 400;\">Deian Stefan</span></a><span style=\"font-weight: 400;\">, and </span><a href=\"https://www.cs.utexas.edu/~hovav/\"><span style=\"font-weight: 400;\">Hovav Shacham</span></a><span style=\"font-weight: 400;\"> for their tireless work in bringing this work from research concept to production. Shipping to hundreds of millions of users is hard, and </span><span style=\"font-weight: 400;\">they did some seriously impressive work.</span></p>\n<p>Read more about RLBox and this announcement on the <a class=\"c-link\" tabindex=\"-1\" href=\"https://jacobsschool.ucsd.edu/news/release/3374\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://jacobsschool.ucsd.edu/news/release/3374\" data-sk=\"tooltip_parent\" data-remove-tab-index=\"true\">UC San Diego Jacobs School of Engineering website</a>.</p>\n<hr />\n<p>[1] Cross-platform sandboxing for Graphite, Hunspell, and Ogg is shipping in Firefox 95, while Expat and Woff2 will ship in Firefox 96.</p>\n<p>[2] By using a syscall to to exploit a vulnerability in the OS, or by using an IPC message to exploit a vulnerability in a process hosting more-privileged parts of the browser.</p>\n<hr />\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/12/webassembly-and-back-again-fine-grained-sandboxing-in-firefox-95/\">WebAssembly and Back Again: Fine-Grained Sandboxing in Firefox 95</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t\t<item>\n\t\t<title>Hacks Decoded: Seyi Akiwowo, Founder of Glitch</title>\n\t\t<link>https://hacks.mozilla.org/2021/11/hacks-decoded-seyi-akiwowo-founder-of-glitch/</link>\n\t\t\n\t\t<dc:creator><![CDATA[Xavier Harding]]></dc:creator>\n\t\t<pubDate>Tue, 30 Nov 2021 16:02:27 +0000</pubDate>\n\t\t\t\t<category><![CDATA[Featured Article]]></category>\n\t\t<category><![CDATA[Firefox]]></category>\n\t\t<category><![CDATA[Hacks Decoded]]></category>\n\t\t<category><![CDATA[Mozilla]]></category>\n\t\t<category><![CDATA[charity]]></category>\n\t\t<category><![CDATA[cyber]]></category>\n\t\t<category><![CDATA[glitch]]></category>\n\t\t<category><![CDATA[internet]]></category>\n\t\t<category><![CDATA[online]]></category>\n\t\t<category><![CDATA[privacy]]></category>\n\t\t<category><![CDATA[trolls]]></category>\n\t\t<guid isPermaLink=\"false\">https://hacks.mozilla.org/?p=47474</guid>\n\n\t\t\t\t\t<description><![CDATA[<p>Seyi Akiwowo’s reputation precedes her. Akiwowo is the founder of Glitch, an organization that seeks to end online abuse. We spoke with Seyi over video chat to learn about what drives her, why she does what she does and what she’d be doing if not battling trolls online for a living. </p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/11/hacks-decoded-seyi-akiwowo-founder-of-glitch/\">Hacks Decoded: Seyi Akiwowo, Founder of Glitch</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></description>\n\t\t\t\t\t\t\t\t\t\t<content:encoded><![CDATA[<p><i><span style=\"font-weight: 400;\">Welcome to our Hacks: Decoded Interview series! </span></i></p>\n<p><i><span style=\"font-weight: 400;\">Once a month, </span></i><a href=\"https://foundation.mozilla.org/\"><i><span style=\"font-weight: 400;\">Mozilla Foundation</span></i></a><i><span style=\"font-weight: 400;\">’s </span></i><a href=\"https://www.xavierharding.com/\"><i><span style=\"font-weight: 400;\">Xavier Harding</span></i></a><i><span style=\"font-weight: 400;\"> speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s </span></i><a href=\"https://hacks.mozilla.org/\"><i><span style=\"font-weight: 400;\">Hacks</span></i></a><i><span style=\"font-weight: 400;\"> blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.</span></i></p>\n<p><b>Meet Seyi Akiwowo (<em>pronounced Shay-ee Aki-wo-wo</em>)</b></p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-47480 size-large\" src=\"https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-500x750.jpg\" alt=\"seyi akiwowo\" width=\"500\" height=\"750\" srcset=\"https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-500x750.jpg 500w, https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-250x375.jpg 250w, https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-768x1152.jpg 768w, https://hacks.mozilla.org/files/2021/11/Head-Shot-Seyi-Dec-2020-1024x1536.jpg 1024w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></p>\n<p><span style=\"font-weight: 400;\">Seyi Akiwowo’s reputation precedes her. Akiwowo is the founder of </span><a href=\"https://glitchcharity.co.uk/\"><span style=\"font-weight: 400;\">Glitch</span></a><span style=\"font-weight: 400;\">, an organization that seeks to end online abuse. Akiwowo is a graduate of the London School of Economics. She’s delivered talks at TED, European Parliament, the U.N. and more and was elected a councillor for the Labor Party in East London — the youngest Black woman ever to do so. </span><i></i></p>\n<p><span style=\"font-weight: 400;\">We spoke with Seyi over video chat to learn about what drives her, why she does what she does and what she’d be doing if not battling trolls online for a living. All that in this month’s Hacks: Decoded.</span></p>\n<p><b>Where do we even begin? What do you consider to be the starting point of your story?</b><i></i></p>\n<p><span style=\"font-weight: 400;\">I’d start with my love for the internet. When you&#8217;re in this anti-troll, human rights space, you’re pitched as anti-tech and anti-change and it&#8217;s simply not true. It’s because I’m such a lover and fan of the internet, I do what I do. I’m part of the ’90s, Microsoft-PC-with-the-big-back-and-a-CD-drive generation. </span></p>\n<p><span style=\"font-weight: 400;\">I had dial-up internet and spent huge amounts of time on MSN and MySpace. There were times I was home alone, my mum out trying to make ends meet, my dad not around, and the computer was my outlet. I adored all of it. The worldwide web was my friend. It was my connection to the rest of the world, all while in a small council flat in East London. </span></p>\n<p><span style=\"font-weight: 400;\">It was when I appeared in a </span><a href=\"https://www.ted.com/talks/seyi_akiwowo_how_to_fix_the_glitch_in_our_online_communities\"><span style=\"font-weight: 400;\">video</span></a><span style=\"font-weight: 400;\"> that went viral that I learned what it meant to be a black woman online. I realized, “Oh, there are people that don’t know me and do not like me and are telling me that they don&#8217;t like me in very violent ways.” So I’d say my journey starts with the love of the internet and innovation. When you&#8217;re in this kind of anti-troll, human rights space, you&#8217;re kind of pitched as anti-tech and anti-change and it&#8217;s not [true], it&#8217;s because I&#8217;m such a lover of the internet is the reason I do what I do.</span></p>\n<p><b>How does where you’re from influence what you do now?</b></p>\n<p><span style=\"font-weight: 400;\">I went to a very good university but the mix of social classes felt almost like a class war! Going to university was the first time I ever felt othered. I didn’t feel it growing up. I grew up in Newham in East London and I didn&#8217;t really realize I was from a poor area. There’s such beauty and safety in that, but there’s also a glass ceiling without you really realizing it.</span></p>\n<p><span style=\"font-weight: 400;\">I grew up with an entrepreneurial spirit and we just made do with what we had and we just were still excellent with the minimum we had. I see how that translates to nowadays. I can really make £1 of funding go far at Glitch! My working-class bargain hunting roots really helped me be frugal with money.</span></p>\n<p><b>It’s interesting how when you throw a Black woman into the mix, people don’t know how to act. Off the internet but specifically on the internet too.</b></p>\n<p><span style=\"font-weight: 400;\">It is, something I’ve been thinking about a lot is that the internet doesn’t need to be this bad if we just listen to Black women many many years ago. If we think about some of the Black activists or campaigners or even the Black women that were just minding their business but got forced into this issue because of their lived experience, like myself. Folks at Facebook, Twitter, Google who weren’t listening — you have </span><a href=\"https://twitter.com/Blackamazon\"><span style=\"font-weight: 400;\">Sydette</span></a><span style=\"font-weight: 400;\"> and Michelle Ferrier. Michelle is a journalist who, after a hate campaign, started </span><a href=\"http://www.troll-busters.com/\"><span style=\"font-weight: 400;\">Troll Busters</span></a><span style=\"font-weight: 400;\"> to help other journalists and women.</span></p>\n<p><span style=\"font-weight: 400;\"> You’ve got </span><a href=\"https://twitter.com/AngryBlackLady\"><span style=\"font-weight: 400;\">Angry Black Lady</span></a><span style=\"font-weight: 400;\"> on Twitter who I remember learning a lot about her experience on social media and she just wasn&#8217;t listened to. I think white men not only have the privilege to make things and get a lot of venture capital and raise a lot of money to make these huge products and break and fail. That’s one. But they&#8217;re also privileged in their echo chamber bubble that they didn&#8217;t have to listen to Black women. And it was only until it started affecting white middle-class upper-class Hollywood that we started paying closer attention to this issue.</span></p>\n<p><b>You’re saying something a lot of us know. Black women encounter harms on these platforms that a white guy may not necessarily encounter. And yet, most of the leadership at these companies are not black women, they’re mostly white men — a group of folks who may not realize how bad these problems are. Why do you think that is? When will it change?</b></p>\n<p><span style=\"font-weight: 400;\">I really don&#8217;t know and I think it&#8217;s a conundrum that isn’t unique to the tech space. You see it everywhere. You see it in conversations in policy discussions about domestic abuse or refugees and you do not have the community that faces it the most, in those conversations. </span></p>\n<p><span style=\"font-weight: 400;\">That&#8217;s the whole reason I went into politics because decisions were being made about my community that’s predominantly people of colour, we have such a high transient population, one of the most diverse boroughs in the world, and yet the council did not look like its community. It&#8217;s a phenomenon that&#8217;s existed in so many places and it&#8217;s even worse in tech because you&#8217;re seeing such the direct harm it&#8217;s having tenfold. </span></p>\n<p><span style=\"font-weight: 400;\">But it&#8217;s everywhere. The erasure and the lack of dignity and respect Black folks and people of colour are given. Issues have to become mainstream enough for people to act on that. Black Lives Matter had to become mainstream enough for people to finally listen. It&#8217;s a lesson for all of us. How do we make sure there is someone in the room who is more, in relative terms, a more minoritized community than you. </span></p>\n<p><span style=\"font-weight: 400;\">How do we make sure that we’re allies offline and online? How are we making sure we’re building community and sharing that legacy and our knowledge and our playbooks and our capital? So that more people from minoritized communities come together.</span></p>\n<p><b>What’s been the most challenging thing about founding and running Glitch?</b></p>\n<p><span style=\"font-weight: 400;\">When you’re a Black founder CEO in a predominantly white charity sector and tech sector, things are just different. I’ve taken meetings where it’s supposed to be a prospective funding meeting about our work and before we can even give the pitch the first thing the prospective funder says is, “If Black lives really matter—” </span><i></i></p>\n<p><b><em>raises eyebrows, confusedly</em></b></p>\n<p><span style=\"font-weight: 400;\">Exactly. So he says, “If Black lives really matter, why are you all not getting the vaccine?” It feels like someone is putting you in an ice bucket or flushing your head down a toilet, comments like that, microaggressions too, are a jarring reminder that you don&#8217;t belong here. It’s like you’re finally at the table, and someone has banged your head against that very table as if to say “you’re stupid for thinking you can be here.” </span></p>\n<p><span style=\"font-weight: 400;\">I think those are moments that really bungee-cord pull you back to reality, That’s what’s really tough. Really, really, really, tough. And I think I got lost in negative thoughts this summer where I thought, “I don’t belong, I don’t know what I’m doing. This privilege of being CEO — how do I use it?” and overall just a massive loss of confidence. And everything. And I think that&#8217;s been really tough.</span></p>\n<p><b>Wow. I’m still stuck on this “If Black Lives really matter—” guy. </b></p>\n<p><b>Seyi, what did you want to be when you grew up? Trolls attacked you online because you made a viral video online so you rose to the occasion and fought back. There are folks out there who don’t grapple with trolls, out there living their truth without a care in the world. What did that look like for you? What did you want to be when you grew up?</b></p>\n<p><span style=\"font-weight: 400;\">I wanted to be a dancer. I wanted to be the next Ciara. I wanted to be in Missy Elliot’s videos. I was like, ‘Move over, sis!’</span><i></i></p>\n<p><b>What’s your favorite Ciara song? </b></p>\n<p><span style=\"font-weight: 400;\">Goodies.</span></p>\n<p><b>Classic. </b></p>\n<p><b></b><b>Back to trolls, what’s a topic regarding online abuse that you wish you saw people talk about more?</b></p>\n<p><span style=\"font-weight: 400;\">The topic of social media whistleblowers and how we should be less worried about making Facebook and Twitter look bad and more about holding them accountable. More specifically, what does this do in the way of changing the system? We shouldn’t have to keep relying on brave individuals who generally tend to be women and women of colour who really put themselves out on the line.</span></p>\n<p><span style=\"font-weight: 400;\"> I don&#8217;t want this to be the trend where we get small bits of reform. We need to have the media holding companies to account, not looking to make Mark Zuckerberg the bad guy because then the narrative becomes ‘when he leaves everything will be sorted out&#8217; and that is not the case.</span><i></i></p>\n<p><b>Seyi, you have the longest resume I’ve ever seen in my life. What motivates you to keep going?</b></p>\n<p><span style=\"font-weight: 400;\">I’m not, I don’t think I want to “keep going” anymore. I grew my organization by 50% in terms of income and more in terms of staff and diversified our income streams before we hit the two-year mark — during a pandemic! I’m ready to rest, I’m ready to sleep more, I’m ready to do work that is still great with minimum viable effort. That’s the sweet spot I’m looking for. </span></p>\n<p>&#8212;</p>\n<p><i>You can keep up with Seyi and Glitch&#8217;s work right </i><a href=\"https://glitchcharity.co.uk/\" target=\"_blank\" rel=\"noopener\"><i>here</i></a><i> and support their special Christmas fundraiser for a safe internet <a href=\"https://www.justgiving.com/campaign/GlitchChristmas\">here</a>.</i></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hacks.mozilla.org/2021/11/hacks-decoded-seyi-akiwowo-founder-of-glitch/\">Hacks Decoded: Seyi Akiwowo, Founder of Glitch</a> appeared first on <a rel=\"nofollow\" href=\"https://hacks.mozilla.org\">Mozilla Hacks - the Web developer blog</a>.</p>\n]]></content:encoded>\n\t\t\t\t\t\n\t\t\n\t\t\n\t\t\t</item>\n\t</channel>\n</rss>\n"
  },
  "description": "hacks.mozilla.org",
  "home_page_url": "https://hacks.mozilla.org",
  "_ext": {
    "date_published": "2022-06-14T15:05:06.000Z",
    "date_modified": "2022-06-14T15:49:04.000Z"
  }
}