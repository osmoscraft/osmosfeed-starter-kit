<?xml version="1.0" encoding="utf-8"?>
   <feed xmlns="http://www.w3.org/2005/Atom">
 
 
   <title>osmosfeed starter</title>
   <link href="http://localhost"/>
   <updated>2022-06-27T04:19:08.448Z</updated>
   <id>http://localhost</id>
   <generator uri="https://github.com/osmoscraft/osmosfeed" version="0.0.10">osmosfeed</generator>
 
     <entry>
       <title>Mobile-First CSS: Is It Time for a Rethink?</title>
         <link href="https://alistapart.com/article/mobile-first-css-is-it-time-for-a-rethink/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">The mobile-first design methodology is great—it focuses on what really matters to the user, it’s well-practiced, and it’s been a common design pattern for years. So developing your CSS mobile-first should also be great, too…right? 
 
 
 
 Well, not necessarily. Classic mobile-first CSS development is based on the principle of overwriting style declarations: you begin your CSS with default style declarations, and overwrite and/or add new styles as you add breakpoints with min-width media queries for larger viewports (for a good overview see “What is Mobile First CSS and Why Does It Rock?”). But all those exceptions create complexity and inefficiency, which in turn can lead to an increased testing effort and a code base that’s harder to maintain. Admit it—how many of us willingly want that?
 
 
 
 On your own projects, mobile-first CSS may yet be the best tool for the job, but first you need to evaluate just how appropriate it is in light of the visual design and user interactions you’re working on. To help you get started, here’s how I go about tackling the factors you need to watch for, and I’ll discuss some alternate solutions if mobile-first doesn’t seem to suit your project.
 
 
 
 Advantages of mobile-first
 
 
 
 Some of the things to like with mobile-first CSS development—and why it’s been the de facto development methodology for so long—make a lot of sense:
 
 
 
 Development hierarchy. One thing you undoubtedly get from mobile-first is a nice development hierarchy—you just focus on the mobile view and get developing. 
 
 
 
 Tried and tested. It’s a tried and tested methodology that’s worked for years for a reason: it solves a problem really well.
 
 
 
 Prioritizes the mobile view. The mobile view is the simplest and arguably the most important, as it encompasses all the key user journeys, and often accounts for a higher proportion of user visits (depending on the project). 
 
 
 
 Prevents desktop-centric development. As development is done using desktop computers, it can be tempting to initially focus on the desktop view. But thinking about mobile from the start prevents us from getting stuck later on; no one wants to spend their time retrofitting a desktop-centric site to work on mobile devices!
 
 
 
 Disadvantages of mobile-first
 
 
 
 Setting style declarations and then overwriting them at higher breakpoints can lead to undesirable ramifications:
 
 
 
 More complexity. The farther up the breakpoint hierarchy you go, the more unnecessary code you inherit from lower breakpoints. 
 
 
 
 Higher CSS specificity. Styles that have been reverted to their browser default value in a class name declaration now have a higher specificity. This can be a headache on large projects when you want to keep the CSS selectors as simple as possible.
 
 
 
 Requires more regression testing. Changes to the CSS at a lower view (like adding a new style) requires all higher breakpoints to be regression tested.
 
 
 
 The browser can’t prioritize CSS downloads. At wider breakpoints, classic mobile-first min-width media queries don’t leverage the browser’s capability to download CSS files in priority order.
 
 
 
 The problem of property value overrides
 
 
 
 There is nothing inherently wrong with overwriting values; CSS was designed to do just that. Still, inheriting incorrect values is unhelpful and can be burdensome and inefficient. It can also lead to increased style specificity when you have to overwrite styles to reset them back to their defaults, something that may cause issues later on, especially if you are using a combination of bespoke CSS and utility classes. We won’t be able to use a utility class for a style that has been reset with a higher specificity.
 
 
 
 With this in mind, I’m developing CSS with a focus on the default values much more these days. Since there’s no specific order, and no chains of specific values to keep track of, this frees me to develop breakpoints simultaneously. I concentrate on finding common styles and isolating the specific exceptions in closed media query ranges (that is, any range with a max-width set). 
 
 
 
 This approach opens up some opportunities, as you can look at each breakpoint as a clean slate. If a component’s layout looks like it should be based on Flexbox at all breakpoints, it’s fine and can be coded in the default style sheet. But if it looks like Grid would be much better for large screens and Flexbox for mobile, these can both be done entirely independently when the CSS is put into closed media query ranges. Also, developing simultaneously requires you to have a good understanding of any given component in all breakpoints up front. This can help surface issues in the design earlier in the development process. We don’t want to get stuck down a rabbit hole building a complex component for mobile, and then get the designs for desktop and find they are equally complex and incompatible with the HTML we created for the mobile view! 
 
 
 
 Though this approach isn’t going to suit everyone, I encourage you to give it a try. There are plenty of tools out there to help with concurrent development, such as Responsively App, Blisk, and many others. 
 
 
 
 Having said that, I don’t feel the order itself is particularly relevant. If you are comfortable with focusing on the mobile view, have a good understanding of the requirements for other breakpoints, and prefer to work on one device at a time, then by all means stick with the classic development order. The important thing is to identify common styles and exceptions so you can put them in the relevant stylesheet—a sort of manual tree-shaking process! Personally, I find this a little easier when working on a component across breakpoints, but that’s by no means a requirement.
 
 
 
 Closed media query ranges in practice 
 
 
 
 In classic mobile-first CSS we overwrite the styles, but we can avoid this by using media query ranges. To illustrate the difference (I’m using SCSS for brevity), let’s assume there are three visual designs: 
 
 
 
 smaller than 768from 768 to below 10241024 and anything larger 
 
 
 
 Take a simple example where a block-level element has a default padding of “20px,” which is overwritten at tablet to be “40px” and set back to “20px” on desktop.
 
 
 
 
 
 
 Classic min-width mobile-first
 .my-block {
   padding: 20px;
   @media (min-width: 768px) {
     padding: 40px;
   }
   @media (min-width: 1024px) {
     padding: 20px;
   }
 }
 Closed media query range
 .my-block {
   padding: 20px;
   @media (min-width: 768px) and (max-width: 1023.98px) {
     padding: 40px;
   }
 }
 
 
 
 
 
 
 The subtle difference is that the mobile-first example sets the default padding to “20px” and then overwrites it at each breakpoint, setting it three times in total. In contrast, the second example sets the default padding to “20px” and only overrides it at the relevant breakpoint where it isn’t the default value (in this instance, tablet is the exception).
 
 
 
 The goal is to: 
 
 
 
 Only set styles when needed. Not set them with the expectation of overwriting them later on, again and again. 
 
 
 
 To this end, closed media query ranges are our best friend. If we need to make a change to any given view, we make it in the CSS media query range that applies to the specific breakpoint. We’ll be much less likely to introduce unwanted alterations, and our regression testing only needs to focus on the breakpoint we have actually edited. 
 
 
 
 Taking the above example, if we find that .my-block spacing on desktop is already accounted for by the margin at that breakpoint, and since we want to remove the padding altogether, we could do this by setting the mobile padding in a closed media query range.
 
 
 
 
 .my-block {
   @media (max-width: 767.98px) {
     padding: 20px;
   }
   @media (min-width: 768px) and (max-width: 1023.98px) {
     padding: 40px;
   }
 }
 
 
 
 
 The browser default padding for our block is “0,” so instead of adding a desktop media query and using unset or “0” for the padding value (which we would need with mobile-first), we can wrap the mobile padding in a closed media query (since it is now also an exception) so it won’t get picked up at wider breakpoints. At the desktop breakpoint, we won’t need to set any padding style, as we want the browser default value.
 
 
 
 Bundling versus separating the CSS
 
 
 
 Back in the day, keeping the number of requests to a minimum was very important due to the browser’s limit of concurrent requests (typically around six). As a consequence, the use of image sprites and CSS bundling was the norm, with all the CSS being downloaded in one go, as one stylesheet with highest priority. 
 
 
 
 With HTTP/2 and HTTP/3 now on the scene, the number of requests is no longer the big deal it used to be. This allows us to separate the CSS into multiple files by media query. The clear benefit of this is the browser can now request the CSS it currently needs with a higher priority than the CSS it doesn’t. This is more performant and can reduce the overall time page rendering is blocked.
 
 
 
 Which HTTP version are you using?
 
 
 
 To determine which version of HTTP you’re using, go to your website and open your browser’s dev tools. Next, select the Network tab and make sure the Protocol column is visible. If “h2” is listed under Protocol, it means HTTP/2 is being used. 
 
 
 
 Note: to view the Protocol in your browser’s dev tools, go to the Network tab, reload your page, right-click any column header (e.g., Name), and check the Protocol column.
 
 
 
 Note: for a summarized comparison, see ImageKit’s “HTTP/2 vs. HTTP/1.”
 
 
 
 Also, if your site is still using HTTP/1...WHY?!! What are you waiting for? There is excellent user support for HTTP/2.
 
 
 
 Splitting the CSS
 
 
 
 Separating the CSS into individual files is a worthwhile task. Linking the separate CSS files using the relevant media attribute allows the browser to identify which files are needed immediately (because they’re render-blocking) and which can be deferred. Based on this, it allocates each file an appropriate priority.
 
 
 
 In the following example of a website visited on a mobile breakpoint, we can see the mobile and default CSS are loaded with “Highest” priority, as they are currently needed to render the page. The remaining CSS files (print, tablet, and desktop) are still downloaded in case they’ll be needed later, but with “Lowest” priority. 
 
 
 
 
 
 
 
 With bundled CSS, the browser will have to download the CSS file and parse it before rendering can start.While, as noted, with the CSS separated into different files linked and marked up with the relevant media attribute, the browser can prioritize the files it currently needs. Using closed media query ranges allows the browser to do this at all widths, as opposed to classic mobile-first min-width queries, where the desktop browser would have to download all the CSS with Highest priority. We can’t assume that desktop users always have a fast connection. For instance, in many rural areas, internet connection speeds are still slow. 
 
 
 
 The media queries and number of separate CSS files will vary from project to project based on project requirements, but might look similar to the example below.
 
 
 
 
 
 
 
 Bundled CSS
 &lt;link href&#x3D;&quot;site.css&quot; rel&#x3D;&quot;stylesheet&quot;&gt;
 This single file contains all the CSS, including all media queries, and it will be downloaded with Highest priority.
 
 
 Separated CSS
 &lt;link href&#x3D;&quot;default.css&quot; rel&#x3D;&quot;stylesheet&quot;&gt;&lt;link href&#x3D;&quot;mobile.css&quot; media&#x3D;&quot;screen and (max-width: 767.98px)&quot; rel&#x3D;&quot;stylesheet&quot;&gt;&lt;link href&#x3D;&quot;tablet.css&quot; media&#x3D;&quot;screen and (min-width: 768px) and (max-width: 1083.98px)&quot; rel&#x3D;&quot;stylesheet&quot;&gt;&lt;link href&#x3D;&quot;desktop.css&quot; media&#x3D;&quot;screen and (min-width: 1084px)&quot; rel&#x3D;&quot;stylesheet&quot;&gt;&lt;link href&#x3D;&quot;print.css&quot; media&#x3D;&quot;print&quot; rel&#x3D;&quot;stylesheet&quot;&gt;
 Separating the CSS and specifying a media attribute value on each link tag allows the browser to prioritize what it currently needs. Out of the five files listed above, two will be downloaded with Highest priority: the default file, and the file that matches the current media query. The others will be downloaded with Lowest priority.
 
 
 
 
 
 
 
 Depending on the project’s deployment strategy, a change to one file (mobile.css, for example) would only require the QA team to regression test on devices in that specific media query range. Compare that to the prospect of deploying the single bundled site.css file, an approach that would normally trigger a full regression test.
 
 
 
 Moving on
 
 
 
 The uptake of mobile-first CSS was a really important milestone in web development; it has helped front-end developers focus on mobile web applications, rather than developing sites on desktop and then attempting to retrofit them to work on other devices.
 
 
 
 I don’t think anyone wants to return to that development model again, but it’s important we don’t lose sight of the issue it highlighted: that things can easily get convoluted and less efficient if we prioritize one particular device—any device—over others. For this reason, focusing on the CSS in its own right, always mindful of what is the default setting and what’s an exception, seems like the natural next step. I’ve started noticing small simplifications in my own CSS, as well as other developers’, and that testing and maintenance work is also a bit more simplified and productive. 
 
 
 
 In general, simplifying CSS rule creation whenever we can is ultimately a cleaner approach than going around in circles of overrides. But whichever methodology you choose, it needs to suit the project. Mobile-first may—or may not—turn out to be the best choice for what’s involved, but first you need to solidly understand the trade-offs you’re stepping into.
 </content>
     </entry>
     <entry>
       <title>Designers, (Re)define Success First</title>
         <link href="https://alistapart.com/article/redefine-success-first/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">About two and a half years ago, I introduced the idea of daily ethical design. It was born out of my frustration with the many obstacles to achieving design that’s usable and equitable; protects people’s privacy, agency, and focus; benefits society; and restores nature. I argued that we need to overcome the inconveniences that prevent us from acting ethically and that we need to elevate design ethics to a more practical level by structurally integrating it into our daily work, processes, and tools.
 
 
 
 Unfortunately, we’re still very far from this ideal. 
 
 
 
 At the time, I didn’t know yet how to structurally integrate ethics. Yes, I had found some tools that had worked for me in previous projects, such as using checklists, assumption tracking, and “dark reality” sessions, but I didn’t manage to apply those in every project. I was still struggling for time and support, and at best I had only partially achieved a higher (moral) quality of design—which is far from my definition of structurally integrated.
 
 
 
 I decided to dig deeper for the root causes in business that prevent us from practicing daily ethical design. Now, after much research and experimentation, I believe that I’ve found the key that will let us structurally integrate ethics. And it’s surprisingly simple! But first we need to zoom out to get a better understanding of what we’re up against.
 
 
 
 Influence the system
 
 
 
 Sadly, we’re trapped in a capitalistic system that reinforces consumerism and inequality, and it’s obsessed with the fantasy of endless growth. Sea levels, temperatures, and our demand for energy continue to rise unchallenged, while the gap between rich and poor continues to widen. Shareholders expect ever-higher returns on their investments, and companies feel forced to set short-term objectives that reflect this. Over the last decades, those objectives have twisted our well-intended human-centered mindset into a powerful machine that promotes ever-higher levels of consumption. When we’re working for an organization that pursues “double-digit growth” or “aggressive sales targets” (which is 99 percent of us), that’s very hard to resist while remaining human friendly. Even with our best intentions, and even though we like to say that we create solutions for people, we’re a part of the problem.
 
 
 
 What can we do to change this?
 
 
 
 We can start by acting on the right level of the system. Donella H. Meadows, a system thinker, once listed ways to influence a system in order of effectiveness. When you apply these to design, you get:
 
 
 
 At the lowest level of effectiveness, you can affect numbers such as usability scores or the number of design critiques. But none of that will change the direction of a company.Similarly, affecting buffers (such as team budgets), stocks (such as the number of designers), flows (such as the number of new hires), and delays (such as the time that it takes to hear about the effect of design) won’t significantly affect a company.Focusing instead on feedback loops such as management control, employee recognition, or design-system investments can help a company become better at achieving its objectives. But that doesn’t change the objectives themselves, which means that the organization will still work against your ethical-design ideals.The next level, information flows, is what most ethical-design initiatives focus on now: the exchange of ethical methods, toolkits, articles, conferences, workshops, and so on. This is also where ethical design has remained mostly theoretical. We’ve been focusing on the wrong level of the system all this time.Take rules, for example—they beat knowledge every time. There can be widely accepted rules, such as how finance works, or a scrum team’s definition of done. But ethical design can also be smothered by unofficial rules meant to maintain profits, often revealed through comments such as “the client didn’t ask for it” or “don’t make it too big.”Changing the rules without holding official power is very hard. That’s why the next level is so influential: self-organization. Experimentation, bottom-up initiatives, passion projects, self-steering teams—all of these are examples of self-organization that improve the resilience and creativity of a company. It’s exactly this diversity of viewpoints that’s needed to structurally tackle big systemic issues like consumerism, wealth inequality, and climate change.Yet even stronger than self-organization are objectives and metrics. Our companies want to make more money, which means that everything and everyone in the company does their best to… make the company more money. And once I realized that profit is nothing more than a measurement, I understood how crucial a very specific, defined metric can be toward pushing a company in a certain direction.
 
 
 
 The takeaway? If we truly want to incorporate ethics into our daily design practice, we must first change the measurable objectives of the company we work for, from the bottom up.
 
 
 
 Redefine success
 
 
 
 Traditionally, we consider a product or service successful if it’s desirable to humans, technologically feasible, and financially viable. You tend to see these represented as equals; if you type the three words in a search engine, you’ll find diagrams of three equally sized, evenly arranged circles.
 
 
 
 
 
 
 
 But in our hearts, we all know that the three dimensions aren’t equally weighted: it’s viability that ultimately controls whether a product will go live. So a more realistic representation might look like this:
 
 
 
 
 
 
 
 Desirability and feasibility are the means; viability is the goal. Companies—outside of nonprofits and charities—exist to make money.
 
 
 
 A genuinely purpose-driven company would try to reverse this dynamic: it would recognize finance for what it was intended for: a means. So both feasibility and viability are means to achieve what the company set out to achieve. It makes intuitive sense: to achieve most anything, you need resources, people, and money. (Fun fact: the Italian language knows no difference between feasibility and viability; both are simply fattibilità.)
 
 
 
 
 
 
 
 But simply swapping viable for desirable isn’t enough to achieve an ethical outcome. Desirability is still linked to consumerism because the associated activities aim to identify what people want—whether it’s good for them or not. Desirability objectives, such as user satisfaction or conversion, don’t consider whether a product is healthy for people. They don’t prevent us from creating products that distract or manipulate people or stop us from contributing to society’s wealth inequality. They’re unsuitable for establishing a healthy balance with nature.
 
 
 
 There’s a fourth dimension of success that’s missing: our designs also need to be ethical in the effect that they have on the world.
 
 
 
 
 
 
 
 This is hardly a new idea. Many similar models exist, some calling the fourth dimension accountability, integrity, or responsibility. What I’ve never seen before, however, is the necessary step that comes after: to influence the system as designers and to make ethical design more practical, we must create objectives for ethical design that are achievable and inspirational. There’s no one way to do this because it highly depends on your culture, values, and industry. But I’ll give you the version that I developed with a group of colleagues at a design agency. Consider it a template to get started.
 
 
 
 Pursue well-being, equity, and sustainability
 
 
 
 We created objectives that address design’s effect on three levels: individual, societal, and global.
 
 
 
 An objective on the individual level tells us what success is beyond the typical focus of usability and satisfaction—instead considering matters such as how much time and attention is required from users. We pursued well-being:
 
 
 
 We create products and services that allow for people’s health and happiness. Our solutions are calm, transparent, nonaddictive, and nonmisleading. We respect our users’ time, attention, and privacy, and help them make healthy and respectful choices.
 
 
 
 An objective on the societal level forces us to consider our impact beyond just the user, widening our attention to the economy, communities, and other indirect stakeholders. We called this objective equity:
 
 
 
 We create products and services that have a positive social impact. We consider economic equality, racial justice, and the inclusivity and diversity of people as teams, users, and customer segments. We listen to local culture, communities, and those we affect.
 
 
 
 Finally, the objective on the global level aims to ensure that we remain in balance with the only home we have as humanity. Referring to it simply as sustainability, our definition was:
 
 
 
 We create products and services that reward sufficiency and reusability. Our solutions support the circular economy: we create value from waste, repurpose products, and prioritize sustainable choices. We deliver functionality instead of ownership, and we limit energy use.
 
 
 
 In short, ethical design (to us) meant achieving wellbeing for each user and an equitable value distribution within society through a design that can be sustained by our living planet. When we introduced these objectives in the company, for many colleagues, design ethics and responsible design suddenly became tangible and achievable through practical—and even familiar—actions.
 
 
 
 Measure impact 
 
 
 
 But defining these objectives still isn’t enough. What truly caught the attention of senior management was the fact that we created a way to measure every design project’s well-being, equity, and sustainability.
 
 
 
 This overview lists example metrics that you can use as you pursue well-being, equity, and sustainability:
 
 
 
 
 
 
 
 There’s a lot of power in measurement. As the saying goes, what gets measured gets done. Donella Meadows once shared this example:
 
 
 
 “If the desired system state is national security, and that is defined as the amount of money spent on the military, the system will produce military spending. It may or may not produce national security.”
 
 
 
 This phenomenon explains why desirability is a poor indicator of success: it’s typically defined as the increase in customer satisfaction, session length, frequency of use, conversion rate, churn rate, download rate, and so on. But none of these metrics increase the health of people, communities, or ecosystems. What if instead we measured success through metrics for (digital) well-being, such as (reduced) screen time or software energy consumption?
 
 
 
 There’s another important message here. Even if we set an objective to build a calm interface, if we were to choose the wrong metric for calmness—say, the number of interface elements—we could still end up with a screen that induces anxiety. Choosing the wrong metric can completely undo good intentions. 
 
 
 
 Additionally, choosing the right metric is enormously helpful in focusing the design team. Once you go through the exercise of choosing metrics for our objectives, you’re forced to consider what success looks like concretely and how you can prove that you’ve reached your ethical objectives. It also forces you to consider what we as designers have control over: what can I include in my design or change in my process that will lead to the right type of success? The answer to this question brings a lot of clarity and focus.
 
 
 
 And finally, it’s good to remember that traditional businesses run on measurements, and managers love to spend much time discussing charts (ideally hockey-stick shaped)—especially if they concern profit, the one-above-all of metrics. For good or ill, to improve the system, to have a serious discussion about ethical design with managers, we’ll need to speak that business language.
 
 
 
 Practice daily ethical design
 
 
 
 Once you’ve defined your objectives and you have a reasonable idea of the potential metrics for your design project, only then do you have a chance to structurally practice ethical design. It “simply” becomes a matter of using your creativity and choosing from all the knowledge and toolkits already available to you.
 
 
 
 
 
 
 
 I think this is quite exciting! It opens a whole new set of challenges and considerations for the design process. Should you go with that energy-consuming video or would a simple illustration be enough? Which typeface is the most calm and inclusive? Which new tools and methods do you use? When is the website’s end of life? How can you provide the same service while requiring less attention from users? How do you make sure that those who are affected by decisions are there when those decisions are made? How can you measure our effects?
 
 
 
 The redefinition of success will completely change what it means to do good design.
 
 
 
 There is, however, a final piece of the puzzle that’s missing: convincing your client, product owner, or manager to be mindful of well-being, equity, and sustainability. For this, it’s essential to engage stakeholders in a dedicated kickoff session.
 
 
 
 Kick it off or fall back to status quo
 
 
 
 The kickoff is the most important meeting that can be so easy to forget to include. It consists of two major phases: 1) the alignment of expectations, and 2) the definition of success.
 
 
 
 In the first phase, the entire (design) team goes over the project brief and meets with all the relevant stakeholders. Everyone gets to know one another and express their expectations on the outcome and their contributions to achieving it. Assumptions are raised and discussed. The aim is to get on the same level of understanding and to in turn avoid preventable miscommunications and surprises later in the project.
 
 
 
 For example, for a recent freelance project that aimed to design a digital platform that facilitates US student advisors’ documentation and communication, we conducted an online kickoff with the client, a subject-matter expert, and two other designers. We used a combination of canvases on Miro: one with questions from “Manual of Me” (to get to know each other), a Team Canvas (to express expectations), and a version of the Project Canvas to align on scope, timeline, and other practical matters.
 
 
 
 The above is the traditional purpose of a kickoff. But just as important as expressing expectations is agreeing on what success means for the project—in terms of desirability, viability, feasibility, and ethics. What are the objectives in each dimension?
 
 
 
 Agreement on what success means at such an early stage is crucial because you can rely on it for the remainder of the project. If, for example, the design team wants to build an inclusive app for a diverse user group, they can raise diversity as a specific success criterion during the kickoff. If the client agrees, the team can refer back to that promise throughout the project. “As we agreed in our first meeting, having a diverse user group that includes A and B is necessary to build a successful product. So we do activity X and follow research process Y.” Compare those odds to a situation in which the team didn’t agree to that beforehand and had to ask for permission halfway through the project. The client might argue that that came on top of the agreed scope—and she’d be right.
 
 
 
 In the case of this freelance project, to define success I prepared a round canvas that I call the Wheel of Success. It consists of an inner ring, meant to capture ideas for objectives, and a set of outer rings, meant to capture ideas on how to measure those objectives. The rings are divided into five dimensions of successful design: healthy, equitable, sustainable, desirable, feasible, and viable.
 
 
 
 
 
 
 
 We went through each dimension, writing down ideas on digital sticky notes. Then we discussed our ideas and verbally agreed on the most important ones. For example, our client agreed that sustainability and progressive enhancement are important success criteria for the platform. And the subject-matter expert emphasized the importance of including students from low-income and disadvantaged groups in the design process.
 
 
 
 After the kickoff, we summarized our ideas and shared understanding in a project brief that captured these aspects:
 
 
 
 the project’s origin and purpose: why are we doing this project?the problem definition: what do we want to solve?the concrete goals and metrics for each success dimension: what do we want to achieve?the scope, process, and role descriptions: how will we achieve it?
 
 
 
 With such a brief in place, you can use the agreed-upon objectives and concrete metrics as a checklist of success, and your design team will be ready to pursue the right objective—using the tools, methods, and metrics at their disposal to achieve ethical outcomes.
 
 
 
 
 
 
 
 Conclusion
 
 
 
 Over the past year, quite a few colleagues have asked me, “Where do I start with ethical design?” My answer has always been the same: organize a session with your stakeholders to (re)define success. Even though you might not always be 100 percent successful in agreeing on goals that cover all responsibility objectives, that beats the alternative (the status quo) every time. If you want to be an ethical, responsible designer, there’s no skipping this step.
 
 
 
 To be even more specific: if you consider yourself a strategic designer, your challenge is to define ethical objectives, set the right metrics, and conduct those kick-off sessions. If you consider yourself a system designer, your starting point is to understand how your industry contributes to consumerism and inequality, understand how finance drives business, and brainstorm which levers are available to influence the system on the highest level. Then redefine success to create the space to exercise those levers.
 
 
 
 And for those who consider themselves service designers or UX designers or UI designers: if you truly want to have a positive, meaningful impact, stay away from the toolkits and meetups and conferences for a while. Instead, gather your colleagues and define goals for well-being, equity, and sustainability through design. Engage your stakeholders in a workshop and challenge them to think of ways to achieve and measure those ethical goals. Take their input, make it concrete and visible, ask for their agreement, and hold them to it.
 
 
 
 Otherwise, I’m genuinely sorry to say, you’re wasting your precious time and creative energy.
 
 
 
 Of course, engaging your stakeholders in this way can be uncomfortable. Many of my colleagues expressed doubts such as “What will the client think of this?,” “Will they take me seriously?,” and “Can’t we just do it within the design team instead?” In fact, a product manager once asked me why ethics couldn’t just be a structured part of the design process—to just do it without spending the effort to define ethical objectives. It’s a tempting idea, right? We wouldn’t have to have difficult discussions with stakeholders about what values or which key-performance indicators to pursue. It would let us focus on what we like and do best: designing.
 
 
 
 But as systems theory tells us, that’s not enough. For those of us who aren’t from marginalized groups and have the privilege to be able to speak up and be heard, that uncomfortable space is exactly where we need to be if we truly want to make a difference. We can’t remain within the design-for-designers bubble, enjoying our privileged working-from-home situation, disconnected from the real world out there. For those of us who have the possibility to speak up and be heard: if we solely keep talking about ethical design and it remains at the level of articles and toolkits—we’re not designing ethically. It’s just theory. We need to actively engage our colleagues and clients by challenging them to redefine success in business.
 
 
 
 With a bit of courage, determination, and focus, we can break out of this cage that finance and business-as-usual have built around us and become facilitators of a new type of business that can see beyond financial value. We just need to agree on the right objectives at the start of each design project, find the right metrics, and realize that we already have everything that we need to get started. That’s what it means to do daily ethical design.
 
 
 
 For their inspiration and support over the years, I would like to thank Emanuela Cozzi Schettini, José Gallegos, Annegret Bönemann, Ian Dorr, Vera Rademaker, Virginia Rispoli, Cecilia Scolaro, Rouzbeh Amini, and many others.
 </content>
     </entry>
     <entry>
       <title>Breaking Out of the Box</title>
         <link href="https://alistapart.com/article/breaking-out-of-the-box/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">CSS is about styling boxes. In fact, the whole web is made of boxes, from the browser viewport to elements on a page. But every once in a while a new feature comes along that makes us rethink our design approach.
 
 
 
 Round displays, for example, make it fun to play with circular clip areas. Mobile screen notches and virtual keyboards offer challenges to best organize content that stays clear of them. And dual screen or foldable devices make us rethink how to best use available space in a number of different device postures.
 
 
 
 Sketches of a round display, a common rectangular mobile display, and a device with a foldable display.
 
 
 
 These recent evolutions of the web platform made it both more challenging and more interesting to design products. They’re great opportunities for us to break out of our rectangular boxes.
 
 
 
 I’d like to talk about a new feature similar to the above: the Window Controls Overlay for Progressive Web Apps (PWAs).
 
 
 
 Progressive Web Apps are blurring the lines between apps and websites. They combine the best of both worlds. On one hand, they’re stable, linkable, searchable, and responsive just like websites. On the other hand, they provide additional powerful capabilities, work offline, and read files just like native apps.
 
 
 
 As a design surface, PWAs are really interesting because they challenge us to think about what mixing web and device-native user interfaces can be. On desktop devices in particular, we have more than 40 years of history telling us what applications should look like, and it can be hard to break out of this mental model.
 
 
 
 At the end of the day though, PWAs on desktop are constrained to the window they appear in: a rectangle with a title bar at the top.
 
 
 
 Here’s what a typical desktop PWA app looks like:
 
 
 
 Sketches of two rectangular user interfaces representing the desktop Progressive Web App status quo on the macOS and Windows operating systems, respectively. 
 
 
 
 Sure, as the author of a PWA, you get to choose the color of the title bar (using the Web Application Manifest theme_color property), but that’s about it.
 
 
 
 What if we could think outside this box, and reclaim the real estate of the app’s entire window? Doing so would give us a chance to make our apps more beautiful and feel more integrated in the operating system.
 
 
 
 This is exactly what the Window Controls Overlay offers. This new PWA functionality makes it possible to take advantage of the full surface area of the app, including where the title bar normally appears.
 
 
 
 About the title bar and window controls
 
 
 
 Let’s start with an explanation of what the title bar and window controls are.
 
 
 
 The title bar is the area displayed at the top of an app window, which usually contains the app’s name. Window controls are the affordances, or buttons, that make it possible to minimize, maximize, or close the app’s window, and are also displayed at the top.
 
 
 
 A sketch of a rectangular application user interface highlighting the title bar area and window control buttons.
 
 
 
 Window Controls Overlay removes the physical constraint of the title bar and window controls areas. It frees up the full height of the app window, enabling the title bar and window control buttons to be overlaid on top of the application’s web content. 
 
 
 
 A sketch of a rectangular application user interface using Window Controls Overlay. The title bar and window controls are no longer in an area separated from the app’s content.
 
 
 
 If you are reading this article on a desktop computer, take a quick look at other apps. Chances are they’re already doing something similar to this. In fact, the very web browser you are using to read this uses the top area to display tabs.
 
 
 
 A screenshot of the top area of a browser’s user interface showing a group of tabs that share the same horizontal space as the app window controls.
 
 
 
 Spotify displays album artwork all the way to the top edge of the application window.
 
 
 
 A screenshot of an album in Spotify’s desktop application. Album artwork spans the entire width of the main content area, all the way to the top and right edges of the window, and the right edge of the main navigation area on the left side. The application and album navigation controls are overlaid directly on top of the album artwork.
 
 
 
 Microsoft Word uses the available title bar space to display the auto-save and search functionalities, and more.
 
 
 
 A screenshot of Microsoft Word’s toolbar interface. Document file information, search, and other functionality appear at the top of the window, sharing the same horizontal space as the app’s window controls.
 
 
 
 The whole point of this feature is to allow you to make use of this space with your own content while providing a way to account for the window control buttons. And it enables you to offer this modified experience on a range of platforms while not adversely affecting the experience on browsers or devices that don’t support Window Controls Overlay. After all, PWAs are all about progressive enhancement, so this feature is a chance to enhance your app to use this extra space when it’s available.
 
 
 
 Let’s use the feature
 
 
 
 For the rest of this article, we’ll be working on a demo app to learn more about using the feature.
 
 
 
 The demo app is called 1DIV. It’s a simple CSS playground where users can create designs using CSS and a single HTML element.
 
 
 
 The app has two pages. The first lists the existing CSS designs you’ve created:
 
 
 
 A screenshot of the 1DIV app displaying a thumbnail grid of CSS designs a user created.
 
 
 
 The second page enables you to create and edit CSS designs:
 
 
 
 A screenshot of the 1DIV app editor page. The top half of the window displays a rendered CSS design, and a text editor on the bottom half of the window displays the CSS used to create it.
 
 
 
 Since I’ve added a simple web manifest and service worker, we can install the app as a PWA on desktop. Here is what it looks like on macOS:
 
 
 
 Screenshots of the 1DIV app thumbnail view and CSS editor view on macOS. This version of the app’s window has a separate control bar at the top for the app name and window control buttons.
 
 
 
 And on Windows:
 
 
 
 Screenshots of the 1DIV app thumbnail view and CSS editor view on the Windows operating system. This version of the app’s window also has a separate control bar at the top for the app name and window control buttons.
 
 
 
 Our app is looking good, but the white title bar in the first page is wasted space. In the second page, it would be really nice if the design area went all the way to the top of the app window.
 
 
 
 Let’s use the Window Controls Overlay feature to improve this.
 
 
 
 Enabling Window Controls Overlay
 
 
 
 The feature is still experimental at the moment. To try it, you need to enable it in one of the supported browsers.
 
 
 
 As of now, it has been implemented in Chromium, as a collaboration between Microsoft and Google. We can therefore use it in Chrome or Edge by going to the internal about://flags page, and enabling the Desktop PWA Window Controls Overlay flag.
 
 
 
 Using Window Controls Overlay
 
 
 
 To use the feature, we need to add the following display_override member to our web app’s manifest file:
 
 
 
 {
   &quot;name&quot;: &quot;1DIV&quot;,
   &quot;description&quot;: &quot;1DIV is a mini CSS playground&quot;,
   &quot;lang&quot;: &quot;en-US&quot;,
   &quot;start_url&quot;: &quot;/&quot;,
   &quot;theme_color&quot;: &quot;#ffffff&quot;,
   &quot;background_color&quot;: &quot;#ffffff&quot;,
   &quot;display_override&quot;: [
     &quot;window-controls-overlay&quot;
   ],
   &quot;icons&quot;: [
     ...
   ]
 }
 
 
 
 
 On the surface, the feature is really simple to use. This manifest change is the only thing we need to make the title bar disappear and turn the window controls into an overlay.
 
 
 
 However, to provide a great experience for all users regardless of what device or browser they use, and to make the most of the title bar area in our design, we’ll need a bit of CSS and JavaScript code.
 
 
 
 Here is what the app looks like now:
 
 
 
 Screenshot of the 1DIV app thumbnail view using Window Controls Overlay on macOS. The separate top bar area is gone, but the window controls are now blocking some of the app’s interface
 
 
 
 The title bar is gone, which is what we wanted, but our logo, search field, and NEW button are partially covered by the window controls because now our layout starts at the top of the window.
 
 
 
 It’s similar on Windows, with the difference that the close, maximize, and minimize buttons appear on the right side, grouped together with the PWA control buttons:
 
 
 
 Screenshot of the 1DIV app thumbnail display using Window Controls Overlay on the Windows operating system. The separate top bar area is gone, but the window controls are now blocking some of the app’s content.
 
 
 
 Using CSS to keep clear of the window controls
 
 
 
 Along with the feature, new CSS environment variables have been introduced:
 
 
 
 titlebar-area-xtitlebar-area-ytitlebar-area-widthtitlebar-area-height
 
 
 
 You use these variables with the CSS env() function to position your content where the title bar would have been while ensuring it won’t overlap with the window controls. In our case, we’ll use two of the variables to position our header, which contains the logo, search bar, and NEW button. 
 
 
 
 header {
   position: absolute;
   left: env(titlebar-area-x, 0);
   width: env(titlebar-area-width, 100%);
   height: var(--toolbar-height);
 }
 
 
 
 
 The titlebar-area-x variable gives us the distance from the left of the viewport to where the title bar would appear, and titlebar-area-width is its width. (Remember, this is not equivalent to the width of the entire viewport, just the title bar portion, which as noted earlier, doesn’t include the window controls.)
 
 
 
 By doing this, we make sure our content remains fully visible. We’re also defining fallback values (the second parameter in the env() function) for when the variables are not defined (such as on non-supporting browsers, or when the Windows Control Overlay feature is disabled).
 
 
 
 Screenshot of the 1DIV app thumbnail view on macOS with Window Controls Overlay and our CSS updated. The app content that the window controls had been blocking has been repositioned.
 
 
 
 Screenshot of the 1DIV app thumbnail view on the Windows operating system with Window Controls Overlay and our updated CSS. The app content that the window controls had been blocking has been repositioned.
 
 
 
 Now our header adapts to its surroundings, and it doesn’t feel like the window control buttons have been added as an afterthought. The app looks a lot more like a native app.
 
 
 
 Changing the window controls background color so it blends in
 
 
 
 Now let’s take a closer look at our second page: the CSS playground editor.
 
 
 
 Screenshots of the 1DIV app CSS editor view with Window Controls Overlay in macOS and Windows, respectively. The window controls overlay areas have a solid white background color, which contrasts with the hot pink color of the example CSS design displayed in the editor.
 
 
 
 Not great. Our CSS demo area does go all the way to the top, which is what we wanted, but the way the window controls appear as white rectangles on top of it is quite jarring.
 
 
 
 We can fix this by changing the app’s theme color. There are a couple of ways to define it:
 
 
 
 PWAs can define a theme color in the web app manifest file using the theme_color manifest member. This color is then used by the OS in different ways. On desktop platforms, it is used to provide a background color to the title bar and window controls.Websites can use the theme-color meta tag as well. It’s used by browsers to customize the color of the UI around the web page. For PWAs, this color can override the manifest theme_color.
 
 
 
 In our case, we can set the manifest theme_color to white to provide the right default color for our app. The OS will read this color value when the app is installed and use it to make the window controls background color white. This color works great for our main page with the list of demos.
 
 
 
 The theme-color meta tag can be changed at runtime, using JavaScript. So we can do that to override the white with the right demo background color when one is opened.
 
 
 
 Here is the function we’ll use:
 
 
 
 function themeWindow(bgColor) {
   document.querySelector(&quot;meta[name&#x3D;theme-color]&quot;).setAttribute(&#x27;content&#x27;, bgColor);
 }
 
 
 
 With this in place, we can imagine how using color and CSS transitions can produce a smooth change from the list page to the demo page, and enable the window control buttons to blend in with the rest of the app’s interface.
 
 
 
 Screenshot of the 1DIV app CSS editor view on the Windows operating system with Window Controls Overlay and updated CSS demonstrating how the window control buttons blend in with the rest of the app’s interface.
 
 
 
 Dragging the window
 
 
 
 Now, getting rid of the title bar entirely does have an important accessibility consequence: it’s much more difficult to move the application window around.
 
 
 
 The title bar provides a sizable area for users to click and drag, but by using the Window Controls Overlay feature, this area becomes limited to where the control buttons are, and users have to very precisely aim between these buttons to move the window.
 
 
 
 Fortunately, this can be fixed using CSS with the app-region property. This property is, for now, only supported in Chromium-based browsers and needs the -webkit- vendor prefix. 
 
 
 
 To make any element of the app become a dragging target for the window, we can use the following: 
 
 
 
 -webkit-app-region: drag;
 
 
 
 It is also possible to explicitly make an element non-draggable: 
 
 
 
 -webkit-app-region: no-drag; 
 
 
 
 These options can be useful for us. We can make the entire header a dragging target, but make the search field and NEW button within it non-draggable so they can still be used as normal.
 
 
 
 However, because the editor page doesn’t display the header, users wouldn’t be able to drag the window while editing code. So let&#x27;s use a different approach. We’ll create another element before our header, also absolutely positioned, and dedicated to dragging the window.
 
 
 
 &lt;div class&#x3D;&quot;drag&quot;&gt;&lt;/div&gt;
 &lt;header&gt;...&lt;/header&gt;
 
 
 
 .drag {
   position: absolute;
   top: 0;
   width: 100%;
   height: env(titlebar-area-height, 0);
   -webkit-app-region: drag;
 }
 
 
 
 With the above code, we’re making the draggable area span the entire viewport width, and using the titlebar-area-height variable to make it as tall as what the title bar would have been. This way, our draggable area is aligned with the window control buttons as shown below.
 
 
 
 And, now, to make sure our search field and button remain usable:
 
 
 
 header .search,
 header .new {
   -webkit-app-region: no-drag;
 }
 
 
 
 With the above code, users can click and drag where the title bar used to be. It is an area that users expect to be able to use to move windows on desktop, and we’re not breaking this expectation, which is good.
 
 
 
 An animated view of the 1DIV app being dragged across a Windows desktop with the mouse.
 
 
 
 Adapting to window resize
 
 
 
 It may be useful for an app to know both whether the window controls overlay is visible and when its size changes. In our case, if the user made the window very narrow, there wouldn’t be enough space for the search field, logo, and button to fit, so we’d want to push them down a bit.
 
 
 
 The Window Controls Overlay feature comes with a JavaScript API we can use to do this: navigator.windowControlsOverlay.
 
 
 
 The API provides three interesting things:
 
 
 
 navigator.windowControlsOverlay.visible lets us know whether the overlay is visible.navigator.windowControlsOverlay.getBoundingClientRect() lets us know the position and size of the title bar area.navigator.windowControlsOverlay.ongeometrychange lets us know when the size or visibility changes.
 
 
 
 Let’s use this to be aware of the size of the title bar area and move the header down if it’s too narrow.
 
 
 
 if (navigator.windowControlsOverlay) {
   navigator.windowControlsOverlay.addEventListener(&#x27;geometrychange&#x27;, () &#x3D;&gt; {
     const { width } &#x3D; navigator.windowControlsOverlay.getBoundingClientRect();
     document.body.classList.toggle(&#x27;narrow&#x27;, width &lt; 250);
   });
 }
 
 
 
 In the example above, we set the narrow class on the body of the app if the title bar area is narrower than 250px. We could do something similar with a media query, but using the windowControlsOverlay API has two advantages for our use case:
 
 
 
 It’s only fired when the feature is supported and used; we don’t want to adapt the design otherwise.We get the size of the title bar area across operating systems, which is great because the size of the window controls is different on Mac and Windows. Using a media query wouldn’t make it possible for us to know exactly how much space remains.
 
 
 
 .narrow header {
   top: env(titlebar-area-height, 0);
   left: 0;
   width: 100%;
 }
 
 
 
 Using the above CSS code, we can move our header down to stay clear of the window control buttons when the window is too narrow, and move the thumbnails down accordingly.
 
 
 
 A screenshot of the 1DIV app on Windows showing the app’s content adjusted for a much narrower viewport.
 
 
 
 Thirty pixels of exciting design opportunities
 
 
 
 Using the Window Controls Overlay feature, we were able to take our simple demo app and turn it into something that feels so much more integrated on desktop devices. Something that reaches out of the usual window constraints and provides a custom experience for its users.
 
 
 
 In reality, this feature only gives us about 30 pixels of extra room and comes with challenges on how to deal with the window controls. And yet, this extra room and those challenges can be turned into exciting design opportunities.
 
 
 
 More devices of all shapes and forms get invented all the time, and the web keeps on evolving to adapt to them. New features get added to the web platform to allow us, web authors, to integrate more and more deeply with those devices. From watches or foldable devices to desktop computers, we need to evolve our design approach for the web. Building for the web now lets us think outside the rectangular box.
 
 
 
 So let’s embrace this. Let’s use the standard technologies already at our disposal, and experiment with new ideas to provide tailored experiences for all devices, all from a single codebase!
 
 
 
 If you get a chance to try the Window Controls Overlay feature and have feedback about it, you can open issues on the spec’s repository. It’s still early in the development of this feature, and you can help make it even better. Or, you can take a look at the feature’s existing documentation, or this demo app and its source code. 
 </content>
     </entry>
     <entry>
       <title>How to Sell UX Research with Two Simple Questions</title>
         <link href="https://alistapart.com/article/how-to-sell-ux-research/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">Do you find yourself designing screens with only a vague idea of how the things on the screen relate to the things elsewhere in the system? Do you leave stakeholder meetings with unclear directives that often seem to contradict previous conversations? You know a better understanding of user needs would help the team get clear on what you are actually trying to accomplish, but time and budget for research is tight. When it comes to asking for more direct contact with your users, you might feel like poor Oliver Twist, timidly asking, “Please, sir, I want some more.” 
 
 
 
 Here’s the trick. You need to get stakeholders themselves to identify high-risk assumptions and hidden complexity, so that they become just as motivated as you to get answers from users. Basically, you need to make them think it’s their idea. 
 
 
 
 In this article, I’ll show you how to collaboratively expose misalignment and gaps in the team’s shared understanding by bringing the team together around two simple questions:
 
 
 
 What are the objects?What are the relationships between those objects?
 
 
 
 A gauntlet between research and screen design
 
 
 
 These two questions align to the first two steps of the ORCA process, which might become your new best friend when it comes to reducing guesswork. Wait, what’s ORCA?! Glad you asked.
 
 
 
 ORCA stands for Objects, Relationships, CTAs, and Attributes, and it outlines a process for creating solid object-oriented user experiences. Object-oriented UX is my design philosophy. ORCA is an iterative methodology for synthesizing user research into an elegant structural foundation to support screen and interaction design. OOUX and ORCA have made my work as a UX designer more collaborative, effective, efficient, fun, strategic, and meaningful.
 
 
 
 The ORCA process has four iterative rounds and a whopping fifteen steps. In each round we get more clarity on our Os, Rs, Cs, and As.
 
 
 
 The four rounds and fifteen steps of the ORCA process. In the OOUX world, we love color-coding. Blue is reserved for objects! (Yellow is for core content, pink is for metadata, and green is for calls-to-action. Learn more about the color-coded object map and connecting CTAs to objects.)
 
 
 
 I sometimes say that ORCA is a “garbage in, garbage out” process. To ensure that the testable prototype produced in the final round actually tests well, the process needs to be fed by good research. But if you don’t have a ton of research, the beginning of the ORCA process serves another purpose: it helps you sell the need for research.
 
 
 
 ORCA strengthens the weak spot between research and design by helping distill research into solid information architecture—scaffolding for the screen design and interaction design to hang on.
 
 
 
 In other words, the ORCA process serves as a gauntlet between research and design. With good research, you can gracefully ride the killer whale from research into design. But without good research, the process effectively spits you back into research and with a cache of specific open questions.
 
 
 
 Getting in the same curiosity-boat
 
 
 
 What gets us into trouble is not what we don’t know. It’s what we know for sure that just ain’t so.Mark Twain
 
 
 
 The first two steps of the ORCA process—Object Discovery and Relationship Discovery—shine a spotlight on the dark, dusty corners of your team’s misalignments and any inherent complexity that’s been swept under the rug. It begins to expose what this classic comic so beautifully illustrates:
 
 
 
 The original “Tree Swing Project Management” cartoon dates back to the 1960s or 1970s and has no artist attribution we could find.
 
 
 
 This is one reason why so many UX designers are frustrated in their job and why many projects fail. And this is also why we often can’t sell research: every decision-maker is confident in their own mental picture. 
 
 
 
 Once we expose hidden fuzzy patches in each picture and the differences between them all, the case for user research makes itself.
 
 
 
 But how we do this is important. However much we might want to, we can’t just tell everyone, “YOU ARE WRONG!” Instead, we need to facilitate and guide our team members to self-identify holes in their picture. When stakeholders take ownership of assumptions and gaps in understanding, BAM! Suddenly, UX research is not such a hard sell, and everyone is aboard the same curiosity-boat.
 
 
 
 Say your users are doctors. And you have no idea how doctors use the system you are tasked with redesigning.
 
 
 
 You might try to sell research by honestly saying: “We need to understand doctors better! What are their pain points? How do they use the current app?” But here’s the problem with that. Those questions are vague, and the answers to them don’t feel acutely actionable.
 
 
 
 Instead, you want your stakeholders themselves to ask super-specific questions. This is more like the kind of conversation you need to facilitate. Let’s listen in:
 
 
 
 “Wait a sec, how often do doctors share patients? Does a patient in this system have primary and secondary doctors?”
 
 
 
 “Can a patient even have more than one primary doctor?”
 
 
 
 “Is it a ‘primary doctor’ or just a ‘primary caregiver’… Can’t that role be a nurse practitioner?”
 
 
 
 “No, caregivers are something else… That’s the patient’s family contacts, right?”
 
 
 
 “So are caregivers in scope for this redesign?”
 
 
 
 “Yeah, because if a caregiver is present at an appointment, the doctor needs to note that. Like, tag the caregiver on the note… Or on the appointment?”
 
 
 
 Now we are getting somewhere. Do you see how powerful it can be getting stakeholders to debate these questions themselves? The diabolical goal here is to shake their confidence—gently and diplomatically.
 
 
 
 When these kinds of questions bubble up collaboratively and come directly from the mouths of your stakeholders and decision-makers, suddenly, designing screens without knowing the answers to these questions seems incredibly risky, even silly.
 
 
 
 If we create software without understanding the real-world information environment of our users, we will likely create software that does not align to the real-world information environment of our users. And this will, hands down, result in a more confusing, more complex, and less intuitive software product.
 
 
 
 The two questions
 
 
 
 But how do we get to these kinds of meaty questions diplomatically, efficiently, collaboratively, and reliably? 
 
 
 
 We can do this by starting with those two big questions that align to the first two steps of the ORCA process:
 
 
 
 What are the objects?What are the relationships between those objects?
 
 
 
 In practice, getting to these answers is easier said than done. I’m going to show you how these two simple questions can provide the outline for an Object Definition Workshop. During this workshop, these “seed” questions will blossom into dozens of specific questions and shine a spotlight on the need for more user research.
 
 
 
 Prep work: Noun foraging
 
 
 
 In the next section, I’ll show you how to run an Object Definition Workshop with your stakeholders (and entire cross-functional team, hopefully). But first, you need to do some prep work.
 
 
 
 Basically, look for nouns that are particular to the business or industry of your project, and do it across at least a few sources. I call this noun foraging.
 
 
 
 Here are just a few great noun foraging sources:
 
 
 
 the product’s marketing sitethe product’s competitors’ marketing sites (competitive analysis, anyone?)the existing product (look at labels!)user interview transcriptsnotes from stakeholder interviews or vision docs from stakeholders
 
 
 
 Put your detective hat on, my dear Watson. Get resourceful and leverage what you have. If all you have is a marketing website, some screenshots of the existing legacy system, and access to customer service chat logs, then use those.
 
 
 
 As you peruse these sources, watch for the nouns that are used over and over again, and start listing them (preferably on blue sticky notes if you’ll be creating an object map later!).
 
 
 
 You’ll want to focus on nouns that might represent objects in your system. If you are having trouble determining if a noun might be object-worthy, remember the acronym SIP and test for:
 
 
 
 StructureInstancesPurpose
 
 
 
 Think of a library app, for example. Is “book” an object?
 
 
 
 Structure: can you think of a few attributes for this potential object? Title, author, publish date… Yep, it has structure. Check!
 
 
 
 Instance: what are some examples of this potential “book” object? Can you name a few? The Alchemist, Ready Player One, Everybody Poops… OK, check!
 
 
 
 Purpose: why is this object important to the users and business? Well, “book” is what our library client is providing to people and books are why people come to the library… Check, check, check!
 
 
 
 SIP: Structure, Instances, and Purpose! (Here’s a flowchart where I elaborate even more on SIP.)
 
 
 
 As you are noun foraging, focus on capturing the nouns that have SIP. Avoid capturing components like dropdowns, checkboxes, and calendar pickers—your UX system is not your design system! Components are just the packaging for objects—they are a means to an end. No one is coming to your digital place to play with your dropdown! They are coming for the VALUABLE THINGS and what they can do with them. Those things, or objects, are what we are trying to identify.
 
 
 
 Let’s say we work for a startup disrupting the email experience. This is how I’d start my noun foraging.
 
 
 
 First I’d look at my own email client, which happens to be Gmail. I’d then look at Outlook and the new HEY email. I’d look at Yahoo, Hotmail…I’d even look at Slack and Basecamp and other so-called “email replacers.” I’d read some articles, reviews, and forum threads where people are complaining about email. While doing all this, I would look for and write down the nouns.
 
 
 
 (Before moving on, feel free to go noun foraging for this hypothetical product, too, and then scroll down to see how much our lists match up. Just don’t get lost in your own emails! Come back to me!)
 
 
 
 Drumroll, please…
 
 
 
 Here are a few nouns I came up with during my noun foraging:
 
 
 
 email messagethreadcontactclientrule/automationemail address that is not a contact?contact groupsattachmentGoogle doc file / other integrated filenewsletter? (HEY treats this differently)saved responses and templates
 
 
 
 In the OOUX world, we love color-coding. Blue is reserved for objects! (Yellow is for core content, pink is for metadata, and green is for calls-to-action. Learn more about the color coded object map and connecting CTAs to objects.)
 
 
 
 Scan your list of nouns and pick out words that you are completely clueless about. In our email example, it might be client or automation. Do as much homework as you can before your session with stakeholders: google what’s googleable. But other terms might be so specific to the product or domain that you need to have a conversation about them.
 
 
 
 Aside: here are some real nouns foraged during my own past project work that I needed my stakeholders to help me understand:
 
 
 
 Record LocatorIncentive HomeAugmented Line ItemCurriculum-Based Measurement Probe
 
 
 
 This is really all you need to prepare for the workshop session: a list of nouns that represent potential objects and a short list of nouns that need to be defined further.
 
 
 
 Facilitate an Object Definition Workshop
 
 
 
 You could actually start your workshop with noun foraging—this activity can be done collaboratively. If you have five people in the room, pick five sources, assign one to every person, and give everyone ten minutes to find the objects within their source. When the time’s up, come together and find the overlap. Affinity mapping is your friend here!
 
 
 
 If your team is short on time and might be reluctant to do this kind of grunt work (which is usually the case) do your own noun foraging beforehand, but be prepared to show your work. I love presenting screenshots of documents and screens with all the nouns already highlighted. Bring the artifacts of your process, and start the workshop with a five-minute overview of your noun foraging journey.
 
 
 
 HOT TIP: before jumping into the workshop, frame the conversation as a requirements-gathering session to help you better understand the scope and details of the system. You don’t need to let them know that you’re looking for gaps in the team’s understanding so that you can prove the need for more user research—that will be our little secret. Instead, go into the session optimistically, as if your knowledgeable stakeholders and PMs and biz folks already have all the answers. 
 
 
 
 Then, let the question whack-a-mole commence.
 
 
 
 1. What is this thing?
 
 
 
 Want to have some real fun? At the beginning of your session, ask stakeholders to privately write definitions for the handful of obscure nouns you might be uncertain about. Then, have everyone show their cards at the same time and see if you get different definitions (you will). This is gold for exposing misalignment and starting great conversations.
 
 
 
 As your discussion unfolds, capture any agreed-upon definitions. And when uncertainty emerges, quietly (but visibly) start an “open questions” parking lot. 😉
 
 
 
 After definitions solidify, here’s a great follow-up:
 
 
 
 2. Do our users know what these things are? What do users call this thing?
 
 
 
 Stakeholder 1: They probably call email clients “apps.” But I’m not sure.
 
 
 
 Stakeholder 2: Automations are often called “workflows,” I think. Or, maybe users think workflows are something different.
 
 
 
 If a more user-friendly term emerges, ask the group if they can agree to use only that term moving forward. This way, the team can better align to the users’ language and mindset.
 
 
 
 OK, moving on. 
 
 
 
 If you have two or more objects that seem to overlap in purpose, ask one of these questions:
 
 
 
 3. Are these the same thing? Or are these different? If they are not the same, how are they different?
 
 
 
 You: Is a saved response the same as a template?
 
 
 
 Stakeholder 1: Yes! Definitely.
 
 
 
 Stakeholder 2: I don’t think so… A saved response is text with links and variables, but a template is more about the look and feel, like default fonts, colors, and placeholder images. 
 
 
 
 Continue to build out your growing glossary of objects. And continue to capture areas of uncertainty in your “open questions” parking lot.
 
 
 
 If you successfully determine that two similar things are, in fact, different, here’s your next follow-up question:
 
 
 
 4. What’s the relationship between these objects?
 
 
 
 You: Are saved responses and templates related in any way?
 
 
 
 Stakeholder 3:  Yeah, a template can be applied to a saved response.
 
 
 
 You, always with the follow-ups: When is the template applied to a saved response? Does that happen when the user is constructing the saved response? Or when they apply the saved response to an email? How does that actually work?
 
 
 
 Listen. Capture uncertainty. Once the list of “open questions” grows to a critical mass, pause to start assigning questions to groups or individuals. Some questions might be for the dev team (hopefully at least one developer is in the room with you). One question might be specifically for someone who couldn’t make it to the workshop. And many questions will need to be labeled “user.” 
 
 
 
 Do you see how we are building up to our UXR sales pitch?
 
 
 
 5. Is this object in scope?
 
 
 
 Your next question narrows the team’s focus toward what’s most important to your users. You can simply ask, “Are saved responses in scope for our first release?,” but I’ve got a better, more devious strategy.
 
 
 
 By now, you should have a list of clearly defined objects. Ask participants to sort these objects from most to least important, either in small breakout groups or individually. Then, like you did with the definitions, have everyone reveal their sort order at once. Surprisingly—or not so surprisingly—it’s not unusual for the VP to rank something like “saved responses” as #2 while everyone else puts it at the bottom of the list. Try not to look too smug as you inevitably expose more misalignment.
 
 
 
 I did this for a startup a few years ago. We posted the three groups’ wildly different sort orders on the whiteboard.
 
 
 
 Here’s a snippet of the very messy middle from this session: three columns of object cards, showing the same cards prioritized completely differently by three different groups.
 
 
 
 The CEO stood back, looked at it, and said, “This is why we haven’t been able to move forward in two years.”
 
 
 
 Admittedly, it’s tragic to hear that, but as a professional, it feels pretty awesome to be the one who facilitated a watershed realization.
 
 
 
 Once you have a good idea of in-scope, clearly defined things, this is when you move on to doing more relationship mapping.
 
 
 
 6. Create a visual representation of the objects’ relationships
 
 
 
 We’ve already done a bit of this while trying to determine if two things are different, but this time, ask the team about every potential relationship. For each object, ask how it relates to all the other objects. In what ways are the objects connected? To visualize all the connections, pull out your trusty boxes-and-arrows technique. Here, we are connecting our objects with verbs. I like to keep my verbs to simple “has a” and “has many” statements.
 
 
 
 A work-in-progress system model of our new email solution.
 
 
 
 This system modeling activity brings up all sorts of new questions:
 
 
 
 Can a saved response have attachments?Can a saved response use a template? If so, if an email uses a saved response with a template, can the user override that template?Do users want to see all the emails they sent that included a particular attachment? For example, “show me all the emails I sent with ProfessionalImage.jpg attached. I’ve changed my professional photo and I want to alert everyone to update it.” 
 
 
 
 Solid answers might emerge directly from the workshop participants. Great! Capture that new shared understanding. But when uncertainty surfaces, continue to add questions to your growing parking lot.
 
 
 
 Light the fuse
 
 
 
 You’ve positioned the explosives all along the floodgates. Now you simply have to light the fuse and BOOM. Watch the buy-in for user research flooooow.
 
 
 
 Before your workshop wraps up, have the group reflect on the list of open questions. Make plans for getting answers internally, then focus on the questions that need to be brought before users.
 
 
 
 Here’s your final step. Take those questions you’ve compiled for user research and discuss the level of risk associated with NOT answering them. Ask, “if we design without an answer to this question, if we make up our own answer and we are wrong, how bad might that turn out?” 
 
 
 
 With this methodology, we are cornering our decision-makers into advocating for user research as they themselves label questions as high-risk. Sorry, not sorry. 
 
 
 
 Now is your moment of truth. With everyone in the room, ask for a reasonable budget of time and money to conduct 6–8 user interviews focused specifically on these questions. 
 
 
 
 HOT TIP: if you are new to UX research, please note that you’ll likely need to rephrase the questions that came up during the workshop before you present them to users. Make sure your questions are open-ended and don’t lead the user into any default answers.
 
 
 
 Final words: Hold the screen design!
 
 
 
 Seriously, if at all possible, do not ever design screens again without first answering these fundamental questions: what are the objects and how do they relate?
 
 
 
 I promise you this: if you can secure a shared understanding between the business, design, and development teams before you start designing screens, you will have less heartache and save more time and money, and (it almost feels like a bonus at this point!) users will be more receptive to what you put out into the world. 
 
 
 
 I sincerely hope this helps you win time and budget to go talk to your users and gain clarity on what you are designing before you start building screens. If you find success using noun foraging and the Object Definition Workshop, there’s more where that came from in the rest of the ORCA process, which will help prevent even more late-in-the-game scope tugs-of-war and strategy pivots. 
 
 
 
 All the best of luck! Now go sell research!
 </content>
     </entry>
     <entry>
       <title>A Content Model Is Not a Design System</title>
         <link href="https://alistapart.com/article/a-content-model-is-not-a-design-system/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">Do you remember when having a great website was enough? Now, people are getting answers from Siri, Google search snippets, and mobile apps, not just our websites. Forward-thinking organizations have adopted an omnichannel content strategy, whose mission is to reach audiences across multiple digital channels and platforms.
 
 
 
 But how do you set up a content management system (CMS) to reach your audience now and in the future? I learned the hard way that creating a content model—a definition of content types, attributes, and relationships that let people and systems understand content—with my more familiar design-system thinking would capsize my customer’s omnichannel content strategy. You can avoid that outcome by creating content models that are semantic and that also connect related content. 
 
 
 
 I recently had the opportunity to lead the CMS implementation for a Fortune 500 company. The client was excited by the benefits of an omnichannel content strategy, including content reuse, multichannel marketing, and robot delivery—designing content to be intelligible to bots, Google knowledge panels, snippets, and voice user interfaces. 
 
 
 
 A content model is a critical foundation for an omnichannel content strategy, and for our content to be understood by multiple systems, the model needed semantic types—types named according to their meaning instead of their presentation. Our goal was to let authors create content and reuse it wherever it was relevant. But as the project proceeded, I realized that supporting content reuse at the scale that my customer needed required the whole team to recognize a new pattern.
 
 
 
 Despite our best intentions, we kept drawing from what we were more familiar with: design systems. Unlike web-focused content strategies, an omnichannel content strategy can’t rely on WYSIWYG tools for design and layout. Our tendency to approach the content model with our familiar design-system thinking constantly led us to veer away from one of the primary purposes of a content model: delivering content to audiences on multiple marketing channels.
 
 
 
 Two essential principles for an effective content model
 
 
 
 We needed to help our designers, developers, and stakeholders understand that we were doing something very different from their prior web projects, where it was natural for everyone to think about content as visual building blocks fitting into layouts. The previous approach was not only more familiar but also more intuitive—at least at first—because it made the designs feel more tangible. We discovered two principles that helped the team understand how a content model differs from the design systems that we were used to:
 
 
 
 Content models must define semantics instead of layout.And content models should connect content that belongs together.
 
 
 
 Semantic content models
 
 
 
 A semantic content model uses type and attribute names that reflect the meaning of the content, not how it will be displayed. For example, in a nonsemantic model, teams might create types like teasers, media blocks, and cards. Although these types might make it easy to lay out content, they don’t help delivery channels understand the content’s meaning, which in turn would have opened the door to the content being presented in each marketing channel. In contrast, a semantic content model uses type names like product, service, and testimonial so that each delivery channel can understand the content and use it as it sees fit. 
 
 
 
 When you’re creating a semantic content model, a great place to start is to look over the types and properties defined by Schema.org, a community-driven resource for type definitions that are intelligible to platforms like Google search.
 
 
 
 A semantic content model has several benefits:
 
 
 
 Even if your team doesn’t care about omnichannel content, a semantic content model decouples content from its presentation so that teams can evolve the website’s design without needing to refactor its content. In this way, content can withstand disruptive website redesigns. A semantic content model also provides a competitive edge. By adding structured data based on Schema.org’s types and properties, a website can provide hints to help Google understand the content, display it in search snippets or knowledge panels, and use it to answer voice-interface user questions. Potential visitors could discover your content without ever setting foot in your website.Beyond those practical benefits, you’ll also need a semantic content model if you want to deliver omnichannel content. To use the same content in multiple marketing channels, delivery channels need to be able to understand it. For example, if your content model were to provide a list of questions and answers, it could easily be rendered on a frequently asked questions (FAQ) page, but it could also be used in a voice interface or by a bot that answers common questions.
 
 
 
 For example, using a semantic content model for articles, events, people, and locations lets A List Apart provide cleanly structured data for search engines so that users can read the content on the website, in Google knowledge panels, and even with hypothetical voice interfaces in the future.
 
 
 
 
 
 
 
 Content models that connect
 
 
 
 After struggling to describe what makes a good content model, I’ve come to realize that the best models are those that are semantic and that also connect related content components (such as a FAQ item’s question and answer pair), instead of slicing up related content across disparate content components. A good content model connects content that should remain together so that multiple delivery channels can use it without needing to first put those pieces back together.
 
 
 
 Think about writing an article or essay. An article’s meaning and usefulness depends upon its parts being kept together. Would one of the headings or paragraphs be meaningful on their own without the context of the full article? On our project, our familiar design-system thinking often led us to want to create content models that would slice content into disparate chunks to fit the web-centric layout. This had a similar impact to an article that were to have been separated from its headline. Because we were slicing content into standalone pieces based on layout, content that belonged together became difficult to manage and nearly impossible for multiple delivery channels to understand.
 
 
 
 To illustrate, let’s look at how connecting related content applies in a real-world scenario. The design team for our customer presented a complex layout for a software product page that included multiple tabs and sections. Our instincts were to follow suit with the content model. Shouldn’t we make it as easy and as flexible as possible to add any number of tabs in the future?
 
 
 
 Because our design-system instincts were so familiar, it felt like we had needed a content type called “tab section” so that multiple tab sections could be added to a page. Each tab section would display various types of content. One tab might provide the software’s overview or its specifications. Another tab might provide a list of resources. 
 
 
 
 Our inclination to break down the content model into “tab section” pieces would have led to an unnecessarily complex model and a cumbersome editing experience, and it would have also created content that couldn’t have been understood by additional delivery channels. For example, how would another system have been able to tell which “tab section” referred to a product’s specifications or its resource list—would that other system have to have resorted to counting tab sections and content blocks? This would have prevented the tabs from ever being reordered, and it would have required adding logic in every other delivery channel to interpret the design system’s layout. Furthermore, if the customer were to have no longer wanted to display this content in a tab layout, it would have been tedious to migrate to a new content model to reflect the new page redesign.
 
 
 
 A content model based on design components is unnecessarily complex, and it’s unintelligible to systems.
 
 
 
 We had a breakthrough when we discovered that our customer had a specific purpose in mind for each tab: it would reveal specific information such as the software product’s overview, specifications, related resources, and pricing. Once implementation began, our inclination to focus on what’s visual and familiar had obscured the intent of the designs. With a little digging, it didn’t take long to realize that the concept of tabs wasn’t relevant to the content model. The meaning of the content that they were planning to display in the tabs was what mattered.
 
 
 
 In fact, the customer could have decided to display this content in a different way—without tabs—somewhere else. This realization prompted us to define content types for the software product based on the meaningful attributes that the customer had wanted to render on the web. There were obvious semantic attributes like name and description as well as rich attributes like screenshots, software requirements, and feature lists. The software’s product information stayed together because it wasn’t sliced across separate components like “tab sections” that were derived from the content’s presentation. Any delivery channel—including future ones—could understand and present this content.
 
 
 
 A good content model connects content that belongs together so it can be easily managed and reused.
 
 
 
 Conclusion
 
 
 
 In this omnichannel marketing project, we discovered that the best way to keep our content model on track was to ensure that it was semantic (with type and attribute names that reflected the meaning of the content) and that it kept content together that belonged together (instead of fragmenting it). These two concepts curtailed our temptation to shape the content model based on the design. So if you’re working on a content model to support an omnichannel content strategy—or even if you just want to make sure that Google and other interfaces understand your content—remember:
 
 
 
 A design system isn’t a content model. Team members may be tempted to conflate them and to make your content model mirror your design system, so you should protect the semantic value and contextual structure of the content strategy during the entire implementation process. This will let every delivery channel consume the content without needing a magic decoder ring.If your team is struggling to make this transition, you can still reap some of the benefits by using Schema.org–based structured data in your website. Even if additional delivery channels aren’t on the immediate horizon, the benefit to search engine optimization is a compelling reason on its own.Additionally, remind the team that decoupling the content model from the design will let them update the designs more easily because they won’t be held back by the cost of content migrations. They’ll be able to create new designs without the obstacle of compatibility between the design and the content, and ​they’ll be ready for the next big thing. 
 
 
 
 By rigorously advocating for these principles, you’ll help your team treat content the way that it deserves—as the most critical asset in your user experience and the best way to connect with your audience.
 </content>
     </entry>
     <entry>
       <title>Design for Safety, An Excerpt</title>
         <link href="https://alistapart.com/article/design-for-safety-excerpt/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">Antiracist economist Kim Crayton says that “intention without strategy is chaos.” We’ve discussed how our biases, assumptions, and inattention toward marginalized and vulnerable groups lead to dangerous and unethical tech—but what, specifically, do we need to do to fix it? The intention to make our tech safer is not enough; we need a strategy.
 
 
 
 This chapter will equip you with that plan of action. It covers how to integrate safety principles into your design work in order to create tech that’s safe, how to convince your stakeholders that this work is necessary, and how to respond to the critique that what we actually need is more diversity. (Spoiler: we do, but diversity alone is not the antidote to fixing unethical, unsafe tech.)
 
 
 
 The process for inclusive safety
 
 
 
 When you are designing for safety, your goals are to:
 
 
 
 identify ways your product can be used for abuse,design ways to prevent the abuse, andprovide support for vulnerable users to reclaim power and control.
 
 
 
 The Process for Inclusive Safety is a tool to help you reach those goals (Fig 5.1). It’s a methodology I created in 2018 to capture the various techniques I was using when designing products with safety in mind. Whether you are creating an entirely new product or adding to an existing feature, the Process can help you make your product safe and inclusive. The Process includes five general areas of action:
 
 
 
 Conducting researchCreating archetypesBrainstorming problemsDesigning solutionsTesting for safety
 
 
 
 Fig 5.1: Each aspect of the Process for Inclusive Safety can be incorporated into your design process where it makes the most sense for you. The times given are estimates to help you incorporate the stages into your design plan.
 
 
 
 The Process is meant to be flexible—it won’t make sense for teams to implement every step in some situations. Use the parts that are relevant to your unique work and context; this is meant to be something you can insert into your existing design practice.
 
 
 
 And once you use it, if you have an idea for making it better or simply want to provide context of how it helped your team, please get in touch with me. It’s a living document that I hope will continue to be a useful and realistic tool that technologists can use in their day-to-day work.
 
 
 
 If you’re working on a product specifically for a vulnerable group or survivors of some form of trauma, such as an app for survivors of domestic violence, sexual assault, or drug addiction, be sure to read Chapter 7, which covers that situation explicitly and should be handled a bit differently. The guidelines here are for prioritizing safety when designing a more general product that will have a wide user base (which, we already know from statistics, will include certain groups that should be protected from harm). Chapter 7 is focused on products that are specifically for vulnerable groups and people who have experienced trauma.
 
 
 
 Step 1: Conduct research
 
 
 
 Design research should include a broad analysis of how your tech might be weaponized for abuse as well as specific insights into the experiences of survivors and perpetrators of that type of abuse. At this stage, you and your team will investigate issues of interpersonal harm and abuse, and explore any other safety, security, or inclusivity issues that might be a concern for your product or service, like data security, racist algorithms, and harassment.
 
 
 
 Broad research
 
 
 
 Your project should begin with broad, general research into similar products and issues around safety and ethical concerns that have already been reported. For example, a team building a smart home device would do well to understand the multitude of ways that existing smart home devices have been used as tools of abuse. If your product will involve AI, seek to understand the potentials for racism and other issues that have been reported in existing AI products. Nearly all types of technology have some kind of potential or actual harm that’s been reported on in the news or written about by academics. Google Scholar is a useful tool for finding these studies.
 
 
 
 Specific research: Survivors
 
 
 
 When possible and appropriate, include direct research (surveys and interviews) with people who are experts in the forms of harm you have uncovered. Ideally, you’ll want to interview advocates working in the space of your research first so that you have a more solid understanding of the topic and are better equipped to not retraumatize survivors. If you’ve uncovered possible domestic violence issues, for example, the experts you’ll want to speak with are survivors themselves, as well as workers at domestic violence hotlines, shelters, other related nonprofits, and lawyers.
 
 
 
 Especially when interviewing survivors of any kind of trauma, it is important to pay people for their knowledge and lived experiences. Don’t ask survivors to share their trauma for free, as this is exploitative. While some survivors may not want to be paid, you should always make the offer in the initial ask. An alternative to payment is to donate to an organization working against the type of violence that the interviewee experienced. We’ll talk more about how to appropriately interview survivors in Chapter 6.
 
 
 
 Specific research: Abusers
 
 
 
 It’s unlikely that teams aiming to design for safety will be able to interview self-proclaimed abusers or people who have broken laws around things like hacking. Don’t make this a goal; rather, try to get at this angle in your general research. Aim to understand how abusers or bad actors weaponize technology to use against others, how they cover their tracks, and how they explain or rationalize the abuse.
 
 
 
 Step 2: Create archetypes
 
 
 
 Once you’ve finished conducting your research, use your insights to create abuser and survivor archetypes. Archetypes are not personas, as they’re not based on real people that you interviewed and surveyed. Instead, they’re based on your research into likely safety issues, much like when we design for accessibility: we don’t need to have found a group of blind or low-vision users in our interview pool to create a design that’s inclusive of them. Instead, we base those designs on existing research into what this group needs. Personas typically represent real users and include many details, while archetypes are broader and can be more generalized.
 
 
 
 The abuser archetype is someone who will look at the product as a tool to perform harm (Fig 5.2). They may be trying to harm someone they don’t know through surveillance or anonymous harassment, or they may be trying to control, monitor, abuse, or torment someone they know personally.
 
 
 
 Fig 5.2: Harry Oleson, an abuser archetype for a fitness product, is looking for ways to stalk his ex-girlfriend through the fitness apps she uses.
 
 
 
 The survivor archetype is someone who is being abused with the product. There are various situations to consider in terms of the archetype’s understanding of the abuse and how to put an end to it: Do they need proof of abuse they already suspect is happening, or are they unaware they’ve been targeted in the first place and need to be alerted (Fig 5.3)?
 
 
 
 Fig 5.3: The survivor archetype Lisa Zwaan suspects her husband is weaponizing their home’s IoT devices against her, but in the face of his insistence that she simply doesn’t understand how to use the products, she’s unsure. She needs some kind of proof of the abuse.
 
 
 
 You may want to make multiple survivor archetypes to capture a range of different experiences. They may know that the abuse is happening but not be able to stop it, like when an abuser locks them out of IoT devices; or they know it’s happening but don’t know how, such as when a stalker keeps figuring out their location (Fig 5.4). Include as many of these scenarios as you need to in your survivor archetype. You’ll use these later on when you design solutions to help your survivor archetypes achieve their goals of preventing and ending abuse.
 
 
 
 Fig 5.4: The survivor archetype Eric Mitchell knows he’s being stalked by his ex-boyfriend Rob but can’t figure out how Rob is learning his location information.
 
 
 
 It may be useful for you to create persona-like artifacts for your archetypes, such as the three examples shown. Instead of focusing on the demographic information we often see in personas, focus on their goals. The goals of the abuser will be to carry out the specific abuse you’ve identified, while the goals of the survivor will be to prevent abuse, understand that abuse is happening, make ongoing abuse stop, or regain control over the technology that’s being used for abuse. Later, you’ll brainstorm how to prevent the abuser’s goals and assist the survivor’s goals.
 
 
 
 And while the “abuser/survivor” model fits most cases, it doesn’t fit all, so modify it as you need to. For example, if you uncovered an issue with security, such as the ability for someone to hack into a home camera system and talk to children, the malicious hacker would get the abuser archetype and the child’s parents would get survivor archetype.
 
 
 
 Step 3: Brainstorm problems
 
 
 
 After creating archetypes, brainstorm novel abuse cases and safety issues. “Novel” means things not found in your research; you’re trying to identify completely new safety issues that are unique to your product or service. The goal with this step is to exhaust every effort of identifying harms your product could cause. You aren’t worrying about how to prevent the harm yet—that comes in the next step.
 
 
 
 How could your product be used for any kind of abuse, outside of what you’ve already identified in your research? I recommend setting aside at least a few hours with your team for this process.
 
 
 
 If you’re looking for somewhere to start, try doing a Black Mirror brainstorm. This exercise is based on the show Black Mirror, which features stories about the dark possibilities of technology. Try to figure out how your product would be used in an episode of the show—the most wild, awful, out-of-control ways it could be used for harm. When I’ve led Black Mirror brainstorms, participants usually end up having a good deal of fun (which I think is great—it’s okay to have fun when designing for safety!). I recommend time-boxing a Black Mirror brainstorm to half an hour, and then dialing it back and using the rest of the time thinking of more realistic forms of harm.
 
 
 
 After you’ve identified as many opportunities for abuse as possible, you may still not feel confident that you’ve uncovered every potential form of harm. A healthy amount of anxiety is normal when you’re doing this kind of work. It’s common for teams designing for safety to worry, “Have we really identified every possible harm? What if we’ve missed something?” If you’ve spent at least four hours coming up with ways your product could be used for harm and have run out of ideas, go to the next step.
 
 
 
 It’s impossible to guarantee you’ve thought of everything; instead of aiming for 100 percent assurance, recognize that you’ve taken this time and have done the best you can, and commit to continuing to prioritize safety in the future. Once your product is released, your users may identify new issues that you missed; aim to receive that feedback graciously and course-correct quickly.
 
 
 
 Step 4: Design solutions
 
 
 
 At this point, you should have a list of ways your product can be used for harm as well as survivor and abuser archetypes describing opposing user goals. The next step is to identify ways to design against the identified abuser’s goals and to support the survivor’s goals. This step is a good one to insert alongside existing parts of your design process where you’re proposing solutions for the various problems your research uncovered.
 
 
 
 Some questions to ask yourself to help prevent harm and support your archetypes include:
 
 
 
 Can you design your product in such a way that the identified harm cannot happen in the first place? If not, what roadblocks can you put up to prevent the harm from happening?How can you make the victim aware that abuse is happening through your product?How can you help the victim understand what they need to do to make the problem stop?Can you identify any types of user activity that would indicate some form of harm or abuse? Could your product help the user access support?
 
 
 
 In some products, it’s possible to proactively recognize that harm is happening. For example, a pregnancy app might be modified to allow the user to report that they were the victim of an assault, which could trigger an offer to receive resources for local and national organizations. This sort of proactiveness is not always possible, but it’s worth taking a half hour to discuss if any type of user activity would indicate some form of harm or abuse, and how your product could assist the user in receiving help in a safe manner.
 
 
 
 That said, use caution: you don’t want to do anything that could put a user in harm’s way if their devices are being monitored. If you do offer some kind of proactive help, always make it voluntary, and think through other safety issues, such as the need to keep the user in-app in case an abuser is checking their search history. We’ll walk through a good example of this in the next chapter.
 
 
 
 Step 5: Test for safety
 
 
 
 The final step is to test your prototypes from the point of view of your archetypes: the person who wants to weaponize the product for harm and the victim of the harm who needs to regain control over the technology. Just like any other kind of product testing, at this point you’ll aim to rigorously test out your safety solutions so that you can identify gaps and correct them, validate that your designs will help keep your users safe, and feel more confident releasing your product into the world.
 
 
 
 Ideally, safety testing happens along with usability testing. If you’re at a company that doesn’t do usability testing, you might be able to use safety testing to cleverly perform both; a user who goes through your design attempting to weaponize the product against someone else can also be encouraged to point out interactions or other elements of the design that don’t make sense to them.
 
 
 
 You’ll want to conduct safety testing on either your final prototype or the actual product if it’s already been released. There’s nothing wrong with testing an existing product that wasn’t designed with safety goals in mind from the onset—“retrofitting” it for safety is a good thing to do.
 
 
 
 Remember that testing for safety involves testing from the perspective of both an abuser and a survivor, though it may not make sense for you to do both. Alternatively, if you made multiple survivor archetypes to capture multiple scenarios, you’ll want to test from the perspective of each one.
 
 
 
 As with other sorts of usability testing, you as the designer are most likely too close to the product and its design by this point to be a valuable tester; you know the product too well. Instead of doing it yourself, set up testing as you would with other usability testing: find someone who is not familiar with the product and its design, set the scene, give them a task, encourage them to think out loud, and observe how they attempt to complete it.
 
 
 
 Abuser testing
 
 
 
 The goal of this testing is to understand how easy it is for someone to weaponize your product for harm. Unlike with usability testing, you want to make it impossible, or at least difficult, for them to achieve their goal. Reference the goals in the abuser archetype you created earlier, and use your product in an attempt to achieve them.
 
 
 
 For example, for a fitness app with GPS-enabled location features, we can imagine that the abuser archetype would have the goal of figuring out where his ex-girlfriend now lives. With this goal in mind, you’d try everything possible to figure out the location of another user who has their privacy settings enabled. You might try to see her running routes, view any available information on her profile, view anything available about her location (which she has set to private), and investigate the profiles of any other users somehow connected with her account, such as her followers.
 
 
 
 If by the end of this you’ve managed to uncover some of her location data, despite her having set her profile to private, you know now that your product enables stalking. Your next step is to go back to step 4 and figure out how to prevent this from happening. You may need to repeat the process of designing solutions and testing them more than once.
 
 
 
 Survivor testing
 
 
 
 Survivor testing involves identifying how to give information and power to the survivor. It might not always make sense based on the product or context. Thwarting the attempt of an abuser archetype to stalk someone also satisfies the goal of the survivor archetype to not be stalked, so separate testing wouldn’t be needed from the survivor’s perspective.
 
 
 
 However, there are cases where it makes sense. For example, for a smart thermostat, a survivor archetype’s goals would be to understand who or what is making the temperature change when they aren’t doing it themselves. You could test this by looking for the thermostat’s history log and checking for usernames, actions, and times; if you couldn’t find that information, you would have more work to do in step 4.
 
 
 
 Another goal might be regaining control of the thermostat once the survivor realizes the abuser is remotely changing its settings. Your test would involve attempting to figure out how to do this: are there instructions that explain how to remove another user and change the password, and are they easy to find? This might again reveal that more work is needed to make it clear to the user how they can regain control of the device or account.
 
 
 
 Stress testing
 
 
 
 To make your product more inclusive and compassionate, consider adding stress testing. This concept comes from Design for Real Life by Eric Meyer and Sara Wachter-Boettcher. The authors pointed out that personas typically center people who are having a good day—but real users are often anxious, stressed out, having a bad day, or even experiencing tragedy. These are called “stress cases,” and testing your products for users in stress-case situations can help you identify places where your design lacks compassion. Design for Real Life has more details about what it looks like to incorporate stress cases into your design as well as many other great tactics for compassionate design.
 </content>
     </entry>
     <entry>
       <title>Sustainable Web Design, An Excerpt</title>
         <link href="https://alistapart.com/article/sustainable-web-design-excerpt/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">In the 1950s, many in the elite running community had begun to believe it wasn’t possible to run a mile in less than four minutes. Runners had been attempting it since the late 19th century and were beginning to draw the conclusion that the human body simply wasn’t built for the task. 
 
 
 
 But on May 6, 1956, Roger Bannister took everyone by surprise. It was a cold, wet day in Oxford, England—conditions no one expected to lend themselves to record-setting—and yet Bannister did just that, running a mile in 3:59.4 and becoming the first person in the record books to run a mile in under four minutes. 
 
 
 
 This shift in the benchmark had profound effects; the world now knew that the four-minute mile was possible. Bannister’s record lasted only forty-six days, when it was snatched away by Australian runner John Landy. Then a year later, three runners all beat the four-minute barrier together in the same race. Since then, over 1,400 runners have officially run a mile in under four minutes; the current record is 3:43.13, held by Moroccan athlete Hicham El Guerrouj.
 
 
 
 We achieve far more when we believe that something is possible, and we will believe it’s possible only when we see someone else has already done it—and as with human running speed, so it is with what we believe are the hard limits for how a website needs to perform.
 
 
 
 Establishing standards for a sustainable web
 
 
 
 In most major industries, the key metrics of environmental performance are fairly well established, such as miles per gallon for cars or energy per square meter for homes. The tools and methods for calculating those metrics are standardized as well, which keeps everyone on the same page when doing environmental assessments. In the world of websites and apps, however, we aren’t held to any particular environmental standards, and only recently have gained the tools and methods we need to even make an environmental assessment.
 
 
 
 The primary goal in sustainable web design is to reduce carbon emissions. However, it’s almost impossible to actually measure the amount of CO2 produced by a web product. We can’t measure the fumes coming out of the exhaust pipes on our laptops. The emissions of our websites are far away, out of sight and out of mind, coming out of power stations burning coal and gas. We have no way to trace the electrons from a website or app back to the power station where the electricity is being generated and actually know the exact amount of greenhouse gas produced. So what do we do? 
 
 
 
 If we can’t measure the actual carbon emissions, then we need to find what we can measure. The primary factors that could be used as indicators of carbon emissions are:
 
 
 
 Data transfer Carbon intensity of electricity
 
 
 
 Let’s take a look at how we can use these metrics to quantify the energy consumption, and in turn the carbon footprint, of the websites and web apps we create.
 
 
 
 Data transfer
 
 
 
 Most researchers use kilowatt-hours per gigabyte (kWh/GB) as a metric of energy efficiency when measuring the amount of data transferred over the internet when a website or application is used. This provides a great reference point for energy consumption and carbon emissions. As a rule of thumb, the more data transferred, the more energy used in the data center, telecoms networks, and end user devices.
 
 
 
 For web pages, data transfer for a single visit can be most easily estimated by measuring the page weight, meaning the transfer size of the page in kilobytes the first time someone visits the page. It’s fairly easy to measure using the developer tools in any modern web browser. Often your web hosting account will include statistics for the total data transfer of any web application (Fig 2.1).
 
 
 
 Fig 2.1: The Kinsta hosting dashboard displays data transfer alongside traffic volumes. If you divide data transfer by visits, you get the average data per visit, which can be used as a metric of efficiency.
 
 
 
 The nice thing about page weight as a metric is that it allows us to compare the efficiency of web pages on a level playing field without confusing the issue with constantly changing traffic volumes. 
 
 
 
 Reducing page weight requires a large scope. By early 2020, the median page weight was 1.97 MB for setups the HTTP Archive classifies as “desktop” and 1.77 MB for “mobile,” with desktop increasing 36 percent since January 2016 and mobile page weights nearly doubling in the same period (Fig 2.2). Roughly half of this data transfer is image files, making images the single biggest source of carbon emissions on the average website. 
 
 
 
 History clearly shows us that our web pages can be smaller, if only we set our minds to it. While most technologies become ever more energy efficient, including the underlying technology of the web such as data centers and transmission networks, websites themselves are a technology that becomes less efficient as time goes on.
 
 
 
 Fig 2.2: The historical page weight data from HTTP Archive can teach us a lot about what is possible in the future.
 
 
 
 You might be familiar with the concept of performance budgeting as a way of focusing a project team on creating faster user experiences. For example, we might specify that the website must load in a maximum of one second on a broadband connection and three seconds on a 3G connection. Much like speed limits while driving, performance budgets are upper limits rather than vague suggestions, so the goal should always be to come in under budget.
 
 
 
 Designing for fast performance does often lead to reduced data transfer and emissions, but it isn’t always the case. Web performance is often more about the subjective perception of load times than it is about the true efficiency of the underlying system, whereas page weight and transfer size are more objective measures and more reliable benchmarks for sustainable web design. 
 
 
 
 We can set a page weight budget in reference to a benchmark of industry averages, using data from sources like HTTP Archive. We can also benchmark page weight against competitors or the old version of the website we’re replacing. For example, we might set a maximum page weight budget as equal to our most efficient competitor, or we could set the benchmark lower to guarantee we are best in class. 
 
 
 
 If we want to take it to the next level, then we could also start looking at the transfer size of our web pages for repeat visitors. Although page weight for the first time someone visits is the easiest thing to measure, and easy to compare on a like-for-like basis, we can learn even more if we start looking at transfer size in other scenarios too. For example, visitors who load the same page multiple times will likely have a high percentage of the files cached in their browser, meaning they don’t need to transfer all of the files on subsequent visits. Likewise, a visitor who navigates to new pages on the same website will likely not need to load the full page each time, as some global assets from areas like the header and footer may already be cached in their browser. Measuring transfer size at this next level of detail can help us learn even more about how we can optimize efficiency for users who regularly visit our pages, and enable us to set page weight budgets for additional scenarios beyond the first visit.
 
 
 
 Page weight budgets are easy to track throughout a design and development process. Although they don’t actually tell us carbon emission and energy consumption analytics directly, they give us a clear indication of efficiency relative to other websites. And as transfer size is an effective analog for energy consumption, we can actually use it to estimate energy consumption too.
 
 
 
 In summary, reduced data transfer translates to energy efficiency, a key factor to reducing carbon emissions of web products. The more efficient our products, the less electricity they use, and the less fossil fuels need to be burned to produce the electricity to power them. But as we’ll see next, since all web products demand some power, it’s important to consider the source of that electricity, too.
 
 
 
 Carbon intensity of electricity
 
 
 
 Regardless of energy efficiency, the level of pollution caused by digital products depends on the carbon intensity of the energy being used to power them. Carbon intensity is a term used to define the grams of CO2 produced for every kilowatt-hour of electricity (gCO2/kWh). This varies widely, with renewable energy sources and nuclear having an extremely low carbon intensity of less than 10 gCO2/kWh (even when factoring in their construction); whereas fossil fuels have very high carbon intensity of approximately 200–400 gCO2/kWh. 
 
 
 
 Most electricity comes from national or state grids, where energy from a variety of different sources is mixed together with varying levels of carbon intensity. The distributed nature of the internet means that a single user of a website or app might be using energy from multiple different grids simultaneously; a website user in Paris uses electricity from the French national grid to power their home internet and devices, but the website’s data center could be in Dallas, USA, pulling electricity from the Texas grid, while the telecoms networks use energy from everywhere between Dallas and Paris.
 
 
 
 We don’t have control over the full energy supply of web services, but we do have some control over where we host our projects. With a data center using a significant proportion of the energy of any website, locating the data center in an area with low carbon energy will tangibly reduce its carbon emissions. Danish startup Tomorrow reports and maps this user-contributed data, and a glance at their map shows how, for example, choosing a data center in France will have significantly lower carbon emissions than a data center in the Netherlands (Fig 2.3).
 
 
 
 Fig 2.3: Tomorrow’s electricityMap shows live data for the carbon intensity of electricity by country.
 
 
 
 That said, we don’t want to locate our servers too far away from our users; it takes energy to transmit data through the telecom’s networks, and the further the data travels, the more energy is consumed. Just like food miles, we can think of the distance from the data center to the website’s core user base as “megabyte miles”—and we want it to be as small as possible.
 
 
 
 Using the distance itself as a benchmark, we can use website analytics to identify the country, state, or even city where our core user group is located and measure the distance from that location to the data center used by our hosting company. This will be a somewhat fuzzy metric as we don’t know the precise center of mass of our users or the exact location of a data center, but we can at least get a rough idea. 
 
 
 
 For example, if a website is hosted in London but the primary user base is on the West Coast of the USA, then we could look up the distance from London to San Francisco, which is 5,300 miles. That’s a long way! We can see that hosting it somewhere in North America, ideally on the West Coast, would significantly reduce the distance and thus the energy used to transmit the data. In addition, locating our servers closer to our visitors helps reduce latency and delivers better user experience, so it’s a win-win.
 
 
 
 Converting it back to carbon emissions
 
 
 
 If we combine carbon intensity with a calculation for energy consumption, we can calculate the carbon emissions of our websites and apps. A tool my team created does this by measuring the data transfer over the wire when loading a web page, calculating the amount of electricity associated, and then converting that into a figure for CO2 (Fig 2.4). It also factors in whether or not the web hosting is powered by renewable energy.
 
 
 
 If you want to take it to the next level and tailor the data more accurately to the unique aspects of your project, the Energy and Emissions Worksheet accompanying this book shows you how.
 
 
 
 Fig 2.4: The Website Carbon Calculator shows how the Riverford Organic website embodies their commitment to sustainability, being both low carbon and hosted in a data center using renewable energy.
 
 
 
 With the ability to calculate carbon emissions for our projects, we could actually take a page weight budget one step further and set carbon budgets as well. CO2 is not a metric commonly used in web projects; we’re more familiar with kilobytes and megabytes, and can fairly easily look at design options and files to assess how big they are. Translating that into carbon adds a layer of abstraction that isn’t as intuitive—but carbon budgets do focus our minds on the primary thing we’re trying to reduce, and support the core objective of sustainable web design: reducing carbon emissions.
 
 
 
 Browser Energy
 
 
 
 Data transfer might be the simplest and most complete analog for energy consumption in our digital projects, but by giving us one number to represent the energy used in the data center, the telecoms networks, and the end user’s devices, it can’t offer us insights into the efficiency in any specific part of the system.
 
 
 
 One part of the system we can look at in more detail is the energy used by end users’ devices. As front-end web technologies become more advanced, the computational load is increasingly moving from the data center to users’ devices, whether they be phones, tablets, laptops, desktops, or even smart TVs. Modern web browsers allow us to implement more complex styling and animation on the fly using CSS and JavaScript. Furthermore, JavaScript libraries such as Angular and React allow us to create applications where the “thinking” work is done partly or entirely in the browser. 
 
 
 
 All of these advances are exciting and open up new possibilities for what the web can do to serve society and create positive experiences. However, more computation in the user’s web browser means more energy used by their devices. This has implications not just environmentally, but also for user experience and inclusivity. Applications that put a heavy processing load on the user’s device can inadvertently exclude users with older, slower devices and cause batteries on phones and laptops to drain faster. Furthermore, if we build web applications that require the user to have up-to-date, powerful devices, people throw away old devices much more frequently. This isn’t just bad for the environment, but it puts a disproportionate financial burden on the poorest in society.
 
 
 
 In part because the tools are limited, and partly because there are so many different models of devices, it’s difficult to measure website energy consumption on end users’ devices. One tool we do currently have is the Energy Impact monitor inside the developer console of the Safari browser (Fig 2.5).
 
 
 
 Fig 2.5: The Energy Impact meter in Safari (on the right) shows how a website consumes CPU energy.
 
 
 
 You know when you load a website and your computer’s cooling fans start spinning so frantically you think it might actually take off? That’s essentially what this tool is measuring. 
 
 
 
 It shows us the percentage of CPU used and the duration of CPU usage when loading the web page, and uses these figures to generate an energy impact rating. It doesn’t give us precise data for the amount of electricity used in kilowatts, but the information it does provide can be used to benchmark how efficiently your websites use energy and set targets for improvement.
 </content>
     </entry>
     <entry>
       <title>Voice Content and Usability</title>
         <link href="https://alistapart.com/article/voice-content-and-usability/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">We’ve been having conversations for thousands of years. Whether to convey information, conduct transactions, or simply to check in on one another, people have yammered away, chattering and gesticulating, through spoken conversation for countless generations. Only in the last few millennia have we begun to commit our conversations to writing, and only in the last few decades have we begun to outsource them to the computer, a machine that shows much more affinity for written correspondence than for the slangy vagaries of spoken language.
 
 
 
 Computers have trouble because between spoken and written language, speech is more primordial. To have successful conversations with us, machines must grapple with the messiness of human speech: the disfluencies and pauses, the gestures and body language, and the variations in word choice and spoken dialect that can stymie even the most carefully crafted human-computer interaction. In the human-to-human scenario, spoken language also has the privilege of face-to-face contact, where we can readily interpret nonverbal social cues.
 
 
 
 In contrast, written language immediately concretizes as we commit it to record and retains usages long after they become obsolete in spoken communication (the salutation “To whom it may concern,” for example), generating its own fossil record of outdated terms and phrases. Because it tends to be more consistent, polished, and formal, written text is fundamentally much easier for machines to parse and understand.
 
 
 
 Spoken language has no such luxury. Besides the nonverbal cues that decorate conversations with emphasis and emotional context, there are also verbal cues and vocal behaviors that modulate conversation in nuanced ways: how something is said, not what. Whether rapid-fire, low-pitched, or high-decibel, whether sarcastic, stilted, or sighing, our spoken language conveys much more than the written word could ever muster. So when it comes to voice interfaces—the machines we conduct spoken conversations with—we face exciting challenges as designers and content strategists.
 
 
 
 Voice Interactions
 
 
 
 We interact with voice interfaces for a variety of reasons, but according to Michael McTear, Zoraida Callejas, and David Griol in The Conversational Interface, those motivations by and large mirror the reasons we initiate conversations with other people, too (http://bkaprt.com/vcu36/01-01). Generally, we start up a conversation because:
 
 
 
 we need something done (such as a transaction),we want to know something (information of some sort), orwe are social beings and want someone to talk to (conversation for conversation’s sake).
 
 
 
 These three categories—which I call transactional, informational, and prosocial—also characterize essentially every voice interaction: a single conversation from beginning to end that realizes some outcome for the user, starting with the voice interface’s first greeting and ending with the user exiting the interface. Note here that a conversation in our human sense—a chat between people that leads to some result and lasts an arbitrary length of time—could encompass multiple transactional, informational, and prosocial voice interactions in succession. In other words, a voice interaction is a conversation, but a conversation is not necessarily a single voice interaction.
 
 
 
 Purely prosocial conversations are more gimmicky than captivating in most voice interfaces, because machines don’t yet have the capacity to really want to know how we’re doing and to do the sort of glad-handing humans crave. There’s also ongoing debate as to whether users actually prefer the sort of organic human conversation that begins with a prosocial voice interaction and shifts seamlessly into other types. In fact, in Voice User Interface Design, Michael Cohen, James Giangola, and Jennifer Balogh recommend sticking to users’ expectations by mimicking how they interact with other voice interfaces rather than trying too hard to be human—potentially alienating them in the process (http://bkaprt.com/vcu36/01-01).
 
 
 
 That leaves two genres of conversations we can have with one another that a voice interface can easily have with us, too: a transactional voice interaction realizing some outcome (“buy iced tea”) and an informational voice interaction teaching us something new (“discuss a musical”).
 
 
 
 Transactional voice interactions
 
 
 
 Unless you’re tapping buttons on a food delivery app, you’re generally having a conversation—and therefore a voice interaction—when you order a Hawaiian pizza with extra pineapple. Even when we walk up to the counter and place an order, the conversation quickly pivots from an initial smattering of neighborly small talk to the real mission at hand: ordering a pizza (generously topped with pineapple, as it should be).
 
 
 
 Alison: Hey, how’s it going?Burhan: Hi, welcome to Crust Deluxe! It’s cold out there. How can I help you?Alison: Can I get a Hawaiian pizza with extra pineapple?Burhan: Sure, what size?Alison: Large.Burhan: Anything else?Alison: No thanks, that’s it.Burhan: Something to drink?Alison: I’ll have a bottle of Coke.Burhan: You got it. That’ll be $13.55 and about fifteen minutes.
 
 
 
 Each progressive disclosure in this transactional conversation reveals more and more of the desired outcome of the transaction: a service rendered or a product delivered. Transactional conversations have certain key traits: they’re direct, to the point, and economical. They quickly dispense with pleasantries.
 
 
 
 Informational voice interactions
 
 
 
 Meanwhile, some conversations are primarily about obtaining information. Though Alison might visit Crust Deluxe with the sole purpose of placing an order, she might not actually want to walk out with a pizza at all. She might be just as interested in whether they serve halal or kosher dishes, gluten-free options, or something else. Here, though we again have a prosocial mini-conversation at the beginning to establish politeness, we’re after much more.
 
 
 
 Alison: Hey, how’s it going?Burhan: Hi, welcome to Crust Deluxe! It’s cold out there. How can I help you?Alison: Can I ask a few questions?Burhan: Of course! Go right ahead.Alison: Do you have any halal options on the menu?Burhan: Absolutely! We can make any pie halal by request. We also have lots of vegetarian, ovo-lacto, and vegan options. Are you thinking about any other dietary restrictions?Alison: What about gluten-free pizzas?Burhan: We can definitely do a gluten-free crust for you, no problem, for both our deep-dish and thin-crust pizzas. Anything else I can answer for you?Alison: That’s it for now. Good to know. Thanks!Burhan: Anytime, come back soon!
 
 
 
 This is a very different dialogue. Here, the goal is to get a certain set of facts. Informational conversations are investigative quests for the truth—research expeditions to gather data, news, or facts. Voice interactions that are informational might be more long-winded than transactional conversations by necessity. Responses tend to be lengthier, more informative, and carefully communicated so the customer understands the key takeaways.
 
 
 
 Voice Interfaces
 
 
 
 At their core, voice interfaces employ speech to support users in reaching their goals. But simply because an interface has a voice component doesn’t mean that every user interaction with it is mediated through voice. Because multimodal voice interfaces can lean on visual components like screens as crutches, we’re most concerned in this book with pure voice interfaces, which depend entirely on spoken conversation, lack any visual component whatsoever, and are therefore much more nuanced and challenging to tackle.
 
 
 
 Though voice interfaces have long been integral to the imagined future of humanity in science fiction, only recently have those lofty visions become fully realized in genuine voice interfaces.
 
 
 
 Interactive voice response (IVR) systems
 
 
 
 Though written conversational interfaces have been fixtures of computing for many decades, voice interfaces first emerged in the early 1990s with text-to-speech (TTS) dictation programs that recited written text aloud, as well as speech-enabled in-car systems that gave directions to a user-provided address. With the advent of interactive voice response (IVR) systems, intended as an alternative to overburdened customer service representatives, we became acquainted with the first true voice interfaces that engaged in authentic conversation.
 
 
 
 IVR systems allowed organizations to reduce their reliance on call centers but soon became notorious for their clunkiness. Commonplace in the corporate world, these systems were primarily designed as metaphorical switchboards to guide customers to a real phone agent (“Say Reservations to book a flight or check an itinerary”); chances are you will enter a conversation with one when you call an airline or hotel conglomerate. Despite their functional issues and users’ frustration with their inability to speak to an actual human right away, IVR systems proliferated in the early 1990s across a variety of industries (http://bkaprt.com/vcu36/01-02, PDF).
 
 
 
 While IVR systems are great for highly repetitive, monotonous conversations that generally don’t veer from a single format, they have a reputation for less scintillating conversation than we’re used to in real life (or even in science fiction).
 
 
 
 Screen readers
 
 
 
 Parallel to the evolution of IVR systems was the invention of the screen reader, a tool that transcribes visual content into synthesized speech. For Blind or visually impaired website users, it’s the predominant method of interacting with text, multimedia, or form elements. Screen readers represent perhaps the closest equivalent we have today to an out-of-the-box implementation of content delivered through voice.
 
 
 
 Among the first screen readers known by that moniker was the Screen Reader for the BBC Micro and NEEC Portable developed by the Research Centre for the Education of the Visually Handicapped (RCEVH) at the University of Birmingham in 1986 (http://bkaprt.com/vcu36/01-03). That same year, Jim Thatcher created the first IBM Screen Reader for text-based computers, later recreated for computers with graphical user interfaces (GUIs) (http://bkaprt.com/vcu36/01-04).
 
 
 
 With the rapid growth of the web in the 1990s, the demand for accessible tools for websites exploded. Thanks to the introduction of semantic HTML and especially ARIA roles beginning in 2008, screen readers started facilitating speedy interactions with web pages that ostensibly allow disabled users to traverse the page as an aural and temporal space rather than a visual and physical one. In other words, screen readers for the web “provide mechanisms that translate visual design constructs—proximity, proportion, etc.—into useful information,” writes Aaron Gustafson in A List Apart. “At least they do when documents are authored thoughtfully” (http://bkaprt.com/vcu36/01-05).
 
 
 
 Though deeply instructive for voice interface designers, there’s one significant problem with screen readers: they’re difficult to use and unremittingly verbose. The visual structures of websites and web navigation don’t translate well to screen readers, sometimes resulting in unwieldy pronouncements that name every manipulable HTML element and announce every formatting change. For many screen reader users, working with web-based interfaces exacts a cognitive toll.
 
 
 
 In Wired, accessibility advocate and voice engineer Chris Maury considers why the screen reader experience is ill-suited to users relying on voice:
 
 
 
 From the beginning, I hated the way that Screen Readers work. Why are they designed the way they are? It makes no sense to present information visually and then, and only then, translate that into audio. All of the time and energy that goes into creating the perfect user experience for an app is wasted, or even worse, adversely impacting the experience for blind users. (http://bkaprt.com/vcu36/01-06)
 
 
 
 In many cases, well-designed voice interfaces can speed users to their destination better than long-winded screen reader monologues. After all, visual interface users have the benefit of darting around the viewport freely to find information, ignoring areas irrelevant to them. Blind users, meanwhile, are obligated to listen to every utterance synthesized into speech and therefore prize brevity and efficiency. Disabled users who have long had no choice but to employ clunky screen readers may find that voice interfaces, particularly more modern voice assistants, offer a more streamlined experience.
 
 
 
 Voice assistants
 
 
 
 When we think of voice assistants (the subset of voice interfaces now commonplace in living rooms, smart homes, and offices), many of us immediately picture HAL from 2001: A Space Odyssey or hear Majel Barrett’s voice as the omniscient computer in Star Trek. Voice assistants are akin to personal concierges that can answer questions, schedule appointments, conduct searches, and perform other common day-to-day tasks. And they’re rapidly gaining more attention from accessibility advocates for their assistive potential.
 
 
 
 Before the earliest IVR systems found success in the enterprise, Apple published a demonstration video in 1987 depicting the Knowledge Navigator, a voice assistant that could transcribe spoken words and recognize human speech to a great degree of accuracy. Then, in 2001, Tim Berners-Lee and others formulated their vision for a Semantic Web “agent” that would perform typical errands like “checking calendars, making appointments, and finding locations” (http://bkaprt.com/vcu36/01-07, behind paywall). It wasn’t until 2011 that Apple’s Siri finally entered the picture, making voice assistants a tangible reality for consumers.
 
 
 
 Thanks to the plethora of voice assistants available today, there is considerable variation in how programmable and customizable certain voice assistants are over others (Fig 1.1). At one extreme, everything except vendor-provided features is locked down; for example, at the time of their release, the core functionality of Apple’s Siri and Microsoft’s Cortana couldn’t be extended beyond their existing capabilities. Even today, it isn’t possible to program Siri to perform arbitrary functions, because there’s no means by which developers can interact with Siri at a low level, apart from predefined categories of tasks like sending messages, hailing rideshares, making restaurant reservations, and certain others.
 
 
 
 At the opposite end of the spectrum, voice assistants like Amazon Alexa and Google Home offer a core foundation on which developers can build custom voice interfaces. For this reason, programmable voice assistants that lend themselves to customization and extensibility are becoming increasingly popular for developers who feel stifled by the limitations of Siri and Cortana. Amazon offers the Alexa Skills Kit, a developer framework for building custom voice interfaces for Amazon Alexa, while Google Home offers the ability to program arbitrary Google Assistant skills. Today, users can choose from among thousands of custom-built skills within both the Amazon Alexa and Google Assistant ecosystems.
 
 
 
 Fig 1.1: Voice assistants like Amazon Alexa and Google Home tend to be more programmable, and thus more flexible, than their counterpart Apple Siri.
 
 
 
 As corporations like Amazon, Apple, Microsoft, and Google continue to stake their territory, they’re also selling and open-sourcing an unprecedented array of tools and frameworks for designers and developers that aim to make building voice interfaces as easy as possible, even without code.
 
 
 
 Often by necessity, voice assistants like Amazon Alexa tend to be monochannel—they’re tightly coupled to a device and can’t be accessed on a computer or smartphone instead. By contrast, many development platforms like Google’s Dialogflow have introduced omnichannel capabilities so users can build a single conversational interface that then manifests as a voice interface, textual chatbot, and IVR system upon deployment. I don’t prescribe any specific implementation approaches in this design-focused book, but in Chapter 4 we’ll get into some of the implications these variables might have on the way you build out your design artifacts.
 
 
 
 Voice Content
 
 
 
 Simply put, voice content is content delivered through voice. To preserve what makes human conversation so compelling in the first place, voice content needs to be free-flowing and organic, contextless and concise—everything written content isn’t.
 
 
 
 Our world is replete with voice content in various forms: screen readers reciting website content, voice assistants rattling off a weather forecast, and automated phone hotline responses governed by IVR systems. In this book, we’re most concerned with content delivered auditorily—not as an option, but as a necessity.
 
 
 
 For many of us, our first foray into informational voice interfaces will be to deliver content to users. There’s only one problem: any content we already have isn’t in any way ready for this new habitat. So how do we make the content trapped on our websites more conversational? And how do we write new copy that lends itself to voice interactions?
 
 
 
 Lately, we’ve begun slicing and dicing our content in unprecedented ways. Websites are, in many respects, colossal vaults of what I call macrocontent: lengthy prose that can extend for infinitely scrollable miles in a browser window, like microfilm viewers of newspaper archives. Back in 2002, well before the present-day ubiquity of voice assistants, technologist Anil Dash defined microcontent as permalinked pieces of content that stay legible regardless of environment, such as email or text messages:
 
 
 
 A day’s weather forcast [sic], the arrival and departure times for an airplane flight, an abstract from a long publication, or a single instant message can all be examples of microcontent. (http://bkaprt.com/vcu36/01-08)
 
 
 
 I’d update Dash’s definition of microcontent to include all examples of bite-sized content that go well beyond written communiqués. After all, today we encounter microcontent in interfaces where a small snippet of copy is displayed alone, unmoored from the browser, like a textbot confirmation of a restaurant reservation. Microcontent offers the best opportunity to gauge how your content can be stretched to the very edges of its capabilities, informing delivery channels both established and novel.
 
 
 
 As microcontent, voice content is unique because it’s an example of how content is experienced in time rather than in space. We can glance at a digital sign underground for an instant and know when the next train is arriving, but voice interfaces hold our attention captive for periods of time that we can’t easily escape or skip, something screen reader users are all too familiar with.
 
 
 
 Because microcontent is fundamentally made up of isolated blobs with no relation to the channels where they’ll eventually end up, we need to ensure that our microcontent truly performs well as voice content—and that means focusing on the two most important traits of robust voice content: voice content legibility and voice content discoverability.
 
 
 
 Fundamentally, the legibility and discoverability of our voice content both have to do with how voice content manifests in perceived time and space.
 </content>
     </entry>
     <entry>
       <title>Designing for the Unexpected</title>
         <link href="https://alistapart.com/article/designing-for-the-unexpected/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">I’m not sure when I first heard this quote, but it’s something that has stayed with me over the years. How do you create services for situations you can’t imagine? Or design products that work on devices yet to be invented?
 
 
 
 Flash, Photoshop, and responsive design
 
 
 
 When I first started designing websites, my go-to software was Photoshop. I created a 960px canvas and set about creating a layout that I would later drop content in. The development phase was about attaining pixel-perfect accuracy using fixed widths, fixed heights, and absolute positioning.
 
 
 
 Ethan Marcotte’s talk at An Event Apart and subsequent article “Responsive Web Design” in A List Apart in 2010 changed all this. I was sold on responsive design as soon as I heard about it, but I was also terrified. The pixel-perfect designs full of magic numbers that I had previously prided myself on producing were no longer good enough.
 
 
 
 The fear wasn’t helped by my first experience with responsive design. My first project was to take an existing fixed-width website and make it responsive. What I learned the hard way was that you can’t just add responsiveness at the end of a project. To create fluid layouts, you need to plan throughout the design phase.
 
 
 
 A new way to design
 
 
 
 Designing responsive or fluid sites has always been about removing limitations, producing content that can be viewed on any device. It relies on the use of percentage-based layouts, which I initially achieved with native CSS and utility classes:
 
 
 
 .column-span-6 {
   width: 49%;
   float: left;
   margin-right: 0.5%;
   margin-left: 0.5%;
 }
 
 
 .column-span-4 {
   width: 32%;
   float: left;
   margin-right: 0.5%;
   margin-left: 0.5%;
 }
 
 .column-span-3 {
   width: 24%;
   float: left;
   margin-right: 0.5%;
   margin-left: 0.5%;
 }
 
 
 
 Then with Sass so I could take advantage of @includes to re-use repeated blocks of code and move back to more semantic markup:
 
 
 
 .logo {
   @include colSpan(6);
 }
 
 .search {
   @include colSpan(3);
 }
 
 .social-share {
   @include colSpan(3);
 }
 
 
 
 Media queries
 
 
 
 The second ingredient for responsive design is media queries. Without them, content would shrink to fit the available space regardless of whether that content remained readable (The exact opposite problem occurred with the introduction of a mobile-first approach).
 
 
 
 Components becoming too small at mobile breakpoints
 
 
 
 Media queries prevented this by allowing us to add breakpoints where the design could adapt. Like most people, I started out with three breakpoints: one for desktop, one for tablets, and one for mobile. Over the years, I added more and more for phablets, wide screens, and so on. 
 
 
 
 For years, I happily worked this way and improved both my design and front-end skills in the process. The only problem I encountered was making changes to content, since with our Sass grid system in place, there was no way for the site owners to add content without amending the markup—something a small business owner might struggle with. This is because each row in the grid was defined using a div as a container. Adding content meant creating new row markup, which requires a level of HTML knowledge.
 
 
 
 Row markup was a staple of early responsive design, present in all the widely used frameworks like Bootstrap and Skeleton.
 
 
 
 &lt;section class&#x3D;&quot;row&quot;&gt;
   &lt;div class&#x3D;&quot;column-span-4&quot;&gt;1 of 7&lt;/div&gt;
   &lt;div class&#x3D;&quot;column-span-4&quot;&gt;2 of 7&lt;/div&gt;
   &lt;div class&#x3D;&quot;column-span-4&quot;&gt;3 of 7&lt;/div&gt;
 &lt;/section&gt;
 
 &lt;section class&#x3D;&quot;row&quot;&gt;
   &lt;div class&#x3D;&quot;column-span-4&quot;&gt;4 of 7&lt;/div&gt;
   &lt;div class&#x3D;&quot;column-span-4&quot;&gt;5 of 7&lt;/div&gt;
   &lt;div class&#x3D;&quot;column-span-4&quot;&gt;6 of 7&lt;/div&gt;
 &lt;/section&gt;
 
 &lt;section class&#x3D;&quot;row&quot;&gt;
   &lt;div class&#x3D;&quot;column-span-4&quot;&gt;7 of 7&lt;/div&gt;
 &lt;/section&gt;
 
 
 
 Components placed in the rows of a Sass grid
 
 
 
 Another problem arose as I moved from a design agency building websites for small- to medium-sized businesses, to larger in-house teams where I worked across a suite of related sites. In those roles I started to work much more with reusable components. 
 
 
 
 Our reliance on media queries resulted in components that were tied to common viewport sizes. If the goal of component libraries is reuse, then this is a real problem because you can only use these components if the devices you’re designing for correspond to the viewport sizes used in the pattern library—in the process not really hitting that “devices that don’t yet exist”  goal.
 
 
 
 Then there’s the problem of space. Media queries allow components to adapt based on the viewport size, but what if I put a component into a sidebar, like in the figure below?
 
 
 
 Components responding to the viewport width with media queries
 
 
 
 Container queries: our savior or a false dawn?
 
 
 
 Container queries have long been touted as an improvement upon media queries, but at the time of writing are unsupported in most browsers. There are JavaScript workarounds, but they can create dependency and compatibility issues. The basic theory underlying container queries is that elements should change based on the size of their parent container and not the viewport width, as seen in the following illustrations.
 
 
 
 Components responding to their parent container with container queries
 
 
 
 One of the biggest arguments in favor of container queries is that they help us create components or design patterns that are truly reusable because they can be picked up and placed anywhere in a layout. This is an important step in moving toward a form of component-based design that works at any size on any device.
 
 
 
 In other words, responsive components to replace responsive layouts.
 
 
 
 Container queries will help us move from designing pages that respond to the browser or device size to designing components that can be placed in a sidebar or in the main content, and respond accordingly.
 
 
 
 My concern is that we are still using layout to determine when a design needs to adapt. This approach will always be restrictive, as we will still need pre-defined breakpoints. For this reason, my main question with container queries is, How would we decide when to change the CSS used by a component? 
 
 
 
 A component library removed from context and real content is probably not the best place for that decision. 
 
 
 
 As the diagrams below illustrate, we can use container queries to create designs for specific container widths, but what if I want to change the design based on the image size or ratio?
 
 
 
 Cards responding to their parent container with container queries
 
 
 
 Cards responding based on their own content
 
 
 
 In this example, the dimensions of the container are not what should dictate the design; rather, the image is.
 
 
 
 It’s hard to say for sure whether container queries will be a success story until we have solid cross-browser support for them. Responsive component libraries would definitely evolve how we design and would improve the possibilities for reuse and design at scale. But maybe we will always need to adjust these components to suit our content.
 
 
 
 CSS is changing
 
 
 
 Whilst the container query debate rumbles on, there have been numerous advances in CSS that change the way we think about design. The days of fixed-width elements measured in pixels and floated div elements used to cobble layouts together are long gone, consigned to history along with table layouts. Flexbox and CSS Grid have revolutionized layouts for the web. We can now create elements that wrap onto new rows when they run out of space, not when the device changes.
 
 
 
 .wrapper {
   display: grid;
   grid-template-columns: repeat(auto-fit, 450px);
   gap: 10px;
 }
 
 
 
 The repeat() function paired with auto-fit or auto-fill allows us to specify how much space each column should use while leaving it up to the browser to decide when to spill the columns onto a new line. Similar things can be achieved with Flexbox, as elements can wrap over multiple rows and “flex” to fill available space. 
 
 
 
 .wrapper {
   display: flex;
   flex-wrap: wrap;
   justify-content: space-between;
 }
 
 .child {
   flex-basis: 32%;
   margin-bottom: 20px;
 }
 
 
 
 The biggest benefit of all this is you don’t need to wrap elements in container rows. Without rows, content isn’t tied to page markup in quite the same way, allowing for removals or additions of content without additional development.
 
 
 
 A traditional Grid layout without the usual row containers
 
 
 
 This is a big step forward when it comes to creating designs that allow for evolving content, but the real game changer for flexible designs is CSS Subgrid. 
 
 
 
 Remember the days of crafting perfectly aligned interfaces, only for the customer to add an unbelievably long header almost as soon as they&#x27;re given CMS access, like the illustration below?
 
 
 
 Cards unable to respond to a sibling’s content changes
 
 
 
 Subgrid allows elements to respond to adjustments in their own content and in the content of sibling elements, helping us create designs more resilient to change.
 
 
 
 Cards responding to content in sibling cards
 
 
 
 .wrapper {
   display: grid;
   grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
      grid-template-rows: auto 1fr auto;
   gap: 10px;
 }
 
 .sub-grid {
   display: grid;
   grid-row: span 3;
   grid-template-rows: subgrid; /* sets rows to parent grid */
 }
 
 
 
 CSS Grid allows us to separate layout and content, thereby enabling flexible designs. Meanwhile, Subgrid allows us to create designs that can adapt in order to suit morphing content. Subgrid at the time of writing is only supported in Firefox but the above code can be implemented behind an @supports feature query. 
 
 
 
 Intrinsic layouts 
 
 
 
 I’d be remiss not to mention intrinsic layouts, the term created by Jen Simmons to describe a mixture of new and old CSS features used to create layouts that respond to available space. 
 
 
 
 Responsive layouts have flexible columns using percentages. Intrinsic layouts, on the other hand, use the fr unit to create flexible columns that won’t ever shrink so much that they render the content illegible.
 
 
 
 fr units is a way to say I want you to distribute the extra space in this way, but...don’t ever make it smaller than the content that’s inside of it.
 
 
 
 —Jen Simmons, “Designing Intrinsic Layouts”
 
 
 
 Intrinsic layouts can also utilize a mixture of fixed and flexible units, allowing the content to dictate the space it takes up.
 
 
 
 Slide from “Designing Intrinsic Layouts” by Jen Simmons
 
 
 
 What makes intrinsic design stand out is that it not only creates designs that can withstand future devices but also helps scale design without losing flexibility. Components and patterns can be lifted and reused without the prerequisite of having the same breakpoints or the same amount of content as in the previous implementation. 
 
 
 
 We can now create designs that adapt to the space they have, the content within them, and the content around them. With an intrinsic approach, we can construct responsive components without depending on container queries.
 
 
 
 Another 2010 moment?
 
 
 
 This intrinsic approach should in my view be every bit as groundbreaking as responsive web design was ten years ago. For me, it’s another “everything changed” moment. 
 
 
 
 But it doesn’t seem to be moving quite as fast; I haven’t yet had that same career-changing moment I had with responsive design, despite the widely shared and brilliant talk that brought it to my attention. 
 
 
 
 One reason for that could be that I now work in a large organization, which is quite different from the design agency role I had in 2010. In my agency days, every new project was a clean slate, a chance to try something new. Nowadays, projects use existing tools and frameworks and are often improvements to existing websites with an existing codebase. 
 
 
 
 Another could be that I feel more prepared for change now. In 2010 I was new to design in general; the shift was frightening and required a lot of learning. Also, an intrinsic approach isn’t exactly all-new; it’s about using existing skills and existing CSS knowledge in a different way. 
 
 
 
 You can’t framework your way out of a content problem
 
 
 
 Another reason for the slightly slower adoption of intrinsic design could be the lack of quick-fix framework solutions available to kick-start the change. 
 
 
 
 Responsive grid systems were all over the place ten years ago. With a framework like Bootstrap or Skeleton, you had a responsive design template at your fingertips.
 
 
 
 Intrinsic design and frameworks do not go hand in hand quite so well because the benefit of having a selection of units is a hindrance when it comes to creating layout templates. The beauty of intrinsic design is combining different units and experimenting with techniques to get the best for your content.
 
 
 
 And then there are design tools. We probably all, at some point in our careers, used Photoshop templates for desktop, tablet, and mobile devices to drop designs in and show how the site would look at all three stages.
 
 
 
 How do you do that now, with each component responding to content and layouts flexing as and when they need to? This type of design must happen in the browser, which personally I’m a big fan of. 
 
 
 
 The debate about “whether designers should code” is another that has rumbled on for years. When designing a digital product, we should, at the very least, design for a best- and worst-case scenario when it comes to content. To do this in a graphics-based software package is far from ideal. In code, we can add longer sentences, more radio buttons, and extra tabs, and watch in real time as the design adapts. Does it still work? Is the design too reliant on the current content?
 
 
 
 Personally, I look forward to the day intrinsic design is the standard for design, when a design component can be truly flexible and adapt to both its space and content with no reliance on device or container dimensions.
 
 
 
 Content first 
 
 
 
 Content is not constant. After all, to design for the unknown or unexpected we need to account for content changes like our earlier Subgrid card example that allowed the cards to respond to adjustments to their own content and the content of sibling elements.
 
 
 
 Thankfully, there’s more to CSS than layout, and plenty of properties and values can help us put content first. Subgrid and pseudo-elements like ::first-line and ::first-letter help to separate design from markup so we can create designs that allow for changes.
 
 
 
 Instead of old markup hacks like this—
 
 
 
 &lt;p&gt;
   &lt;span class&#x3D;&quot;first-line&quot;&gt;First line of text with different styling&lt;/span&gt;...
 &lt;/p&gt;
 
 
 
 —we can target content based on where it appears.
 
 
 
 .element::first-line {
   font-size: 1.4em;
 }
 
 .element::first-letter {
   color: red;
 }
 
 
 
 Much bigger additions to CSS include logical properties, which change the way we construct designs using logical dimensions (start and end) instead of physical ones (left and right), something CSS Grid also does with functions like min(), max(), and clamp().
 
 
 
 This flexibility allows for directional changes according to content, a common requirement when we need to present content in multiple languages. In the past, this was often achieved with Sass mixins but was often limited to switching from left-to-right to right-to-left orientation.
 
 
 
 In the Sass version, directional variables need to be set.
 
 
 
 $direction: rtl;
 $opposite-direction: ltr;
 
 $start-direction: right;
 $end-direction: left;
 
 
 
 These variables can be used as values—
 
 
 
 body {
   direction: $direction;
   text-align: $start-direction;
 }
 
 
 
 —or as properties.
 
 
 
 margin-#{$end-direction}: 10px;
 padding-#{$start-direction}: 10px;
 
 
 
 However, now we have native logical properties, removing the reliance on both Sass (or a similar tool) and pre-planning that necessitated using variables throughout a codebase. These properties also start to break apart the tight coupling between a design and strict physical dimensions, creating more flexibility for changes in language and in direction.
 
 
 
 margin-block-end: 10px;
 padding-block-start: 10px;
 
 
 
 There are also native start and end values for properties like text-align, which means we can replace text-align: right with text-align: start.
 
 
 
 Like the earlier examples, these properties help to build out designs that aren’t constrained to one language; the design will reflect the content’s needs.
 
 
 
 
 
 
 
 Fixed and fluid 
 
 
 
 We briefly covered the power of combining fixed widths with fluid widths with intrinsic layouts. The min() and max() functions are a similar concept, allowing you to specify a fixed value with a flexible alternative. 
 
 
 
 For min() this means setting a fluid minimum value and a maximum fixed value.
 
 
 
 .element {
   width: min(50%, 300px);
 }
 
 
 
 
 
 
 
 The element in the figure above will be 50% of its container as long as the element’s width doesn’t exceed 300px.
 
 
 
 For max() we can set a flexible max value and a minimum fixed value.
 
 
 
 .element {
   width: max(50%, 300px);
 }
 
 
 
 
 
 
 
 Now the element will be 50% of its container as long as the element’s width is at least 300px. This means we can set limits but allow content to react to the available space. 
 
 
 
 The clamp() function builds on this by allowing us to set a preferred value with a third parameter. Now we can allow the element to shrink or grow if it needs to without getting to a point where it becomes unusable.
 
 
 
 .element {
   width: clamp(300px, 50%, 600px);
 }
 
 
 
 
 
 
 
 This time, the element’s width will be 50% (the preferred value) of its container but never less than 300px and never more than 600px.
 
 
 
 With these techniques, we have a content-first approach to responsive design. We can separate content from markup, meaning the changes users make will not affect the design. We can start to future-proof designs by planning for unexpected changes in language or direction. And we can increase flexibility by setting desired dimensions alongside flexible alternatives, allowing for more or less content to be displayed correctly.
 
 
 
 Situation first
 
 
 
 Thanks to what we’ve discussed so far, we can cover device flexibility by changing our approach, designing around content and space instead of catering to devices. But what about that last bit of Jeffrey Zeldman’s quote, “...situations you haven’t imagined”?
 
 
 
 It’s a very different thing to design for someone seated at a desktop computer as opposed to someone using a mobile phone and moving through a crowded street in glaring sunshine. Situations and environments are hard to plan for or predict because they change as people react to their own unique challenges and tasks.
 
 
 
 This is why choice is so important. One size never fits all, so we need to design for multiple scenarios to create equal experiences for all our users.
 
 
 
 Thankfully, there is a lot we can do to provide choice.
 
 
 
 Responsible design 
 
 
 
 “There are parts of the world where mobile data is prohibitively expensive, and where there is little or no broadband infrastructure.”“I Used the Web for a Day on a 50 MB Budget”Chris Ashton
 
 
 
 One of the biggest assumptions we make is that people interacting with our designs have a good wifi connection and a wide screen monitor. But in the real world, our users may be commuters traveling on trains or other forms of transport using smaller mobile devices that can experience drops in connectivity. There is nothing more frustrating than a web page that won’t load, but there are ways we can help users use less data or deal with sporadic connectivity.
 
 
 
 The srcset attribute allows the browser to decide which image to serve. This means we can create smaller ‘cropped’ images to display on mobile devices in turn using less bandwidth and less data.
 
 
 
 &lt;img 
   src&#x3D;&quot;image-file.jpg&quot;
   srcset&#x3D;&quot;large.jpg 1024w,
              medium.jpg 640w,
              small.jpg 320w&quot;
      alt&#x3D;&quot;Image alt text&quot; /&gt;
 
 
 
 The preload attribute can also help us to think about how and when media is downloaded. It can be used to tell a browser about any critical assets that need to be downloaded with high priority, improving perceived performance and the user experience. 
 
 
 
 &lt;link rel&#x3D;&quot;stylesheet&quot; href&#x3D;&quot;style.css&quot;&gt; &lt;!--Standard stylesheet markup--&gt;
 &lt;link rel&#x3D;&quot;preload&quot; href&#x3D;&quot;style.css&quot; as&#x3D;&quot;style&quot;&gt; &lt;!--Preload stylesheet markup--&gt;
 
 
 
 There’s also native lazy loading, which indicates assets that should only be downloaded when they are needed.
 
 
 
 &lt;img src&#x3D;&quot;image.png&quot; loading&#x3D;&quot;lazy&quot; alt&#x3D;&quot;…&quot;&gt;
 
 
 
 With srcset, preload, and lazy loading, we can start to tailor a user’s experience based on the situation they find themselves in. What none of this does, however, is allow the user themselves to decide what they want downloaded, as the decision is usually the browser’s to make. 
 
 
 
 So how can we put users in control?
 
 
 
 The return of media queries 
 
 
 
 Media queries have always been about much more than device sizes. They allow content to adapt to different situations, with screen size being just one of them.
 
 
 
 We’ve long been able to check for media types like print and speech and features such as hover, resolution, and color. These checks allow us to provide options that suit more than one scenario; it’s less about one-size-fits-all and more about serving adaptable content. 
 
 
 
 As of this writing, the Media Queries Level 5 spec is still under development. It introduces some really exciting queries that in the future will help us design for multiple other unexpected situations.
 
 
 
 For example, there’s a light-level feature that allows you to modify styles if a user is in sunlight or darkness. Paired with custom properties, these features allow us to quickly create designs or themes for specific environments.
 
 
 
 @media (light-level: normal) {
   --background-color: #fff;
   --text-color: #0b0c0c;  
 }
 
 @media (light-level: dim) {
   --background-color: #efd226;
   --text-color: #0b0c0c;
 }
 
 
 
 Another key feature of the Level 5 spec is personalization. Instead of creating designs that are the same for everyone, users can choose what works for them. This is achieved by using features like prefers-reduced-data, prefers-color-scheme, and prefers-reduced-motion, the latter two of which already enjoy broad browser support. These features tap into preferences set via the operating system or browser so people don’t have to spend time making each site they visit more usable. 
 
 
 
 Media queries like this go beyond choices made by a browser to grant more control to the user.
 
 
 
 Expect the unexpected
 
 
 
 In the end, the one thing we should always expect is for things to change. Devices in particular change faster than we can keep up, with foldable screens already on the market.
 
 
 
 We can’t design the same way we have for this ever-changing landscape, but we can design for content. By putting content first and allowing that content to adapt to whatever space surrounds it, we can create more robust, flexible designs that increase the longevity of our products. 
 
 
 
 A lot of the CSS discussed here is about moving away from layouts and putting content at the heart of design. From responsive components to fixed and fluid units, there is so much more we can do to take a more intrinsic approach. Even better, we can test these techniques during the design phase by designing in-browser and watching how our designs adapt in real-time.
 
 
 
 When it comes to unexpected situations, we need to make sure our products are usable when people need them, whenever and wherever that might be. We can move closer to achieving this by involving users in our design decisions, by creating choice via browsers, and by giving control to our users with user-preference-based media queries. 
 
 
 
 Good design for the unexpected should allow for change, provide choice, and give control to those we serve: our users themselves.
 </content>
     </entry>
     <entry>
       <title>Asynchronous Design Critique: Getting Feedback</title>
         <link href="https://alistapart.com/article/asynchronous-design-critique-giving-feedback-part2/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">“Any comment?” is probably one of the worst ways to ask for feedback. It’s vague and open ended, and it doesn’t provide any indication of what we’re looking for. Getting good feedback starts earlier than we might expect: it starts with the request. 
 
 
 
 It might seem counterintuitive to start the process of receiving feedback with a question, but that makes sense if we realize that getting feedback can be thought of as a form of design research. In the same way that we wouldn’t do any research without the right questions to get the insights that we need, the best way to ask for feedback is also to craft sharp questions.
 
 
 
 Design critique is not a one-shot process. Sure, any good feedback workflow continues until the project is finished, but this is particularly true for design because design work continues iteration after iteration, from a high level to the finest details. Each level needs its own set of questions.
 
 
 
 And finally, as with any good research, we need to review what we got back, get to the core of its insights, and take action. Question, iteration, and review. Let’s look at each of those.
 
 
 
 The question
 
 
 
 Being open to feedback is essential, but we need to be precise about what we’re looking for. Just saying “Any comment?”, “What do you think?”, or “I’d love to get your opinion” at the end of a presentation—whether it’s in person, over video, or through a written post—is likely to get a number of varied opinions or, even worse, get everyone to follow the direction of the first person who speaks up. And then... we get frustrated because vague questions like those can turn a high-level flows review into people instead commenting on the borders of buttons. Which might be a hearty topic, so it might be hard at that point to redirect the team to the subject that you had wanted to focus on.
 
 
 
 But how do we get into this situation? It’s a mix of factors. One is that we don’t usually consider asking as a part of the feedback process. Another is how natural it is to just leave the question implied, expecting the others to be on the same page. Another is that in nonprofessional discussions, there’s often no need to be that precise. In short, we tend to underestimate the importance of the questions, so we don’t work on improving them.
 
 
 
 The act of asking good questions guides and focuses the critique. It’s also a form of consent: it makes it clear that you’re open to comments and what kind of comments you’d like to get. It puts people in the right mental state, especially in situations when they weren’t expecting to give feedback.
 
 
 
 There isn’t a single best way to ask for feedback. It just needs to be specific, and specificity can take many shapes. A model for design critique that I’ve found particularly useful in my coaching is the one of stage versus depth.
 
 
 
 
 
 
 
 “Stage” refers to each of the steps of the process—in our case, the design process. In progressing from user research to the final design, the kind of feedback evolves. But within a single step, one might still review whether some assumptions are correct and whether there’s been a proper translation of the amassed feedback into updated designs as the project has evolved. A starting point for potential questions could derive from the layers of user experience. What do you want to know: Project objectives? User needs? Functionality? Content? Interaction design? Information architecture? UI design? Navigation design? Visual design? Branding?
 
 
 
 Here’re a few example questions that are precise and to the point that refer to different layers:
 
 
 
 Functionality: Is automating account creation desirable?Interaction design: Take a look through the updated flow and let me know whether you see any steps or error states that I might’ve missed.Information architecture: We have two competing bits of information on this page. Is the structure effective in communicating them both?UI design: What are your thoughts on the error counter at the top of the page that makes sure that you see the next error, even if the error is out of the viewport? Navigation design: From research, we identified these second-level navigation items, but once you’re on the page, the list feels too long and hard to navigate. Are there any suggestions to address this?Visual design: Are the sticky notifications in the bottom-right corner visible enough?
 
 
 
 The other axis of specificity is about how deep you’d like to go on what’s being presented. For example, we might have introduced a new end-to-end flow, but there was a specific view that you found particularly challenging and you’d like a detailed review of that. This can be especially useful from one iteration to the next where it’s important to highlight the parts that have changed.
 
 
 
 There are other things that we can consider when we want to achieve more specific—and more effective—questions.
 
 
 
 A simple trick is to remove generic qualifiers from your questions like “good,” “well,” “nice,” “bad,” “okay,” and “cool.” For example, asking, “When the block opens and the buttons appear, is this interaction good?” might look specific, but you can spot the “good” qualifier, and convert it to an even better question: “When the block opens and the buttons appear, is it clear what the next action is?”
 
 
 
 Sometimes we actually do want broad feedback. That’s rare, but it can happen. In that sense, you might still make it explicit that you’re looking for a wide range of opinions, whether at a high level or with details. Or maybe just say, “At first glance, what do you think?” so that it’s clear that what you’re asking is open ended but focused on someone’s impression after their first five seconds of looking at it.
 
 
 
 Sometimes the project is particularly expansive, and some areas may have already been explored in detail. In these situations, it might be useful to explicitly say that some parts are already locked in and aren’t open to feedback. It’s not something that I’d recommend in general, but I’ve found it useful to avoid falling again into rabbit holes of the sort that might lead to further refinement but aren’t what’s most important right now.
 
 
 
 Asking specific questions can completely change the quality of the feedback that you receive. People with less refined critique skills will now be able to offer more actionable feedback, and even expert designers will welcome the clarity and efficiency that comes from focusing only on what’s needed. It can save a lot of time and frustration.
 
 
 
 The iteration
 
 
 
 Design iterations are probably the most visible part of the design work, and they provide a natural checkpoint for feedback. Yet a lot of design tools with inline commenting tend to show changes as a single fluid stream in the same file, and those types of design tools make conversations disappear once they’re resolved, update shared UI components automatically, and compel designs to always show the latest version—unless these would-be helpful features were to be manually turned off. The implied goal that these design tools seem to have is to arrive at just one final copy with all discussions closed, probably because they inherited patterns from how written documents are collaboratively edited. That’s probably not the best way to approach design critiques, but even if I don’t want to be too prescriptive here: that could work for some teams.
 
 
 
 The asynchronous design-critique approach that I find most effective is to create explicit checkpoints for discussion. I’m going to use the term iteration post for this. It refers to a write-up or presentation of the design iteration followed by a discussion thread of some kind. Any platform that can accommodate this structure can use this. By the way, when I refer to a “write-up or presentation,” I’m including video recordings or other media too: as long as it’s asynchronous, it works.
 
 
 
 Using iteration posts has many advantages:
 
 
 
 It creates a rhythm in the design work so that the designer can review feedback from each iteration and prepare for the next.It makes decisions visible for future review, and conversations are likewise always available.It creates a record of how the design changed over time.Depending on the tool, it might also make it easier to collect feedback and act on it.
 
 
 
 These posts of course don’t mean that no other feedback approach should be used, just that iteration posts could be the primary rhythm for a remote design team to use. And other feedback approaches (such as live critique, pair designing, or inline comments) can build from there.
 
 
 
 I don’t think there’s a standard format for iteration posts. But there are a few high-level elements that make sense to include as a baseline:
 
 
 
 The goalThe designThe list of changesThe questions
 
 
 
 Each project is likely to have a goal, and hopefully it’s something that’s already been summarized in a single sentence somewhere else, such as the client brief, the product manager’s outline, or the project owner’s request. So this is something that I’d repeat in every iteration post—literally copy and pasting it. The idea is to provide context and to repeat what’s essential to make each iteration post complete so that there’s no need to find information spread across multiple posts. If I want to know about the latest design, the latest iteration post will have all that I need.
 
 
 
 This copy-and-paste part introduces another relevant concept: alignment comes from repetition. So having posts that repeat information is actually very effective toward making sure that everyone is on the same page.
 
 
 
 The design is then the actual series of information-architecture outlines, diagrams, flows, maps, wireframes, screens, visuals, and any other kind of design work that’s been done. In short, it’s any design artifact. For the final stages of work, I prefer the term blueprint to emphasize that I’ll be showing full flows instead of individual screens to make it easier to understand the bigger picture. 
 
 
 
 It can also be useful to label the artifacts with clear titles because that can make it easier to refer to them. Write the post in a way that helps people understand the work. It’s not too different from organizing a good live presentation. 
 
 
 
 For an efficient discussion, you should also include a bullet list of the changes from the previous iteration to let people focus on what’s new, which can be especially useful for larger pieces of work where keeping track, iteration after iteration, could become a challenge.
 
 
 
 And finally, as noted earlier, it’s essential that you include a list of the questions to drive the design critique in the direction you want. Doing this as a numbered list can also help make it easier to refer to each question by its number.
 
 
 
 Not all iterations are the same. Earlier iterations don’t need to be as tightly focused—they can be more exploratory and experimental, maybe even breaking some of the design-language guidelines to see what’s possible. Then later, the iterations start settling on a solution and refining it until the design process reaches its end and the feature ships.
 
 
 
 I want to highlight that even if these iteration posts are written and conceived as checkpoints, by no means do they need to be exhaustive. A post might be a draft—just a concept to get a conversation going—or it could be a cumulative list of each feature that was added over the course of each iteration until the full picture is done.
 
 
 
 Over time, I also started using specific labels for incremental iterations: i1, i2, i3, and so on. This might look like a minor labelling tip, but it can help in multiple ways:
 
 
 
 Unique—It’s a clear unique marker. Within each project, one can easily say, “This was discussed in i4,” and everyone knows where they can go to review things.Unassuming—It works like versions (such as v1, v2, and v3) but in contrast, versions create the impression of something that’s big, exhaustive, and complete. Iterations must be able to be exploratory, incomplete, partial.Future proof—It resolves the “final” naming problem that you can run into with versions. No more files named “final final complete no-really-its-done.” Within each project, the largest number always represents the latest iteration.
 
 
 
 To mark when a design is complete enough to be worked on, even if there might be some bits still in need of attention and in turn more iterations needed, the wording release candidate (RC) could be used to describe it: “with i8, we reached RC” or “i12 is an RC.”
 
 
 
 The review
 
 
 
 What usually happens during a design critique is an open discussion, with a back and forth between people that can be very productive. This approach is particularly effective during live, synchronous feedback. But when we work asynchronously, it’s more effective to use a different approach: we can shift to a user-research mindset. Written feedback from teammates, stakeholders, or others can be treated as if it were the result of user interviews and surveys, and we can analyze it accordingly.
 
 
 
 This shift has some major benefits that make asynchronous feedback particularly effective, especially around these friction points:
 
 
 
 It removes the pressure to reply to everyone.It reduces the frustration from swoop-by comments.It lessens our personal stake.
 
 
 
 The first friction point is feeling a pressure to reply to every single comment. Sometimes we write the iteration post, and we get replies from our team. It’s just a few of them, it’s easy, and it doesn’t feel like a problem. But other times, some solutions might require more in-depth discussions, and the amount of replies can quickly increase, which can create a tension between trying to be a good team player by replying to everyone and doing the next design iteration. This might be especially true if the person who’s replying is a stakeholder or someone directly involved in the project who we feel that we need to listen to. We need to accept that this pressure is absolutely normal, and it’s human nature to try to accommodate people who we care about. Sometimes replying to all comments can be effective, but if we treat a design critique more like user research, we realize that we don’t have to reply to every comment, and in asynchronous spaces, there are alternatives:
 
 
 
 One is to let the next iteration speak for itself. When the design evolves and we post a follow-up iteration, that’s the reply. You might tag all the people who were involved in the previous discussion, but even that’s a choice, not a requirement. Another is to briefly reply to acknowledge each comment, such as “Understood. Thank you,” “Good points—I’ll review,” or “Thanks. I’ll include these in the next iteration.” In some cases, this could also be just a single top-level comment along the lines of “Thanks for all the feedback everyone—the next iteration is coming soon!”Another is to provide a quick summary of the comments before moving on. Depending on your workflow, this can be particularly useful as it can provide a simplified checklist that you can then use for the next iteration.
 
 
 
 The second friction point is the swoop-by comment, which is the kind of feedback that comes from someone outside the project or team who might not be aware of the context, restrictions, decisions, or requirements—or of the previous iterations’ discussions. On their side, there’s something that one can hope that they might learn: they could start to acknowledge that they’re doing this and they could be more conscious in outlining where they’re coming from. Swoop-by comments often trigger the simple thought “We’ve already discussed this…”, and it can be frustrating to have to repeat the same reply over and over.
 
 
 
 Let’s begin by acknowledging again that there’s no need to reply to every comment. If, however, replying to a previously litigated point might be useful, a short reply with a link to the previous discussion for extra details is usually enough. Remember, alignment comes from repetition, so it’s okay to repeat things sometimes!
 
 
 
 Swoop-by commenting can still be useful for two reasons: they might point out something that still isn’t clear, and they also have the potential to stand in for the point of view of a user who’s seeing the design for the first time. Sure, you’ll still be frustrated, but that might at least help in dealing with it.
 
 
 
 The third friction point is the personal stake we could have with the design, which could make us feel defensive if the review were to feel more like a discussion. Treating feedback as user research helps us create a healthy distance between the people giving us feedback and our ego (because yes, even if we don’t want to admit it, it’s there). And ultimately, treating everything in aggregated form allows us to better prioritize our work.
 
 
 
 Always remember that while you need to listen to stakeholders, project owners, and specific advice, you don’t have to accept every piece of feedback. You have to analyze it and make a decision that you can justify, but sometimes “no” is the right answer. 
 
 
 
 As the designer leading the project, you’re in charge of that decision. Ultimately, everyone has their specialty, and as the designer, you’re the one who has the most knowledge and the most context to make the right decision. And by listening to the feedback that you’ve received, you’re making sure that it’s also the best and most balanced decision.
 
 
 
 Thanks to Brie Anne Demkiw and Mike Shelton for reviewing the first draft of this article.
 </content>
     </entry>
     <entry>
       <title>Asynchronous Design Critique: Giving Feedback</title>
         <link href="https://alistapart.com/article/async-design-critique-giving-feedback/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">Feedback, in whichever form it takes, and whatever it may be called, is one of the most effective soft skills that we have at our disposal to collaboratively get our designs to a better place while growing our own skills and perspectives.
 
 
 
 Feedback is also one of the most underestimated tools, and often by assuming that we’re already good at it, we settle, forgetting that it’s a skill that can be trained, grown, and improved. Poor feedback can create confusion in projects, bring down morale, and affect trust and team collaboration over the long term. Quality feedback can be a transformative force. 
 
 
 
 Practicing our skills is surely a good way to improve, but the learning gets even faster when it’s paired with a good foundation that channels and focuses the practice. What are some foundational aspects of giving good feedback? And how can feedback be adjusted for remote and distributed work environments? 
 
 
 
 On the web, we can identify a long tradition of asynchronous feedback: from the early days of open source, code was shared and discussed on mailing lists. Today, developers engage on pull requests, designers comment in their favorite design tools, project managers and scrum masters exchange ideas on tickets, and so on.
 
 
 
 Design critique is often the name used for a type of feedback that’s provided to make our work better, collaboratively. So it shares a lot of the principles with feedback in general, but it also has some differences.
 
 
 
 The content
 
 
 
 The foundation of every good critique is the feedback’s content, so that’s where we need to start. There are many models that you can use to shape your content. The one that I personally like best—because it’s clear and actionable—is this one from Lara Hogan.
 
 
 
 
 
 
 
 While this equation is generally used to give feedback to people, it also fits really well in a design critique because it ultimately answers some of the core questions that we work on: What? Where? Why? How? Imagine that you’re giving some feedback about some design work that spans multiple screens, like an onboarding flow: there are some pages shown, a flow blueprint, and an outline of the decisions made. You spot something that could be improved. If you keep the three elements of the equation in mind, you’ll have a mental model that can help you be more precise and effective.
 
 
 
 Here is a comment that could be given as a part of some feedback, and it might look reasonable at a first glance: it seems to superficially fulfill the elements in the equation. But does it?
 
 
 
 Not sure about the buttons’ styles and hierarchy—it feels off. Can you change them?
 
 
 
 Observation for design feedback doesn’t just mean pointing out which part of the interface your feedback refers to, but it also refers to offering a perspective that’s as specific as possible. Are you providing the user’s perspective? Your expert perspective? A business perspective? The project manager’s perspective? A first-time user’s perspective?
 
 
 
 When I see these two buttons, I expect one to go forward and one to go back.
 
 
 
 Impact is about the why. Just pointing out a UI element might sometimes be enough if the issue may be obvious, but more often than not, you should add an explanation of what you’re pointing out.
 
 
 
 When I see these two buttons, I expect one to go forward and one to go back. But this is the only screen where this happens, as before we just used a single button and an “×” to close. This seems to be breaking the consistency in the flow.
 
 
 
 The question approach is meant to provide open guidance by eliciting the critical thinking in the designer receiving the feedback. Notably, in Lara’s equation she provides a second approach: request, which instead provides guidance toward a specific solution. While that’s a viable option for feedback in general, for design critiques, in my experience, defaulting to the question approach usually reaches the best solutions because designers are generally more comfortable in being given an open space to explore.
 
 
 
 The difference between the two can be exemplified with, for the question approach:
 
 
 
 When I see these two buttons, I expect one to go forward and one to go back. But this is the only screen where this happens, as before we just used a single button and an “×” to close. This seems to be breaking the consistency in the flow. Would it make sense to unify them?
 
 
 
 Or, for the request approach:
 
 
 
 When I see these two buttons, I expect one to go forward and one to go back. But this is the only screen where this happens, as before we just used a single button and an “×” to close. This seems to be breaking the consistency in the flow. Let’s make sure that all screens have the same pair of forward and back buttons.
 
 
 
 At this point in some situations, it might be useful to integrate with an extra why: why you consider the given suggestion to be better.
 
 
 
 When I see these two buttons, I expect one to go forward and one to go back. But this is the only screen where this happens, as before we just used a single button and an “×” to close. This seems to be breaking the consistency in the flow. Let’s make sure that all screens have the same two forward and back buttons so that users don’t get confused.
 
 
 
 Choosing the question approach or the request approach can also at times be a matter of personal preference. A while ago, I was putting a lot of effort into improving my feedback: I did rounds of anonymous feedback, and I reviewed feedback with other people. After a few rounds of this work and a year later, I got a positive response: my feedback came across as effective and grounded. Until I changed teams. To my shock, my next round of feedback from one specific person wasn’t that great. The reason is that I had previously tried not to be prescriptive in my advice—because the people who I was previously working with preferred the open-ended question format over the request style of suggestions. But now in this other team, there was one person who instead preferred specific guidance. So I adapted my feedback for them to include requests.
 
 
 
 One comment that I heard come up a few times is that this kind of feedback is quite long, and it doesn’t seem very efficient. No… but also yes. Let’s explore both sides.
 
 
 
 No, this style of feedback is actually efficient because the length here is a byproduct of clarity, and spending time giving this kind of feedback can provide exactly enough information for a good fix. Also if we zoom out, it can reduce future back-and-forth conversations and misunderstandings, improving the overall efficiency and effectiveness of collaboration beyond the single comment. Imagine that in the example above the feedback were instead just, “Let’s make sure that all screens have the same two forward and back buttons.” The designer receiving this feedback wouldn’t have much to go by, so they might just apply the change. In later iterations, the interface might change or they might introduce new features—and maybe that change might not make sense anymore. Without the why, the designer might imagine that the change is about consistency… but what if it wasn’t? So there could now be an underlying concern that changing the buttons would be perceived as a regression.
 
 
 
 Yes, this style of feedback is not always efficient because the points in some comments don’t always need to be exhaustive, sometimes because certain changes may be obvious (“The font used doesn’t follow our guidelines”) and sometimes because the team may have a lot of internal knowledge such that some of the whys may be implied.
 
 
 
 So the equation above isn’t meant to suggest a strict template for feedback but a mnemonic to reflect and improve the practice. Even after years of active work on my critiques, I still from time to time go back to this formula and reflect on whether what I just wrote is effective.
 
 
 
 The tone
 
 
 
 Well-grounded content is the foundation of feedback, but that’s not really enough. The soft skills of the person who’s providing the critique can multiply the likelihood that the feedback will be well received and understood. Tone alone can make the difference between content that’s rejected or welcomed, and it’s been demonstrated that only positive feedback creates sustained change in people.
 
 
 
 Since our goal is to be understood and to have a positive working environment, tone is essential to work on. Over the years, I’ve tried to summarize the required soft skills in a formula that mirrors the one for content: the receptivity equation.
 
 
 
 
 
 
 
 Respectful feedback comes across as grounded, solid, and constructive. It’s the kind of feedback that, whether it’s positive or negative, is perceived as useful and fair.
 
 
 
 Timing refers to when the feedback happens. To-the-point feedback doesn’t have much hope of being well received if it’s given at the wrong time. Questioning the entire high-level information architecture of a new feature when it’s about to ship might still be relevant if that questioning highlights a major blocker that nobody saw, but it’s way more likely that those concerns will have to wait for a later rework. So in general, attune your feedback to the stage of the project. Early iteration? Late iteration? Polishing work in progress? These all have different needs. The right timing will make it more likely that your feedback will be well received.
 
 
 
 Attitude is the equivalent of intent, and in the context of person-to-person feedback, it can be referred to as radical candor. That means checking before we write to see whether what we have in mind will truly help the person and make the project better overall. This might be a hard reflection at times because maybe we don’t want to admit that we don’t really appreciate that person. Hopefully that’s not the case, but that can happen, and that’s okay. Acknowledging and owning that can help you make up for that: how would I write if I really cared about them? How can I avoid being passive aggressive? How can I be more constructive?
 
 
 
 Form is relevant especially in a diverse and cross-cultural work environments because having great content, perfect timing, and the right attitude might not come across if the way that we write creates misunderstandings. There might be many reasons for this: sometimes certain words might trigger specific reactions; sometimes nonnative speakers might not understand all the nuances of some sentences; sometimes our brains might just be different and we might perceive the world differently—neurodiversity must be taken into consideration. Whatever the reason, it’s important to review not just what we write but how.
 
 
 
 A few years back, I was asking for some feedback on how I give feedback. I received some good advice but also a comment that surprised me. They pointed out that when I wrote “Oh, […],” I made them feel stupid. That wasn’t my intent! I felt really bad, and I just realized that I provided feedback to them for months, and every time I might have made them feel stupid. I was horrified… but also thankful. I made a quick fix: I added “oh” in my list of replaced words (your choice between: macOS’s text replacement, aText, TextExpander, or others) so that when I typed “oh,” it was instantly deleted. 
 
 
 
 Something to highlight because it’s quite frequent—especially in teams that have a strong group spirit—is that people tend to beat around the bush. It’s important to remember here that a positive attitude doesn’t mean going light on the feedback—it just means that even when you provide hard, difficult, or challenging feedback, you do so in a way that’s respectful and constructive. The nicest thing that you can do for someone is to help them grow.
 
 
 
 We have a great advantage in giving feedback in written form: it can be reviewed by another person who isn’t directly involved, which can help to reduce or remove any bias that might be there. I found that the best, most insightful moments for me have happened when I’ve shared a comment and I’ve asked someone who I highly trusted, “How does this sound?,” “How can I do it better,” and even “How would you have written it?”—and I’ve learned a lot by seeing the two versions side by side.
 
 
 
 The format
 
 
 
 Asynchronous feedback also has a major inherent advantage: we can take more time to refine what we’ve written to make sure that it fulfills two main goals: the clarity of communication and the actionability of the suggestions.
 
 
 
 
 
 
 
 Let’s imagine that someone shared a design iteration for a project. You are reviewing it and leaving a comment. There are many ways to do this, and of course context matters, but let’s try to think about some elements that may be useful to consider.
 
 
 
 In terms of clarity, start by grounding the critique that you’re about to give by providing context. Specifically, this means describing where you’re coming from: do you have a deep knowledge of the project, or is this the first time that you’re seeing it? Are you coming from a high-level perspective, or are you figuring out the details? Are there regressions? Which user’s perspective are you taking when providing your feedback? Is the design iteration at a point where it would be okay to ship this, or are there major things that need to be addressed first?
 
 
 
 Providing context is helpful even if you’re sharing feedback within a team that already has some information on the project. And context is absolutely essential when giving cross-team feedback. If I were to review a design that might be indirectly related to my work, and if I had no knowledge about how the project arrived at that point, I would say so, highlighting my take as external.
 
 
 
 We often focus on the negatives, trying to outline all the things that could be done better. That’s of course important, but it’s just as important—if not more—to focus on the positives, especially if you saw progress from the previous iteration. This might seem superfluous, but it’s important to keep in mind that design is a discipline where there are hundreds of possible solutions for every problem. So pointing out that the design solution that was chosen is good and explaining why it’s good has two major benefits: it confirms that the approach taken was solid, and it helps to ground your negative feedback. In the longer term, sharing positive feedback can help prevent regressions on things that are going well because those things will have been highlighted as important. As a bonus, positive feedback can also help reduce impostor syndrome.
 
 
 
 There’s one powerful approach that combines both context and a focus on the positives: frame how the design is better than the status quo (compared to a previous iteration, competitors, or benchmarks) and why, and then on that foundation, you can add what could be improved. This is powerful because there’s a big difference between a critique that’s for a design that’s already in good shape and a critique that’s for a design that isn’t quite there yet.
 
 
 
 Another way that you can improve your feedback is to depersonalize the feedback: the comments should always be about the work, never about the person who made it. It’s “This button isn’t well aligned” versus “You haven’t aligned this button well.” This is very easy to change in your writing by reviewing it just before sending.
 
 
 
 In terms of actionability, one of the best approaches to help the designer who’s reading through your feedback is to split it into bullet points or paragraphs, which are easier to review and analyze one by one. For longer pieces of feedback, you might also consider splitting it into sections or even across multiple comments. Of course, adding screenshots or signifying markers of the specific part of the interface you’re referring to can also be especially useful.
 
 
 
 One approach that I’ve personally used effectively in some contexts is to enhance the bullet points with four markers using emojis. So a red square 🟥 means that it’s something that I consider blocking; a yellow diamond 🔶 is something that I can be convinced otherwise, but it seems to me that it should be changed; and a green circle 🟢 is a detailed, positive confirmation. I also use a blue spiral 🌀 for either something that I’m not sure about, an exploration, an open alternative, or just a note. But I’d use this approach only on teams where I’ve already established a good level of trust because if it happens that I have to deliver a lot of red squares, the impact could be quite demoralizing, and I’d reframe how I’d communicate that a bit.
 
 
 
 Let’s see how this would work by reusing the example that we used earlier as the first bullet point in this list:
 
 
 
 🔶 Navigation—When I see these two buttons, I expect one to go forward and one to go back. But this is the only screen where this happens, as before we just used a single button and an “×” to close. This seems to be breaking the consistency in the flow. Let’s make sure that all screens have the same two forward and back buttons so that users don’t get confused.🟢 Overall—I think the page is solid, and this is good enough to be our release candidate for a version 1.0.🟢 Metrics—Good improvement in the buttons on the metrics area; the improved contrast and new focus style make them more accessible. 🟥  Button Style—Using the green accent in this context creates the impression that it’s a positive action because green is usually perceived as a confirmation color. Do we need to explore a different color?🔶Tiles—Given the number of items on the page, and the overall page hierarchy, it seems to me that the tiles shouldn’t be using the Subtitle 1 style but the Subtitle 2 style. This will keep the visual hierarchy more consistent.🌀 Background—Using a light texture works well, but I wonder whether it adds too much noise in this kind of page. What is the thinking in using that?
 
 
 
 What about giving feedback directly in Figma or another design tool that allows in-place feedback? In general, I find these difficult to use because they hide discussions and they’re harder to track, but in the right context, they can be very effective. Just make sure that each of the comments is separate so that it’s easier to match each discussion to a single task, similar to the idea of splitting mentioned above.
 
 
 
 One final note: say the obvious. Sometimes we might feel that something is obviously good or obviously wrong, and so we don’t say it. Or sometimes we might have a doubt that we don’t express because the question might sound stupid. Say it—that’s okay. You might have to reword it a little bit to make the reader feel more comfortable, but don’t hold it back. Good feedback is transparent, even when it may be obvious.
 
 
 
 There’s another advantage of asynchronous feedback: written feedback automatically tracks decisions. Especially in large projects, “Why did we do this?” could be a question that pops up from time to time, and there’s nothing better than open, transparent discussions that can be reviewed at any time. For this reason, I recommend using software that saves these discussions, without hiding them once they are resolved. 
 
 
 
 Content, tone, and format. Each one of these subjects provides a useful model, but working to improve eight areas—observation, impact, question, timing, attitude, form, clarity, and actionability—is a lot of work to put in all at once. One effective approach is to take them one by one: first identify the area that you lack the most (either from your perspective or from feedback from others) and start there. Then the second, then the third, and so on. At first you’ll have to put in extra time for every piece of feedback that you give, but after a while, it’ll become second nature, and your impact on the work will multiply.
 
 
 
 Thanks to Brie Anne Demkiw and Mike Shelton for reviewing the first draft of this article.
 </content>
     </entry>
     <entry>
       <title>That’s Not My Burnout</title>
         <link href="https://alistapart.com/article/thats-not-my-burnout/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">Are you like me, reading about people fading away as they burn out, and feeling unable to relate? Do you feel like your feelings are invisible to the world because you’re experiencing burnout differently? When burnout starts to push down on us, our core comes through more. Beautiful, peaceful souls get quieter and fade into that distant and distracted burnout we’ve all read about. But some of us, those with fires always burning on the edges of our core, get hotter. In my heart I am fire. When I face burnout I double down, triple down, burning hotter and hotter to try to best the challenge. I don’t fade—I am engulfed in a zealous burnout. 
 
 
 
 So what on earth is a zealous burnout?
 
 
 
 Imagine a woman determined to do it all. She has two amazing children whom she, along with her husband who is also working remotely, is homeschooling during a pandemic. She has a demanding client load at work—all of whom she loves. She gets up early to get some movement in (or often catch up on work), does dinner prep as the kids are eating breakfast, and gets to work while positioning herself near “fourth grade” to listen in as she juggles clients, tasks, and budgets. Sound like a lot? Even with a supportive team both at home and at work, it is. 
 
 
 
 Sounds like this woman has too much on her plate and needs self-care. But no, she doesn’t have time for that. In fact, she starts to feel like she’s dropping balls. Not accomplishing enough. There’s not enough of her to be here and there; she is trying to divide her mind in two all the time, all day, every day. She starts to doubt herself. And as those feelings creep in more and more, her internal narrative becomes more and more critical.
 
 
 
 Suddenly she KNOWS what she needs to do! She should DO MORE. 
 
 
 
 This is a hard and dangerous cycle. Know why? Because once she doesn’t finish that new goal, that narrative will get worse. Suddenly she’s failing. She isn’t doing enough. SHE is not enough. She might fail, she might fail her family...so she’ll find more she should do. She doesn’t sleep as much, move as much, all in the efforts to do more. Caught in this cycle of trying to prove herself to herself, never reaching any goal. Never feeling “enough.” 
 
 
 
 So, yeah, that’s what zealous burnout looks like for me. It doesn’t happen overnight in some grand gesture but instead slowly builds over weeks and months. My burning out process looks like speeding up, not a person losing focus. I speed up and up and up...and then I just stop.
 
 
 
 I am the one who could
 
 
 
 It’s funny the things that shape us. Through the lens of childhood, I viewed the fears, struggles, and sacrifices of someone who had to make it all work without having enough. I was lucky that my mother was so resourceful and my father supportive; I never went without and even got an extra here or there. 
 
 
 
 Growing up, I did not feel shame when my mother paid with food stamps; in fact, I’d have likely taken on any debate on the topic, verbally eviscerating anyone who dared to criticize the disabled woman trying to make sure all our needs were met with so little. As a child, I watched the way the fear of not making those ends meet impacted people I love. As the non-disabled person in my home, I would take on many of the physical tasks because I was “the one who could” make our lives a little easier. I learned early to associate fears or uncertainty with putting more of myself into it—I am the one who can. I learned early that when something frightens me, I can double down and work harder to make it better. I can own the challenge. When people have seen this in me as an adult, I’ve been told I seem fearless, but make no mistake, I’m not. If I seem fearless, it’s because this behavior was forged from other people’s fears. 
 
 
 
 And here I am, more than 30 years later still feeling the urge to mindlessly push myself forward when faced with overwhelming tasks ahead of me, assuming that I am the one who can and therefore should. I find myself driven to prove that I can make things happen if I work longer hours, take on more responsibility, and do more. 
 
 
 
 I do not see people who struggle financially as failures, because I have seen how strong that tide can be—it pulls you along the way. I truly get that I have been privileged to be able to avoid many of the challenges that were present in my youth. That said, I am still “the one who can” who feels she should, so if I were faced with not having enough to make ends meet for my own family, I would see myself as having failed. Though I am supported and educated, most of this is due to good fortune. I will, however, allow myself the arrogance of saying I have been careful with my choices to have encouraged that luck. My identity stems from the idea that I am “the one who can” so therefore feel obligated to do the most. I can choose to stop, and with some quite literal cold water splashed in my face, I’ve made the choice to before. But that choosing to stop is not my go-to; I move forward, driven by a fear that is so a part of me that I barely notice it’s there until I’m feeling utterly worn away.
 
 
 
 So why all the history? You see, burnout is a fickle thing. I have heard and read a lot about burnout over the years. Burnout is real. Especially now, with COVID, many of us are balancing more than we ever have before—all at once! It’s hard, and the procrastinating, the avoidance, the shutting down impacts so many amazing professionals. There are important articles that relate to what I imagine must be the majority of people out there, but not me. That’s not what my burnout looks like.
 
 
 
 The dangerous invisibility of zealous burnout
 
 
 
 A lot of work environments see the extra hours, extra effort, and overall focused commitment as an asset (and sometimes that’s all it is). They see someone trying to rise to challenges, not someone stuck in their fear. Many well-meaning organizations have safeguards in place to protect their teams from burnout. But in cases like this, those alarms are not always tripped, and then when the inevitable stop comes, some members of the organization feel surprised and disappointed. And sometimes maybe even betrayed. 
 
 
 
 Parents—more so mothers, statistically speaking—are praised as being so on top of it all when they can work, be involved in the after-school activities, practice self-care in the form of diet and exercise, and still meet friends for coffee or wine. During COVID many of us have binged countless streaming episodes showing how it’s so hard for the female protagonist, but she is strong and funny and can do it. It’s a “very special episode” when she breaks down, cries in the bathroom, woefully admits she needs help, and just stops for a bit. Truth is, countless people are hiding their tears or are doom-scrolling to escape. We know that the media is a lie to amuse us, but often the perception that it’s what we should strive for has penetrated much of society.
 
 
 
 Women and burnout
 
 
 
 I love men. And though I don’t love every man (heads up, I don’t love every woman or nonbinary person either), I think there is a beautiful spectrum of individuals who represent that particular binary gender. 
 
 
 
 That said, women are still more often at risk of burnout than their male counterparts, especially in these COVID stressed times. Mothers in the workplace feel the pressure to do all the “mom” things while giving 110%. Mothers not in the workplace feel they need to do more to “justify” their lack of traditional employment. Women who are not mothers often feel the need to do even more because they don’t have that extra pressure at home. It’s vicious and systemic and so a part of our culture that we’re often not even aware of the enormity of the pressures we put on ourselves and each other. 
 
 
 
 And there are prices beyond happiness too. Harvard Health Publishing released a study a decade ago that “uncovered strong links between women’s job stress and cardiovascular disease.” The CDC noted, “Heart disease is the leading cause of death for women in the United States, killing 299,578 women in 2017—or about 1 in every 5 female deaths.” 
 
 
 
 This relationship between work stress and health, from what I have read, is more dangerous for women than it is for their non-female counterparts.
 
 
 
 But what if your burnout isn’t like that either?
 
 
 
 That might not be you either. After all, each of us is so different and how we respond to stressors is too. It’s part of what makes us human. Don’t stress what burnout looks like, just learn to recognize it in yourself. Here are a few questions I sometimes ask friends if I am concerned about them.
 
 
 
 Are you happy? This simple question should be the first thing you ask yourself. Chances are, even if you’re burning out doing all the things you love, as you approach burnout you’ll just stop taking as much joy from it all.
 
 
 
 Do you feel empowered to say no? I have observed in myself and others that when someone is burning out, they no longer feel they can say no to things. Even those who don’t “speed up” feel pressure to say yes to not disappoint the people around them.
 
 
 
 What are three things you’ve done for yourself? Another observance is that we all tend to stop doing things for ourselves. Anything from skipping showers and eating poorly to avoiding talking to friends. These can be red flags. 
 
 
 
 Are you making excuses? Many of us try to disregard feelings of burnout. Over and over I have heard, “It’s just crunch time,” “As soon as I do this one thing, it will all be better,” and “Well I should be able to handle this, so I’ll figure it out.” And it might really be crunch time, a single goal, and/or a skill set you need to learn. That happens—life happens. BUT if this doesn’t stop, be honest with yourself. If you’ve worked more 50-hour weeks since January than not, maybe it’s not crunch time—maybe it’s a bad situation that you’re burning out from.
 
 
 
 Do you have a plan to stop feeling this way? If something is truly temporary and you do need to just push through, then it has an exit route with adefined end.
 
 
 
 Take the time to listen to yourself as you would a friend. Be honest, allow yourself to be uncomfortable, and break the thought cycles that prevent you from healing. 
 
 
 
 So now what?
 
 
 
 What I just described is a different path to burnout, but it’s still burnout. There are well-established approaches to working through burnout:
 
 
 
 Get enough sleep.Eat healthy.Work out.Get outside.Take a break.Overall, practice self-care.
 
 
 
 Those are hard for me because they feel like more tasks. If I’m in the burnout cycle, doing any of the above for me feels like a waste. The narrative is that if I’m already failing, why would I take care of myself when I’m dropping all those other balls? People need me, right? 
 
 
 
 If you’re deep in the cycle, your inner voice might be pretty awful by now. If you need to, tell yourself you need to take care of the person your people depend on. If your roles are pushing you toward burnout, use them to help make healing easier by justifying the time spent working on you. 
 
 
 
 To help remind myself of the airline attendant message about putting the mask on yourself first, I have come up with a few things that I do when I start feeling myself going into a zealous burnout.
 
 
 
 Cook an elaborate meal for someone! 
 
 
 
 OK, I am a “food-focused” individual so cooking for someone is always my go-to. There are countless tales in my home of someone walking into the kitchen and turning right around and walking out when they noticed I was “chopping angrily.” But it’s more than that, and you should give it a try. Seriously. It’s the perfect go-to if you don’t feel worthy of taking time for yourself—do it for someone else. Most of us work in a digital world, so cooking can fill all of your senses and force you to be in the moment with all the ways you perceive the world. It can break you out of your head and help you gain a better perspective. In my house, I’ve been known to pick a place on the map and cook food that comes from wherever that is (thank you, Pinterest). I love cooking Indian food, as the smells are warm, the bread needs just enough kneading to keep my hands busy, and the process takes real attention for me because it’s not what I was brought up making. And in the end, we all win!
 
 
 
 Vent like a foul-mouthed fool
 
 
 
 Be careful with this one! 
 
 
 
 I have been making an effort to practice more gratitude over the past few years, and I recognize the true benefits of that. That said, sometimes you just gotta let it all out—even the ugly. Hell, I’m a big fan of not sugarcoating our lives, and that sometimes means that to get past the big pile of poop, you’re gonna wanna complain about it a bit. 
 
 
 
 When that is what’s needed, turn to a trusted friend and allow yourself some pure verbal diarrhea, saying all the things that are bothering you. You need to trust this friend not to judge, to see your pain, and, most importantly, to tell you to remove your cranium from your own rectal cavity. Seriously, it’s about getting a reality check here! One of the things I admire the most about my husband (though often after the fact) is his ability to break things down to their simplest. “We’re spending our lives together, of course you’re going to disappoint me from time to time, so get over it” has been his way of speaking his dedication, love, and acceptance of me—and I could not be more grateful. It also, of course, has meant that I needed to remove my head from that rectal cavity. So, again, usually those moments are appreciated in hindsight.
 
 
 
 Pick up a book! 
 
 
 
 There are many books out there that aren’t so much self-help as they are people just like you sharing their stories and how they’ve come to find greater balance. Maybe you’ll find something that speaks to you. Titles that have stood out to me include:
 
 
 
 Thrive by Arianna HuffingtonTools of Titans by Tim FerrissGirl, Stop Apologizing by Rachel HollisDare to Lead by Brené Brown
 
 
 
 Or, another tactic I love to employ is to read or listen to a book that has NOTHING to do with my work-life balance. I’ve read the following books and found they helped balance me out because my mind was pondering their interesting topics instead of running in circles:
 
 
 
 The Drunken Botanist by Amy StewartSuperlife by Darin OlienA Brief History of Everyone Who Ever Lived by Adam RutherfordGaia’s Garden by Toby Hemenway 
 
 
 
 If you’re not into reading, pick up a topic on YouTube or choose a podcast to subscribe to. I’ve watched countless permaculture and gardening topics in addition to how to raise chickens and ducks. For the record, I do not have a particularly large food garden, nor do I own livestock of any kind...yet. I just find the topic interesting, and it has nothing to do with any aspect of my life that needs anything from me.
 
 
 
 Forgive yourself 
 
 
 
 You are never going to be perfect—hell, it would be boring if you were. It’s OK to be broken and flawed. It’s human to be tired and sad and worried. It’s OK to not do it all. It’s scary to be imperfect, but you cannot be brave if nothing were scary.
 
 
 
 This last one is the most important: allow yourself permission to NOT do it all. You never promised to be everything to everyone at all times. We are more powerful than the fears that drive us. 
 
 
 
 This is hard. It is hard for me. It’s what’s driven me to write this—that it’s OK to stop. It’s OK that your unhealthy habit that might even benefit those around you needs to end. You can still be successful in life.
 
 
 
 I recently read that we are all writing our eulogy in how we live. Knowing that your professional accomplishments won’t be mentioned in that speech, what will yours say? What do you want it to say? 
 
 
 
 Look, I get that none of these ideas will “fix it,” and that’s not their purpose. None of us are in control of our surroundings, only how we respond to them. These suggestions are to help stop the spiral effect so that you are empowered to address the underlying issues and choose your response. They are things that work for me most of the time. Maybe they’ll work for you.
 
 
 
 Does this sound familiar? 
 
 
 
 If this sounds familiar, it’s not just you. Don’t let your negative self-talk tell you that you “even burn out wrong.” It’s not wrong. Even if rooted in fear like my own drivers, I believe that this need to do more comes from a place of love, determination, motivation, and other wonderful attributes that make you the amazing person you are. We’re going to be OK, ya know. The lives that unfold before us might never look like that story in our head—that idea of “perfect” or “done” we’re looking for, but that’s OK. Really, when we stop and look around, usually the only eyes that judge us are in the mirror. 
 
 
 
 Do you remember that Winnie the Pooh sketch that had Pooh eat so much at Rabbit’s house that his buttocks couldn’t fit through the door? Well, I already associate a lot with Rabbit, so it came as no surprise when he abruptly declared that this was unacceptable. But do you recall what happened next? He put a shelf across poor Pooh’s ankles and decorations on his back, and made the best of the big butt in his kitchen. 
 
 
 
 At the end of the day we are resourceful and know that we are able to push ourselves if we need to—even when we are tired to our core or have a big butt of fluff ‘n’ stuff in our room. None of us has to be afraid, as we can manage any obstacle put in front of us. And maybe that means we will need to redefine success to allow space for being uncomfortably human, but that doesn’t really sound so bad either. 
 
 
 
 So, wherever you are right now, please breathe. Do what you need to do to get out of your head. Forgive and take care.
 </content>
     </entry>
     <entry>
       <title>Beware the Cut ‘n’ Paste Persona</title>
         <link href="https://alistapart.com/article/beware-the-cut-n-paste-persona/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">This Person Does Not Exist is a website that generates human faces with a machine learning algorithm. It takes real portraits and recombines them into fake human faces. We recently scrolled past a LinkedIn post stating that this website could be useful “if you are developing a persona and looking for a photo.” 
 
 
 
 We agree: the computer-generated faces could be a great match for personas—but not for the reason you might think. Ironically, the website highlights the core issue of this very common design method: the person(a) does not exist. Like the pictures, personas are artificially made. Information is taken out of natural context and recombined into an isolated snapshot that’s detached from reality. 
 
 
 
 But strangely enough, designers use personas to inspire their design for the real world. 
 
 
 
 Personas: A step back
 
 
 
 Most designers have created, used, or come across personas at least once in their career. In their article “Personas - A Simple Introduction,” the Interaction Design Foundation defines personas as “fictional characters, which you create based upon your research in order to represent the different user types that might use your service, product, site, or brand.” In their most complete expression, personas typically consist of a name, profile picture, quotes, demographics, goals, needs, behavior in relation to a certain service/product, emotions, and motivations (for example, see Creative Companion’s Persona Core Poster). The purpose of personas, as stated by design agency Designit, is “to make the research relatable, [and] easy to communicate, digest, reference, and apply to product and service development.”
 
 
 
 The decontextualization of personas
 
 
 
 Personas are popular because they make “dry” research data more relatable, more human. However, this method constrains the researcher’s data analysis in such a way that the investigated users are removed from their unique contexts. As a result, personas don’t portray key factors that make you understand their decision-making process or allow you to relate to users’ thoughts and behavior; they lack stories. You understand what the persona did, but you don’t have the background to understand why. You end up with representations of users that are actually less human.
 
 
 
 This “decontextualization” we see in personas happens in four ways, which we’ll explain below. 
 
 
 
 Personas assume people are static 
 
 
 
 Although many companies still try to box in their employees and customers with outdated personality tests (referring to you, Myers-Briggs), here’s a painfully obvious truth: people are not a fixed set of features. You act, think, and feel differently according to the situations you experience. You appear different to different people; you might act friendly to some, rough to others. And you change your mind all the time about decisions you’ve taken. 
 
 
 
 Modern psychologists agree that while people generally behave according to certain patterns, it’s actually a combination of background and environment that determines how people act and take decisions. The context—the environment, the influence of other people, your mood, the entire history that led up to a situation—determines the kind of person you are in each specific moment. 
 
 
 
 In their attempt to simplify reality, personas do not take this variability into account; they present a user as a fixed set of features. Like personality tests, personas snatch people away from real life. Even worse, people are reduced to a label and categorized as “that kind of person” with no means to exercise their innate flexibility. This practice reinforces stereotypes, lowers diversity, and doesn’t reflect reality. 
 
 
 
 Personas focus on individuals, not the environment
 
 
 
 In the real world, you’re designing for a context, not for an individual. Each person lives in a family, a community, an ecosystem, where there are environmental, political, and social factors you need to consider. A design is never meant for a single user. Rather, you design for one or more particular contexts in which many people might use that product. Personas, however, show the user alone rather than describe how the user relates to the environment. 
 
 
 
 Would you always make the same decision over and over again? Maybe you’re a committed vegan but still decide to buy some meat when your relatives are coming over. As they depend on different situations and variables, your decisions—and behavior, opinions, and statements—are not absolute but highly contextual. The persona that “represents” you wouldn’t take into account this dependency, because it doesn’t specify the premises of your decisions. It doesn’t provide a justification of why you act the way you do. Personas enact the well-known bias called fundamental attribution error: explaining others’ behavior too much by their personality and too little by the situation.
 
 
 
 As mentioned by the Interaction Design Foundation, personas are usually placed in a scenario that’s a “specific context with a problem they want to or have to solve”—does that mean context actually is considered? Unfortunately, what often happens is that you take a fictional character and based on that fiction determine how this character might deal with a certain situation. This is made worse by the fact that you haven’t even fully investigated and understood the current context of the people your persona seeks to represent; so how could you possibly understand how they would act in new situations? 
 
 
 
 Personas are meaningless averages
 
 
 
 As mentioned in Shlomo Goltz’s introductory article on Smashing Magazine, “a persona is depicted as a specific person but is not a real individual; rather, it is synthesized from observations of many people.” A well-known critique to this aspect of personas is that the average person does not exist, as per the famous example of the USA Air Force designing planes based on the average of 140 of their pilots’ physical dimensions and not a single pilot actually fitting within that average seat. 
 
 
 
 The same limitation applies to mental aspects of people. Have you ever heard a famous person say, “They took what I said out of context! They used my words, but I didn’t mean it like that.” The celebrity’s statement was reported literally, but the reporter failed to explain the context around the statement and didn’t describe the non-verbal expressions. As a result, the intended meaning was lost. You do the same when you create personas: you collect somebody’s statement (or goal, or need, or emotion), of which the meaning can only be understood if you provide its own specific context, yet report it as an isolated finding. 
 
 
 
 But personas go a step further, extracting a decontextualized finding and joining it with another decontextualized finding from somebody else. The resulting set of findings often does not make sense: it’s unclear, or even contrasting, because it lacks the underlying reasons on why and how that finding has arisen. It lacks meaning. And the persona doesn’t give you the full background of the person(s) to uncover this meaning: you would need to dive into the raw data for each single persona item to find it. What, then, is the usefulness of the persona?
 
 
 
 
 
 
 
 The relatability of personas is deceiving
 
 
 
 To a certain extent, designers realize that a persona is a lifeless average. To overcome this, designers invent and add “relatable” details to personas to make them resemble real individuals. Nothing captures the absurdity of this better than a sentence by the Interaction Design Foundation: “Add a few fictional personal details to make the persona a realistic character.” In other words, you add non-realism in an attempt to create more realism. You deliberately obscure the fact that “John Doe” is an abstract representation of research findings; but wouldn’t it be much more responsible to emphasize that John is only an abstraction? If something is artificial, let’s present it as such.
 
 
 
 It’s the finishing touch of a persona’s decontextualization: after having assumed that people’s personalities are fixed, dismissed the importance of their environment, and hidden meaning by joining isolated, non-generalizable findings, designers invent new context to create (their own) meaning. In doing so, as with everything they create, they introduce a host of biases. As phrased by Designit, as designers we can “contextualize [the persona] based on our reality and experience. We create connections that are familiar to us.” This practice reinforces stereotypes, doesn’t reflect real-world diversity, and gets further away from people’s actual reality with every detail added. 
 
 
 
 To do good design research, we should report the reality “as-is” and make it relatable for our audience, so everyone can use their own empathy and develop their own interpretation and emotional response.
 
 
 
 Dynamic Selves: The alternative to personas
 
 
 
 If we shouldn’t use personas, what should we do instead? 
 
 
 
 Designit has proposed using Mindsets instead of personas. Each Mindset is a “spectrum of attitudes and emotional responses that different people have within the same context or life experience.” It challenges designers to not get fixated on a single user’s way of being. Unfortunately, while being a step in the right direction, this proposal doesn’t take into account that people are part of an environment that determines their personality, their behavior, and, yes, their mindset. Therefore, Mindsets are also not absolute but change in regard to the situation. The question remains, what determines a certain Mindset?
 
 
 
 Another alternative comes from Margaret P., author of the article “Kill Your Personas,” who has argued for replacing personas with persona spectrums that consist of a range of user abilities. For example, a visual impairment could be permanent (blindness), temporary (recovery from eye surgery), or situational (screen glare). Persona spectrums are highly useful for more inclusive and context-based design, as they’re based on the understanding that the context is the pattern, not the personality. Their limitation, however, is that they have a very functional take on users that misses the relatability of a real person taken from within a spectrum. 
 
 
 
 In developing an alternative to personas, we aim to transform the standard design process to be context-based. Contexts are generalizable and have patterns that we can identify, just like we tried to do previously with people. So how do we identify these patterns? How do we ensure truly context-based design? 
 
 
 
 Understand real individuals in multiple contexts
 
 
 
 Nothing is more relatable and inspiring than reality. Therefore, we have to understand real individuals in their multi-faceted contexts, and use this understanding to fuel our design. We refer to this approach as Dynamic Selves.
 
 
 
 Let’s take a look at what the approach looks like, based on an example of how one of us applied it in a recent project that researched habits of Italians around energy consumption. We drafted a design research plan aimed at investigating people’s attitudes toward energy consumption and sustainable behavior, with a focus on smart thermostats. 
 
 
 
 1. Choose the right sample
 
 
 
 When we argue against personas, we’re often challenged with quotes such as “Where are you going to find a single person that encapsulates all the information from one of these advanced personas[?]” The answer is simple: you don’t have to. You don’t need to have information about many people for your insights to be deep and meaningful. 
 
 
 
 In qualitative research, validity does not derive from quantity but from accurate sampling. You select the people that best represent the “population” you’re designing for. If this sample is chosen well, and you have understood the sampled people in sufficient depth, you’re able to infer how the rest of the population thinks and behaves. There’s no need to study seven Susans and five Yuriys; one of each will do. 
 
 
 
 Similarly, you don’t need to understand Susan in fifteen different contexts. Once you’ve seen her in a couple of diverse situations, you’ve understood the scheme of Susan’s response to different contexts. Not Susan as an atomic being but Susan in relation to the surrounding environment: how she might act, feel, and think in different situations. 
 
 
 
 Given that each person is representative of a part of the total population you’re researching, it becomes clear why each should be represented as an individual, as each already is an abstraction of a larger group of individuals in similar contexts. You don’t want abstractions of abstractions! These selected people need to be understood and shown in their full expression, remaining in their microcosmos—and if you want to identify patterns you can focus on identifying patterns in contexts.
 
 
 
 Yet the question remains: how do you select a representative sample? First of all, you have to consider what’s the target audience of the product or service you are designing: it might be useful to look at the company’s goals and strategy, the current customer base, and/or a possible future target audience. 
 
 
 
 In our example project, we were designing an application for those who own a smart thermostat. In the future, everyone could have a smart thermostat in their house. Right now, though, only early adopters own one. To build a significant sample, we needed to understand the reason why these early adopters became such. We therefore recruited by asking people why they had a smart thermostat and how they got it. There were those who had chosen to buy it, those who had been influenced by others to buy it, and those who had found it in their house. So we selected representatives of these three situations, from different age groups and geographical locations, with an equal balance of tech savvy and non-tech savvy participants. 
 
 
 
 2. Conduct your research
 
 
 
 After having chosen and recruited your sample, conduct your research using ethnographic methodologies. This will make your qualitative data rich with anecdotes and examples. In our example project, given COVID-19 restrictions, we converted an in-house ethnographic research effort into remote family interviews, conducted from home and accompanied by diary studies.
 
 
 
 To gain an in-depth understanding of attitudes and decision-making trade-offs, the research focus was not limited to the interviewee alone but deliberately included the whole family. Each interviewee would tell a story that would then become much more lively and precise with the corrections or additional details coming from wives, husbands, children, or sometimes even pets. We also focused on the relationships with other meaningful people (such as colleagues or distant family) and all the behaviors that resulted from those relationships. This wide research focus allowed us to shape a vivid mental image of dynamic situations with multiple actors. 
 
 
 
 It’s essential that the scope of the research remains broad enough to be able to include all possible actors. Therefore, it normally works best to define broad research areas with macro questions. Interviews are best set up in a semi-structured way, where follow-up questions will dive into topics mentioned spontaneously by the interviewee. This open-minded “plan to be surprised” will yield the most insightful findings. When we asked one of our participants how his family regulated the house temperature, he replied, “My wife has not installed the thermostat’s app—she uses WhatsApp instead. If she wants to turn on the heater and she is not home, she will text me. I am her thermostat.”
 
 
 
 3. Analysis: Create the Dynamic Selves
 
 
 
 During the research analysis, you start representing each individual with multiple Dynamic Selves, each “Self” representing one of the contexts you have investigated. The core of each Dynamic Self is a quote, which comes supported by a photo and a few relevant demographics that illustrate the wider context. The research findings themselves will show which demographics are relevant to show. In our case, as our research focused on families and their lifestyle to understand their needs for thermal regulation, the important demographics were family type, number and nature of houses owned, economic status, and technological maturity. (We also included the individual’s name and age, but they’re optional—we included them to ease the stakeholders’ transition from personas and be able to connect multiple actions and contexts to the same person).
 
 
 
 
 
 
 
 To capture exact quotes, interviews need to be video-recorded and notes need to be taken verbatim as much as possible. This is essential to the truthfulness of the several Selves of each participant. In the case of real-life ethnographic research, photos of the context and anonymized actors are essential to build realistic Selves. Ideally, these photos should come directly from field research, but an evocative and representative image will work, too, as long as it’s realistic and depicts meaningful actions that you associate with your participants. For example, one of our interviewees told us about his mountain home where he used to spend every weekend with his family. Therefore, we portrayed him hiking with his little daughter. 
 
 
 
 At the end of the research analysis, we displayed all of the Selves’ “cards” on a single canvas, categorized by activities. Each card displayed a situation, represented by a quote and a unique photo. All participants had multiple cards about themselves.
 
 
 
 
 
 
 
 4. Identify design opportunities
 
 
 
 Once you have collected all main quotes from the interview transcripts and diaries, and laid them all down as Self cards, you will see patterns emerge. These patterns will highlight the opportunity areas for new product creation, new functionalities, and new services—for new design. 
 
 
 
 In our example project, there was a particularly interesting insight around the concept of humidity. We realized that people don’t know what humidity is and why it is important to monitor it for health: an environment that’s too dry or too wet can cause respiratory problems or worsen existing ones. This highlighted a big opportunity for our client to educate users on this concept and become a health advisor.
 
 
 
 Benefits of Dynamic Selves
 
 
 
 When you use the Dynamic Selves approach in your research, you start to notice unique social relations, peculiar situations real people face and the actions that follow, and that people are surrounded by changing environments. In our thermostat project, we have come to know one of the participants, Davide, as a boyfriend, dog-lover, and tech enthusiast. 
 
 
 
 Davide is an individual we might have once reduced to a persona called “tech enthusiast.” But we can have tech enthusiasts who have families or are single, who are rich or poor. Their motivations and priorities when deciding to purchase a new thermostat can be opposite according to these different frames. 
 
 
 
 Once you have understood Davide in multiple situations, and for each situation have understood in sufficient depth the underlying reasons for his behavior, you’re able to generalize how he would act in another situation. You can use your understanding of him to infer what he would think and do in the contexts (or scenarios) that you design for.
 
 
 
 
 
 
 
 The Dynamic Selves approach aims to dismiss the conflicted dual purpose of personas—to summarize and empathize at the same time—by separating your research summary from the people you’re seeking to empathize with. This is important because our empathy for people is affected by scale: the bigger the group, the harder it is to feel empathy for others. We feel the strongest empathy for individuals we can personally relate to.  
 
 
 
 If you take a real person as inspiration for your design, you no longer need to create an artificial character. No more inventing details to make the character more “realistic,” no more unnecessary additional bias. It’s simply how this person is in real life. In fact, in our experience, personas quickly become nothing more than a name in our priority guides and prototype screens, as we all know that these characters don’t really exist. 
 
 
 
 Another powerful benefit of the Dynamic Selves approach is that it raises the stakes of your work: if you mess up your design, someone real, a person you and the team know and have met, is going to feel the consequences. It might stop you from taking shortcuts and will remind you to conduct daily checks on your designs.
 
 
 
 And finally, real people in their specific contexts are a better basis for anecdotal storytelling and therefore are more effective in persuasion. Documentation of real research is essential in achieving this result. It adds weight and urgency behind your design arguments: “When I met Alessandra, the conditions of her workplace struck me. Noise, bad ergonomics, lack of light, you name it. If we go for this functionality, I’m afraid we’re going to add complexity to her life.”
 
 
 
 Conclusion
 
 
 
 Designit mentioned in their article on Mindsets that “design thinking tools offer a shortcut to deal with reality’s complexities, but this process of simplification can sometimes flatten out people’s lives into a few general characteristics.” Unfortunately, personas have been culprits in a crime of oversimplification. They are unsuited to represent the complex nature of our users’ decision-making processes and don’t account for the fact that humans are immersed in contexts. 
 
 
 
 Design needs simplification but not generalization. You have to look at the research elements that stand out: the sentences that captured your attention, the images that struck you, the sounds that linger. Portray those, use them to describe the person in their multiple contexts. Both insights and people come with a context; they cannot be cut from that context because it would remove meaning. 
 
 
 
 It’s high time for design to move away from fiction, and embrace reality—in its messy, surprising, and unquantifiable beauty—as our guide and inspiration.
 </content>
     </entry>
     <entry>
       <title>Immersive Content Strategy</title>
         <link href="https://alistapart.com/article/immersive-content-strategy/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">Beyond the severe toll of the coronavirus pandemic, perhaps no other disruption has transformed user experiences quite like how the tethers to our formerly web-biased era of content have frayed. We’re transitioning to a new world of remote work and digital content. We’re also experimenting with unprecedented content channels that, not too long ago, elicited chuckles at the watercooler, like voice interfaces, digital signage, augmented reality, and virtual reality.
 
 
 
 Many factors are responsible. Perhaps it’s because we yearn for immersive spaces that temporarily resurrect the Before Times, or maybe it’s due to the boredom and tedium of our now-cemented stuck-at-home routines. But aural user experiences slinging voice content, and immersive user experiences unlocking new forms of interacting with formerly web-bound content, are no longer figments of science fiction. They’re fast becoming a reality in the here and now.
 
 
 
 The idea of immersive experiences is all the rage these days, and content strategists and designers are now seriously examining this still-amorphous trend. Immersive experiences embrace concepts like geolocation, digital signage, and extended reality (XR). XR encompasses augmented reality (AR) and virtual reality (VR) as well as their fusion: mixed reality (MR). Sales of immersive equipment like gaming and VR headsets have skyrocketed during the pandemic, and content strategists are increasingly attuned to the kaleidoscope of devices and interfaces users now interact with on a daily basis to acquire information.
 
 
 
 Immersive user experiences are becoming commonplace, and, more importantly, new tools and frameworks are emerging for designers and developers looking to get their hands dirty. But that doesn’t mean our content is ready for prime time in settings unbound from the web like physical spaces, digital signage, or extended reality. Recasting your fixed web content in more immersive ways will enable more than just newfangled user experiences; it’ll prepare you for flexibility in an unpredictable future as well.
 
 
 
 Agnostic content for immersive experiences
 
 
 
 These days, we interact with content through a slew of devices. It’s no longer the case that we navigate information on a single desktop computer screen. In my upcoming book Voice Content and Usability (A Book Apart, coming June 2021), I draw a distinction between what I call macrocontent—the unwieldy long-form copy plastered across browser viewports—and Anil Dash’s definition of microcontent: the kind of brisk, contextless bursts of content that we find nowadays on Apple Watches, Samsung TVs, and Amazon Alexas.
 
 
 
 Today, content also has to be ready for contextless situations—not only in truncated form when we struggle to make out tiny text on our smartwatches or scroll through new television series on Roku but also in places it’s never ended up before. As the twenty-first century continues apace, our clients and our teams are beginning to come to terms with the fact that the way copy is consumed in just a few decades will bear no resemblance whatsoever to the prosaic browsers and even smartphones of today.
 
 
 
 What do we mean by immersive content?
 
 
 
 Immersive experiences are those that, according to Forrester, blur “the boundaries between the human, digital, physical, and virtual realms” to facilitate smarter, more interactive user experiences. But what do we mean by immersive content? I define immersive content as content that plays in the sandbox of physical and virtual space—copy and media that are situationally or locationally aware rather than rooted in a static, unmoving computer screen.
 
 
 
 Whether a space is real or virtual, immersive content (or spatialcontent) will be a key way in which our customers and users deal with information in the coming years. Unlike voice content, which deals with time and sound, immersive content works with space and sight. Immersive content operates not along the axis of links and page changes but rather along situational changes, as the following figure illustrates.
 
 
 
 In this illustration, each rectangle represents different displays that appear based on situational changes such as movement in space or adjustment of perspective that result in the delivery of different content from the previous context. One of these, such as the rightmost display, can be a web-enabled content display with links to other content presented in the same display. This illustration thus demonstrates two forms of navigation: traditional link navigation and immersive situational navigation.
 
 
 
 Acknowledging the actual or imagined surroundings of where we are as human beings will have vast implications for content strategy, omnichannel marketing, usability testing, and accessibility. Before we dig deeper, let’s define a few clear categories of immersive content:
 
 
 
 Digital signage content. Though it may seem a misnomer, digital signage is one of the most widespread examples of immersive content already in use today. For example, you may have seen it used to display a guide of stores at a mall or to aid wayfinding in an airport. While still largely bound to flat screens, it’s an example of content in space.Locational content. Locational content involves copy that is delivered to a user on a personal device based on their current location in the world or within an identified physical space. Most often mediated through Bluetooth low-energy (BLE) beacon technology or GPS location services, it’s an example of content at a point in space.Augmented reality content. Unlike locational content, which doesn’t usually adjust itself seamlessly based on how users move in real-world space, AR content is now common in museums and other environments—typically as overlays that are superimposed over actual physical surroundings and adjust dynamically according to the user’s position and perspective. It’s content projected into real-world space.Virtual reality content. Like AR content, VR content is dependent on its imagined surroundings in terms of how it displays, but it’s part of a nonexistent space that is fully immersive, an example of content projected into virtual space.Navigable content. Long a gimmicky playground for designers and developers interested in pushing the envelope, navigable content is copy that users can move across and sift through as if it were a physical space itself: true content as space.
 
 
 
 The following illustration depicts these types of immersive content in their typical habitats.
 
 
 
 
 
 
 
 Why auditing immersive content is important
 
 
 
 Alongside conversational and voice content, immersive content is a compelling example of breaking content out of the limiting box where it has long lived: the browser viewport, the computer screen, and the 8.5”x11” or broadsheet borders of print media. For centuries, our written copy has been affixed to the staid standards of whatever bookbinders, newspaper printing presses, and screen manufacturers decided. Today, however, for the first time, we’re surmounting those arbitrary barriers and situating content in contexts that challenge all the assumptions we’ve made since the era of Gutenberg—and, arguably, since clay tablets, papyrus manuscripts, and ancient scrolls.
 
 
 
 Today, it’s never been more pressing to implement an omnichannel content strategy that centers the reality our customers increasingly live in: a world in which information can end up on any device, even if it has no tether to a clickable or scrollable setting. One of the most important elements of such a future-proof content strategy is an omnichannel content audit that evaluates your content from a variety of standpoints so you can manage and plan it effectively. These audits generally consist of several steps:
 
 
 
 Write a questionnaire. Each content item needs to be examined from the perspective of each channel through a series of channel-relevant questions, like whether content is legible or discoverable on every conduit through which it travels.Settle the criteria. No questionnaire is complete for a content audit without evaluation criteria that measure how the content performs and recommendation criteria that determine necessary steps to improve its efficacy.Discuss with stakeholders. At the end of any content audit, it’s important to leaf through the results and any recommendations in a frank discussion with stakeholders, including content strategists, editors, designers, and others.
 
 
 
 In my previous article for A List Apart, I shared the work we did on a conversational content audit for Ask GeorgiaGov, the first (but now decommissioned) Alexa skill for residents of the state of Georgia. Such a content audit is just one facet of the multifaceted omnichannel content strategy along various dimensions you’ll need to consider. Nonetheless, there are a few things all content audits share in terms of foundational evaluation criteria across all content delivery channels:
 
 
 
 Content legibility. Is the content readable or easily consumable from a variety of vantage points and perspectives? In the case of immersive content, this can include examining verbosity tolerance (how long content can be before users zone out, a big factor in digital signage) and phantom references (like links and calls to action that make sense on the web but not on a VR headset).Content discoverability. It’s no longer guaranteed in immersive content experiences that every piece of content can be accessed from other content items, and content loses almost all of its context when displayed unmoored from other content in digital signs or AR overlays. For discoverability’s sake, avoid relegating content to unreachable siloes, whether content is inaccessible due to physical conditions (like walls or other obstacles) or technical ones (like a finicky VR headset).
 
 
 
 Like voice content, immersive content requires ample attention to the ways in which users approach and interact with content in physical and virtual spaces. And as I write in Voice Content and Usability, it’s also the case that cross-channel interactions can influence how we work with copy and media. After all, how often do subway and rail commuters glance up while scrolling through service advisories on their smartphones to consult a potentially more up-to-date alert on a digital sign?
 
 
 
 Digital signage content: Content in space
 
 
 
 Signage has long been a fixture of how we find our way through physical spaces, ever since the earliest roads crisscrossed civilizations. Today, digital signs are becoming ubiquitous across shopping centers, university campuses, and especially transit systems, with the New York City subway recently introducing countdown clocks that display service advisories on a ticker along the bottom of the screen, just below train arrival times.
 
 
 
 Digital signs can deliver critical content at important times, such as during emergencies, without the limitations imposed by the static nature of analog signs. News tickers on digital signs, for instance, can stretch for however long they need to, though succinctness is still highly prized. But digital signage’s rich potential to deliver immersive content also presents challenges when it comes to content modeling and governance.
 
 
 
 Are news items delivered to digital signs simply teaser or summary versions of full articles? Without a fully functional and configurable digital sign in your office, how will you preview them in context before they go live? To solve this problem for the New York City subway, the Metropolitan Transportation Authority (MTA) manages all digital signage content across all signs within a central Drupal content management system (CMS), which synthesizes data such as train arrival times from real-time feeds and transit messages administered in the CMS for arbitrary delivery to any platform across the network.
 
 
 
 How to present content items in digital signs also poses problems. As the following figure illustrates, do you overtake the entire screen at the risk of obscuring other information, do you leave it in a ticker that may be ignored, or do you use both depending on the priority or urgency of the content you’re presenting?
 
 
 
 
 
 
 
 While some digital signs have the benefit of touch screens and occupying entire digital kiosks, many are tasked with providing key information in as little space as possible, where users don’t have the luxury of manipulating the interface to customize the content they wish to view. The New York City subway makes a deliberate choice to allow urgent alerts to spill across the entire screen, which limits the sign’s usefulness for those who simply need to know when the next train is arriving in the interest of more important information that is relevant to all passengers—and those who need captions for loudspeaker announcements.
 
 
 
 Auditing for digital signage content
 
 
 
 Because digital signs value brevity and efficiency, digital signage content often isn’t the main focus of what’s displayed. Digital signs on the São Paulo metro, for instance, juggle service alerts, breaking news, and health advisories. For this reason, auditing digital signage content for legibility and discoverability is key to ensuring users can interact with it gracefully, regardless of how often it appears, how highly prioritized it is, or what it covers.
 
 
 
 When it comes to legibility, ask yourself these questions and consider the digital sign content you’re authoring based on these concerns:
 
 
 
 Font size and typography. Many digital signs use sans-serif typefaces, which are easier to read from a distance, and many also employ uppercase for all text, especially in tickers. Consider which typefaces advance rather than obscure legibility, even when the digital sign content overtakes the entire screen.Angles and perspective. Is your digital sign content readily readable from various angles and various vantage points? Does the reflectivity of the screen impact your content’s legibility when standing just below the sign? How does your content look when it’s displayed to a user craning their neck and peering at it askew?Color contrast and lighting. Digital signs are no longer just fixtures of subterranean worlds; they’re above-ground and in well-lit spaces too. Color contrast and lighting strongly influence how legible your digital sign content can be.
 
 
 
 As for discoverability, digital signs present challenges of both physical discoverability (can the sign itself be easily found and consulted?) and content discoverability (how long does a reader have to stare at the sign for the content they need to show up?):
 
 
 
 Physical discoverability. Are signs placed in prominent locations where users will come across them? The MTA was criticized for the poor placement of many of its digital countdown clocks in the New York City subway, something that can block a user from ever accessing content they need.Content discoverability. Because digital signs can only display so much content at once, even if there’s a large amount of copy to deliver eventually, users of digital signs may need to wait too long for their desired content to appear, or the content they seek may be too deprioritized for it to show up while they’re looking at the sign.
 
 
 
 Both legibility and discoverability of digital sign content require thorough approaches when authoring, designing, and implementing content for digital signs.
 
 
 
 Usability and accessibility in digital signage content
 
 
 
 In addition to audits, in any physical environment, immersive content on digital signs requires a careful and bespoke approach to consider not only how content will be consumed on the sign itself but also all the ways in which users move around and refer to digital signage as they consult it for information. After all, our content is no longer couched in a web page or recited by a screen reader, both objects we can control ourselves; instead, it’s flashed and displayed on flat screens and kiosks in physical spaces. 
 
 
 
 Consider how the digital sign and the content it presents appear to people who use mobility aids such as wheelchairs or walkers. Is the surrounding physical environment accessible enough so that wheelchair users can easily read and discover the content they seek on a digital sign, which may be positioned too high for a seated reader? By the same token, can colorblind and dyslexic people read the chosen typeface in the color scheme it’s rendered in? Is there an aural equivalent of the content for Blind people navigating your digital signage, in close proximity to the sign itself, serving as synchronized captions?
 
 
 
 Locational content: Content at a point in space
 
 
 
 Unlike digital signage content, which is copy or media displayed in a space, locational (or geolocational) content is copy or media delivered to a device—usually a phone or watch—based on a point in space (if precise location is acquired through GPS location services) or a swath of space (typically driven by Bluetooth Low Energy beacons that have certain ranges). For smartphone and smartwatch users, GPS location services can often pinpoint a relatively accurate sense of where a person is, while Bluetooth Low Energy (BLE) beacons can triangulate their position based on devices that have Bluetooth enabled.
 
 
 
 
 
 
 
 Though BLE beacons remain a fairly finicky and untested realm of spatial technology, they’ve quickly gained traction in large shopping centers and public spaces such as airports where users agree to receive content relevant to their current location, most often in the form of push notifications that whisk users away into a separate view with more comprehensive information. But because these tiny chunks of copy are often tightly contained and contextless, teams designing for locational content need to focus on how users interact with their devices as they move through physical spaces.
 
 
 
 Auditing for locational content
 
 
 
 Fortunately, because locational content is often delivered to the same visual devices that we use on a regular basis—smartphones, smartwatches, and tablets—auditing for content legibility can embrace many of the same principles we employ to evaluate other content. For discoverability, some of the most important considerations include:
 
 
 
 Locational discoverability. BLE beacons are notorious for their imprecision, though they continue to improve in quality. GPS location, too, can be an inaccurate measure of where someone is at any given time. The last thing you want your customers to experience is an incorrect triangulation of where they are leading to embarrassing mistakes and bewilderment when unexpected content travels down the wire.Proximity. Because of the relative lack of precision when it comes to BLE beacons and GPS location services, placing content items too close together in a coordinate map may trigger too many notifications or resource deliveries to a user, thus overwhelming them, or a certain content item may inadvertently supersede another because they’re spaced too closely together.
 
 
 
 As push notifications and location sharing become more common, locational content is rapidly becoming an important way to funnel users toward somewhat longer-form content that might otherwise go unnoticed when a customer is in a brick-and-mortar store.
 
 
 
 Usability and accessibility in locational content
 
 
 
 Because locational content requires users to move around physical spaces and trigger triangulation, consider how different types of users will move and also whether unforeseen issues can arise. For example, researchers in Japan found that users who walk while staring at their phones are highly disruptive to the flow and movement of those around them. Is your locational content possibly creating a situation where users bump into others, or worse, get into accidents? For instance, writing copy that’s quick and to the point or preventing notifications from being prematurely dismissed could allow users to ignore their devices until they have time to safely glance at them.
 
 
 
 Limited mobility and cognitive disabilities can place many disabled users of locational content at a deep disadvantage. While gamification may encourage users to seek as many items of locational content as possible in a given span of time for promotional purposes, consider whether it excludes wheelchair users or people who encounter obstacles when switching between contexts rapidly. There are good use cases for locational content, but what’s compelling for some users might be confounding for others.
 
 
 
 AR and VR content: Content projected into space
 
 
 
 Augmented reality, once the stuff of science fiction holograms and futuristic cityscapes, is becoming more available to the masses thanks to wearable AR devices, high-performing smartphones and tablets, and innovation in machine vision capabilities, though the utopian future of true “holographic” content remains as yet unrealized. Meanwhile, virtual reality has seen incredible growth over the pandemic as homebound users—by interacting with copy and media in fictional worlds—increasingly seek escapist ways to access content normally spread across flat screens.
 
 
 
 While AR and VR content is still in its infancy, the vast majority is currently couched in overlays that are superimposed over real-world environments or objects and can be opaque (occupying some of a device’s field of vision) or semi-transparent (creating an eerie, shimmery film on which text or media is displayed). Thanks to advancements in machine vision, these content overlays can track the motion of perceived objects in the physical or virtual world, bamboozling us into thinking these overlays are traveling in our fields of vision just like the things we see around us do.
 
 
 
 Formerly restricted to realms like museums, expensive video games, and gimmicky prototypes, AR and VR content is now becoming much more popular among companies that are interested in more immersive experiences capable of delivering content alongside objects in real-life brick-and-mortar environments, as well as virtual or imagined landscapes, like fully immersive brand experiences that transport customers to a pop-up store in their living room.
 
 
 
 To demonstrate this, my former team at Acquia Labs built an experimental proof of concept that examines how VR content can be administered within a CMS and a pilot project for grocery stores that explores what can happen when product information is displayed as AR content next to consumer goods in supermarket aisles. The following illustration shows, in the context of this latter experiment, how a smartphone camera interacts with a machine vision service and a Drupal CMS to acquire information to render alongside the item.
 
 
 
 
 
 
 
 Auditing for AR and VR content
 
 
 
 Because AR and VR content, unlike other forms of immersive content, fundamentally plays in the same sandbox as the real world (or an imaginary one), legibility and discoverability can become challenging. The potential risks for AR and VR content are in many regards a fusion of the problems found in both digital signage and locational content, encompassing both physical placement and visual perspective, especially when it comes to legibility:
 
 
 
 Content visibility. Is the AR or VR overlay too transparent to comfortably read the copy or view the image contained therein, or is it so opaque that it obscures its surroundings? AR and VR content must coexist gracefully with its exterior, and the two must enhance rather than obfuscate each other. Does the way your content is delivered compromise a user’s feeling of immersion in the environment behind it?Content perspective. Unless you’re limited to a smartphone or similar handheld device, many AR and VR overlays, especially in immersive headsets, don’t display content or media as an immobile rectangular box, as it defeats the purpose of the illusion and can be jarring to users as they adjust their field of vision, breaking them out of the fantasy you’re hoping to create. For this reason, your AR or VR experience must not only dictate how environments and objects are angled and lit but also how the content associated with them is perceived. Is your content readable from various angles and points in the AR view or VR world?
 
 
 
 When it comes to discoverability of your AR and VR content, issues like accuracy in machine vision and triangulation of your user’s location and orientation become much more important:
 
 
 
 Machine vision. Most relevantly for AR content, if your copy or media is predicated on machine vision that perceives an object by identifying it according to certain characteristics, how accurate is that prediction? Does some content go undiscovered because certain objects go undetected in your AR-enabled device?Location accuracy. If your content relies on the user’s current location and orientation in relation to some point in space, as is common in both AR and VR content use cases, how accurately do devices dictate correct delivery at just the right time and place? Are the ranges within which content is accessible too limited, leading to flashes of content as you take a step to the left or right? Are there locations that simply can’t be reached, leading to forever-siloed copy or media?
 
 
 
 Due to the intersection of technical considerations and design concerns, AR and VR content, like voice content and indeed other forms of immersive content, requires a concerted effort across multiple teams to ensure resources are delivered not just legibly but also discoverably.
 
 
 
 Usability and accessibility in AR and VR content
 
 
 
 Out of all the forms of immersive content we’ve covered so far, AR and VR content is possibly the medium that demands the most assiduously crafted solutions in accessibility testing and usability testing. Because AR and VR content, especially in headsets or wearable devices, requires motion through real or imagined space, its impact on accessibility cannot be overstated. Adding a third dimension—and arguably, a fourth: time—to our perception of content requires attention not only to how content is accessed but also all the other elements that comprise a fully immersive visual experience.
 
 
 
 VR headsets commonly induce virtual reality motion sickness in many individuals. Poorly implemented transitions between states occurring in quick succession where content is visible and then invisible, and then visible again, can lead to epileptic seizures if not built with the utmost care. Finally, users moving quickly through spaces may inadvertently trigger vertigo in themselves or even collide with hazardous objects, resulting in potentially serious injuries. There’s a reason we aren’t wearing wearable headsets outside carefully secured environments.
 
 
 
 Navigable content: Content as space
 
 
 
 This is only the beginning of immersive content. Increasingly, we’re also toying with ideas that seemed harebrained even a few decades ago, like navigable content—copy and media that can be traversed as if the content itself were a navigable space. Imagine zooming in and out of tracts of text and stepping across glyphs like hopping between islands in a Super Mario game. Ambitious designers and developers are exploring this emerging concept of navigable content in exciting ways, both in and out of AR and VR. In many ways, truly navigable content is the endgame of how virtual reality presents information.
 
 
 
 Imagining an encyclopedia that we can browse like the classic 1990s opening sequence of the BBC’s Eyewitness television episodes is no longer as far-fetched as we think. Consider, for instance, Robby Leonardi’s interactive résumé, which invites you to play a character as you learn about his career, or Bruno Simon’s ambitious portfolio, where you drive an animated truck around his website. For navigable content, the risks and rewards for user experience and accessibility remain largely unexplored, just like the hazy fringes of the infinite maps VR worlds make possible.
 
 
 
 Conclusion
 
 
 
 The story of immersive content is in its early stages. As newly emerging channels for content see greater adoption, requiring us to relay resources like text and media to never-before-seen destinations like digital signage, location-enabled devices, and AR and VR overlays, the demands on our content strategy and design approaches will become both fascinating and frustrating. As seemingly fantastical new interfaces continue to emerge over the horizon, we’ll need an omnichannel content strategy to guide our own journeys as creatives and to orient the voyages of our users into the immersive.
 
 
 
 Content audits and effective content strategies aren’t just the domain of staid websites and boxy mobile or tablet interfaces—or even aurally rooted voice interfaces. They’re a key component of our increasingly digitized spaces, too, cornerstones of immersive experiences that beckon us to consume content where we are at any moment, unmoored from a workstation or a handheld. Because it lacks long-standing motifs of the web like context and clickable links, immersive content invites us to revisit our content with a fresh perspective. How will immersive content reinvent how we deliver information like the web did only a few decades ago, like voice has done in the past ten years?
 
 
 
 Only the test of time, and the allure of immersion, will tell.
 </content>
     </entry>
     <entry>
       <title>Do You Need to Localize Your Website?</title>
         <link href="https://alistapart.com/article/do-you-need-to-localize-your-website/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">Global markets give you access to new customers. All you need to do is inform potential buyers about your product or service. 
 
 
 
 Your website is a good place to introduce your product or service outside your locale. Localizing your web content sounds like the right way to reach out to the global market. Localization will bridge the language barriers, or the wider scope of differing cultures. 
 
 
 
 Before we move on further with the discussion, let’s focus on the definition of “localization.” 
 
 
 
 What is localization?
 
 
 
 According to the Cambridge Dictionary, localization (as a marketing term) is “the process of making a product or service more suitable for a particular country, area, etc.,” while translation is “something that is translated, or the process of translating something, from one language to another.” 
 
 
 
 In practice, the difference can be a little blurred. While it’s true that localization includes both language and non-language aspects, most cultural adjustments in the localization process are done through the language. Hence, the two terms are often interchangeable. 
 
 
 
 Good translators will not simply find an equivalent of a word in another language. They will actively research their materials and have an in-depth understanding of the languages they work in. 
 
 
 
 Depending on the situation, they may or may not convert measurement units and date formats. Technical guide books may need accurate unit conversion, but changing “Fahrenheit 451” to “Celsius 233” would be simply awkward. A good translator will suggest what to change and what to leave as it is. 
 
 
 
 Some people call this conversion process “localization.” The truth is, unit conversions had become a part of translation, long before the word “localization” was used to describe the process. 
 
 
 
 When we talk about linguistic versus non-linguistic aspects of a medium, and view them as separate entities, localization and translation may look different. However, when we look at the whole process of translating the message, seeing both elements as translatable items, the terms are interchangeable. 
 
 
 
 In this article, the terms “localization” and “translation” will be used interchangeably. We are going to discuss how to use a website as a communication tool to gain a new market in different cultures. 
 
 
 
 Localization: who is it for?
 
 
 
 A good localization is not cheap, so it would be wise to ask yourselves several questions beforehand: 
 
 
 
 Who is your audience?What kind of culture do they live in?What kind of problems may arise during the localization process? 
 
 
 
 I will explain the details below. 
 
 
 
 Who is your ideal audience?
 
 
 
 Knowing your target audience should be at the top of your business plan. 
 
 
 
 For some, localization is not needed because they live in the same region and speak the same language as their target market. For example, daycare services, local coffee shops, and restaurants. 
 
 
 
 In some cases, people who live in the same region may speak different languages. In a bilingual society, you may want to cater to speakers of both languages as a sign of respect. In a multilingual society, aim to translate to the lingua franca and/or the language used by the majority. It makes people feel seen and it can create a positive image to your brand. 
 
 
 
 Sometimes, website translation is required by law. In Quebec, for instance, where French is spoken as the provincial language, you’ll need to include a French version of your website. You may also want to check other types of linguistic experiences you need to provide. 
 
 
 
 If your target market lives across the sea and speaks a different language, you may not have any choice but to localize. However, if those people can speak your  language, consider other aspects (cultural and/or legal) to make an informed decision on whether to translate.  
 
 
 
 Although there are many benefits of website translation, you don’t always have to do it now. Especially when your budget is tight or you can spend it on something more urgent. It’s better to postpone than to have a badly translated website. The price of cheap translation is costly.  
 
 
 
 If you’re legally required to launch a bilingual website but you don’t have the budget, you may want to check if you can be exempted. If you are not exempted, hire volunteers or seek government support, if possible. 
 
 
 
 Unless required otherwise by law, there is nothing wrong with using your current language in your product or service. You can maintain the already-formed relationship by focusing on what you have in common: the same interest. 
 
 
 
 Understanding cultural and linguistic intricacies
 
 
 
 For example, you have a coding tutorial website. Your current audience is IT professionals—mostly college graduates. You see an opportunity to expand to India. 
 
 
 
 Localization is unlikely to be needed in this case, as most Indian engineers have a good grasp of English. So, instead of doing a web translation project, you can use your money to improve or develop a new product or service for your Indian audience. Maybe you want to set up a workshop or a meetup in India. Or a bootcamp retreat in the country. 
 
 
 
 You can achieve this by focusing on the similarities you have with your audience. 
 
 
 
 The same rule applies to other countries where English language is commonly used by IT professionals. In the developing world, where English is rarely used, some self-taught programmers become “good hackers” to earn some money. You may wonder how, despite their lack of English skill, they can learn programming.
 
 
 
 There’s an explanation for it. 
 
 
 
 There are two types of language skills: passive (listening, reading) and active (speaking, writing). Passive language skills are usually learned first. Active language skills are developed later. You learn to speak by listening, and learn to write by reading. You go through this process as a child and, again, when you learn a new language as an adult. (This is not to confuse language acquisition with language learning, but to note that the process is relatively the same.) 
 
 
 
 As most free IT course materials are available online in English, some programmers may have to adapt and study English (passively) as they go. They may not be considered “fluent” in a formal way, but it doesn’t mean they lack the ability to grasp the language. They may not be able to speak or write perfectly, but they can understand technical texts. 
 
 
 
 In short, passive and active language skills can grow at different speeds. This fact leads you to a new group of potential audience: those who can understand English, but only passively. 
 
 
 
 If your product is in a text format, translation won’t be necessary for this type of audience. If it’s an audio or video format, you may need to add subtitles, since native English speakers speak in so many different accents and at various speeds. Captioning will also help the hard of hearing. It may be required by regional or national accessibility legislation too. And it’s the right thing to do.
 
 
 
 One might argue that if these people can understand English, they will understand the text better in their native tongue. 
 
 
 
 Well, if all the programs you’re using or referring to are available in their native language version, it may not be a problem. But in reality, this is often not the case. 
 
 
 
 Linguistic consistency helps programmers work faster. And this alone should trump the presumed ease that comes with translation. 
 
 
 
 Some problems with localization 
 
 
 
 I was once involved in a global internet company’s localization project in Indonesia. 
 
 
 
 Indonesian SMEs mostly speak Indonesian since they mainly serve the domestic market. So, it was the right decision to target Indonesian SMEs using Indonesian language. 
 
 
 
 The company had the budget to target Indonesia’s market of 58 million SMEs, and there weren’t too many competitors yet. I think the localization plan was justified. But even with this generally well-thought-out plan, there were some problems in its execution. 
 
 
 
 The materials were filled with jargon and annoying outlinks. You could not just read an instruction until it was completed, because after a few words, you would be confronted with a “smart term.” Now to understand this smart term, you would have to follow a link that would take you to a separate page that was supposed to explain everything, but in that page you would find more smart terms that you’d need to click. At this point, the scent of information would have grown cold, and you’d likely have forgotten what you were reading or why. 
 
 
 
 Small business owners are among the busiest folks you can find. They do almost everything by themselves. They would not waste their time trying to read pages of instructions that throw them right and left. 
 
 
 
 Language-wise, the instructions could have been simplified. Design-wise, a hover/focus pop-up containing a brief definition or description could have been used to explain special terms. 
 
 
 
 I agree pop-ups can be distracting, but in terms of ease, for this use case, they would have worked far better than outlinks. There are some ways to improve hover/focus pop-ups to make them more readable. 
 
 
 
 However, if the content of those pop-ups (definition, description, etc.) cannot be brief, it is wiser to write it down as a separate paragraph. 
 
 
 
 In my client’s case, they could have started each instruction by describing the definitions of those special terms. Those definitions ought to be written in one page so as to reduce the amount of time spent on clicking and returning to the intended page. This solution can also be applied when a definition is too long to be put inside a hover/focus bubble. 
 
 
 
 The text problem, in my client’s case, came with the source language. It was later transferred to the target language thanks to localization. They could have solved the problem at the source language level, but I think it would have been too late at that point. 
 
 
 
 Transcreation, i.e., “taking a concept in one language and completely recreating it in another language,” doesn’t solve a problem like this because the issue is more technical than linguistic. Translators would still have to adjust their work to the given environment. They’d still have to retain all the links and translate all jargon-laden content. 
 
 
 
 The company should have hired a local writer to rewrite the content in the target language. It would have worked better. They didn’t take this route for a reason: namely, those “smart terms” were used as keywords. So as much as we hated them, we had to keep them there.  
 
 
 
 How to prepare a web localization project
 
 
 
 Let’s say you have considered everything. You’ve learned about your target audience, how your product will solve their problem, and that you have the budget to reach out to them. Naturally, you want to reach them now before your competitors do. 
 
 
 
 Now you can proceed with your web localization project plan. 
 
 
 
 One thing I want to repeat is that localization will transfer any errors you have in your original content to the translated pages. So you’ll need to do some content pre-checks before starting a web translation project. It will be cheaper to fix the problems before the translation project commences. 
 
 
 
 Pre-localization checks should include assessing the text you intend to translate. Ask someone outside the team to read the text and ask them to give their feedback. It’s even better if that someone represents the target audience. 
 
 
 
 Then make corrections, if need be. Use as little jargon as possible. Let readers focus on one article with no interruption. 
 
 
 
 Some companies like to coin new terms to create keywords that will lead people to their sites. This can be a smart move, and it is arguably good for search engine optimization. But if you want to build rapport with your audience, you must make your message clear and understandable. Clear communication, not the invention of new words, should be your priority. 
 
 
 
 Following this course of action might mean sacrificing keywords for clarity, but it also promises a lower bounce rate since visitors will stay longer on your site. After all, people are more likely to read your writing to the end if they are not being frustrated by difficult terms.
 
 
 
 Once your text is ready, you can start your localization project. You can hire a language agency or build your own team. 
 
 
 
 If you have a lot of content, it may be wise to outsource your project to a language agency. Doing so can save you time and money. An outside specialist consultancy will have the technology and skills to work on various types of localization projects. They can also translate your website to different languages at once. 
 
 
 
 As an alternative, you might directly hire freelance editors and translators to work on your project. Depending on many factors, this might end up less or more expensive than hiring an agency. 
 
 
 
 Make sure that the translators you hire, whether directly or through an agency, have relevant experience. If your text is about marketing, for instance, the translators and editors must be experts in this field. This is to make sure they can get your message across. 
 
 
 
 Most translation tools used today can retain sentence formatting, links, and HTML code, so you don’t need to worry about these. 
 
 
 
 Focus on the message you want to carry to your target audience. Be sensitive about cultural remarks and be careful about any potential misunderstanding caused by your translation. Consult with your language team about certain phrases that may become problematic when translated. Pick your words carefully. Choose the right expressions. 
 
 
 
 If you localize a website, you must be sure to provide customer service support in target-friendly language. This allows you to reply to customers immediately, rather than having to wait for a translator to become involved.  
 
 
 
 In summary, don’t be hasty when doing a web localization/translation project. There are a lot of things to consider beforehand. A well prepared plan will yield a better result. A good quality translation will not only bridge the language gap but it can also build trust and solidify your brand image in the mind of your target audience.
 </content>
     </entry>
     <entry>
       <title>Human-Readable JavaScript: A Tale of Two Experts</title>
         <link href="https://alistapart.com/article/human-readable-javascript/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">Everyone wants to be an expert. But what does that even mean? Over the years I’ve seen two types of people who are referred to as “experts.” Expert 1 is someone who knows every tool in the language and makes sure to use every bit of it, whether it helps or not. Expert 2 also knows every piece of syntax, but they’re pickier about what they employ to solve problems, considering a number of factors, both code-related and not. 
 
 
 
 Can you take a guess at which expert we want working on our team? If you said Expert 2, you’d be right. They’re a developer focused on delivering readable code—lines of JavaScript others can understand and maintain. Someone who can make the complex simple. But “readable” is rarely definitive—in fact, it’s largely based on the eyes of the beholder. So where does that leave us? What should experts aim for when writing readable code? Are there clear right and wrong choices? The answer is, it depends.
 
 
 
 The obvious choice
 
 
 
 In order to improve developer experience, TC39 has been adding lots of new features to ECMAScript in recent years, including many proven patterns borrowed from other languages. One such addition, added in ES2019, is Array.prototype.flat() It takes an argument of depth or Infinity, and flattens an array. If no argument is given, the depth defaults to 1.
 
 
 
 Prior to this addition, we needed the following syntax to flatten an array to a single level.
 
 
 
 let arr &#x3D; [1, 2, [3, 4]];
 
 [].concat.apply([], arr);
 // [1, 2, 3, 4]
 
 
 
 When we added flat(), that same functionality could be expressed using a single, descriptive function.
 
 
 
 arr.flat();
 // [1, 2, 3, 4]
 
 
 
 Is the second line of code more readable? The answer is emphatically yes. In fact, both experts would agree.
 
 
 
 Not every developer is going to be aware that flat() exists. But they don’t need to because flat() is a descriptive verb that conveys the meaning of what is happening. It’s a lot more intuitive than concat.apply().
 
 
 
 This is the rare case where there is a definitive answer to the question of whether new syntax is better than old. Both experts, each of whom is familiar with the two syntax options, will choose the second. They’ll choose the shorter, clearer, more easily maintained line of code.
 
 
 
 But choices and trade-offs aren’t always so decisive.
 
 
 
 The gut check
 
 
 
 The wonder of JavaScript is that it’s incredibly versatile. There is a reason it’s all over the web. Whether you think that’s a good or bad thing is another story.
 
 
 
 But with that versatility comes the paradox of choice. You can write the same code in many different ways. How do you determine which way is “right”? You can’t even begin to make a decision unless you understand the available options and their limitations.
 
 
 
 Let’s use functional programming with map() as the example. I’ll walk through various iterations that all yield the same result.
 
 
 
 This is the tersest version of our map() examples. It uses the fewest characters, all fit into one line. This is our baseline.
 
 
 
 const arr &#x3D; [1, 2, 3];
 let multipliedByTwo &#x3D; arr.map(el &#x3D;&gt; el * 2);
 // multipliedByTwo is [2, 4, 6]
 
 
 
 This next example adds only two characters: parentheses. Is anything lost? How about gained? Does it make a difference that a function with more than one parameter will always need to use the parentheses? I’d argue that it does. There is little to no detriment  in adding them here, and it improves consistency when you inevitably write a function with multiple parameters. In fact, when I wrote this, Prettier enforced that constraint; it didn’t want me to create an arrow function without the parentheses.
 
 
 
 let multipliedByTwo &#x3D; arr.map((el) &#x3D;&gt; el * 2);
 
 
 
 Let’s take it a step further. We’ve added curly braces and a return. Now this is starting to look more like a traditional function definition. Right now, it may seem like overkill to have a keyword as long as the function logic. Yet, if the function is more than one line, this extra syntax is again required. Do we presume that we will not have any other functions that go beyond a single line? That seems dubious.
 
 
 
 let multipliedByTwo &#x3D; arr.map((el) &#x3D;&gt; {
   return el * 2;
 });
 
 
 
 Next we’ve removed the arrow function altogether. We’re using the same syntax as before, but we’ve swapped out for the function keyword. This is interesting because there is no scenario in which this syntax won’t work; no number of parameters or lines will cause problems, so consistency is on our side. It’s more verbose than our initial definition, but is that a bad thing? How does this hit a new coder, or someone who is well versed in something other than JavaScript? Is someone who knows JavaScript well going to be frustrated by this syntax in comparison?
 
 
 
 let multipliedByTwo &#x3D; arr.map(function(el) {
   return el * 2;
 });
 
 
 
 Finally we get to the last option: passing just the function. And timesTwo can be written using any syntax we like. Again, there is no scenario in which passing the function name causes a problem. But step back for a moment and think about whether or not this could be confusing. If you’re new to this codebase, is it clear that timesTwo is a function and not an object? Sure, map() is there to give you a hint, but it’s not unreasonable to miss that detail. How about the location of where timesTwo is declared and initialized? Is it easy to find? Is it clear what it’s doing and how it’s affecting this result? All of these are important considerations.
 
 
 
 const timesTwo &#x3D; (el) &#x3D;&gt; el * 2;
 let multipliedByTwo &#x3D; arr.map(timesTwo);
 
 
 
 As you can see, there is no obvious answer here. But making the right choice for your codebase means understanding all the options and their limitations. And knowing that consistency requires parentheses and curly braces and return keywords.
 
 
 
 There are a number of questions you have to ask yourself when writing code. Questions of performance are typically the most common. But when you’re looking at code that is functionally identical, your determination should be based on humans—how humans consume code.
 
 
 
 Maybe newer isn’t always better
 
 
 
 So far we’ve found a clear-cut example of where both experts would reach for the newest syntax, even if it’s not universally known. We’ve also looked at an example that poses a lot of questions but not as many answers.
 
 
 
 Now it’s time to dive into code that I’ve written before...and removed. This is code that made me the first expert, using a little-known piece of syntax to solve a problem to the detriment of my colleagues and the maintainability of our codebase.
 
 
 
 Destructuring assignment lets you unpack values from objects (or arrays). It typically looks something like this.
 
 
 
 const {node} &#x3D; exampleObject;
 
 
 
 It initializes a variable and assigns it a value all in one line. But it doesn’t have to.
 
 
 
 let node
 ;({node} &#x3D; exampleObject)
 
 
 
 The last line of code assigns a variable to a value using destructuring, but the variable declaration takes place one line before it. It’s not an uncommon thing to want to do, but many people don’t realize you can do it.
 
 
 
 But look at that code closely. It forces an awkward semicolon for code that doesn’t use semicolons to terminate lines. It wraps the command in parentheses and adds the curly braces; it’s entirely unclear what this is doing. It’s not easy to read, and, as an expert, it shouldn’t be in code that I write.
 
 
 
 let node
 node &#x3D; exampleObject.node
 
 
 
 This code solves the problem. It works, it’s clear what it does, and my colleagues will understand it without having to look it up. With the destructuring syntax, just because I can doesn’t mean I should.
 
 
 
 Code isn’t everything
 
 
 
 As we’ve seen, the Expert 2 solution is rarely obvious based on code alone; yet there are still clear distinctions between which code each expert would write. That’s because code is for machines to read and humans to interpret. So there are non-code factors to consider!
 
 
 
 The syntax choices you make for a team of JavaScript developers is different than those you should make for a team of polyglots who aren’t steeped in the minutiae. 
 
 
 
 Let’s take spread vs. concat() as an example.
 
 
 
 Spread was added to ECMAScript a few years ago, and it’s enjoyed wide adoption. It’s sort of a utility syntax in that it can do a lot of different things. One of them is concatenating a number of arrays.
 
 
 
 const arr1 &#x3D; [1, 2, 3];
 const arr2 &#x3D; [9, 11, 13];
 const nums &#x3D; [...arr1, ...arr2];
 
 
 
 As powerful as spread is, it isn’t a very intuitive symbol. So unless you already know what it does, it’s not super helpful. While both experts may safely assume a team of JavaScript specialists are familiar with this syntax, Expert 2 will probably question whether that’s true of a team of polyglot programmers. Instead, Expert 2 may select the concat() method instead, as it’s a descriptive verb that you can probably understand from the context of the code.
 
 
 
 This code snippet gives us the same nums result as the spread example above.
 
 
 
 const arr1 &#x3D; [1, 2, 3];
 const arr2 &#x3D; [9, 11, 13];
 const nums &#x3D; arr1.concat(arr2);
 
 
 
 And that’s but one example of how human factors influence code choices. A codebase that’s touched by a lot of different teams, for example, may have to hold more stringent standards that don’t necessarily keep up with the latest and greatest syntax. Then you move beyond the main source code and consider other factors in your tooling chain that make life easier, or harder, for the humans who work on that code. There is code that can be structured in a way that’s hostile to testing. There is code that backs you into a corner for future scaling or feature addition. There is code that’s less performant, doesn’t handle different browsers, or isn’t accessible. All of these factor into the recommendations Expert 2 makes.
 
 
 
 Expert 2 also considers the impact of naming. But let’s be honest, even they can’t get that right most of the time.
 
 
 
 Conclusion
 
 
 
 Experts don’t prove themselves by using every piece of the spec; they prove themselves by knowing the spec well enough to deploy syntax judiciously and make well-reasoned decisions. This is how experts become multipliers—how they make new experts.
 
 
 
 So what does this mean for those of us who consider ourselves experts or aspiring experts? It means that writing code involves asking yourself a lot of questions. It means considering your developer audience in a real way. The best code you can write is code that accomplishes something complex, but is inherently understood by those who examine your codebase.
 
 
 
 And no, it’s not easy. And there often isn’t a clear-cut answer. But it’s something you should consider with every function you write.
 </content>
     </entry>
     <entry>
       <title>Now THAT’S What I Call Service Worker!</title>
         <link href="https://alistapart.com/article/now-thats-what-i-call-service-worker/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">The Service Worker API is the Dremel of the web platform. It offers incredibly broad utility while also yielding resiliency and better performance. If you’ve not used Service Worker yet—and you couldn’t be blamed if so, as it hasn’t seen wide adoption as of 2020—it goes something like this:
 
 
 
 On the initial visit to a website, the browser registers what amounts to a client-side proxy powered by a comparably paltry amount of JavaScript that—like a Web Worker—runs on its own thread.After the Service Worker’s registration, you can intercept requests and decide how to respond to them in the Service Worker’s fetch() event.
 
 
 
 What you decide to do with requests you intercept is a) your call and b) depends on your website. You can rewrite requests, precache static assets during install, provide offline functionality, and—as will be our eventual focus—deliver smaller HTML payloads and better performance for repeat visitors.
 
 
 
 Getting out of the woods
 
 
 
 Weekly Timber is a client of mine that provides logging services in central Wisconsin. For them, a fast website is vital. Their business is located in Waushara County, and like many rural stretches in the United States, network quality and reliability isn’t great.
 
 
 
 Figure 1. A wireless coverage map of Waushara County, Wisconsin. The tan areas of the map indicate downlink speeds between 3 and 9.99 Mbps. Red areas are even slower, while the pale and dark blue areas are faster.
 
 
 
 Wisconsin has farmland for days, but it also has plenty of forests. When you need a company that cuts logs, Google is probably your first stop. How fast a given logging company’s website is might be enough to get you looking elsewhere if you’re left waiting too long on a crappy network connection.
 
 
 
 I initially didn’t believe a Service Worker was necessary for Weekly Timber’s website. After all, if things were plenty fast to start with, why complicate things? On the other hand, knowing that my client services not just Waushara County, but much of central Wisconsin, even a barebones Service Worker could be the kind of progressive enhancement that adds resilience in the places it might be needed most.
 
 
 
 The first Service Worker I wrote for my client’s website—which I’ll refer to henceforth as the “standard” Service Worker—used three well-documented caching strategies:
 
 
 
 Precache CSS and JavaScript assets for all pages when the Service Worker is installed when the window’s load event fires.Serve static assets out of CacheStorage if available. If a static asset isn’t in CacheStorage, retrieve it from the network, then cache it for future visits.For HTML assets, hit the network first and place the HTML response into CacheStorage. If the network is unavailable the next time the visitor arrives, serve the cached markup from CacheStorage.
 
 
 
 These are neither new nor special strategies, but they provide two benefits:
 
 
 
 Offline capability, which is handy when network conditions are spotty.A performance boost for loading static assets.
 
 
 
 That performance boost translated to a 42% and 48% decrease in the median time to First Contentful Paint (FCP) and Largest Contentful Paint (LCP), respectively. Better yet, these insights are based on Real User Monitoring (RUM). That means these gains aren’t just theoretical, but a real improvement for real people.
 
 
 
 Figure 2. A breakdown of request/response timings depicted in Chrome’s developer tools. The request is for a static asset from CacheStorage. Because the Service Worker doesn’t need to access the network, it takes about 23 milliseconds to “download” the asset from CacheStorage.
 
 
 
 This performance boost is from bypassing the network entirely for static assets already in CacheStorage—particularly render-blocking stylesheets. A similar benefit is realized when we rely on the HTTP cache, only the FCP and LCP improvements I just described are in comparison to pages with a primed HTTP cache without an installed Service Worker.
 
 
 
 If you’re wondering why CacheStorage and the HTTP cache aren’t equal, it’s because the HTTP cache—at least in some cases—may still involve a trip to the server to verify asset freshness. Cache-Control’s immutable flag gets around this, but immutable doesn’t have great support yet. A long max-age value works, too, but the combination of Service Worker API and CacheStorage gives you a lot more flexibility.
 
 
 
 Details aside, the takeaway is that the simplest and most well-established Service Worker caching practices can improve performance. Potentially more than what well-configured Cache-Control headers can provide. Even so, Service Worker is an incredible technology with far more possibilities. It’s possible to go farther, and I’ll show you how.
 
 
 
 A better, faster Service Worker
 
 
 
 The web loves itself some “innovation,” which is a word we equally love to throw around. To me, true innovation isn’t when we create new frameworks or patterns solely for the benefit of developers, but whether those inventions benefit people who end up using whatever it is we slap up on the web. The priority of constituencies is a thing we ought to respect. Users above all else, always.
 
 
 
 The Service Worker API’s innovation space is considerable. How you work within that space can have a big effect on how the web is experienced. Things like navigation preload and ReadableStream have taken Service Worker from great to killer. We can do the following with these new capabilities, respectively:
 
 
 
 Reduce Service Worker latency by parallelizing Service Worker startup time and navigation requests.Stream content in from CacheStorage and the network.
 
 
 
 Moreover, we’re going to combine these capabilities and pull out one more trick: precache header and footer partials, then combine them with content partials from the network. This not only reduces how much data we download from the network, but it also improves perceptual performance for repeat visits. That’s innovation that helps everyone.
 
 
 
 Grizzled, I turn to you and say “let’s do this.”
 
 
 
 Laying the groundwork
 
 
 
 If the idea of combining precached header and footer partials with network content on the fly seems like a Single Page Application (SPA), you’re not far off. Like an SPA, you’ll need to apply the “app shell” model to your website. Only instead of a client-side router plowing content into one piece of minimal markup, you have to think of your website as three separate parts:
 
 
 
 The header.The content.The footer.
 
 
 
 For my client’s website, that looks like this:
 
 
 
 Figure 3. A color coding of the Weekly Timber website’s different partials. The Footer and Header partials are stored in CacheStorage, while the Content partial is retrieved from the network unless the user is offline.
 
 
 
 The thing to remember here is that the individual partials don’t have to be valid markup in the sense that all tags need to be closed within each partial. The only thing that counts in the final sense is that the combination of these partials must be valid markup.
 
 
 
 To start, you’ll need to precache separate header and footer partials when the Service Worker is installed. For my client’s website, these partials are served from the /partial-header and /partial-footer pathnames:
 
 
 
 self.addEventListener(&quot;install&quot;, event &#x3D;&gt; {
   const cacheName &#x3D; &quot;fancy_cache_name_here&quot;;
   const precachedAssets &#x3D; [
     &quot;/partial-header&quot;,  // The header partial
     &quot;/partial-footer&quot;,  // The footer partial
     // Other assets worth precaching
   ];
 
   event.waitUntil(caches.open(cacheName).then(cache &#x3D;&gt; {
     return cache.addAll(precachedAssets);
   }).then(() &#x3D;&gt; {
     return self.skipWaiting();
   }));
 });
 
 
 
 Every page must be fetchable as a content partial minus the header and footer, as well as a full page with the header and footer. This is key because the initial visit to a page won’t be controlled by a Service Worker. Once the Service Worker takes over, then you serve content partials and assemble them into complete responses with the header and footer partials from CacheStorage.
 
 
 
 If your site is static, this means generating a whole other mess of markup partials that you can rewrite requests to in the Service Worker’s fetch() event. If your website has a back end—as is the case with my client—you can use an HTTP request header to instruct the server to deliver full pages or content partials.
 
 
 
 The hard part is putting all the pieces together—but we’ll do just that.
 
 
 
 Putting it all together
 
 
 
 Writing even a basic Service Worker can be challenging, but things get real complicated real fast when assembling multiple responses into one. One reason for this is that in order to avoid the Service Worker startup penalty, we’ll need to set up navigation preload.
 
 
 
 Implementing navigation preload
 
 
 
 Navigation preload addresses the problem of Service Worker startup time, which delays navigation requests to the network. The last thing you want to do with a Service Worker is hold up the show.
 
 
 
 Navigation preload must be explicitly enabled. Once enabled, the Service Worker won’t hold up navigation requests during startup. Navigation preload is enabled in the Service Worker’s activate event:
 
 
 
 self.addEventListener(&quot;activate&quot;, event &#x3D;&gt; {
   const cacheName &#x3D; &quot;fancy_cache_name_here&quot;;
   const preloadAvailable &#x3D; &quot;navigationPreload&quot; in self.registration;
 
   event.waitUntil(caches.keys().then(keys &#x3D;&gt; {
     return Promise.all([
       keys.filter(key &#x3D;&gt; {
         return key !&#x3D;&#x3D; cacheName;
       }).map(key &#x3D;&gt; {
         return caches.delete(key);
       }),
       self.clients.claim(),
       preloadAvailable ? self.registration.navigationPreload.enable() : true
     ]);
   }));
 });
 
 
 
 Because navigation preload isn’t supported everywhere, we have to do the usual feature check, which we store in the above example in the preloadAvailable variable.
 
 
 
 Additionally, we need to use Promise.all() to resolve multiple asynchronous operations before the Service Worker activates. This includes pruning those old caches, as well as waiting for both clients.claim() (which tells the Service Worker to assert control immediately rather than waiting until the next navigation) and navigation preload to be enabled.
 
 
 
 A ternary operator is used to enable navigation preload in supporting browsers and avoid throwing errors in browsers that don’t. If preloadAvailable is true, we enable navigation preload. If it isn’t, we pass a Boolean that won’t affect how Promise.all() resolves.
 
 
 
 With navigation preload enabled, we need to write code in our Service Worker’s fetch() event handler to make use of the preloaded response:
 
 
 
 self.addEventListener(&quot;fetch&quot;, event &#x3D;&gt; {
   const { request } &#x3D; event;
 
   // Static asset handling code omitted for brevity
   // ...
 
   // Check if this is a request for a document
   if (request.mode &#x3D;&#x3D;&#x3D; &quot;navigate&quot;) {
     const networkContent &#x3D; Promise.resolve(event.preloadResponse).then(response &#x3D;&gt; {
       if (response) {
         addResponseToCache(request, response.clone());
 
         return response;
       }
 
       return fetch(request.url, {
         headers: {
           &quot;X-Content-Mode&quot;: &quot;partial&quot;
         }
       }).then(response &#x3D;&gt; {
         addResponseToCache(request, response.clone());
 
         return response;
       });
     }).catch(() &#x3D;&gt; {
       return caches.match(request.url);
     });
 
     // More to come...
   }
 });
 
 
 
 Though this isn’t the entirety of the Service Worker’s fetch() event code, there’s a lot that needs explaining:
 
 
 
 The preloaded response is available in event.preloadResponse. However, as Jake Archibald notes, the value of event.preloadResponse will be undefined in browsers that don’t support navigation preload. Therefore, we must pass event.preloadResponse to Promise.resolve() to avoid compatibility issues.We adapt in the resulting then callback. If event.preloadResponse is supported, we use the preloaded response and add it to CacheStorage via an addResponseToCache() helper function. If not, we send a fetch() request to the network to get the content partial using a custom X-Content-Mode header with a value of partial.Should the network be unavailable, we fall back to the most recently accessed content partial in CacheStorage.The response—regardless of where it was procured from—is then returned to a variable named networkContent that we use later.
 
 
 
 How the content partial is retrieved is tricky. With navigation preload enabled, a special Service-Worker-Navigation-Preload header with a value of true is added to navigation requests. We then work with that header on the back end to ensure the response is a content partial rather than the full page markup.
 
 
 
 However, because navigation preload isn’t available in all browsers, we send a different header in those scenarios. In Weekly Timber’s case, we fall back to a custom X-Content-Mode header. In my client’s PHP back end, I’ve created some handy constants:
 
 
 
 &lt;?php
 
 // Is this a navigation preload request?
 define(&quot;NAVIGATION_PRELOAD&quot;, isset($_SERVER[&quot;HTTP_SERVICE_WORKER_NAVIGATION_PRELOAD&quot;]) &amp;&amp; stristr($_SERVER[&quot;HTTP_SERVICE_WORKER_NAVIGATION_PRELOAD&quot;], &quot;true&quot;) !&#x3D;&#x3D; false);
 
 // Is this an explicit request for a content partial?
 define(&quot;PARTIAL_MODE&quot;, isset($_SERVER[&quot;HTTP_X_CONTENT_MODE&quot;]) &amp;&amp; stristr($_SERVER[&quot;HTTP_X_CONTENT_MODE&quot;], &quot;partial&quot;) !&#x3D;&#x3D; false);
 
 // If either is true, this is a request for a content partial
 define(&quot;USE_PARTIAL&quot;, NAVIGATION_PRELOAD &#x3D;&#x3D;&#x3D; true || PARTIAL_MODE &#x3D;&#x3D;&#x3D; true);
 
 ?&gt;
 
 
 
 From there, the USE_PARTIAL constant is used to adapt the response:
 
 
 
 &lt;?php
 
 if (USE_PARTIAL &#x3D;&#x3D;&#x3D; false) {
   require_once(&quot;partial-header.php&quot;);
 }
 
 require_once(&quot;includes/home.php&quot;);
 
 if (USE_PARTIAL &#x3D;&#x3D;&#x3D; false) {
   require_once(&quot;partial-footer.php&quot;);
 }
 
 ?&gt;
 
 
 
 The thing to be hip to here is that you should specify a Vary header for HTML responses to take the Service-Worker-Navigation-Preload (and in this case, the X-Content-Mode header) into account for HTTP caching purposes—assuming you’re caching HTML at all, which may not be the case for you.
 
 
 
 With our handling of navigation preloads complete, we can then move onto the work of streaming content partials from the network and stitching them together with the header and footer partials from CacheStorage into a single response that the Service Worker will provide.
 
 
 
 Streaming partial content and stitching together responses
 
 
 
 While the header and footer partials will be available almost instantaneously because they’ve been in CacheStorage since the Service Worker’s installation, it’s the content partial we retrieve from the network that will be the bottleneck. It’s therefore vital that we stream responses so we can start pushing markup to the browser as quickly as possible. ReadableStream can do this for us.
 
 
 
 This ReadableStream business is a mind-bender. Anyone who tells you it’s “easy” is whispering sweet nothings to you. It’s hard. After I wrote my own function to merge streamed responses and messed up a critical step—which ended up not improving page performance, mind you—I modified Jake Archibald’s mergeResponses() function to suit my needs:
 
 
 
 async function mergeResponses (responsePromises) {
   const readers &#x3D; responsePromises.map(responsePromise &#x3D;&gt; {
     return Promise.resolve(responsePromise).then(response &#x3D;&gt; {
       return response.body.getReader();
     });
   });
 
   let doneResolve,
       doneReject;
 
   const done &#x3D; new Promise((resolve, reject) &#x3D;&gt; {
     doneResolve &#x3D; resolve;
     doneReject &#x3D; reject;
   });
 
   const readable &#x3D; new ReadableStream({
     async pull (controller) {
       const reader &#x3D; await readers[0];
 
       try {
         const { done, value } &#x3D; await reader.read();
 
         if (done) {
           readers.shift();
 
           if (!readers[0]) {
             controller.close();
             doneResolve();
 
             return;
           }
 
           return this.pull(controller);
         }
 
         controller.enqueue(value);
       } catch (err) {
         doneReject(err);
         throw err;
       }
     },
     cancel () {
       doneResolve();
     }
   });
 
   const headers &#x3D; new Headers();
   headers.append(&quot;Content-Type&quot;, &quot;text/html&quot;);
 
   return {
     done,
     response: new Response(readable, {
       headers
     })
   };
 }
 
 
 
 As usual, there’s a lot going on:
 
 
 
 mergeResponses() accepts an argument named responsePromises, which is an array of Response objects returned from either a navigation preload, fetch(), or caches.match(). Assuming the network is available, this will always contain three responses: two from caches.match() and (hopefully) one from the network.Before we can stream the responses in the responsePromises array, we must map responsePromises to an array containing one reader for each response. Each reader is used later in a ReadableStream() constructor to stream each response’s contents.A promise named done is created. In it, we assign the promise’s resolve() and reject() functions to the external variables doneResolve and doneReject, respectively. These will be used in the ReadableStream() to signal whether the stream is finished or has hit a snag.The new ReadableStream() instance is created with a name of readable. As responses stream in from CacheStorage and the network, their contents will be appended to readable.The stream’s pull() method streams the contents of the first response in the array. If the stream isn’t canceled somehow, the reader for each response is discarded by calling the readers array’s shift() method when the response is fully streamed. This repeats until there are no more readers to process.When all is done, the merged stream of responses is returned as a single response, and we return it with a Content-Type header value of text/html.
 
 
 
 This is much simpler if you use TransformStream, but depending on when you read this, that may not be an option for every browser. For now, we’ll have to stick with this approach.
 
 
 
 Now let’s revisit the Service Worker’s fetch() event from earlier, and apply the mergeResponses() function:
 
 
 
 self.addEventListener(&quot;fetch&quot;, event &#x3D;&gt; {
   const { request } &#x3D; event;
 
   // Static asset handling code omitted for brevity
   // ...
 
   // Check if this is a request for a document
   if (request.mode &#x3D;&#x3D;&#x3D; &quot;navigate&quot;) {
     // Navigation preload/fetch() fallback code omitted.
     // ...
 
     const { done, response } &#x3D; await mergeResponses([
       caches.match(&quot;/partial-header&quot;),
       networkContent,
       caches.match(&quot;/partial-footer&quot;)
     ]);
 
     event.waitUntil(done);
     event.respondWith(response);
   }
 });
 
 
 
 At the end of the fetch() event handler, we pass the header and footer partials from CacheStorage to the mergeResponses() function, and pass the result to the fetch() event’s respondWith() method, which serves the merged response on behalf of the Service Worker.
 
 
 
 Are the results worth the hassle?
 
 
 
 This is a lot of stuff to do, and it’s complicated! You might mess something up, or maybe your website’s architecture isn’t well-suited to this exact approach. So it’s important to ask: are the performance benefits worth the work? In my view? Yes! The synthetic performance gains aren’t bad at all:
 
 
 
 Figure 4. A bar chart of median FCP and LCP synthetic performance data across various Service Worker types for the Weekly Timber website.
 
 
 
 Synthetic tests don’t measure performance for anything except the specific device and internet connection they’re performed on. Even so, these tests were conducted on a staging version of my client’s website with a low-end Nokia 2 Android phone on a throttled “Fast 3G” connection in Chrome’s developer tools. Each category was tested ten times on the homepage. The takeaways here are:
 
 
 
 No Service Worker at all is slightly faster than the “standard” Service Worker with simpler caching patterns than the streaming variant. Like, ever so slightly faster. This may be due to the delay introduced by Service Worker startup, however, the RUM data I’ll go over shows a different case.Both LCP and FCP are tightly coupled in scenarios where there’s no Service Worker or when the “standard” Service Worker is used. This is because the content of the page is pretty simple and the CSS is fairly small. The Largest Contentful Paint is usually the opening paragraph on a page.However, the streaming Service Worker decouples FCP and LCP because the header content partial streams in right away from CacheStorage.Both FCP and LCP are lower in the streaming Service Worker than in other cases.
 
 
 
 Figure 5. A bar chart of median FCP and LCP RUM performance data across various Service Worker types for the Weekly Timber website.
 
 
 
 The benefits of the streaming Service Worker for real users is pronounced. For FCP, we receive an 79% improvement over no Service Worker at all, and a 63% improvement over the “standard” Service Worker. The benefits for LCP are more subtle. Compared to no Service Worker at all, we realize a 41% improvement in LCP—which is incredible! However, compared to the “standard” Service Worker, LCP is a touch slower.
 
 
 
 Because the long tail of performance is important, let’s look at the 95th percentile of FCP and LCP performance:
 
 
 
 Figure 6. A bar chart of 95th percentile FCP and LCP RUM performance data across various Service Worker types for the Weekly Timber website.
 
 
 
 The 95th percentile of RUM data is a great place to assess the slowest experiences. In this case, we see that the streaming Service Worker confers a 40% and 51% improvement in FCP and LCP, respectively, over no Service Worker at all. Compared to the “standard” Service Worker, we see a reduction in FCP and LCP by 19% and 43%, respectively. If these results seem a bit squirrely compared to synthetic metrics, remember: that’s RUM data for you! You never know who’s going to visit your website on which device on what network.
 
 
 
 While both FCP and LCP are boosted by the myriad benefits of streaming, navigation preload (in Chrome’s case), and sending less markup by stitching together partials from both CacheStorage and the network, FCP is the clear winner. Perceptually speaking, the benefit is pronounced, as this video would suggest:
 
 
 
 Figure 7. Three WebPageTest videos of a repeat view of the Weekly Timber homepage. On the left is the page not controlled by a Service Worker, with a primed HTTP cache. On the right is the same page controlled by a streaming Service Worker, with CacheStorage primed.
 
 
 
 Now ask yourself this: If this is the kind of improvement we can expect on such a small and simple website, what might we expect on a website with larger header and footer markup payloads?
 
 
 
 Caveats and conclusions
 
 
 
 Are there trade-offs with this on the development side? Oh yeah.
 
 
 
 As Philip Walton has noted, a cached header partial means the document title must be updated in JavaScript on each navigation by changing the value of document.title. It also means you’ll need to update the navigation state in JavaScript to reflect the current page if that’s something you do on your website. Note that this shouldn’t cause indexing issues, as Googlebot crawls pages with an unprimed cache.
 
 
 
 There may also be some challenges on sites with authentication. For example, if your site’s header displays the current authenticated user on log in, you may have to update the header partial markup provided by CacheStorage in JavaScript on each navigation to reflect who is authenticated. You may be able to do this by storing basic user data in localStorage and updating the UI from there.
 
 
 
 There are certainly other challenges, but it’ll be up to you to weigh the user-facing benefits versus the development costs. In my opinion, this approach has broad applicability in applications such as blogs, marketing websites, news websites, ecommerce, and other typical use cases.
 
 
 
 All in all, though, it’s akin to the performance improvements and efficiency gains that you’d get from an SPA. Only the difference is that you’re not replacing time-tested navigation mechanisms and grappling with all the messiness that entails, but enhancing them. That’s the part I think is really important to consider in a world where client-side routing is all the rage.
 
 
 
 “What about Workbox?,” you might ask—and you’d be right to. Workbox simplifies a lot when it comes to using the Service Worker API, and you’re not wrong to reach for it. Personally, I prefer to work as close to the metal as I can so I can gain a better understanding of what lies beneath abstractions like Workbox. Even so, Service Worker is hard. Use Workbox if it suits you. As far as frameworks go, its abstraction cost is very low.
 
 
 
 Regardless of this approach, I think there’s incredible utility and power in using the Service Worker API to reduce the amount of markup you send. It benefits my client and all the people that use their website. Because of Service Worker and the innovation around its use, my client’s website is faster in the far-flung parts of Wisconsin. That’s something I feel good about.
 
 
 
 Special thanks to Jake Archibald for his valuable editorial advice, which, to put it mildly, considerably improved the quality of this article.
 </content>
     </entry>
     <entry>
       <title>Keeping Your Design Mind New and Fresh</title>
         <link href="https://alistapart.com/article/keeping-a-fresh-design-mind/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">“Only a fool knows everything.”— African Proverb
 
 
 
 Since March 2020, most of us have been working from home, and the days blend into each other and look the same. This is not the first time I have experienced this type of feeling. 
 
 
 
 My commute — New York to New Jersey — is what folks in my area call the reverse commute.While going to the office, my days began to look the same: riding the subway to a bus to a shuttle to get to my job. Have you ever arrived at a destination and not even realized how you got there? This is how I began to experience the world everyday. I stopped paying attention to my surroundings.
 
 
 
 Because I worked a lot, the only time I would take off was for the holidays. During this time, I was a consultant and was coming to the end of an existing contract. For six years straight, I did this, until I decided to take six weeks off work to travel to Europe and visit places I had not seen before.
 
 
 
 A family friend let me stay with her in Munich, Germany; I did not speak German, and so began my adventure. I was in a new place, where I did not know anyone, and I got lost every single day. My eyes were opened to the fact that every day is an opportunity. It just took me going on a trip and traveling halfway around the world to realize it. There are new things to experience each and every day.
 
 
 
 When I returned to the U.S. and went back to work, I made a conscious decision to make each day different. Sometimes I would walk a new route. Some days I would take another train. Each change meant I saw something new: new clothing, new buildings, and new faces. It really impacted the way I viewed myself in the world.
 
 
 
 But what do you do when you cannot travel? Seeing a situation with new eyes takes practice, and you can still create the opportunity to see something by not taking your surroundings for granted.
 
 
 
 How do we do this? For me, I adopted a new philosophy of being WOQE: watching, observing, questioning, and exploring.
 
 
 
 
 
 
 
 Watching
 
 
 
 Let go of assumptions to open up your mind. This takes looking at yourself and understanding your beliefs.
 
 
 
 When I am looking to design something, I always have to tell myself that I am not the user. I don’t know where they come from, and I don’t know their reason for making the decisions they do. I begin the work to understand where they are coming from. It all starts with why.
 
 
 
 Observing
 
 
 
 View the situation from different angles. Architects think about the details of a building and look at different viewpoints and perspectives (i.e., outside the building, different sides of the building, etc.)
 
 
 
 How can you apply this approach to your designs? Here’s an example. I sketched something once as part of an augmented reality experience. Using my mobile device, I was able to walk around the sketch and see it from all sides, including the top and bottom. As a UX Designer, I have had to view items from both a user’s perspective and the business’ perspective. If I am giving a talk at a conference, I look at the talk from an audience perspective and my own.
 
 
 
 Questioning
 
 
 
 Use the “5 Why Technique” to get to the root of the problem. This involves asking “why” 5 times.
 
 
 
 You know how kids keep asking “why” when you answer a question from them? This approach is how you can get to the root of problems. For example, a friend of mine who is blind expressed interest in playing a popular augmented reality game. This intrigued me and I used a whiteboard as I worked through the 5 Whys with my friend. Here is the process we took:
 
 
 
 “Why can’t someone who is blind play Pokémon Go?” I asked.“Because the game is visual and requires someone to see what is on the screen.”“Why is the game only a visual perspective?”“Because this is the way it was designed.”“Why was it designed this way?”“Because frequently designers are creating for themselves and may not think about who they might be excluding.”“Why are designers excluding people?”“Because they were never taught to include them.”“Why were they never taught?”“Design programs often do not include an inclusive and accessible curriculum.”
 
 
 
 This may not be a scientific way of approaching a problem, but it is a starting point. My friend could not play this augmented reality game because designers were not taught to make this game for someone who is blind. After this exercise, I was able to work with a group of students who worked with my friend to create an augmented reality concept and ultimately a game using audio and haptic feedback.
 
 
 
 It all started with why.
 
 
 
 Exploring
 
 
 
 Collaborate with others to learn from others and teach others what you know. Let your friends and colleagues know what you are working on, and perhaps talk it through with them.When I was a freelance designer, I worked on my own and found it challenging when I would get stuck on a design. I searched online and found a group of designers who would come and share their work with each other for feedback. Through this group, I was able to get some insightful comments on my designs and explain some of my decisions. I began to collaborate with the folks in the group and found it very helpful. When talking to clients, this made me feel more confident explaining my designs because I had already been through the process with my online group.
 
 
 
 With all of our days blending into each other in this pandemic, we as designers have an unprecedented opportunity to really shake things up. Furthermore, we are problem solvers. As you move forward with your design practice, consider being WOQE to design with a fresh mind.
 </content>
     </entry>
     <entry>
       <title>How to Get a Dysfunctional Team Back on Track</title>
         <link href="https://alistapart.com/article/dysfunctional-teams-back-on-track/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">Maybe you’ve been part of a team that you’ve seen slowly slide into a rut. You didn’t notice it happen, but you’re now not shipping anything, no one’s talking to each other, and the management’s Eye of Sauron has cast its gaze upon you.
 
 
 
 Maybe you’ve just joined a team that’s in the doldrums.
 
 
 
 Maybe the people who used to oil the wheels that kept everyone together have moved on and you’re having to face facts—you all hate each other.
 
 
 
 However you’ve ended up in this situation, the fact is that you’re now here and it’s up to someone to do something about it. And that person might be you.
 
 
 
 You’re not alone
 
 
 
 The first thing to understand is that you’re not the only person to ever encounter problems. Things like this happen all the time at work, but there are simple steps you can take and habits you can form to ease the situation and even dig yourself (and your team) out of the hole. I’ll share some techniques that have helped me, and maybe they can work for you, too.
 
 
 
 So let me tell you a story about a hot mess I found myself in and how we turned it around. Names and details have been changed to protect the innocent.
 
 
 
 It always starts out great
 
 
 
 An engineer called Jen was working with me on a new feature on our product that lets people create new meal recipes themselves. I was the Project Manager. We were working in six-week cycles.
 
 
 
 She had to rely on an API that was managed by Tom (who was in another team) to allow her to get and set the new recipe information on a central database. Before we kicked off, everyone knew the overall objective and everyone was all smiles and ready to go.
 
 
 
 The system architecture was a legacy mishmash of different parts of local databases and API endpoints. And, no prizes for guessing what’s coming next, the API documentation was like Swiss cheese.
 
 
 
 Two weeks into a six-week cycle, Jen hit Tom up with a list of her dream API calls that she wanted to use to build her feature. She asked him to confirm or deny they would work—or even if they existed at all—because once she started digging into the docs, it wasn’t clear to her if the API could support her plans.
 
 
 
 However, Tom had form for sticking his head in the sand and not responding to requests he didn’t like. Tom went to ground and didn’t respond. Tom’s manager, Frankie, was stretched too thin, and hence wasn’t paying attention to this until I was persistently asking about it, in increasingly fraught tones.
 
 
 
 In the meantime, Jen tried to do as much as she could. Every day she built a bit more based on her as-yet unapproved design, hoping it would all work out.
 
 
 
 With two weeks left to go, Tom eventually responded with a short answer—which boiled down to “The API doesn’t support these calls and I don’t see why I should build something that does. Why don’t you get the data from the other part of the system? And by the way, if I’m forced to do this, it will take at least six weeks.”
 
 
 
 And as we know, six weeks into two weeks doesn’t go. Problem.
 
 
 
 How did we sort it?
 
 
 
 Step 1 — Accept
 
 
 
 When things go south, what do you do?
 
 
 
 Accept it.
 
 
 
 Acknowledge whatever has happened to get you into this predicament. Take some notes about it to use in team appraisals and retrospectives. Take a long hard look at yourself, too.
 
 
 
 Write a concise, impersonal summary of where you are. Try not to write it from your point of view. Imagine that you’re in your boss’ seat and just give them the facts as they are. Don’t dress things up to make them sound better. Don’t over-exaggerate the bad. Leave the emotions to the side.
 
 
 
 When you can see your situation clearly, you’ll make better decisions.
 
 
 
 Now, pointing out the importance of taking some time to cool down and gather your thoughts seems obvious, but it’s based on the study of some of the most basic circuitry in our brains. Daniel Goleman’s 1995 book, Emotional Intelligence: Why It Can Matter More Than IQ, introduces the concept of emotional hijacking; the idea that the part of our brain that deals with emotion—the limbic system—can biologically interrupt rational thinking when it is overstimulated. For instance, experiments show that the angrier men get, the poorer are the decisions they make at the casino. And another study found that people in a negative emotional state are more likely to deviate from logical norms. To put it another way, if you’re pissed off, you can’t think straight.
 
 
 
 So when you are facing up to the facts, avoid the temptation to keep it off-the-record and only discuss it on the telephone or in person with your colleagues. There’s nothing to be scared of by writing it down. If it turns out that you’re wrong about something, you can always admit it and update your notes. If you don’t write it down, then there’s always scope for misunderstanding or misremembering in future.
 
 
 
 In our case, we summarized how we’d ended up at that juncture; the salient points were:
 
 
 
 I hadn’t checked to ensure we had scoped it properly before committing to the work. It wasn’t a surprise that the API coverage was patchy, but I turned a blind eye because we were excited about the new feature.Jen should have looked for the hard problem first rather than do a couple of weeks’ worth of nice, easy work around the edges. That’s why we lost two weeks off the top.Tom and Frankie’s communication was poor. The reasons for that don’t form part of this discussion, but something wasn’t right in that team.
 
 
 
 And that’s step one.
 
 
 
 Step 2 — Rejoice
 
 
 
 Few people like to make mistakes, but everyone will make one at some point in their life. Big ones, small ones, important ones, silly ones—we all do it. Don’t beat yourself up.
 
 
 
 
 
 
 
 At the start of my career, I worked on a team whose manager had a very high opinion of himself. He was good, but what I learned from him was that he spread that confidence around the team. If something was looking shaky, he insisted that if we could “smell smoke,” that he had to be the first to know so he could do something about it. If we made a mistake, there was no hiding from it. We learned how to face up to it and accept responsibility, but what was more important was learning from him the feeling we were the best people to fix it.
 
 
 
 There was no holding of grudges. What was done, was done. It was all about putting it behind us.
 
 
 
 He would tell us that we were only in this team because he had handpicked us because we were the best and he only wanted the best around him. Now, that might all have been manipulative nonsense, but it worked.
 
 
 
 The only thing you can control is what you do now, so try not to fret about what happened in the past or get anxious about what might happen in the future.
 
 
 
 With that in mind, once you’ve written the summary of your sticky situation, set it aside!
 
 
 
 I’ll let you in on a secret. No one else is interested in how you got here. They might be asking you about it (probably because they are scared that someone will ask them), but they’re always going to be more interested in how you’re going to sort the problem out.
 
 
 
 So don’t waste time pointing fingers. Don’t prepare slide decks to throw someone under the bus. Tag that advice with a more general “don’t be an asshole” rule.
 
 
 
 If you’re getting consistent heat about the past, it’s because you’re not doing a good enough job filling the bandwidth with a solid, robust, and realistic plan for getting out of the mess.
 
 
 
 So focus on the future.
 
 
 
 Sometimes it’s not easy to do that, but remember that none of this is permanent. Trust in the fact that if you pull it together, you’ll be in a much more powerful position to decide what to do next.
 
 
 
 Maybe the team will hold together with a new culture or, if it is irretrievably broken, once you’re out of the hole then you can do something about it and switch teams or even switch jobs. But be the person who sorted it out, or at the very least, be part of the gang who sorted it out. That will be obvious to outsiders and makes for a much better interview question response.
 
 
 
 In our story with Jen, we had a short ten-minute call with everyone involved on the line. We read out the summary and asked if anyone had anything to add.
 
 
 
 Tom spoke up and said that he never gets time to update the API documentation because he always has to work on emergencies. We added that to our summary:
 
 
 
 Tom has an ongoing time management problem. He doesn’t have enough time allocated to maintain and improve the API documentation.
 
 
 
 After that was added, everyone agreed that the summary was accurate.
 
 
 
 I explained that the worst thing that could now happen was that we had to report back to the wider business that we’d messed up and couldn’t hit our deadline.
 
 
 
 If we did that, we’d lose face. There would be real financial consequences. It would show up on our appraisals. It wouldn’t be good. It wouldn’t be the end of the world, but it wasn’t something that we wanted. Everyone probably knew all that already, but there’s a power in saying it out loud. Suddenly, it doesn’t seem so scary.
 
 
 
 Jen spoke up to say that she was new here and really didn’t want to start out like this. There was some murmuring in general support. I wrapped up that part of the discussion.
 
 
 
 I purposefully didn’t enter into a discussion about the solution yet. We had all come together to admit the circumstances we were in. We’d done that. It was enough for now.
 
 
 
 Step 3 — Move on
 
 
 
 Stepping back for a second, as the person who is going to lead the team out of the wilderness, you may want to start getting in everyone’s face. You’ll be tempted to rely on your unlimited reserves of personal charm or enthusiasm to vibe everyone up. Resist the urge! Don’t do it!
 
 
 
 Your job is to give people the space to let them do their best work.
 
 
 
 I learned this the hard way. I’m lucky enough that I can bounce back quickly, but when someone is under pressure, funnily enough, a super-positive person who wants to throw the curtains open and talk about what a wonderful day it is might not be the most motivational person to be around. I’ve unwittingly walked into some short-tempered conversations that way.
 
 
 
 Don’t micromanage. In fact, scrap all of your management tricks. Your job is to listen to what people are telling you—even if they’re telling you things by not talking.
 
 
 
 Reframe the current problem. Break it up into manageable chunks.
 
 
 
 The first task to add to your list of things to do is simply to “Decide what we’re going to do about [the thing].”
 
 
 
 It’s likely that there’s a nasty old JIRA ticket that everyone has been avoiding or has been bounced back and forth between different team members. Set that aside. There’s too much emotional content invested in that ticket now.
 
 
 
 Create a new task that’s entirely centered on making a decision. Now, break it down into subtasks for each member of the team, like “Submit a proposal for what to do next.” Put your own suggestions in the mix but do your best to dissociate yourself from them.
 
 
 
 Once you start getting some suggestions back and can tick those tasks off the list, you start to generate positive momentum. Nurture that.
 
 
 
 If a plan emerges, champion it. Be wary of naysayers. Challenge them respectfully with “How do you think we should…?” questions. If they have a better idea, champion that instead; if they don’t respond at all, then gently suggest “Maybe we should go with this if no one else has a better idea.”
 
 
 
 Avoid words like “need,” “just,” “one,” or “small.” Basically, anything that imposes a view of other people’s work. It seems trivial, but try to see it from the other side.
 
 
 
 Saying, “I just need you to change that one small thing” hits the morale-killing jackpot. It unthinkingly diminishes someone else’s efforts. An engineer or a designer could reasonably react by thinking “What do you know about how to do this?!” Your job is to help everyone drop their guard and feel safe enough to contribute.
 
 
 
 Instead, try “We’re all looking at you here because you’re good at this and this is a nasty problem. Maybe you know a way to make this part work?”
 
 
 
 More often than not, people want to help.
 
 
 
 So I asked Jen, Tom, and Frankie to submit their proposals for a way through the mess.
 
 
 
 It wasn’t straightforward. Just because we’d all agreed how we got here didn’t just magically make all the problems disappear. Tom was still digging his heels in about not wanting to write more code, and kept pushing back on Jen.
 
 
 
 There was a certain amount of back and forth. Although, with some constant reminders that we should maybe focus on what will move us forward, we eventually settled on a plan.
 
 
 
 Like most compromises, it wasn’t pretty or simple. Jen was going to have to rely on using the local database for a certain amount of the lower-priority features. Tom was going to have to create some additional API functions and would end up with some unnecessary traffic that might create too much load on the API.
 
 
 
 And even with the compromise, Tom wouldn’t be finished in time. He’d need another couple of weeks.
 
 
 
 But it was a plan!
 
 
 
 N.B. Estimating is a whole other subject that I won’t cover here. Check out the Shape Up process for some great advice on that.
 
 
 
 Step 4 — Spread the word
 
 
 
 Once you’ve got a plan, commit to it and tell everyone affected what’s going on.
 
 
 
 When communicating with people who are depending on you, take the last line of your email, which usually contains the summary or the “ask,” and put it at the top. When your recipient reads the message, the opener is the meat. Good news or bad news, that’s what they’re interested in. They’ll read on if they want more.
 
 
 
 If it’s bad news, set someone up for it with a simple “I’m sorry to say I’ve got bad news” before you break it to them. No matter who they are, kindly framing the conversation will help them digest it.
 
 
 
 When discussing it with the team, put the plan somewhere everyone can see it. Transparency is key.
 
 
 
 Don’t pull any moves—like publishing deadline dates to the team that are two weeks earlier than the date you’ve told the business. Teams aren’t stupid. They’ll know that’s what you do.
 
 
 
 Publish the new deadlines in a place where everyone on the team can see them, and say we’re aiming for this date but we’re telling the business that we’ll definitely be done by that date.
 
 
 
 In our case, I posted an update to the rest of the business as part of our normal weekly reporting cycle to announce we’d hit a bump that was going to affect our end date.
 
 
 
 Here’s an extract:
 
 
 
 Hi everyone,Here’s the update for the week. I’m afraid there’s a bit of bad news to start but there is some good news too.First:We uncovered a misunderstanding between Jen and Tom this week. The outcome is that Tom has more API work to do than he anticipated. This affects the delivery date and means we’re now planning to finish 10 working days later on November 22.**Expected completion date ** CHANGED ****Original estimate: November 8Current estimate: November 22Second: We successfully released version 1.3 of the app into the App Store 🎉.
 
 
 
 And so on...
 
 
 
 That post was available for everyone within the team to see. Everyone knew what was to be done and what the target was.
 
 
 
 I had to field some questions from above, but I was ready with my summary of what went wrong and what we’d all agreed to do as a course of action. All I had to do was refer to it. Then I could focus on sharing the plan.
 
 
 
 And all manner of things shall be well
 
 
 
 Now, I’d like to say that we then had tea and scones every day for the next month and it was all rather spiffing. But that would be a lie.
 
 
 
 There was some more wailing and gnashing of teeth, but we all got through it and—even though we tried to finish early but failed—we did manage to finish by the November 22 date.
 
 
 
 And then, after a bit of a tidy up, we all moved on to the next project, a bit older and a bit wiser. I hope that helps you if you’re in a similar scenario. Send me a tweet or email me at liam.nugent@hey.com with any questions or comments. I’d love to hear about your techniques and advice.
 </content>
     </entry>
     <entry>
       <title>The Future of Web Software Is HTML-over-WebSockets</title>
         <link href="https://alistapart.com/article/the-future-of-web-software-is-html-over-websockets/"/>
       <updated>2022-06-27T04:19:06.774Z</updated>
       <content type="text">The future of web-based software architectures is already taking form, and this time it’s server-rendered (again). Papa’s got a brand new bag: HTML-over-WebSockets and broadcast everything all the time.
 
 
 
 The dual approach of marrying a Single Page App with an API service has left many dev teams mired in endless JSON wrangling and state discrepancy bugs across two layers. This costs dev time, slows release cycles, and saps the bandwidth for innovation.
 
 
 
 But a new WebSockets-driven approach is catching web developers’ attention. One that reaffirms the promises of classic server-rendered frameworks: fast prototyping, server-side state management, solid rendering performance, rapid feature development, and straightforward SEO. One that enables multi-user collaboration and reactive, responsive designs without building two separate apps. The end result is a single-repo application that feels to users just as responsive as a client-side all-JavaScript affair, but with straightforward templating and far fewer loading spinners, and no state misalignments, since state only lives in one place. All of this sets us up for a considerably easier (and faster!) development path. 
 
 
 
 Reclaiming all of that time spent addressing architecture difficulties grants you a pool of surplus hours that you can use to do awesome. Spend your dev budget, and your company’s salary budget, happily building full-stack features yourself, and innovating on things that benefit your company and customers. 
 
 
 
 And in my opinion, there’s no better app framework for reclaiming tedious development time than Ruby on Rails. Take another look at the underappreciated Stimulus. Beef up the View in your MVC with ViewComponents. Add in the CableReady and StimulusReflex libraries for that Reactive Rails (as it has been dubbed) new car smell, and you’re off to the races. But we’ll get back to Rails in a bit...
 
 
 
 This all started with web frameworks...
 
 
 
 Web frameworks burst onto the scene around 2005 amidst a sea of mostly figure-it-out-for-yourself scripting language libraries glued together and thrown onto hand-maintained Apache servers. This new architecture promised developers a more holistic approach that wrapped up all the fiddly stuff in no-touch conventions, freeing developers to focus on programming ergonomics, code readability, and fast-to-market features. All a developer had to do was learn the framework’s core language, get up to speed on the framework itself and its conventions, and then start churning out sophisticated web apps while their friends were still writing XML configuration files for all those other approaches.
 
 
 
 Despite the early criticisms that always plague new approaches, these server-rendered frameworks became tools of choice, especially for fast-moving startups—strapped for resources—that needed an attractive, feature-rich app up yesterday.
 
 
 
 But then the JavaScript everything notion took hold...
 
 
 
 As the web development world pushed deeper into the 2010s, the tides began to turn, and server-rendered frameworks took something of a backseat to the Single Page Application, wholly built in JavaScript and run entirely on the client’s computer. At many companies, the “server” became relegated to hosting an API data service only, with most of the business logic and all of the HTML rendering happening on the client, courtesy of the big ’ol package of JavaScript that visitors were forced to download when they first hit the site. 
 
 
 
 This is where things started to get ugly.
 
 
 
 Fast-forward to 2020 and the web isn’t getting any faster, as we were promised it would with SPAs. Shoving megabytes of JavaScript down an iPhone 4’s throat doesn’t make for a great user experience. And if you thought building a professional web app took serious resources, what about building a web app and an API service and a communication layer between them? Do we really believe that every one of our users is going to have a device capable of digesting 100 kB of JSON and rendering a complicated HTML table faster than a server-side app could on even a mid-grade server?
 
 
 
 Developing and hosting these JavaScript-forward apps didn’t get any cheaper either. In many cases we’re now doing twice the work, and maybe even paying twice the developers, to achieve the same results we had before with server-side app development.
 
 
 
 In 2005, app frameworks blew everyone’s minds with “build a blog app in 15 minutes” videos. Fifteen years later, doing the same thing with an SPA approach can require two codebases, a JSON serialization layer, and dozens of spinners all over the place so we can still claim a 50ms First Contentful Paint. Meanwhile, the user watches some blank gray boxes, hoping for HTML to finally render from all the JSON their browser is requesting and digesting. 
 
 
 
 How did we get here? This is not my beautiful house! Were we smart in giving up all of that server-rendered developer happiness and doubling down on staff and the time to implement in order to chase the promise of providing our users some fancier user interfaces?
 
 
 
 Well. Yes. Sort of.
 
 
 
 We’re not building web software for us. We’re building it for them. The users of our software have expectations of how it’s going to work for them. We have to meet them where they are. Our users are no longer excited about full-page refreshes and ugly Rube Goldberg-ian multi-form workflows. The SPA approach was the next logical leap from piles of unorganized spaghetti JavaScript living on the server. The problem, though: it was a 5% improvement, not a 500% improvement. 
 
 
 
 Is 5% better worth twice the work? What about the developer cost?
 
 
 
 Bedazzling the web app certainly makes things fancier from the user’s perspective. Done well, it can make the app feel slicker and more interactive, and it opens up a wealth of new non-native interaction elements. Canonizing those elements as components was the next natural evolution. Gone are the days of thinking through how an entire HTML document could be mutated to give the illusion of the user interacting with an atomic widget on the page—now, that can be implemented directly, and we can think about our UX in terms of component breakdowns. But, alas, the costs begin to bite us almost immediately.
 
 
 
 Go ahead, write that slick little rating stars component. Add some cool animations, make the mouseover and click area feel good, give some endorphin-generating feedback when a selection is made. But now what? In a real app, we need to persist that change, right? The database has to be changed to reflect this new state, and the app in front of the user’s eyes needs to reflect that new reality too. 
 
 
 
 In the old days, we’d give the user a couple star GIFs, each a link that hit the same server endpoint with a different param value. Server-side, we’d save that change to the database, then send back a whole new HTML page for their browser to re-render; maybe we’d even get fancy and use AJAX to do it behind the scenes, obviating the need for the full HTML and render. Let’s say the former costs x in developer time and salary (and we won’t even talk about lost opportunity cost for features rolled out too late for the market). In that case, the fancy AJAX-based approach costs x + n (you know, some “extra JavaScript sprinkles”), but the cost of lots and lots of n grows as our app becomes more and more of a JavaScript spaghetti sprinkles mess.
 
 
 
 Over in the SPA world, we’re now writing JavaScript in the client-side app and using JSX or Handlebars templates to render the component, then code to persist that change to the front-end data store, then a PUT request to the API, where we’re also writing an API endpoint to handle the request, a JSON serializer (probably with its own pseudo-template) to package up our successful response, and then front-end code to ensure we re-render the component (and some branching logic to maybe rollback and re-render the client-side state change if the backend failed on us). This costs a lot more than even x + n in developer time and salary. And if you’ve split your team into “front-end” and “back-end” people, you might as well go ahead and double that cost (both time and money) for many non-trivial components where you need two different people to finish the implementation. Sure, the SPA mitigates some of the ever-growing spaghetti problem, but at what cost for a business racing to be relevant in the market or get something important out to the people who need it?
 
 
 
 One of the other arguments we hear in support of the SPA is the reduction in cost of cyber infrastructure. As if pushing that hosting burden onto the client (without their consent, for the most part, but that’s another topic) is somehow saving us on our cloud bills. But that’s ridiculous. For any non-trivial application, you’re still paying for a server to host the API and maybe another for the database, not to mention load balancers, DNS, etc. And here’s the thing: none of that cost even comes close to what a software company pays its developers! Seriously, think about it. I’ve yet to work at any business where our technical infrastructure was anything more than a fraction of our salary overhead. And good developers expect raises. Cloud servers generally just get cheaper over time.
 
 
 
 If you want to be efficient with your money—especially as a cash-strapped startup—you don’t need to cheap out on cloud servers; you need to get more features faster out of your existing high-performance team.
 
 
 
 In the old, old days, before the web frameworks, you’d pay a developer for six weeks to finally unveil…the log-in page. Cue the sad trombone. Then frameworks made that log-in page an hour of work, total, and people were launching web startups overnight. The trumpets sound! Now, with our SPA approach, we’re back to a bunch of extra work. It’s costing us more money because we’re writing two apps at once. There’s that trombone again...
 
 
 
 We’re paying a lot of money for that 5% user experience improvement.
 
 
 
 But what if we could take the best client-side JavaScript ideas and libraries from that 5% improvement and reconnect them with the developer ergonomics and salary savings of a single codebase? What if components and organized JavaScript could all live in one rock-solid app framework optimized for server-side rendering? What if there is a path to a 500% jump?
 
 
 
 Sound impossible? It’s not. I’ve seen it, like C-beams glittering in the dark near the Tannhäuser Gate. I’ve built that 500% app, in my free time, with my kids running around behind me barking like dogs. Push broadcasts to logged-in users. Instant updates to the client-side DOM in milliseconds. JavaScript-driven 3D animations that interact with real-time chat windows. All in a single codebase, running on the same server hardware I’d use for a “classic” server-rendered app (and maybe I can even scale that hardware down since I’m rendering HTML fragments more often than full-page documents). No separate front-end app. Clean, componentized JavaScript and server-side code, married like peanut butter and jelly. It’s real, I tell you!
 
 
 
 Socket to me! (Get it? Get it? Ah, nevermind...)
 
 
 
 Finalized in 2011, support for WebSockets in modern browsers ramped up throughout the 2010s and is now fully supported in all modern browsers. With the help of a small bit of client-side JavaScript, you get a full-duplex socket connection between browser and server. Data can pass both ways, and can be pushed from either side at any time, no user-initiated request needed.
 
 
 
 Like the game industry’s ever-expanding moves into cloud-based gaming, the future of web apps is not going to be about pushing even heavier obligations onto the user/client, but rather the opposite: let the client act as a thin terminal that renders the state of things for the human. WebSockets provide the communication layer, seamless and fast; a direct shot from the server to the human.
 
 
 
 But this wasn’t terribly easy for many developers to grok at first. I sure didn’t. And the benefits weren’t exactly clear either. After years (decades, even) of wrapping our heads around the HTTP request cycle, to which all server-handled features must conform, adopting this WebSocket tech layer required a lot of head scratching. As with many clever new technologies or protocols, we needed a higher-level abstraction that provided something really effective for getting a new feature in front of a user, fast.
 
 
 
 Enter HTML-over-WebSockets...
 
 
 
 Want a hyper-responsive datalist typeahead that is perfectly synced with the database? On every keystroke, send a query down the WebSocket and get back precisely the changed set of option tags, nothing more, nothing less.
 
 
 
 How about client-side validations? Easy. On every input change, round up the form values and send ’em down the WebSocket. Let your server framework validate and send back changes to the HTML of the form, including any errors that need to be rendered. No need for JSON or complicated error objects.
 
 
 
 User presence indicators? Dead simple. Just check who has an active socket connection.
 
 
 
 What about multi-user chat? Or document collaboration? In classic frameworks and SPAs, these are the features we put off because of their difficulty and the code acrobatics needed to keep everyone’s states aligned. With HTML-over-the-wire, we’re just pushing tiny bits of HTML based on one user’s changes to every other user currently subscribed to the channel. They’ll see exactly the same thing as if they hit refresh and asked the server for the entire HTML page anew. And you can get those bits to every user in under 30ms.
 
 
 
 We’re not throwing away the promise of components either. Where this WebSockets-based approach can be seen as a thick server/thin client, so too can our components. It’s fractal, baby! Make that component do delightful things for the user with smart JavaScript, and then just ask the server for updated HTML, and mutate the DOM. No need for a client-side data store to manage the component’s state since it’ll render itself to look exactly like what the server knows it should look like now. The HTML comes from the server, so no need for JSX or Handlebars or &lt;insert other JavaScript templating library here&gt;. The server is always in control: rendering the initial component’s appearance and updating it in response to any state change, all through the socket. 
 
 
 
 And there’s nothing saying you have to use those socket channels to send only HTML. Send a tiny bit of text, and have the client do something smart. Send a chat message from one user to every other user, and have their individual clients render that message in whatever app theme they’re currently using. Imagine the possibilities!
 
 
 
 But it’s complex/expensive/requires a bunch of new infrastructure, right?
 
 
 
 Nope. Prominent open-source web servers support it natively, generally without needing any kind of extra configuration or setup. Many server-side frameworks will automatically ship the JS code to the client for native support in communicating over the socket. In Rails, for example, setting up your app to use WebSockets is as easy as configuring the built-in ActionCable and then deploying as usual on the same hardware you would have used otherwise. Anecdotally, the typical single Rails server process seems to be perfectly happy supporting nearly 4,000 active connections. And you can easily swap in the excellent AnyCable to bump that up to around 10,000+ connections per node by not relying on the built-in Ruby WebSocket server. Again, this is on the usual hardware you’d be running your web server on in the first place. You don’t need to set up any extra hardware or increase your cloud infrastructure.
 
 
 
 This new approach is quickly appearing as extensions, libraries, or alternative configurations in a variety of languages and web frameworks, from Django’s Sockpuppet to Phoenix’s LiveView and beyond. Seriously, go dig around for WebSockets-based libraries for your favorite app framework and then step into a new way of thinking about your app architectures. Build something amazing and marvel at the glorious HTML bits zipping along on the socket, like jet fighters passing in the night. It’s more than a new technical approach; it’s a new mindset, and maybe even a new wellspring of key app features that will drive your startup success.
 
 
 
 But I’d be remiss if I didn’t highlight for the reader my contender for Best Framework in a Leading Role. Sure, any app framework can adopt this approach, but for my money, there’s a strong case to be made that the vanguard could be Ruby on Rails. 
 
 
 
 So we come back around to Rails, 15 years on from its launch...
 
 
 
 Set up a Rails 6 app with the latest versions of Turbolinks, Stimulus, StimulusReflex, CableReady, and GitHub’s ViewComponent gem, and you can be working with Reactive Rails in a way that simultaneously feels like building a classic Rails app and like building a modern, componentized SPA, in a single codebase, with all the benefits of server-side rendering, HTML fragment caching, easy SEO, rock-solid security, and the like. You’ll suddenly find your toolbelt bursting with straightforward tools to solve previously daunting challenges.
 
 
 
 Oh, and with Turbolinks, you also get wrappers allowing for hybrid native/HTML UIs in the same codebase. Use a quick deploy solution like Heroku or Hatchbox, and one developer can build a responsive, reactive, multi-platform app in their spare time. Just see this Twitter clone if you don’t believe me. 
 
 
 
 OK, that all sounds exciting, but why Rails specifically? Isn’t it old and boring? You already said any framework can benefit from this new WebSocket, DOM-morphing approach, right? 
 
 
 
 Sure. But where Rails has always shined is in its ability to make rapid prototyping, well…rapid, and in its deep ecosystem of well-polished gems. Rails also hasn’t stopped pushing the envelope forward, with the latest version 6.1.3 of the framework boasting a ton of cool features. 
 
 
 
 If you’ve got a small, resource-strapped team, Rails (and Ruby outside of the framework) still serves as a potent force multiplier that lets you punch way above your weight, which probably explains the $92 billion in revenue it’s helped drive over the years. With this new approach, there’s a ton more weight behind that punch. While your competitors are fiddling with their JSON serializers and struggling to optimize away all the loading spinners, you’re rolling out a new multi-user collaborative feature every week…or every day. 
 
 
 
 You win. Your fellow developers win. Your business wins. And, most importantly, your users win.
 
 
 
 That’s what Rails promised from the day it was released to the community. That’s why Rails spawned so many imitators in other languages, and why it saw such explosive growth in the startup world for years. And that same old rapid prototyping spirit, married to this new HTML-over-the-wire approach, positions Rails for a powerful resurgence. 
 
 
 
 Ruby luminary and author of The Ruby Way, Obie Fernandez, seems to think so.
 
 
 
 Heck, even Russ Hanneman thinks this approach with StimulusReflex is the new hotness.
 
 
 
 And the good folks over at Basecamp (creators of Rails in the first place), dropped their own take on the concept, Hotwire, just in time for the 2020 holidays, so your options for tackling this new and exciting technique continue to expand.
 
 
 
 Don’t call it a comeback, because Rails has been here for years. With this new architectural approach, brimming with HTML-over-WebSockets and full-duplex JavaScript interactions, Rails becomes something new, something beautiful, something that demands attention (again). 
 
 
 
 Reactive Rails, with StimulusReflex and friends, is a must-look for anyone exhausted from toiling with JSON endpoints or JSX, and I’m super excited to see the new crop of apps that it enables.
 </content>
     </entry>
     <entry>
       <title>Faster page loads using server think-time with Early Hints</title>
         <link href="https://developer.chrome.com/en/blog/early-hints/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text"># What is Early Hints?
 Websites have become more sophisticated over time. As such, it&#x27;s not unusual that a server needs to perform non-trivial work (for example, access to databases, or CDNs accessing the origin server) to produce the HTML for the requested page. Unfortunately, this &quot;server think-time&quot; results in extra latency before the browser can start rendering the page. Indeed, the connection effectively goes idle for as long as it takes the server to prepare the response.
 
 
 Without Early Hints: everything is blocked on the server determining how to respond for the main resource.
 
 Early Hints is an HTTP status code (103 Early Hints) used to send a preliminary HTTP response ahead of a final response. This allows a server to send hints to the browser about critical sub-resources (for example,  stylesheet for the page, critical JavaScript) or origins that will be likely used by the page, while the server is busy generating the main resource. The browser can use those hints to warm up connections, and request sub-resources, while waiting for the main resource. In other words, Early Hints helps the browser take advantage of such &quot;server think-time&quot; by doing some work in advance, thereby speeding up page loads.
 
 
 With Early Hints: the server can serve a partial response with resource hints while it determines the final response
 
 In some cases, the performance improvement to the Largest Contentful Paint can go from several hundred milliseconds, as observed by Shopify and by Cloudflare, and up to a second faster, as seen in this before/after comparison:
 
 
 Before/After comparison of Early Hints on a test website done with WebPageTest (Moto G4 - DSL)
 
 # Implementing Early Hints
 Early Hints is available from Chrome version 103, as a response to navigation requests, or user interactions that change the url in the status bar, with support for both preconnect and preload hints.
 Before going deep into the topic, please note that Early Hints are not useful if your server can send a 200 (or other final responses) right away. Instead, consider using the regular link rel&#x3D;preload or link rel&#x3D;preconnect on the main response (Link rel HTTP header), or in the main response (&lt;link&gt;  elements), in such situations. For the cases where your server needs a little time to generate the main response, read on!
 The first step to take advantage of Early Hints consists of identifying the top landing pages, that is, the pages where your users typically start when they visit your website. This could be the homepage, or popular product listing pages if you have lots of users coming from other websites. The reason these entry points matter more than other pages is because Early Hints&#x27; usefulness decreases as the user navigates around your website (that is, the browser is more likely to have all the sub-resources it needs on the second or third subsequent navigation). It&#x27;s also always a good idea to deliver a great first impression!
 Now that you have this prioritized list of landing pages, the next step consists of identifying which origins or sub-resources would be good candidates for preconnect or preload hints, as a first approximation. Typically, those would be origins and sub-resources that contribute the most to key user metrics such as Largest Contentful Paint, or First Contentful Paint. More concretely, look for render-blocking sub-resources such as synchronous JavaScript, stylesheets, or even web fonts. Similarly, look for origins that host sub-resources that contribute a lot to key user metrics. Note: if your main resources are already using  or , you may consider these  origins or resources among the candidates for Early Hints. See this article for more details.
 While this represents a decent starting point, it&#x27;s not necessarily sufficient. The second step consists of minimizing the risk of using Early Hints on resources or origins that might be obsolete, or no longer used by the main resource. For instance, resources that are frequently updated and versioned (for example,  example.com/css/main.fa231e9c.css) may not be the best choice.  Note that this concern isn&#x27;t specific to Early Hints, it applies to any link rel&#x3D;preload or rel&#x3D;preconnect wherever they might be present. This is the sort of detail that&#x27;s best dealt with automation or templating (for example, a manual process is more likely to lead to mismatched hash or version urls between link rel&#x3D;preload and the actual HTML tag using the resource).
 As an example, consider the following flow:
 GET /main.htmlHost: example.comUser-Agent: [....] Chrome/103.0.0.0 [...]
 The server predicts that main.abcd100.css will be needed, and suggests preloading it via Early Hints:
 103 Early HintsLink: &lt;/main.abcd100.css&gt;; rel&#x3D;preload; as&#x3D;style[...]
 A few moments later, the webpage, including the linked CSS is served. Unfortunately, this CSS resource is frequently updated, and the main resource is already five versions ahead (abcd105) of the predicted CSS resource (abcd100).
 200 OK[...]&lt;HTML&gt;&lt;head&gt;   &lt;title&gt;Example&lt;/title&gt;   &lt;link rel&#x3D;&quot;stylesheet&quot; href&#x3D;&quot;/main.abcd105.css&quot;&gt;
 In general, aim for resources and origins that are fairly stable, and largely independent of the outcome for the main resource. If necessary, you may consider splitting your key resources in two: a stable part designed to be used with Early Hints, and a more dynamic part left to be fetched after the main resource is received by the browser:
 &lt;HTML&gt;&lt;head&gt;   &lt;title&gt;Example&lt;/title&gt;   &lt;link rel&#x3D;&quot;stylesheet&quot; href&#x3D;&quot;/main.css&quot;&gt;   &lt;link rel&#x3D;&quot;stylesheet&quot; href&#x3D;&quot;/experimental.3eab3290.css&quot;&gt;
 Finally, on the server side, look for main resource requests sent by browsers known to support Early Hints, and respond immediately with 103 Early Hints. In the 103 response, include the relevant preconnect and preload hints. Once the main resource is ready, follow up with the usual response (for example, 200 OK if successful). For backward compatibility, it&#x27;s good practice to also include LINK HTTP headers in the final response, perhaps even augmenting with critical resources that became evident as part of generating the main resource (for example, the dynamic part of a key resource if you followed the &quot;split in two&quot; suggestion). Here is what this would look like:
 GET /main.htmlHost: example.comUser-Agent: [....] Chrome/103.0.0.0 [...]103 Early HintsLink: &lt;https://fonts.google.com&gt;; rel&#x3D;preconnectLink: &lt;/main.css&gt;; rel&#x3D;preload; as&#x3D;styleLink: &lt;/common.js&gt;; rel&#x3D;preload; as&#x3D;script
 A few moments later:
 200 OKContent-Length: 7531Content-Type: text/html; charset&#x3D;UTF-8Content-encoding: brLink: &lt;https://fonts.google.com&gt;; rel&#x3D;preconnectLink: &lt;/main.css&gt;; rel&#x3D;preload; as&#x3D;styleLink: &lt;/common.js&gt;; rel&#x3D;preload; as&#x3D;scriptLink: &lt;/experimental.3eab3290.css&gt;; rel&#x3D;preload; as&#x3D;style&lt;HTML&gt;&lt;head&gt;   &lt;title&gt;Example&lt;/title&gt;   &lt;link rel&#x3D;&quot;stylesheet&quot; href&#x3D;&quot;/main.css&quot;&gt;   &lt;link rel&#x3D;&quot;stylesheet&quot; href&#x3D;&quot;/experimental.3eab3290.css&quot;&gt;   &lt;script src&#x3D;&quot;/common.js&quot;&gt;&lt;/script&gt;   &lt;link rel&#x3D;&quot;preconnect&quot; href&#x3D;&quot;https://fonts.googleapis.com&quot;&gt;
 # Support for Early Hints by HTTP server software
 Here is a quick summary of the level of support for Early Hints among popular OSS HTTP server software:
 
 Apache: supported via mod_http2.
 H2O: supported.
 NGINX: experimental module.
 Node: not yet supported by core. Available as a draft plugin for Fastify.
 
 # Enabling Early Hints, the easy way
 If you are using one of the following CDNs or platforms, you may not need to manually implement Early Hints. Refer to your solution provider&#x27;s online documentation to find out if it supports Early Hints, or refer to the non-exhaustive list here:
 
 Early Hints at Cloudflare
 Early Hints at Fastly.
 
 # Advanced pattern
 If you have fully applied Early Hints to your key landing pages and find yourself looking for more opportunities, you might be interested in the following advanced pattern.
 For visitors who are on their nth page request as part of a typical user journey, you may want to adapt the Early Hints response to content that is lower and deeper in the page, in other words using Early Hints on lower priority resources. This may sound counter-intuitive given that we recommended focussing on high priority, render-blocking sub-resources or origins. However, by the time a visitor has navigated for a while, it&#x27;s very likely that their browser already has all the critical resources. From there on, it might make sense to switch your attention toward lower priority resources. For instance, this could mean using Early Hints to load product images, or additional JS/CSS that are only needed for less common user interactions.
 # Current limitations
 Here are the limitations of Early Hints as implemented in Chrome 103 and future releases until further notice:
 
 Only available for navigation requests (that is,. main resource for the top level document).
 Only supports preconnect and preload (that is, prefetch isn&#x27;t supported).
 Early Hint followed by a cross-origin redirect on the final response will result in Chrome dropping the resources and connections it obtained via Early Hints.
 
 # What&#x27;s next?
 Depending on interest from the community, we may augment our implementation of Early Hints with the following capabilities:
 
 Early Hints sent on sub-resource requests.
 Early Hints sent on iframe main resource requests.
 Support for prefetch in Early Hints.
 
 We welcome your input on which aspects to prioritize, and how to further improve Early Hints.
 # Relationship to H2/Push
 If you are familiar with the deprecated HTTP2/Push feature, you may wonder how Early Hints differs. While Early Hints requires a round trip for the browser to start fetching critical sub-resources, with HTTP2/Push the server could start pushing sub-resources alongside the response. While this sounds amazing, this resulted in a key structural downside. Indeed, with HTTP2/Push it was extremely hard to avoid pushing sub-resources that the browser already had. This &quot;over-pushing&quot; effect resulted in a less efficient usage of the network bandwidth, which significantly hindered the performance benefits. Overall, Chrome data showed that HTTP2/Push was in fact a net negative for performance across the web.
 By contrast, Early Hints performs better in practice because it combines the ability to send a preliminary response with hints that leave the browser in charge of fetching, or connecting to, what it actually needs. While Early Hints doesn&#x27;t cover all the use cases that HTTP2/Push could address in theory, we believe that Early Hints is a more practical solution for speeding up navigations.
 Hero image by Pierre Bamin.</content>
     </entry>
     <entry>
       <title>Deprecations and removals in Chrome 104</title>
         <link href="https://developer.chrome.com/en/blog/deps-rems-104/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text">
 Visit ChromeStatus.com for lists of current deprecations and previous removals.
 
 Chrome 104 beta was released on June 23, 2022 and is expected to become the
 stable version in early August, 2022.
 # Block third-party contexts navigating to filesystem URLs
 iframes can no longer navigate to filesystem URLs. Top frame support for navigating to filesystem URLs was dropped in Chrome 68.
 # Remove non-standard client hint mode
 Four client hints (dpr, width, viewport-width, and device-memory) have a default allowlist of self but behave as though they have a default allowlist of * on Android, contrary to the spec. This is now fixed, increasing privacy on Android by requiring explicit delegation of these hints.
 # Remove U2F API (cryptotoken)
 Chrome&#x27;s legacy U2F API for interacting with security keys is no longer supported. U2F security keys themselves are not deprecated and will continue to work.
 Affected sites should migrate to the Web Authentication API. Credentials that were originally registered via the U2F API can be challenged via web authentication. USB security keys that are supported by the U2F API are also supported by the Web Authentication API.
 U2F is Chrome&#x27;s original security key API. It allows sites to register public key credentials on USB security keys and challenge them for building phishing-resistant two-factor authentication systems. U2F never became an open web standard and was subsumed by the Web Authentication API (launched in Chrome 67). Chrome never directly supported the FIDO U2F JavaScript API, but rather shipped a component extension called cryptotoken, which exposes an equivalent chrome.runtime.sendMessage() method. U2F and Cryptotoken are firmly in maintenance mode and have encouraged sites to migrate to the Web Authentication API for the last two years.
 # Deprecation policy
 To keep the platform healthy, we sometimes remove APIs from the Web Platform
 which have run their course. There can be many reasons why we would remove an
 API, such as:
 
 They are superseded by newer APIs.
 They are updated to reflect changes to specifications to bring alignment and
 consistency with other browsers.
 They are early experiments that never came to fruition in other browsers and
 thus can increase the burden of support for web developers.
 
 Some of these changes will have an effect on a very small number of sites. To
 mitigate issues ahead of time, we try to give developers advanced notice so they
 can make the required changes to keep their sites running.
 Chrome currently has a process for deprecations and removals of
 API&#x27;s,
 essentially:
 
 Announce on the
 blink-dev
 mailing list.
 Set warnings and give time scales in the Chrome DevTools Console when usage
 is detected on the page.
 Wait, monitor, and then remove the feature as usage drops.
 
 You can find a list of all deprecated features on chromestatus.com using the
 deprecated filter and
 removed features by applying the removed
 filter. We will also try to
 summarize some of the changes, reasoning, and migration paths in these posts.</content>
     </entry>
     <entry>
       <title>Attribution Reporting updates in June 2022</title>
         <link href="https://developer.chrome.com/en/blog/attribution-reporting-updates-june-2022/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text">The Attribution Reporting proposal is changing for Chrome version 104,
 with new API mechanisms, functionality, and updates to the aggregation service.
 To get the latest information on versions, sign up for Chrome OS release
 notifications.
 # Who are these updates for?
 These updates are for you if:
 
 You already are familiar with the API—for example, if you&#x27;ve been observing
 or participating in the discussions on the WICG repository and want to
 understand the changes made to the API.
 You&#x27;re using the Attribution Reporting API in a demo or plan to test in the
 origin trial.
 
 If you&#x27;re just getting started with this API and/or have not experimented with
 it yet, go directly to the introduction to the
 API instead.
 
 The Google Chrome team will host developer office
 hours,
 where developers who are testing and integrating the Privacy Sandbox
 technologies can learn more about Attribution Reporting in Chrome&#x27;s Privacy
 Sandbox.
 
 # Attribution Reporting API updates
 The API handbook
 and Attribution Reporting demo
 have been updated to reflect the latest changes to the Attribution Reporting
 client-side API.
 Most changes don&#x27;t require action. Those that do require updates for your
 implementation have been highlighted below.
 # (Action required) unified headers for registration
 The headers have been unified. There is now just one header for sources and one
 for triggers, formatted in JSON.
 
 To register attribution sources, you can respond to registration requests
 with the header Attribution-Reporting-Register-Source.
 To complete trigger registration, set the
 Attribution-Reporting-Register-Trigger header.
 
 This change requires action. Refer to the
 API handbook
 for more information.
 # (Action required) aggregation keys are now a dictionary
 To register attribution sources,
 continue to use aggregation_keys, but now stored as a JSON dictionary instead
 of a list.
 For example:
 &quot;aggregation_keys&quot;: {    // Generate a &quot;0x159&quot; key piece for the key named &quot;campaignCounts&quot;.    &quot;campaignCounts&quot;: &quot;0x159&quot;, // User saw ad from campaign 345 (out of 511)    // Generates a &quot;0x5&quot; key piece (low order bits of the key) for     // the key named &quot;geoValue&quot;.    &quot;geoValue&quot;: &quot;0x5&quot; // Source-side geo region &#x3D; 5 (US), out of a possible ~100 regions }
 This change requires action. Refer to the
 API handbook
 for more information.
 # Report generation
 You can choose to generate only aggregatable reports, which can be aggregated
 into summary reports. If your filters don&#x27;t match any event triggers, then no
 event-level reports will be generated.
 # Unified debug key setting
 The debug key should now be set in the source and trigger headers, instead of
 with separate headers. Learn more about how to debug
 reports.
 # Register attribution sources
 Script tags can now be used to register attribution sources, similar to support
 for the &lt;img&gt; tag.
 # More API updates
 Other changes that have been made and cited in the API handbook include:
 
 Sources can be registered with JavaScript request APIs.
 window.registerSource was removed.
 It is now optional to include a value for attributionsrc when registering
 sources.
 Attribution-Reporting-Eligible header added to incoming source
 registration requests.
 There was a minor change to encodeURIComponent.
 The privacy budget key was removed
 from the shared_info field in aggregatable reports.
 
 # Support for the Aggregation Service
 In Chrome 104, we intend to update the format of some information inside of
 aggregatable reports. We are currently building support for this change in the
 Aggregation Service. This document will be updated, as well the
 changelog,
 after the changes are shipped.
 We&#x27;ve gathered a document of practical tips and strategies to generate summary
 reports.
 There are a number of insights, including:
 
 Overview of noise in summary report generation
 A detailed explanation of dimensions, keys, and values
 Aggregation keys in practice, including a key structure map
 Aggregatable values in practice, and implications of the contribution budget
 Guide to experimenting with epsilon
 
 # Read more about the updates
 
 Read the API handbook.
 Read What you should know about the API.
 Read Experiment with Attribution Reporting: Strategy and tips for summary reports.
 
 The header image is from Diana Polekhina on Unsplash.</content>
     </entry>
     <entry>
       <title>New in Chrome 103</title>
         <link href="https://developer.chrome.com/en/blog/new-in-chrome-103/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text">
   
 
 Here&#x27;s what you need to know:
 
 There&#x27;s a new HTTP 103 status code that helps the browser
 decide what content to preload before the page has even started to arrive.
 The Local Font Access API gives web applications the ability
 to enumerate and use fonts installed on the user&#x27;s computer.
 AbortSignal.timeout() is an easier way to implement
 timeouts on asynchronous APIs.
 And there&#x27;s plenty more.
 
 I&#x27;m Pete LePage. Let&#x27;s dive in and
 see what&#x27;s new for developers in Chrome 103.
 # HTTP 103 status code 103 - early hints
 One way you can improve page performance is to use resource hints. They give
 the browser &quot;hints&quot; about what stuff it might need later. For example,
 preloading files, or connecting to a different server.
 &lt;link as&#x3D;&quot;font&quot; crossorigin&#x3D;&quot;anonymous&quot;      href&#x3D;&quot;...&quot; rel&#x3D;&quot;preload&quot;&gt;&lt;link as&#x3D;&quot;font&quot; crossorigin&#x3D;&quot;anonymous&quot;      href&#x3D;&quot;...&quot; rel&#x3D;&quot;preload&quot;&gt;&lt;link href&#x3D;&quot;https://web-dev.imgix.net&quot;      rel&#x3D;&quot;preconnect&quot;&gt;
 But the browser can&#x27;t act on those hints until the server sends at least
 part of the page.
 Imagine the browser requests a page, but the server requires a few hundred
 milliseconds to generate it. Until the browser starts to receive the page,
 it just sits there and waits. But, if the server knows the page will always
 need a certain set of subresources, for example, a CSS file, some JavaScript,
 and a few images, it can immediately respond with the new HTTP 103
 Early Hints status code, and ask the browser to preload those subresources.
 Then, once the server has generated the page, it can send it with the normal
 HTTP 200 response. As the page comes in, the browser has already started
 loading the required resources.
 Since this is a new HTTP status code, using it requires updates to your server.
 Get started with HTTP 103 Early hints:
 
 Explainer for Early Hints
 Apache 2 Early Hints configuration
 Using Early Hints on Cloudflare
 Fastly Beyond Server Push: The 103 Early Hints Status Code
 
 # Local Font Access API
 Fonts on the web have always been a challenge, and especially so for apps
 that let users create their own graphics and designs. Until now, web apps
 could only really use web fonts. There was no way to get a list of fonts the user had installed on their
 computer. And, there was no way to access the full font table data, critical
 if you need to implement your own custom text stack.
 The new Local Font Access API gives web applications the ability to enumerate
 the local fonts on the user&#x27;s device, and provides access to the font table data.
 To get a list of fonts installed on the device, you&#x27;ll first need to request
 permission.
 // Ask for permission to use the APItry {  const status &#x3D; await navigator.permissions.request({    name: &#x27;local-fonts&#x27;,  });  if (status.state !&#x3D;&#x3D; &#x27;granted&#x27;) {    throw new Error(&#x27;No Permission.&#x27;);  }} catch (err) {  if (err.name !&#x3D;&#x3D; &#x27;TypeError&#x27;) {    throw err;  }}
 Then, call window.queryLocalFonts(). It returns an array of all the fonts
 installed on the users device.
 const opts &#x3D; {};const pickedFonts &#x3D; await self.queryLocalFonts();for (const fontData of pickedFonts) {  console.log(fontData.postscriptName);  console.log(fontData.fullName);  console.log(fontData.family);  console.log(fontData.style);}
 If you&#x27;re only interested in a subset of fonts, you can filter them by adding
 a postscriptNames parameter.
 const opts &#x3D; {  postscriptNames: [    &#x27;Verdana&#x27;,    &#x27;Verdana-Bold&#x27;,    &#x27;Verdana-Italic&#x27;,  ],};const pickedFonts &#x3D; await self.queryLocalFonts(opts);
 Check out Tom&#x27;s article Use advanced typography with local fonts
 on web.dev for complete details.
 # Easier Timeouts with AbortSignal.timeout()
 In JavaScript, AbortController and AbortSignal are used to cancel an
 asynchronous call.
 For example, when making a fetch() request, you can create an
 AbortSignal, and pass it to fetch(). If you want to cancel the fetch()
 before it returns, call abort() on the instance of the AbortSignal. Up
 until now, if you wanted it to abort after a specific amount of time, you&#x27;d
 need to wrap it in a setTimeout().
 const controller &#x3D; new AbortController();const signal &#x3D; controller.signal;const resp &#x3D; fetch(url, { signal });setTimeout(() &#x3D;&gt; {  // abort the fetch after 6 seconds  controller.abort();}, 6000);
 Thankfully, that just got easier with a new timeout() static method on
 AbortSignal. It returns an AbortSignal object that is automatically
 aborted after the given number of milliseconds. What used to be a handful of
 lines of code, is now just one.
 const signal &#x3D; AbortSignal.timeout(6000);const resp &#x3D; fetch(url, { signal });
 AbortSignal.timeout()
 is supported in Chrome 103, and is already in Firefox, and Safari.
 # And more!
 Of course there&#x27;s plenty more.
 
 The avif image file format is now sharable by Web Share
 Chromium now matches Firefox by firing popstate immediately after URL
 changes. The order of events is now: popstate then hashchange on both
 platforms.
 And Element.isVisible() tells you whether an element is visible or not.
 
 
 In the video, I mentioned Element.isVisible() is available in stable, but
 it is currently only available behind a feature flag.
 
 # Further reading
 This covers only some of the key highlights. Check the links below for
 additional changes in Chrome 103.
 
 What&#x27;s new in Chrome DevTools (103)
 Chrome 103 deprecations and removals
 ChromeStatus.com updates for Chrome 103
 Chromium source repository change list
 Chrome release calendar
 
 # Subscribe
 To stay up to date, subscribe to the
 Chrome Developers YouTube channel,
 and you&#x27;ll get an email notification whenever we launch a new video.
 I&#x27;m Pete LePage, and as soon as Chrome 104 is released, I&#x27;ll be right here to
 tell you what&#x27;s new in Chrome!</content>
     </entry>
     <entry>
       <title>New syntax for range media queries in Chrome 104</title>
         <link href="https://developer.chrome.com/en/blog/media-query-range-syntax/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text">Media Queries enabled responsive design, and the range features that enable testing the minimum and maximum size of the viewport are used by around 80% of sites that use media queries. The Media Queries Level 4 specification includes a new syntax for these range queries.
 The new syntax has been available in Firefox since Firefox 63, and will be available in Chrome from 104. Let’s take a look at how it can streamline your queries.
 A typical media query testing for a minimum viewport width, would be written as follows:
 @media (min-width: 400px) {  // Styles for viewports with a width of 400 pixels or greater.}
 The new syntax allows for the use of a comparison operator:
 @media (width &gt;&#x3D; 400px) {  // Styles for viewports with a width of 400 pixels or greater.}
 Testing for a maximum width:
 @media (max-width: 30em) {  // Styles for viewports with a width of 30em or less.}
 And, the version using the level 4 syntax:
 @media (width &lt;&#x3D; 30em) {  // Styles for viewports with a width of 30em or less.}
 This syntax has the potential to streamline queries, in particular when testing between two widths. In the following example, the media query tests for a viewport with a minimum width of 400px, and a maximum width of 600px.
 @media (min-width: 400px) and (max-width: 600px) {  // Styles for viewports between 400px and 600px.}
 This can be rewritten in the new syntax as:
 @media (400px &lt;&#x3D; width &lt;&#x3D; 600px )  {  // Styles for viewports between 400px and 600px.}
 The feature that you are testing, in this case width, goes between the two values.
 In addition to making media queries less verbose, the new syntax has the advantage of accuracy. The min- and max- queries are inclusive of the specified values, for example min-width: 400px tests for a width of 400px or greater. The new syntax allows you to be more explicit about what you mean and avoid the potential of clashing queries.
 To use the new range syntax while accounting for browsers that have not yet implemented it, there is a PostCSS plugin that will rewrite the new syntax to the old in your stylesheets.
 Hero image by William Warby.</content>
     </entry>
     <entry>
       <title>What&#x27;s New In DevTools (Chrome 103)</title>
         <link href="https://developer.chrome.com/en/blog/new-in-devtools-103/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text">
 Interested in helping improve DevTools? Sign up to participate in Google User Research here.
 
 
   
 
 # Capture double-click and right-click events in the Recorder panel
 The Recorder panel can now capture double-click and right-click events.
 
 In this example, start a recording and try to perform the following steps:
 
 Double-click the card to enlarge it
 Right-click the card and select an action from the context menu
 
 To understand how Recorder captured these events, expand the steps:
 
 Double-click is captured as type: doubleClick.
 Right-click event is captured as type: click but with the button property is set to secondary. The button value of a normal mouse click is primary.
 
 Chromium issues: 1300839, 1322879, 1299701, 1323688
 # New timespan and snapshot mode in the Lighthouse panel
 You can now use Lighthouse to measure your website’s performance beyond page load.
 
 The Lighthouse panel now supports 3 modes of user flow measurement:
 
 Navigation reports analyze a single page load. Navigation is the most common report type. All Lighthouse reports before the current version are navigation reports.
 Timespans reports analyze an arbitrary time period, typically containing user interactions.
 Snapshots reports analyze the page in a particular state, typically after the user has interacted with it.
 
 For example, let’s measure the performance of adding items to cart on this demo page. Select the Timespan mode and click Start timespan. Scroll and add a few items to the cart. Once you are done, click on End timespan to generate a Lighthouse report of the user interactions.
 
 See User flows in Lighthouse to learn about the unique use cases, benefits, and limitations of each mode.
 Chromium issue: 1291284
 # Performance Insights updates
 # Improved zoom control in the Performance Insights panel
 DevTools will now zoom in based on your mouse cursor rather than the playhead position.With the latest cursor-based zoom, you can move your mouse to anywhere in the track, and zoom in to the desired area right away.
           
 See Performance Insights to learn how to get actionable insights and improve your website’s performance with the panel.
 Chromium issue: 1313382
 # Confirm to delete a performance recording
 DevTools now shows a confirmation dialog before deleting a performance recording.
 
 Chromium issue: 1318087
 # Reorder panes in the Elements panel
 You can now reorder panes in the Elements panel based on your preference.
 For example, when you open DevTools on a narrow screen, the Accessibility pane is hidden under the Show more button. If you frequently debug accessibility issues, you can now drag the pane to the front for easier access.
 
 Chromium issue: 1146146
 # Picking a color outside of the browser
 DevTools now supports picking a color outside of the browser. Previously, you could only pick a color within the browser.
 In the Styles pane, click on any color preview to open a color picker. Use the eyedropper to pick color from anywhere.
 
 Chromium issue: 1245191
 # Improved inline value preview during debugging
 The debugger now shows the inline values preview correctly.
 In this example, the double function has an input parameter  a and a variable x. Put a breakpoint at the return line and run the code. The inline preview shows values a and x correctly. Previously, the debugger did not show the value x in the inline preview.
 
 Chromium issue: 1316340
 # Support large blobs for virtual authenticators
 The WebAuthn tab now has the new Supports large blob checkbox for virtual authenticators.
 This checkbox is disabled by default. You can enable it only for the authenticators with ctap2 protocol that support resident keys.
 
 Chromium issue: 1321803
 # New keyboard shortcuts in the Sources panel
 Two new keyboard shortcuts are now available in the  Sources panel:
 
 Toggle navigation sidebar (left) with Control / Command + Shift + Y
 Toggle debugger sidebar (right) with Control / Command + Shift + H
 
 
 Chromium issues: 1226363
 # Sourcemaps improvements
 Previously, developers experience random failure during:
 
 Debugging with Codepen example
 Identifying source location of performance issues in a Codepen example
 Missing Component tab when React DevTools is enabled
 
 Here are a few fixes on sourcemaps to improve the overall debugging experience:
 
 Correct mapping between location and offset for inline scripts and source location
 Use fallback information for frame’s text location
 Properly resolve relative urls with frame&#x27;s URL
 
 Chromium issues: 1319828, 1318635, 1305475
 # Download the preview channels
 Consider using the Chrome Canary, Dev or Beta as your default development browser. These preview channels give you access to the latest DevTools features, test cutting-edge web platform APIs, and find issues on your site before your users do!
 # Getting in touch with the Chrome DevTools team
 Use the following options to discuss the new features and changes in the post, or anything else related to DevTools.
 
 Submit a suggestion or feedback to us via crbug.com.
 Report a DevTools issue using the More options      &gt; Help &gt; Report a DevTools issues in DevTools.
 Tweet at @ChromeDevTools.
 Leave comments on our What&#x27;s new in DevTools YouTube videos.
 
 # What&#x27;s New in DevTools
 A list of everything that has been covered in the What&#x27;s New In DevTools series.
 # Chrome 103
 
 Capture double-click and right-click events in the Recorder panel
 New timespan and snapshot mode in the Lighthouse panel
 Improved zoom control in the Performance Insights panel
 Confirm to delete a performance recording
 Picking a color outside of the browser
 Improved inline value preview during debugging
 Support large blobs for virtual authenticators
 New keyboard shortcuts in the Sources panel
 Sourcemaps improvements
 
 # Chrome 102
 
 Preview feature: New Performance insights panel
 New shortcuts to emulate light and dark themes
 Improved security on the Network Preview tab
 Improved reloading at breakpoint
 Console updates
 Cancel user flow recording at the start
 Display inherited highlight pseudo-elements in the Styles pane
 Miscellaneous highlights
 [Experimental] Copy CSS changes
 [Experimental] Picking color outside of browser
 
 # Chrome 101
 
 Import and export recorded user flows as a JSON file
 View cascade layers in the Styles pane
 Support for the hwb() color function
 Improved the display of private properties
 Miscellaneous highlights
 [Experimental] New timespan and snapshot mode in the Lighthouse panel
 
 # Chrome 100
 
 View and edit @supports at rules in the Styles pane
 Support common selectors by default
 Customize the recording’s selector
 Rename a recording
 Preview class/function properties on hover
 Partially presented frames in the Performance panel
 Miscellaneous highlights
 
 # Chrome 99
 
 Throttling WebSocket requests
 New Reporting API pane in the Application panel
 Support wait until element is visible/clickable in the Recorder panel
 Better console styling, formatting and filtering
 Debug Chrome extension with sourcemap files
 Improved source folder tree in the Sources panel
 Display worker source files in the Sources panel
 Chrome’s Auto Dark Theme updates
 Touch-friendly color-picker and split pane
 Miscellaneous highlights
 
 # Chrome 98
 
 Preview feature: Full-page accessibility tree
 More precise changes in the Changes tab
 Set longer timeout for user flow recording
 Ensure your pages are cacheable with the Back/forward cache tab
 New Properties pane filter
 Emulate the CSS forced-colors media feature
 Show rulers on hover command
 Support row-reverse and column-reverse in the Flexbox editor
 New keyboard shortcuts to replay XHR and expand all search results
 Lighthouse 9 in the Lighthouse panel
 Improved Sources panel
 Miscellaneous highlights
 [Experimental] Endpoints in the Reporting API pane
 
 # Chrome 97
 
 Preview feature: New Recorder panel
 Refresh device list in Device Mode
 Autocomplete with Edit as HTML
 Improved code debugging experience
 Syncing DevTools settings across devices
 
 # Chrome 96
 
 Preview feature: New CSS Overview panel
 Restored and improved CSS length edit and copy experince
 Emulate the CSS prefers-contrast media feature
 Emulate the Chrome’s Auto Dark Theme feature
 Copy declarations as JavaScript in the Styles pane
 New Payload tab in the Network panel
 Improved the display of properties in the Properties pane
 Option to hide CORS errors in the Console
 Proper Intl objects preview and evaluation in the Console
 Consistent async stack traces
 Retain the Console sidebar
 Deprecated Application cache pane in the Application panel
 [Experimental] New Reporting API pane in the Application panel
 
 # Chrome 95
 
 New CSS length authoring tools
 Hide issues in the Issues tab
 Improved the display of properties
 Lighthouse 8.4 in the Lighthouse panel
 Sort snippets in the Sources panel
 New links to translated release notes and report a translation bug
 Improved UI for DevTools command menu
 
 # Chrome 94
 
 Use DevTools in your preferred language
 New Nest Hub devices in the Device list
 Origin trials in the Frame details view
 New CSS container queries badge
 New checkbox to invert the network filters
 Upcoming deprecation of the Console sidebar
 Display raw Set-Cookies headers in the Issues tab and Network panel
 Consistent display native accessors as own properties in the Console
 Proper error stack traces for inline scripts with #sourceURL
 Change color format in the Computed pane
 Replace custom tooltips with native HTML tooltips
 [Experimental] Hide issues in the Issues tab
 
 # Chrome 93
 
 Editable CSS container queries  in the Styles pane
 Web bundle preview in the Network panel
 Attribution Reporting API debugging
 Better string handling in the Console
 Improved CORS debugging
 Lighthouse 8.1
 New note URL in the Manifest pane
 Fixed CSS matching selectors
 Pretty-printing JSON responses in the Network panel
 
 # Chrome 92
 
 CSS grid editor
 Support for const redeclarations in the Console
 Source order viewer
 New shortcut to view frame details
 Enhanced CORS debugging support
 Rename XHR label to Fetch/XHR
 Filter Wasm resource type in the Network panel
 User-Agent Client Hints for devices in the Network conditions tab
 Report Quirks mode issues in the Issues tab
 Include Compute Intersections in the Performance panel
 Lighthouse 7.5 in the Lighthouse panel
 Deprecated &quot;Restart frame&quot; context menu in the call stack
 [Experimental] Protocol monitor
 [Experimental] Puppeteer Recorder
 
 # Chrome 91
 
 Web Vitals information pop up
 New Memory inspector
 Visualize CSS scroll-snap
 New badge settings pane
 Enhanced image preview with aspect ratio information
 New network conditions button with options to configure Content-Encodings
 shortcut to view computed value
 accent-color keyword
 Categorize issue types with colors and icons
 Delete Trust tokens
 Blocked features in the Frame details view
 Filter experiments in the Experiments setting
 New Vary Header column in the Cache storage pane
 Support JavaScript private brand check
 Enhanced support for breakpoints debugging
 Support hover preview with [] notation
 Improved outline of HTML files
 Proper error stack traces for Wasm debugging
 
 # Chrome 90
 
 New CSS flexbox debugging tools
 New Core Web Vitals overlay
 Moved issue count to the Console status bar
 Report Trusted Web Activity issues
 Format strings as (valid) JavaScript string literals in the Console
 New Trust Tokens pane in the Application panel
 Emulate the CSS color-gamut media feature
 Improved Progressive Web Apps tooling
 New Remote Address Space column in the Network panel
 Performance improvements
 Display allowed/disallowed features in the Frame details view
 New SameParty column in the Cookies pane
 Deprecated non-standard fn.displayName support
 Deprecation of Don&#x27;t show Chrome Data Saver warning in the Settings menu
 [Experimental] Automatic low-contrast issue reporting in the Issues tab
 [Experimental] Full accessibility tree view in the Elements panel
 
 # Chrome 89
 
 Debugging support for Trusted Types violations
 Capture node screenshot beyond viewport
 New Trust Tokens tab for network requests
 Lighthouse 7 in the Lighthouse panel
 Support forcing the CSS :target state
 New shortcut to duplicate element
 Color pickers for custom CSS properties
 New shortcuts to copy CSS properties
 New option to show URL-decoded cookies
 Clear only visible cookies
 New option to clear third-party cookies in the Storage pane
 Edit User-Agent Client Hints for custom devices
 Persist &quot;record network log&quot; setting
 View WebTransport connections in the Network panel
 &quot;Online&quot; renamed to &quot;No throttling&quot;
 New copy options in the Console, Sources panel, and Styles pane
 New Service Workers information in the Frame details view
 Measure Memory information in the Frame details view
 Provide feedback from the Issues tab
 Dropped frames in the Performance panel
 Emulate foldable and dual-screen in Device Mode
 [Experimental] Automate browser testing with Puppeteer Recorder
 [Experimental] Font editor in the Styles pane
 [Experimental] CSS flexbox debugging tools
 [Experimental] New CSP Violations tab
 [Experimental] New color contrast calculation - Advanced Perceptual Contrast Algorithm (APCA)
 
 # Chrome 88
 
 Faster DevTools startup
 New CSS angle visualization tools
 Emulate unsupported image types
 Simulate storage quota size in the Storage pane
 New Web Vitals lane in the Performance panel
 Report CORS errors in the Network panel
 Cross-origin isolation information in the Frame details view
 New Web Workers information in the Frame details view
 Display opener frame details for opened windows
 Open Network panel from the Service Workers pane
 Copy property value
 Copy stacktrace for network initiator
 Preview Wasm variable value on mouseover
 Evaluate Wasm variable in the Console
 Consistent units of measurement for file/memory sizes
 Highlight pseudo elements in the Elements panel
 [Experimental] CSS Flexbox debugging tools
 [Experimental] Customize chords keyboard shortcuts
 
 # Chrome 87
 
 New CSS Grid debugging tools
 New WebAuthn tab
 Move tools between top and bottom panel
 New Computed sidebar pane in the Styles pane
 Grouping CSS properties in the Computed pane
 Lighthouse 6.3 in the Lighthouse panel
 performance.mark() events in the Timings section
 New resource-type and url filters in the Network panel
 Frame details view updates
 Deprecation of Settings in the More tools menu
 [Experimental] View and fix color contrast issues in the CSS Overview panel
 [Experimental] Customize keyboard shortcuts in DevTools
 
 # Chrome 86
 
 New Media panel
 Capture node screenshots via Elements panel context menu
 Issues tab updates
 Emulate missing local fonts
 Emulate inactive users
 Emulate prefers-reduced-data
 Support for new JavaScript features
 Lighthouse 6.2 in the Lighthouse panel
 Deprecation of &quot;other origins&quot; listing in the Service Workers pane
 Show coverage summary for filtered items
 New frame details view in Application panel
 Accessible color suggestion in the Styles pane
 Reinstate Properties pane in the Elements panel
 Human-readable X-Client-Data header values in the Network panel
 Auto-complete custom fonts in the Styles pane
 Consistently display resource type in Network panel
 Clear buttons in the Elements and Network panels
 
 # Chrome 85
 
 Style editing for CSS-in-JS frameworks
 Lighthouse 6 in the Lighthouse panel
 First Meaningful Paint (FMP) deprecation
 Support for new JavaScript features
 New app shortcut warnings in the Manifest pane
 Service worker respondWith events in the Timing tab
 Consistent display of the Computed pane
 Bytecode offsets for WebAssembly files
 Line-wise copy and cut in Sources Panel
 Console settings updates
 Performance panel updates
 New icons for breakpoints, conditional breakpoints, and logpoints
 
 # Chrome 84
 
 Fix site issues with the new Issues tab
 View accessibility information in the Inspect Mode tooltip
 Performance panel updates
 More accurate promise terminology in the Console
 Styles pane updates
 Deprecation of the Properties pane in the Elements panel
 App shortcuts support in the Manifest pane
 
 # Chrome 83
 
 Emulate vision deficiencies
 Emulate locales
 Cross-Origin Embedder Policy (COEP) debugging
 New icons for breakpoints, conditional breakpoints, and logpoints
 View network requests that set a specific cookie
 Dock to left from the Command Menu
 The Settings option in the Main Menu has moved
 The Audits panel is now the Lighthouse panel
 Delete all Local Overrides in a folder
 Updated Long Tasks UI
 Maskable icon support in the Manifest pane
 
 # Chrome 82
 Chrome 82 was cancelled.
 # Chrome 81
 
 Moto G4 support in Device Mode
 Cookie-related updates
 More accurate web app manifest icons
 Hover over CSS content properties to see unescaped values
 Source map errors in the Console
 Setting for disabling scrolling past the end of a file
 
 # Chrome 80
 
 Support for let and class redeclarations in the Console
 Improved WebAssembly debugging
 Request Initiator Chains in the Initiator tab
 Highlight the selected network request in the Overview
 URL and path columns in the Network panel
 Updated User-Agent strings
 New Audits panel configuration UI
 Per-function or per-block code coverage modes
 Code coverage must now be initiated by a page reload
 
 # Chrome 79
 
 Debug why a cookie was blocked
 View cookie values
 Simulate different prefers-color-scheme and prefers-reduced-motion preferences
 Code coverage updates
 Debug why a network resource was requested
 Console and Sources panels respect indentation preferences again
 New shortcuts for cursor navigation
 
 # Chrome 78
 
 Multi-client support in the Audits panel
 Payment Handler debugging
 Lighthouse 5.2 in the Audits panel
 Largest Contentful Paint in the Performance panel
 File DevTools issues from the Main Menu
 
 # Chrome 77
 
 Copy element styles
 Visualize layout shifts
 Lighthouse 5.1 in the Audits panel
 OS theme syncing
 Keyboard shortcut for opening the Breakpoint Editor
 Prefetch cache in the Network panel
 Private properties when viewing objects
 Notifications and push messages in the Application panel
 
 # Chrome 76
 
 Autocomplete with CSS values
 A new UI for network settings
 WebSocket messages in HAR exports
 HAR import and export buttons
 Real-time memory usage
 Service worker registration port numbers
 Inspect Background Fetch and Background Sync events
 Puppeteer for Firefox
 
 # Chrome 75
 
 Meaningful presets when autocompleting CSS functions
 Clear site data from the Command Menu
 View all IndexedDB databases
 View a resource&#x27;s uncompressed size on hover
 Inline breakpoints in the Breakpoints pane
 IndexedDB and Cache resource counts
 Setting for disabling the detailed Inspect tooltip
 Setting for toggling tab indentation in the Editor
 
 # Chrome 74
 
 Highlight all nodes affected by CSS property
 Lighthouse v4 in the Audits panel
 WebSocket binary message viewer
 Capture area screenshot in the Command Menu
 Service worker filters in the Network panel
 Performance panel updates
 Long tasks in Performance panel recordings
 First Paint in the Timing section
 Bonus tip: Shortcut for viewing RGB and HSL color codes (video)
 
 # Chrome 73
 
 Logpoints
 Detailed tooltips in Inspect Mode
 Export code coverage data
 Navigate the Console with a keyboard
 AAA contrast ratio line in the Color Picker
 Save custom geolocation overrides
 Code folding
 Frames tab renamed to Messages tab
 Bonus tip: Network panel filtering by property (video)
 
 # Chrome 72
 
 Visualize performance metrics in the Performance panel
 Highlight text nodes in the DOM Tree
 Copy the JS path to a DOM node
 Audits panel updates, including a new audit
 that detects JS libraries and new keywords for accessing the Audits panel from the Command Menu
 Bonus tip: Use Device Mode to inspect media queries (video)
 
 # Chrome 71
 
 Hover over a Live Expression result to highlight a DOM node
 Store DOM nodes as global variables
 Initiator and priority information now in HAR imports and exports
 Access the Command Menu from the Main Menu
 Picture-in-Picture breakpoints
 Bonus tip: Use monitorEvents() to log a node&#x27;s fired events in the
 Console (video)
 
 # Chrome 70
 
 Live Expressions in the Console
 Highlight DOM nodes during Eager Evaluation
 Performance panel optimizations
 More reliable debugging
 Enable network throttling from the Command Menu
 Autocomplete Conditional Breakpoints
 Break on AudioContext events
 Debug Node.js apps with ndb
 Bonus tip: Measure real world user interactions with the User Timing API
 
 # Chrome 68
 
 Eager Evaluation
 Argument hints
 Function autocompletion
 ES2017 keywords
 Lighthouse 3.0 in the Audits panel
 BigInt support
 Adding property paths to the Watch pane
 &quot;Show timestamps&quot; moved to Settings
 Bonus tip: Lesser-known Console methods (video)
 
 # Chrome 67
 
 Search across all network headers
 CSS variable value previews
 Copy as fetch
 New audits, desktop configuration options, and viewing traces
 Stop infinite loops
 User Timing in the Performance tabs
 JavaScript VM instances clearly listed in the Memory panel
 Network tab renamed to Page tab
 Dark theme updates
 Certificate transparency information in the Security panel
 Site isolation features in the Performance panel
 Bonus tip: Layers panel + Animations Inspector (video)
 
 # Chrome 66
 
 Blackboxing in the Network panel
 Auto-adjust zooming in Device Mode
 Pretty-printing in the Preview and Response tabs
 Previewing HTML content in the Preview tab
 Local Overrides support for styles inside of HTML
 Bonus tip: Blackbox framework scripts to make Event Listener Breakpoints more useful
 
 # Chrome 65
 
 Local Overrides
 New accessibility tools
 The Changes tab
 New SEO and performance audits
 Multiple recordings in the Performance panel
 Reliable code stepping with workers in async code
 Bonus tip: Automate DevTools actions with Puppeteer (video)
 
 # Chrome 64
 
 Performance Monitor
 Console Sidebar
 Group similar Console messages
 Bonus tip: Toggle hover pseudo-class (video)
 
 # Chrome 63
 
 Multi-client remote debugging support
 Workspaces 2.0
 4 new audits
 Simulate push notifications with custom data
 Trigger background sync events with custom tags
 Bonus tip: Event listener breakpoints (video)
 
 # Chrome 62
 
 Top-level await in the Console
 New screenshot workflows
 CSS Grid highlighting
 A new Console API for querying objects
 New Console filters
 HAR imports in the Network panel
 Previewable cache resources
 More predictable cache debugging
 Block-level code coverage
 
 # Chrome 61
 
 Mobile device throttling simulation
 View storage usage
 View when a service worker cached responses
 Enable the FPS meter from the Command Menu
 Set mousewheel behavior to zoom or scroll
 Debugging support for ES6 modules
 
 # Chrome 60
 
 New Audits panel
 3rd-Party Badges
 A new gesture for Continue To Here
 Step into async
 More informative object previews in the Console
 More informative context selection in the Console
 Real-time updates in the Coverage tab
 Simpler network throttling options
 Async stacks on by default
 
 # Chrome 59
 
 CSS and JS code coverage
 Full-page screenshots
 Block requests
 Step over async await
 Unified Command Menu
 </content>
     </entry>
     <entry>
       <title>New in Chrome 102</title>
         <link href="https://developer.chrome.com/en/blog/new-in-chrome-102/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text">
   
 
 Here&#x27;s what you need to know:
 
 Installed PWAs can register as file handlers, making it
 easy for users to open files directly from disk.
 The inert attribute allows you to mark parts of the DOM as inert.
 The Navigation API makes it easier for single page apps
 to handle navigation and updates to the URL
 And there&#x27;s plenty more.
 
 I&#x27;m Pete LePage. Let&#x27;s dive in and
 see what&#x27;s new for developers in Chrome 102.
 # File Handling API
 The File Handling API allows installed PWAs to register with the OS as a file
 handler. Once registered, a user can click on a file to open it with the
 installed PWA. This is perfect for PWAs that interact with files, for example,
 image editors, IDEs, text editors, and so on.
 To add file handling functionality to your PWA, you&#x27;ll need to update your
 web app manifest, adding a file_handlers array with details about the types
 of files your PWA can handle. You&#x27;ll need to specify the URL to open, the
 mime types, an icon for the file type, and the launch type. The launch type
 defines whether multiple files should be opened in a single client, or in
 multiple clients.
 &quot;file_handlers&quot;: [  {    &quot;action&quot;: &quot;/open-csv&quot;,    &quot;accept&quot;: {&quot;text/csv&quot;: [&quot;.csv&quot;]},    &quot;icons&quot;: [      {        &quot;src&quot;: &quot;csv-icon.png&quot;,        &quot;sizes&quot;: &quot;256x256&quot;,        &quot;type&quot;: &quot;image/png&quot;      }    ],    &quot;launch_type&quot;: &quot;single-client&quot;  }]
 Then, to access those files when the PWA is launched, you need to specify a
 consumer for the launchQueue object. Launches are queued until they are
 handled by the consumer.
 // Access from Window.launchQueue.launchQueue.setConsumer((launchParams) &#x3D;&gt; {  if (!launchParams.files.length) {    // Nothing to do when the queue is empty.    return;  }  for (const fileHandle of launchParams.files) {    // Handle the file.    openFile(fileHandle);  }});
 Check out Let installed web applications be file handlers
 for all the details.
 # The inert property
 The inert property is a global HTML attribute that tells the browser to
 ignore user input events for an element, including focus events, and events
 from assistive technologies.
 This can be useful when building UIs. For example, with a modal dialog, you
 want to &quot;trap&quot; the focus inside the modal when it&#x27;s visible. Or, for a drawer
 that is not always visible to the user, adding inert ensures that while
 the drawer is offscreen, a keyboard user cannot accidentally interact with it.
 &lt;div&gt;  &lt;label for&#x3D;&quot;button1&quot;&gt;Button 1&lt;/label&gt;  &lt;button id&#x3D;&quot;button1&quot;&gt;I am not inert&lt;/button&gt;&lt;/div&gt;&lt;div inert&gt;  &lt;label for&#x3D;&quot;button2&quot;&gt;Button 2&lt;/label&gt;  &lt;button id&#x3D;&quot;button2&quot;&gt;I am inert&lt;/button&gt;&lt;/div&gt;
 Here, inert has been declared on the second &lt;div&gt; element, so all content
 contained within, including the &lt;button&gt; and &lt;label&gt;, cannot receive
 focus or be clicked.
 inert is supported in Chrome 102, and is coming to both Firefox and Safari.
 Check out Introducing inert for more details.
 # Navigation API
 Many web apps depend on the ability to update the URL without a page
 navigation. Today, we use the History API, but it&#x27;s clunky and
 doesn&#x27;t always work as expected. Rather than trying to patch the History API&#x27;s
 rough edges, the Navigation API completely overhauls this space.
 To use the Navigation API, add a navigate listener on the global navigation
 object.
 navigation.addEventListener(&#x27;navigate&#x27;, (navigateEvent) &#x3D;&gt; {  switch (navigateEvent.destination.url) {    case &#x27;https://example.com/&#x27;:      navigateEvent.transitionWhile(loadIndexPage());      break;    case &#x27;https://example.com/cats&#x27;:      navigateEvent.transitionWhile(loadCatsPage());      break;  }});
 The event is fundamentally centralized and it will fire for all types of
 navigations, whether the user performed an action, such as clicking a link,
 submitting a form, or going back and forward, even when navigation is
 triggered programmatically. In most cases, it lets your code override the
 browser&#x27;s default behavior for that action.
 
   
 
 Check out the Modern client-side routing: the Navigation API
 for complete details and a demo you can try out.
 # And more!
 Of course there&#x27;s plenty more.
 
 The new Sanitizer API aims to build a robust processor for arbitrary strings
 to be safely inserted into a page.
 The hidden&#x3D;until-found attribute makes it possible for the browser to
 search text in hidden regions, and reveal that section if a match is found.
 
 # Further reading
 This covers only some of the key highlights. Check the links below for
 additional changes in Chrome 102.
 
 What&#x27;s new in Chrome DevTools (102)
 Chrome 102 deprecations and removals
 ChromeStatus.com updates for Chrome 102
 Chromium source repository change list
 Chrome release calendar
 
 # Subscribe
 To stay up to date, subscribe to the
 Chrome Developers YouTube channel,
 and you&#x27;ll get an email notification whenever we launch a new video.
 I&#x27;m Pete LePage, and as soon as Chrome 103 is released, I&#x27;ll be right here to
 tell you what&#x27;s new in Chrome!</content>
     </entry>
     <entry>
       <title>Generate summary reports with the aggregation service</title>
         <link href="https://developer.chrome.com/en/blog/generate-summary-reports/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text">You can now experiment and participate in an origin
 trial for Attribution
 Reporting. There are a number of available developer resources and code samples
 to get started.
 To generate summary reports in the origin trial, you&#x27;ll first need to set up
 the aggregation service. This blog post gives an overview of those steps.
 
 If you ran an origin trial with this API in 2021, follow the migration
 guide
 to prepare for the latest origin trial.
 
 # Key terms
 If you haven&#x27;t already, we recommend you read the overview of Attribution
 Reporting and more
 about summary reports
 before learning about the aggregation service set up.
 
 A summary report is a compiled output for the Attribution Reporting API,
 which aggregates information about a group of users. Summary reports offer
 detailed conversion data, such as purchase value and cart contents, with flexibility for click and view data.
 An aggregatable report is an encrypted report, generated by an individual
 user&#x27;s browser. These contain data about individual conversions and
 associated metrics, as defined by the advertiser or adtech. Learn more about
 aggregatable reports.
 The aggregation service processes data from aggregatable reports to
 create a summary report.
 
 # Set up the aggregation service
 The aggregation service
 proposal
 asks each adtech provider to operate their own instance of the aggregation
 service in a Trusted Execution Environment (TEE) deployed on a cloud service
 that supports needed security features.
 For the initial implementation and origin trial, adtechs can set up
 local testing
 or test in TEEs with Amazon Web Services (AWS):
 
 Register for the Privacy Sandbox Relevance and Measurement origin trial (OT).
 Create or have an Amazon Web Services account.
 Complete the aggregation service
 onboarding form. After filling out
 this form we will send a verification email and instructions for how to set
 up the aggregation service.
 
 Both the local testing tool and the aggregation service running on AWS expect
 aggregatable reports to be batched in the
 Apache Avro format.
 Review code snippets
 which demonstrate how to collect and batch aggregatable reports.
 # Generate summary reports with TEEs on AWS
 
 This is a general overview of how to generate summary reports in TEEs. Read the
 detailed developer documentation on testing the aggregation service with
 AWS.
 
 To generate summary reports, you&#x27;ll need to set up the aggregation service in
 AWS with Terraform. Terraform will help provision,
 manage, and decommission the aggregation service on AWS
 Nitro Enclave instances.
 
 Install
 and set up
 the latest AWS client.
 Set up Terraform.
 Download the aggregation service
 dependencies.
 Set up a deployment
 environment.
 Test your
 system.
 
 # Get support
 For more support with the Attribution Reporting API:
 
 Open a new Issue on the Privacy Sandbox Developer Support GitHub
 repository.
 Select the Issue template for Attribution Reporting.
 Or join the Attribution Reporting mailing list for
 developers
 and ask there.
 
 If you notice any unexpected behavior:
 
 View Chrome
 issues reported for the API implementation.
 Open a Chrome bug.
 Raise a new Issue for the Trusted Execution Aggregation
 Service.
 </content>
     </entry>
     <entry>
       <title>Deprecations and removals in Chrome 103</title>
         <link href="https://developer.chrome.com/en/blog/deps-rems-103/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text">
 Visit ChromeStatus.com for lists of current deprecations and previous removals.
 
 Chrome 103 beta was released on May 26, 2022 and is expected to become the
 stable version in late June, 2022.
 # Block external protocol in sandboxed iframes
 Sandboxed iframes are not blocked from opening external applications. Currently, developers sandbox untrusted content and block user navigation. Blocking probably should have also included links to external apps or to the Play store. This has now been fixed.
 Sites that need navigation can add the following values to the &lt;iframe&gt; element&#x27;s sandbox property:
 
 allow-popups
 allow-top-navigation
 allow-top-navigation-with-user-activation
 
 # Remove Battery Status API on insecure origins
 The Battery Status API is no longer supported on insecure contexts, specifically HTTP pages and HTTPS iframes embedded in HTTP pages. This is being removed in accordance with our policy of deprecating powerful features on insecure origins, This also follows a spec change.
 # Remove  element
 Given the removal of plugins from the web platform, and the relative lack of use of &lt;param&gt;, it is being removed from the web platform.
 # Deprecation policy
 To keep the platform healthy, we sometimes remove APIs from the Web Platform
 which have run their course. There can be many reasons why we would remove an
 API, such as:
 
 They are superseded by newer APIs.
 They are updated to reflect changes to specifications to bring alignment and
 consistency with other browsers.
 They are early experiments that never came to fruition in other browsers and
 thus can increase the burden of support for web developers.
 
 Some of these changes will have an effect on a very small number of sites. To
 mitigate issues ahead of time, we try to give developers advanced notice so they
 can make the required changes to keep their sites running.
 Chrome currently has a process for deprecations and removals of
 API&#x27;s,
 essentially:
 
 Announce on the
 blink-dev
 mailing list.
 Set warnings and give time scales in the Chrome DevTools Console when usage
 is detected on the page.
 Wait, monitor, and then remove the feature as usage drops.
 
 You can find a list of all deprecated features on chromestatus.com using the
 deprecated filter and
 removed features by applying the removed
 filter. We will also try to
 summarize some of the changes, reasoning, and migration paths in these posts.</content>
     </entry>
     <entry>
       <title>Cookies Having Independent State (CHIPS) origin trial extended</title>
         <link href="https://developer.chrome.com/en/blog/chips-origin-trial-extended/"/>
       <updated>2022-06-27T04:19:06.487Z</updated>
       <content type="text">CHIPS is a Privacy
 Sandbox proposal that introduces a mechanism to opt-in to having third-party
 cookies partitioned by top-level site using a new cookie attribute,
 Partitioned.
 The experiment started in Chrome 100 on March 29, 2022 and was scheduled to run
 until June 14, 2022.
 To give developers more time to test the feature and gather more feedback, an
 extension to the experiment will run through Chrome 104 until the release of Chrome 105, on August 30, 2022.
 Starting in version 103, Chrome will include an alternative CHIPS origin trial design for HTTP
 cookies, which should enable an opt-in mechanism that will make deployment
 easier for large organizations.
 In the new design, sending Accept-CH: Sec-CH-Partitioned-Cookies header will
 no longer be required to enroll in the origin trial. Sites will only need to
 send the Origin-Trial header with their CHIPS origin trial token when they are sending
 responses that include the Set-Cookie header with the Partitioned
 attribute.
 To enroll in the origin trial and start experimenting, head over to
 CHIPS origin trial instructions.</content>
     </entry>
     <entry>
       <title>UI Interactions &amp; Animations Roundup #25</title>
         <link href="https://tympanus.net/codrops/2022/06/26/ui-interactions-animations-roundup-25/"/>
       <updated>2022-06-26T11:09:02.000Z</updated>
       <content type="text">University Website Faculty Page
 
 
 
 by tubik UX
 
 
 
 
 
 
 ui Animation Anzo Clothing – Landing Page
 
 
 
 by ©mAnOff
 
 
 
 
 
 
 Aesthetica Magazine
 
 
 
 by Evgeny UPROCK
 
 
 
 
 
 
 Quiosque Lisboa
 
 
 
 by Ruslan Siiz
 
 
 
 
 
 
 GUUULP! Landing Hero Animation
 
 
 
 by Margaret Lunina
 
 
 
 
 
 
 Plain – swapping interaction
 
 
 
 by thrc.eth
 
 
 
 
 
 
 Scroll video
 
 
 
 by Slava Kornilov
 
 
 
 
 
 
 Annalisa – Web Design
 
 
 
 by QClay
 
 
 
 
 
 
 skye // /
 
 
 
 by Andrew Baygulov
 
 
 
 
 
 
 XPeng Motors
 
 
 
 by Evgeny UPROCK
 
 
 
 
 
 
 Apartment Booking Website
 
 
 
 by Dmitry Lauretsky
 
 
 
 
 
 
 Anime // E-Commerce
 
 
 
 by blacklead studio
 
 
 
 
 
 
 Travel Istanbul city Icons
 
 
 
 by Musemind Branding
 
 
 
 
 
 
 Resonance exploration animation
 
 
 
 by Resonance Studio
 
 
 
 
 
 
 Le Labo Another 13/ Product landing page
 
 
 
 by Tien Anh Dang
 
 
 
 
 
 
 NFT Space // Website
 
 
 
 by blacklead studio
 
 
 
 
 
 
 Re←Born – website concept
 
 
 
 by Vice Rukavina
 
 
 
 
 
 
 Cuberto Development Dept
 
 
 
 by Cuberto
 
 
 
 
 
 
 Landing Skyrent
 
 
 
 by Desire Creative Agency
 
 
 
 
 
 
 Temple – The future of Web3 Creator Economy
 
 
 
 by thrc.eth
 
 
 
 
 
 
 Seagulls 22
 
 
 
 by Elegant Seagulls
 
 
 
 The post UI Interactions &amp; Animations Roundup #25 appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>Single Element Loaders: The Bars</title>
         <link href="https://css-tricks.com/single-element-loaders-the-bars/"/>
       <updated>2022-06-24T20:00:29.000Z</updated>
       <content type="text">We’ve looked at spinners. We’ve looked at dots. Now we’re going to tackle another common pattern for loaders: bars. And we’re going to do the same thing in this third article of the series as we have the others by making it with only one element and with flexible CSS that makes it easy to create variations.
 
 
 
 
 
 
 
 
 Article series
 
 
 
 Single Element Loaders: The SpinnerSingle Element Loaders: The DotsSingle Element Loaders: The Bars — you are hereSingle Element Loaders: Going 3D — coming July 1
 
 
 
 
 Let’s start with not one, not two, but 20 examples of bar loaders.
 
 
 
 CodePen Embed Fallback
 
 
 
 CodePen Embed Fallback
 
 
 
 What?! Are you going to detail each one of them? That’s too much for an article!
 
 
 
 It might seem like that at first glance! But all of them rely on the same code structure and we only update a few values to create variations. That’s all the power of CSS. We don’t learn how to create one loader, but we learn different techniques that allow us to create as much loader as we want using merely the same code structure.
 
 
 
 Let’s make some bars!
 
 
 
 We start by defining the dimensions for them using width (or height) with aspect-ratio to maintain proportion:
 
 
 
 .bars {
   width: 45px;
   aspect-ratio: 1;
 }
 
 
 
 We sort of “fake” three bars with a linear gradient on the background — very similar to how we created dot loaders in Part 2 of this series.
 
 
 
 .bars {
   width: 45px;
   aspect-ratio: 1;
   --c: no-repeat linear-gradient(#000 0 0); /* we define the color here */
   background: 
     var(--c) 0%   50%,
     var(--c) 50%  50%,
     var(--c) 100% 50%;
   background-size: 20% 100%; /* 20% * (3 bars + 2 spaces) &#x3D; 100% */
 }
 
 
 
 The above code will give us the following result:
 
 
 
 
 
 
 
 Like the other articles in this series, we are going to deal with a lot of background trickery. So, if you ever feel like we’re jumping around too fast or feel you need a little more detail, please do check those out. You can also read my Stack Overflow answer where I give a detailed explanation on how all this works.
 
 
 
 Animating the bars
 
 
 
 We either animate the element’s size or position to create the bar loader. Let’s animate the size by defining the following animation keyframes:
 
 
 
 @keyframes load {
   0%   { background-size: 20% 100%, 20% 100%, 20% 100%; }  /* 1 */
   33%  { background-size: 20% 10% , 20% 100%, 20% 100%; }  /* 2 */
   50%  { background-size: 20% 100%, 20% 10% , 20% 100%; }  /* 3 */
   66%  { background-size: 20% 100%, 20% 100%, 20% 10%;  }  /* 4 */
   100% { background-size: 20% 100%, 20% 100%, 20% 100%; }  /* 5 */
 }
 
 
 
 See what’s happening there? Between 0% and 100%, the animation changes the background-size of the element’s background gradient. Each keyframe sets three background sizes (one for each gradient).
 
 
 
 
 
 
 
 And here’s what we get:
 
 
 
 CodePen Embed Fallback
 
 
 
 Can you start to imagine all the possible variations we can get by playing with different animation configurations for the sizes or the positions?
 
 
 
 Let’s fix the size to 20% 50% and update the positions this time:
 
 
 
 .loader {
   width: 45px;
   aspect-ratio: .75;
   --c: no-repeat linear-gradient(#000 0 0);
   background: 
     var(--c),
     var(--c),
     var(--c);
   background-size: 20% 50%;
   animation: load 1s infinite linear;
 }
 @keyframes load {
   0%   { background-position: 0% 100%, 50% 100%, 100% 100%; } /* 1 */
   20%  { background-position: 0% 50% , 50% 100%, 100% 100%; } /* 2 */
   40%  { background-position: 0% 0%  , 50% 50% , 100% 100%; } /* 3 */
   60%  { background-position: 0% 100%, 50% 0%  , 100% 50%;  } /* 4 */
   80%  { background-position: 0% 100%, 50% 100%, 100% 0%;   } /* 5 */ 
   100% { background-position: 0% 100%, 50% 100%, 100% 100%; } /* 6 */
 }
 
 
 
 
 
 
 
 …which gets us another loader!
 
 
 
 CodePen Embed Fallback
 
 
 
 You’ve probably got the trick by now. All you need is to define a timeline that you translate into a keyframe. By animating the size, the position — or both! — there’s an infinite number of loader possibilities at our fingertips.
 
 
 
 And once we get comfortable with such a technique we can go further and use a more complex gradient to create even more loaders.
 
 
 
 CodePen Embed Fallback
 
 
 
 Expect for the last two examples in that demo, all of the bar loaders use the same underlying markup and styles and different combinations of animations. Open the code and try to visualize each frame independently; you’ll see how relatively trivial it is to make dozens — if not hundreds — of variations.
 
 
 
 Getting fancy
 
 
 
 Did you remember the mask trick we did with the dot loaders in the second article of this series? We can do the same here!
 
 
 
 If we apply all the above logic inside the mask property we can use any background configuration to add a fancy coloration to our loaders.
 
 
 
 Let’s take one demo and update it:
 
 
 
 CodePen Embed Fallback
 
 
 
 All I did is updating all the background-* with mask-* and I added a gradient coloration. As simple as that and yet we get another cool loader.
 
 
 
 So there is no difference between the dots and the bars?
 
 
 
 No difference! I wrote two different articles to cover as many examples as possible but in both, I am relying on the same techniques:
 
 
 
 Gradients to create the shapes (dots or bars or maybe something else)Animating background-size and/or background-position to create the loader animationAdding mask to add a touch of colors
 
 
 
 Rounding the bars
 
 
 
 Let’s try something different this time where we can round the edges of our bars.
 
 
 
 CodePen Embed Fallback
 
 
 
 Using one element and its ::before and ::after pseudos, we define three identical bars:
 
 
 
 .loader {
   --s: 100px; /* control the size */
 
   display: grid;
   place-items: center;
   place-content: center;
   margin: 0 calc(var(--s) / 2); /* 50px */
 }
 .loader::before,
 .loader::after {
   content: &quot;&quot;;
   grid-area: 1/1;
 }
 .loader,
 .loader::before,
 .loader::after {
   height: var(--s);
   width: calc(var(--s) / 5); /* 20px */
   border-radius: var(--s);
   transform: translate(calc(var(--_i, 0) * 200%));
 }
 .loader::before { --_i: -1; }
 .loader::after { --_i:  1; }
 
 
 
 That gives us three bars, this time without relying on a linear gradient:
 
 
 
 
 
 
 
 Now the trick is to fill in those bars with a lovely gradient. To simulate a continuous gradient, we need to play with background properties. In the above figure, the green area defines the area covered by the loader. That area should be the size of the gradient and, if we do the math, it’s equal to multiplying both sides labeled S in the diagram, or background-size: var(--s) var(--s).
 
 
 
 Since our elements are individually placed, we need to update the position of the gradient inside each one to make sure all of them overlap. This way, we’re simulating one continuous gradient even though it’s really three of them.
 
 
 
 For the main element (placed at the center), the background needs to be at the center. We use the following:
 
 
 
 .loader {
   /* etc. */
   background: linear-gradient() 50% / var(--s) var(--s);
 }
 
 
 
 For the pseudo-element on the left, we need the background on the left
 
 
 
 .loader::before {
   /* etc. */
   background: linear-gradient() 0% / var(--s) var(--s);
 }
 
 
 
 And for the pseudo on the right, the background needs to be positioned to the right:
 
 
 
 .loader::after {
   background: linear-gradient() 100% / var(--s) var(--s);
 }
 
 
 
 Using the same CSS variable, --_i, that we used for the translate, we can write the code like this:
 
 
 
 .loader {
   --s: 100px; /* control the size */
   --c: linear-gradient(/* etc. */); /* control the coloration */
 
   display: grid;
   place-items: center;
   place-content: center;
 }
 .loader::before,
 .loader::after{
   content: &quot;&quot;;
   grid-area: 1/1;
 }
 .loader,
 .loader::before,
 .loader::after{
   height: var(--s);
   width: calc(var(--s) / 5);
   border-radius: var(--s);
   background: var(--c) calc(50% + var(--_i, 0) * 50%) / var(--s) var(--s);
   transform: translate(calc(var(--_i, 0) * 200%));
 }
 .loader::before { --_i: -1; }
 .loader::after  { --_i:  1; }
 
 
 
 Now, all we have to do is to animate the height and add some delays! Here are three examples where all that’s different are the colors and sizes:
 
 
 
 CodePen Embed Fallback
 
 
 
 Wrapping up
 
 
 
 I hope so far you are feeling super encouraged by all the powers you have to make complex-looking loading animations. All we need is one element, either gradients or pseudos to draw the bars, then some keyframes to move things around. That’s the entire recipe for getting an endless number of possibilities, so go out and starting cooking up some neat stuff!
 
 
 
 Until the next article, I will leave you with a funny collection of loaders where I am combining the dots and the bars!
 
 
 
 CodePen Embed Fallback
 
 
 
 CodePen Embed Fallback
 
 
 
 
 Article series
 
 
 
 Single Element Loaders: The SpinnerSingle Element Loaders: The DotsSingle Element Loaders: The Bars — you are hereSingle Element Loaders: Going 3D — coming July 1
 
 
 Single Element Loaders: The Bars originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>How to start an ordered list with a number other than one</title>
         <link href="https://gomakethings.com/how-to-start-an-ordered-list-with-a-number-other-than-one/"/>
       <updated>2022-06-24T14:30:00.000Z</updated>
       <content type="text">This week, Marcus Herrmann reminded me of one of my favorite HTML tricks.
 
 If you have an ordered list (ol) and want it to start with a number other than one, you can add the start attribute with the desired first number as its value.
 &lt;ol start&#x3D;&quot;4&quot;&gt;
 	&lt;li&gt;Merlin&lt;/li&gt;
 	&lt;li&gt;Ursula&lt;/li&gt;
 	&lt;li&gt;Radgast&lt;/li&gt;
 &lt;/ol&gt;
 The example above would start with the number four. Here’s a demo.
 
 This is particularly useful if you have a list split across a few columns and want to keep them consistently numbered. Here, the first .column would run from one to three, and the second would go from four to six.
 &lt;div class&#x3D;&quot;row&quot;&gt;
 	&lt;div class&#x3D;&quot;column&quot;&gt;
 		&lt;ol&gt;
 			&lt;li&gt;Neville&lt;/li&gt;
 			&lt;li&gt;Hermione&lt;/li&gt;
 			&lt;li&gt;Cedric&lt;/li&gt;
 		&lt;/ol&gt;
 	&lt;/div&gt;
 	&lt;div class&#x3D;&quot;column&quot;&gt;
 		&lt;ol start&#x3D;&quot;4&quot;&gt;
 			&lt;li&gt;Merlin&lt;/li&gt;
 			&lt;li&gt;Ursula&lt;/li&gt;
 			&lt;li&gt;Radgast&lt;/li&gt;
 		&lt;/ol&gt;
 	&lt;/div&gt;
 &lt;/div&gt;⏰🦉 Early Bird Sale! Today through Monday, get 40% off registration in the next session of the Vanilla JS Academy.</content>
     </entry>
     <entry>
       <title>Voice Control Usability Considerations For Partially Visually Hidden Link Names</title>
         <link href="https://smashingmagazine.com/2022/06/voice-control-usability-considerations-partially-visually-hidden-link-names/"/>
       <updated>2022-06-24T09:00:00.000Z</updated>
       <content type="text">Digital accessibility tends to be taught through the lens of how your experience works (or fails to work) with a screen reader. It makes sense to think that, if it works for a screen reader, it will also work for a lot of other kinds of assistive technology.
 However, this approach also indirectly reinforces the narrative that blindness is the majority experience. Within this narrative, there is also some subtlety in the fact that not everyone who uses a screen reader is blind. 
 The majority disability experience is actually depression, which is a complicated disability with highly variable symptoms. One of the most notable symptoms of depression is that it negatively impacts your cognition, which affects your ability to understand things.
 The other salient bit is that the majority experience is not default. The point I’m getting at is that overcorrecting for one form of disability may unintentionally negatively impact the experience for other forms of disability — voice control software being one example.
 Unique Link Names
 Making each link’s accessible name unique is an important thing to do. It helps clarify where each link goes to someone who is navigating via a specialized screen reader browsing mode. For example, all major screen readers have the ability to list all the links on the current page or view, with the links being removed from their surrounding text content and context.
 Eleven links called “learn more” don’t make a lot of sense when separated from their surrounding context. It’s far better to tell the reader what they’ll be learning about:
 
 Visually Hidden Text
 If you use CSS libraries such as Bootstrap or Tailwind, you may be familiar with a specialized class that a screen reader can read, but is not displayed visually. These classes help provide additional context, such as supplying additional heading elements to help with way-finding.
 &lt;h2 class&#x3D;&quot;sr-only&quot;&gt;
   Footer
 &lt;/h2&gt;
 
 
 Partially Visually Hidden Text
 Another thing visually hidden classes are commonly used for is to hide the portion of an interactive control that makes its name unique. A common pattern for this is a call-to-action (CTA) link in a card component:
 
 Who Uses It?
 There are more people who use voice control than you’d think. If you’ve ever asked Siri to set a timer, congratulations! You’re a voice control user!
 That being said, software like Voice Control, Dragon, and Talon are designed for more long-form, specialized use. People who use these applications may temporarily or permanently, and circumstantially or biologically be:
 
 Fully or partially paralyzed;
 Unable to use their hands;
 Unable to make fine motor movements;
 Unable to make repetitive movements over a long duration;
 Operating with lowered cognition.
 
 Disability is also not a binary state, so it is entirely possible (and relatively common) that multiple disability conditions may be present at any given time.  
 Workarounds
 Some voice control software has the functionality to work around this issue, the same way screen readers do. Dragon, for example, uses heuristics to identify links that contain the term “learn more.” It then lists a number for each link with that term, which you can then say “choose number” to activate.
 
 If you need to update your existing work to accommodate these kinds of real-world considerations, you’ll probably have to collaborate with content writers, developers, and project managers. Design thrives in this kind of collaborative environment, where you can identify and work with the limits of the systems you work inside of.
 Sometimes you’ll be able to tweak what you already have. Sometimes you’ll have to create net-new content. Sometimes you’ll even have to throw it all out and go back to the drawing board. These efforts all speak to the value of a Shift Left methodology, where these kinds of considerations are named and dealt with early in the conception phase.
 How Do We Make Sure Our Experiences Are Easy To Use By Voice Control Software?
 For interactive controls such as links and buttons, you’ll want to:
 
 Show the full text of the control’s name,
 Use a unique, accessible name for each control on the current page or view, 
 Avoid overriding accessible names with aria-label, and
 Avoid using just a cryptic or abstract icon — especially if the control’s functionality is exotic.
 
 These steps will go a long way towards making using voice control software easier and more pleasant.  
 It’s Not Really About Voice Control Software
 The three overarching themes that I hope you’re picking up on:
 
 Testing with a range of assistive technology is important to understanding actual support,
 Agency can be granted by letting go of control, and 
 Design decisions carry a tremendous amount of power.
 
 Your designs need to be flexible and adaptable, as well as be able to accommodate the many different ways people can interact with them. This includes voice control, as well as numerous other forms of assistive technology.
 Further Reading On Smashing Magazine
 
 “Everything You Want To Know About Creating Voice User Interfaces,” Nick Babich &amp; Gleb Kuznetsov 
 “Equivalent Experiences: What Are They?,” Eric Bailey
 “Making A Strong Case For Accessibility,” Todd Libby
 “The Guide To Windows High Contrast Mode,” Cristian Díaz
 </content>
     </entry>
     <entry>
       <title>Fuzzing rust-minidump for Embarrassment and Crashes – Part 2</title>
         <link href="https://hacks.mozilla.org/2022/06/fuzzing-rust-minidump-for-embarrassment-and-crashes/"/>
       <updated>2022-06-23T17:19:31.000Z</updated>
       <content type="text">This is part 2 of a series of articles on rust-minidump. For part 1, see here.
 So to recap, we rewrote breakpad’s minidump processor in Rust, wrote a ton of tests, and deployed to production without any issues. We killed it, perfect job.
 And we still got massively dunked on by the fuzzer. Just absolutely destroyed.
 I was starting to pivot off of rust-minidump work because I needed a bit of palette cleanser before tackling round 2 (handling native debuginfo, filling in features for other groups who were interested in rust-minidump, adding extra analyses that we’d always wanted but were too much work to do in Breakpad, etc etc etc).
 I was still getting some PRs from people filling in the corners they needed, but nothing that needed too much attention, and then @5225225 smashed through the windows and released a bunch of exploding fuzzy rabbits into my office. 
 I had no idea who they were or why they were there. When I asked they just lowered one of their seven pairs of sunglasses and said “Because I can. Now hold this bunny”. I did as I was told and held the bunny. It was a good bun. Dare I say, it was a true bnnuy: it was libfuzzer. (Huh? You thought it was gonna be AFL? Weird.)
 As it turns out, several folks had built out some really nice infrastructure for quickly setting up a decent fuzzer for some Rust code: cargo-fuzz. They even wrote a little book that walks you through the process.
 Apparently those folks had done such a good job that 5225225 had decided it would be a really great hobby to just pick up a random rust project and implement fuzzing for it. And then to fuzz it. And file issues. And PRs that fix those issues. And then implement even more fuzzing for it.
 Please help my office is drowning in rabbits and I haven’t seen my wife in weeks.
 As far as I can tell, the process seems to genuinely be pretty easy! I think their first fuzzer for rust-minidump was basically just:
 
 checked out the project
 run cargo fuzz init (which autogenerates a bunch of config files)
 write a file with this:
 
 #![no_main]
 
 use libfuzzer_sys::fuzz_target;
 use minidump::*;
 
 fuzz_target!(|data: &amp;[u8]| {
     // Parse a minidump like a normal user of the library
     if let Ok(dump) &#x3D; minidump::Minidump::read(data) {
         // Ask the library to get+parse several streams like a normal user.
 
         let _ &#x3D; dump.get_stream::&lt;MinidumpAssertion&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpBreakpadInfo&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpCrashpadInfo&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpException&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpLinuxCpuInfo&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpLinuxEnviron&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpLinuxLsbRelease&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpLinuxMaps&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpLinuxProcStatus&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpMacCrashInfo&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpMemoryInfoList&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpMemoryList&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpMiscInfo&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpModuleList&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpSystemInfo&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpThreadNames&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpThreadList&gt;();
         let _ &#x3D; dump.get_stream::&lt;MinidumpUnloadedModuleList&gt;();
     }
 });
 And that’s… it? And all you have to do is type cargo fuzz run and it downloads, builds, and spins up an instance of libfuzzer and finds bugs in your project overnight?
 Surely that won’t find anything interesting. Oh it did? It was largely all bugs in code I wrote? Nice.
 cargo fuzz is clearly awesome but let’s not downplay the amount of bafflingly incredible work that 5225225 did here! Fuzzers, sanitizers, and other code analysis tools have a very bad reputation for drive-by contributions. 
 I think we’ve all heard stories of someone running a shiny new tool on some big project they know nothing about, mass filing a bunch of issues that just say “this tool says your code has a problem, fix it” and then disappearing into the mist and claiming victory.
 This is not a pleasant experience for someone trying to maintain a project. You’re dumping a lot on my plate if I don’t know the tool, have trouble running the tool, don’t know exactly how you ran it, etc. 
 It’s also very easy to come up with a huge pile of issues with very little sense of how significant they are. 
 Some things are only vaguely dubious, while others are horribly terrifying exploits. We only have so much time to work on stuff, you’ve gotta help us out!
 And in this regard 5225225’s contributions were just, bloody beautiful. 
 Like, shockingly fantastic.
 They wrote really clear and detailed issues. When I skimmed those issues and misunderstood them, they quickly clarified and got me on the same page. And then they submitted a fix for the issue before I even considered working on the fix. And quickly responded to review comments. I didn’t even bother asking them to squashing their commits because damnit they earned those 3 commits in the tree to fix one overflow.
 Then they submitted a PR to merge the fuzzer. They helped me understand how to use it and debug issues. Then they started asking questions about the project and started writing more fuzzers for other parts of it. And now there’s like 5 fuzzers and a bunch of fixed issues!
 I don’t care how good cargo fuzz is, that’s a lot of friggin’ really good work! Like I am going to cry!! This was so helpful??? 
 That said, I will take a little credit for this going so smoothly: both Rust itself and rust-minidump are written in a way that’s very friendly to fuzzing. Specifically, rust-minidump is riddled with assertions for “hmm this seems messed up and shouldn’t happen but maybe?” and Rust turns integer overflows into panics (crashes) in debug builds (and index-out-of-bounds is always a panic).
 Having lots of assertions everywhere makes it a lot easier to detect situations where things go wrong. And when you do detect that situation, the crash will often point pretty close to where things went wrong.
 As someone who has worked on detecting bugs in Firefox with sanitizer and fuzzing folks, let me tell you what really sucks to try to do anything with: “Hey so on my machine this enormous complicated machine-generated input caused Firefox to crash somewhere this one time. No, I can’t reproduce it. You won’t be able to reproduce it either. Anyway, try to fix it?”
 That’s not me throwing shade on anyone here. I am all of the people in that conversation. The struggle of productively fuzzing Firefox is all too real, and I do not have a good track record of fixing those kinds of bugs. 
 By comparison I am absolutely thriving under “Yeah you can deterministically trip this assertion with this tiny input you can just check in as a unit test”.
 And what did we screw up? Some legit stuff! It’s Rust code, so I am fairly confident none of the issues were security concerns, but they were definitely quality of implementation issues, and could have been used to at very least denial-of-service the minidump processor.
 Now let’s dig into the issues they found!
 #428: Corrupt stacks caused infinite loops until OOM on ARM64
 Issue
 As noted in the background, stackwalking is a giant heuristic mess and you can find yourself going backwards or stuck in an infinite loop. To keep this under control, stackwalkers generally require forward progress. 
 Specifically, they require the stack pointer to move down the stack. If the stack pointer ever goes backwards or stays the same, we just call it quits and end the stackwalk there.
 However, you can’t be so strict on ARM because leaf functions may not change the stack size at all. Normally this would be impossible because every function call at least has to push the return address to the stack, but ARM has the link register which is basically an extra buffer for the return address. 
 The existence of the link register in conjunction with an ABI that makes the callee responsible for saving and restoring it means leaf functions can have 0-sized stack frames!
 To handle this, an ARM stackwalker must allow for there to be no forward progress for the first frame of a stackwalk, and then become more strict. Unfortunately I hand-waved that second part and ended up allowing infinite loops with no forward progress:
 // If the new stack pointer is at a lower address than the old,
 // then that&#x27;s clearly incorrect. Treat this as end-of-stack to
 // enforce progress and avoid infinite loops.
 //
 // NOTE: this check allows for equality because arm leaf functions
 // may not actually touch the stack (thanks to the link register
 // allowing you to &quot;push&quot; the return address to a register).
 if frame.context.get_stack_pointer() &lt; self.get_register_always(&quot;sp&quot;) as u64 {
     trace!(&quot;unwind: stack pointer went backwards, assuming unwind complete&quot;);
     return None;
 }
 So if the ARM64 stackwalker ever gets stuck in an infinite loop on one frame, it will just build up an infinite backtrace until it’s killed by an OOM. This is very nasty because it’s a potentially very slow denial-of-service that eats up all the memory on the machine!
 This issue was actually originally discovered and fixed in #300 without a fuzzer, but when I fixed it for ARM (32-bit) I completely forgot to do the same for ARM64. Thankfully the fuzzer was evil enough to discover this infinite looping situation on its own, and the fix was just “copy-paste the logic from the 32-bit impl”.
 Because this issue was actually encountered in the wild, we know this was a serious concern! Good job, fuzzer!
 (This issue specifically affected minidump-processor and minidump-stackwalk)
 #407: MinidumpLinuxMaps address-based queries didn’t work at all
 Issue
 MinidumpLinuxMaps is an interface for querying the dumped contents of Linux’s /proc/self/maps file. This provides metadata on the permissions and allocation state for mapped ranges of memory in the crashing process.
 There are two usecases for this: just getting a full dump of all the process state, and specifically querying the memory properties for a specific address (“hey is this address executable?”). The dump usecase is handled by just shoving everything in a Vec. The address usecase requires us to create a RangeMap over the entries.
 Unfortunately, a comparison was flipped in the code that created the keys to the RangeMap, which resulted in every correct memory range being discarded AND invalid memory ranges being accepted. The fuzzer was able to catch this because the invalid ranges tripped an assertion when they got fed into the RangeMap (hurray for redundant checks!).
 // OOPS
 if self.base_address &lt; self.final_address { 
  return None; 
 }
 Although tests were written for MinidumpLinuxMaps, they didn’t include any invalid ranges, and just used the dump interface, so the fact that the RangeMap was empty went unnoticed!
 This probably would have been quickly found as soon as anyone tried to actually use this API in practice, but it’s nice that we caught it beforehand! Hooray for fuzzers!
 (This issue specifically affected the minidump crate which technically could affect minidump-processor and minidump-stackwalk. Although they didn’t yet actually do address queries, they may have crashed when fed invalid ranges.)
 #381: OOM from reserving memory based on untrusted list length.
 Issue
 Minidumps have lots of lists which we end up collecting up in a Vec or some other collection. It’s quite natural and more efficient to start this process with something like Vec::with_capacity(list_length). Usually this is fine, but if the minidump is corrupt (or malicious), then this length could be impossibly large and cause us to immediately OOM.
 We were broadly aware that this was a problem, and had discussed the issue in #326, but then everyone left for the holidays. #381 was a nice kick in the pants to actually fix it, and gave us a free simple test case to check in.
 Although the naive solution would be to fix this by just removing the reserves, we opted for a solution that guarded against obviously-incorrect array lengths. This allowed us to keep the performance win of reserving memory while also making rust-minidump fast-fail instead of vaguely trying to do something and hallucinating a mess.
 Specifically, @Swatinem introduced a function for checking that the amount of memory left in the section we’re parsing is large enough to even hold the claimed amount of items (based on their known serialized size). This should mean the minidump crate can only be induced to reserve O(n) memory, where n is the size of the minidump itself.
 For some scale:
 
 A minidump for Firefox’s main process with about 100 threads is about 3MB.
 A minidump for a stackoverflow from infinite recursion (8MB stack, 9000 calls) is about 8MB.
 A breakpad symbol file for Firefox’s main module can be about 200MB.
 
 If you’re symbolicating, Minidumps probably won’t be your memory bottleneck. 
 (This issue specifically affected the minidump crate and therefore also minidump-processor and minidump-stackwalk.)
 The Many Integer Overflows and My Greatest Defeat
 The rest of the issues found were relatively benign integer overflows. I claim they’re benign because rust-minidump should already be working under the assumption that all the values it reads out of the minidump could be corrupt garbage. This means its code is riddled with “is this nonsense” checks and those usually very quickly catch an overflow (or at worst print a nonsense value for some pointer).
 We still fixed them all, because that’s shaky as heck logic and we want to be robust. But yeah none of these were even denial-of-service issues, as far as I know.
 To demonstrate this, let’s discuss the most evil and embarrassing overflow which was definitely my fault and I am still mad about it but in a like “how the heck” kind of way!?
 The overflow is back in our old friend the stackwalker. Specifically in the code that attempts to unwind using frame pointers. Even more specifically, when offsetting the supposed frame-pointer to get the location of the supposed return address:
 let caller_ip &#x3D; stack_memory.get_memory_at_address(last_bp + POINTER_WIDTH)?;
 let caller_bp &#x3D; stack_memory.get_memory_at_address(last_bp)?;
 let caller_sp &#x3D; last_bp + POINTER_WIDTH * 2;
 If the frame pointer (last_bp) was ~u64::MAX, the offset on the first line would overflow and we would instead try to load ~null. All of our loads are explicitly fallible (we assume everything is corrupt garbage!), and nothing is ever mapped to the null page in normal applications, so this load would reliably fail as if we had guarded the overflow. Hooray!
 …but the overflow would panic in debug builds because that’s how debug builds work in Rust!
 This was actually found, reported, and fixed without a fuzzer in #251. All it took was a simple guard:
 (All the casts are because this specific code is used in the x86 impl and the x64 impl.)
 if last_bp as u64 &gt;&#x3D; u64::MAX - POINTER_WIDTH as u64 * 2 {
     // Although this code generally works fine if the pointer math overflows,
     // debug builds will still panic, and this guard protects against it without
     // drowning the rest of the code in checked_add.
     return None;
 }
 
 let caller_ip &#x3D; stack_memory.get_memory_at_address(last_bp as u64 + POINTER_WIDTH as u64)?;
 let caller_bp &#x3D; stack_memory.get_memory_at_address(last_bp as u64)?;
 let caller_sp &#x3D; last_bp + POINTER_WIDTH * 2;
 And then it was found, reported, and fixed again with a fuzzer in #422.
 Wait what?
 Unlike the infinite loop bug, I did remember to add guards to all the unwinders for this problem… but I did the overflow check in 64-bit even for the 32-bit platforms.
 slaps forehead
 This made the bug report especially confusing at first because the overflow was like 3 lines away from a guard for that exact overflow. As it turns out, the mistake wasn’t actually as obvious as it sounds! To understand what went wrong, let’s talk a bit more about pointer width in minidumps.
 A single instance of rust-minidump has to be able to handle crash reports from any platform, even ones it isn’t natively running on. This means it needs to be able to handle both 32-bit and 64-bit platforms in one binary. To avoid the misery of copy-pasting everything or making everything generic over pointer size, rust-minidump prefers to work with 64-bit values wherever possible, even for 32-bit plaftorms.
 This isn’t just us being lazy: the minidump format itself does this! Regardless of the platform, a minidump will refer to ranges of memory with a MINIDUMP_MEMORY_DESCRIPTOR whose base address is a 64-bit value, even on 32-bit platforms!
 typedef struct _MINIDUMP_MEMORY_DESCRIPTOR {
   ULONG64                      StartOfMemoryRange;
   MINIDUMP_LOCATION_DESCRIPTOR Memory;
 } MINIDUMP_MEMORY_DESCRIPTOR, *PMINIDUMP_MEMORY_DESCRIPTOR;
 So quite naturally rust-minidump’s interface for querying saved regions of memory just operates on 64-bit (u64) addresses unconditionally, and 32-bit-specific code casts its u32 address to a u64 before querying memory.
 That means the code with the overflow guard was manipulating those values as u64s on x86! The problem is that after all the memory loads we would then go back to “native” sizes and compute caller_sp &#x3D; last_bp + POINTER_WIDTH * 2. This would overflow a u32 and crash in debug builds. 
 But here’s the really messed up part: getting to that point meant we were successfully loading memory up to that address. The first line where we compute caller_ip reads it! So this overflow means… we were… loading memory… from an address that was beyond u32::MAX…!?
 Yes!!!!!!!!
 The fuzzer had found an absolutely brilliantly evil input.
 It abused the fact that MINIDUMP_MEMORY_DESCRIPTOR technically lets 32-bit minidumps define memory ranges beyond u32::MAX even though they could never actually access that memory! It could then have the u64-based memory accesses succeed but still have the “native” 32-bit operation overflow!
 This is so messed up that I didn’t even comprehend that it had done this until I wrote my own test and realized that it wasn’t actually failing because I foolishly had limited the range of valid memory to the mere 4GB a normal x86 process is restricted to.
 And I mean that quite literally: this is exactly the issue that creates Parallel Universes in Super Mario 64.
 But hey my code was probably just bad. I know google loves sanitizers and fuzzers, so I bet google breakpad found this overflow ages ago and fixed it:
 uint32_t last_esp &#x3D; last_frame-&gt;context.esp;
 uint32_t last_ebp &#x3D; last_frame-&gt;context.ebp;
 uint32_t caller_eip, caller_esp, caller_ebp;
 
 if (memory_-&gt;GetMemoryAtAddress(last_ebp + 4, &amp;caller_eip) &amp;&amp;
     memory_-&gt;GetMemoryAtAddress(last_ebp, &amp;caller_ebp)) {
     caller_esp &#x3D; last_ebp + 8;
     trust &#x3D; StackFrame::FRAME_TRUST_FP;
 } else {
     ...
 Ah. Hmm. They don’t guard for any kind of overflow for those uint32_t’s (or the uint64_t’s in the x64 impl).
 Well ok GetMemoryAtAddress does actual bounds checks so the load from ~null will generally fail like it does in rust-minidump. But what about the Parallel Universe overflow that lets GetMemoryAtAddress succeed?
 Ah well surely breakpad is more principled with integer width than I was–
 virtual bool GetMemoryAtAddress(uint64_t address, uint8_t*  value) const &#x3D; 0;
 virtual bool GetMemoryAtAddress(uint64_t address, uint16_t* value) const &#x3D; 0;
 virtual bool GetMemoryAtAddress(uint64_t address, uint32_t* value) const &#x3D; 0;
 virtual bool GetMemoryAtAddress(uint64_t address, uint64_t* value) const &#x3D; 0;
 
 Whelp congrats to 5225225 for finding an overflow that’s portable between two implementations in two completely different languages by exploiting the very nature of the file format itself!
 In case you’re wondering what the implications of this overflow are: it’s still basically benign. Both rust-minidump and google-breakpad will successfully complete the frame pointer analysis and yield a frame with a ~null stack pointer.
 Then the outer layer of the stackwalker which runs all the different passes in sequence will see something succeeded but that the frame pointer went backwards. At this point it will discard the stack frame and terminate the stackwalk normally and just calmly output whatever the backtrace was up to that point. Totally normal and reasonable operation.
 I expect this is why no one would notice this in breakpad even if you run fuzzers and sanitizers on it: nothing in the code actually does anything wrong. Unsigned integers are defined to wrap, the program behaves reasonably, everything is kinda fine. We only noticed this in rust-minidump because all integer overflows panic in Rust debug builds.
 However this “benign” behaviour is slightly different from properly guarding the overflow. Both implementations will normally try to move on to stack scanning when the frame pointer analysis fails, but in this case they give up immediately. It’s important that the frame pointer analysis properly identifies failures so that this cascading can occur. Failing to do so is definitely a bug!
 However in this case the stack is partially in a parallel universe, so getting any kind of useful backtrace out of it is… dubious to say the least.
 So I totally stand by “this is totally benign and not actually a problem” but also “this is sketchy and we should have the bounds check so we can be confident in this code’s robustness and correctness”.
 Minidumps are all corner cases — they literally get generated when a program encounters an unexpected corner case! It’s so tempting to constantly shrug off situations as “well no reasonable program would ever do this, so we can ignore it”… but YOU CAN’T.
 You would not have a minidump at your doorstep if the program had behaved reasonably! The fact that you are trying to inspect a minidump means something messed up happened, and you need to just deal with it!
 That’s why we put so much energy into testing this thing, it’s a nightmare!
 I am extremely paranoid about this stuff, but that paranoia is based on the horrors I have seen. There are always more corner cases. 
 There are ALWAYS more corner cases. ALWAYS.
  
 The post Fuzzing rust-minidump for Embarrassment and Crashes – Part 2 appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>Collective #717</title>
         <link href="https://tympanus.net/codrops/collective/collective-717/"/>
       <updated>2022-06-23T16:56:38.000Z</updated>
       <content type="text">
 
 
  
 Inspirational Website of the Week: Hisami Kurita Portfolio
 A really unique design and web experience that shines with amazing animations. Our pick this week.
 Get inspired
 
 
 
 
 
         
 This content is sponsored via BuySellAds
 Perfect for Hackathons
 Learn how to speed up your development using low-code databases.
         Check it out
 
 
 
 
 
  
 State of GraphQL
 The new annual developer survey of the GraphQL ecosystem. Fill out the survey to help identify which GraphQL tools and features are actually being used.
 Check it out
 
 
 
 
 
  
 The cost of convenience
 Surma’s mental model of how to develop for developers without getting tempted to over-do it with abstractions.
 Read it
 
 
 
 
 
  
 Fun Parallax Scrolling CSS for Matterday
 Journey through the parallax scrolling CSS effects featured on the site of the Matterday project. By Lynn Fisher.
 Read it
 
 
 
 
 
  
 Managing Specificity With CSS Cascade Layers
 An in-depth guide to CSS Cascade Layers that will help you write easily maintainable stylesheets and deal with CSS specificity more efficiently.
 Watch it
 
 
 
 
 
  
 Stack Overflow Developer Survey 2022
 The results of Stack Overflow’s survey insights about the attitudes, tools, and environments that are shaping the art and practice of software today.
 Check it out
 
 
 
 
 
  
 Building forms with custom elements
 Maggie Wachs reviews what we can do now with custom elements in forms to ensure they behave as expected, and what’s on the horizon to simplify this process.
 Read it
 
 
 
 
 
  
 Style scoping versus shadow DOM: which is fastest?
 Nolan Lawson presents a new benchmark that provides a better answer to the question if shadow DOM improves style performance.
 Read it
 
 
 
 
 
  
 Building a Quiz with Eleventy and Eleventy Serverless
 Learn how to built an Eleventy site driven by dynamic quiz data. By Raymond Camden.
 Read it
 
 
 
 
 
  
 CSS card shadow effects
 Chen Hui Jing shows how to create a card component with a comic style look.
 Read it
 
 
 
 
 
  
 ATMOS
 Get on board and discover the most surreal facts about the aviation industry. A fantastic experiment by the folks of Leeroy.
 Check it out
 
 
 
 
 
  
 Spider
 A cool spider experiment coded by Fabio Ottaviani.
 Check it out
 
 
 
 
 
  
 SVG Spinners
 A collection of 24 x 24 dp SVG spinners.
 Check it out
 
 
 
 
 
  
 Markwhen
 In case you didn’t know about it: Markwhen is a text-to-timeline tool. You write markdown-ish text and it gets converted into a nice looking cascading timeline.
 Check it out
 
 
 
 
 
  
 Chunshik’s observation diary
 An absolutely adorable WebGL site.
 Check it out
 
 
 
 
 
  
 Modern CSS Reset
 CSS Reset that uses modern CSS features such as :where(), logical properties, prefers-reduced-motion and more. By Elly.
 Check it out
 
 
 
 
 
  
 Conditionally Styling Selected Elements in a Grid Container
 Fantastic CSS magic by Preethi that allows for selecting multiple items in a grid.
 Read it
 
 
 
 
 
  
 Three.js Architectural Visualization
 Anderson Mancini has made another amazing #archiviz project in Three.js.
 Check it out
 
 
 
 
 
  
 The designer’s gaze
 Srishti Mehrotra’s opinion on the role of designers and the impact of their work.
 Read it
 
 
 
 
 
  
 From Our Blog
 Creating a Particles Galaxy with Three.js
 A coding session where you’ll learn how to recreate the galaxy made of particles from the Viverse website.
 Check it out
 
 
 The post Collective #717 appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>How I create my JavaScript books and courses</title>
         <link href="https://gomakethings.com/how-i-create-my-javascript-books-and-courses/"/>
       <updated>2022-06-23T14:30:00.000Z</updated>
       <content type="text">Over the last two weeks, I release a new course on accessible components and a new course on native web components, and completely redid my course on writing JavaScript libraries.
 
 Today, I wanted to talk about how I create my courses and ebooks.
 
 This article is focused on the actual content and finished books/videos. If you want to learn more about how I sell and deliver them, I wrote about the tech behind my learning platform a few years ago.
 
 Let’s dig in!
 
 The process
 
 Every pocket guide I create follows the same process…
 
 
 Learn how to do a thing
 Outline the stuff you need to know: the basics, some advanced topics, and weird gotchas or edge cases that most tutorials miss
 Write the ebook
 Create source code from the ebook
 Use the source code to create the video course
 Create the book cover and course image
 Convert the ebook into PDF, EPUB, MOBI, and HTML formats
 Upload the videos to a streaming service and get them captioned
 Sell the course!
 
 
 Let’s look at each step in more detail.
 
 Learn how to do a thing
 
 My pocket guides are short and focused. Every one starts off as a narrow topic that I’m personally interested in or learning about.
 
 My code base was getting unruly, so I learned about ES modules. I wanted to improve the resilience and offline capabilities of my sites, so I learned about Service Workers.
 
 The process for me always starts by Duck Duck Going a whole bunch, reading a bunch of tutorials, and trying to implement what I’ve read into a simple working project.
 
 There’s almost always a bunch of assumed knowledge in the tutorials, or edge cases they don’t mention, or gotchas or weird bugs that pop up that weren’t discussed. This is the kind of stuff that makes self-taught learning so hard, and one of the reasons I started creating courses in the first place.
 
 After I’ve built a few small projects, I have a pretty good grasp on how things work. But I’m also close enough to “being a new learner” that the pain points and “oh, I wish someone had told me that!” stuff is still really fresh in my mind.
 
 This is the perfect time to start actually writing the course.
 
 Outline the stuff that you need to know
 
 I find most tutorials on the web fall into one of two categories:
 
 
 Hand wave over some essential knowledge and jump right into the interesting stuff.
 Go into way too much detail about every aspect of a topic.
 
 
 Both approaches suck, for different reasons.
 
 The hand-wavy tutorials are impossible for beginners, because you can never get started. There’s all this stuff you need to know first that’s not mentioned or covered.
 
 The too much detail tutorials can be confusing and overwhelming. They often cover stuff that’s not likely to come up, or not important at first (but good to know later). They’re hard for beginners, too, mostly because they overload you with information.
 
 I like to break my outline into a few common parts:
 
 
 The essential stuff you need to start doing the thing.
 Advanced topics. After you’ve got the essentials down, now lets talk about those advanced features, edge-cases, and so on.
 A project. I’ve found that for most folks, learning doesn’t really stick until you apply it. I include a project in every course, because it dramatically improves learning retention.
 
 
 Writing the ebook
 
 I start with the ebook, because it’s easier for me to write and edit and clarify my thoughts in text first.
 
 I write my ebooks as a collection of markdown files. Each chapter gets its own file.
 
 Markdown works really well for me because my books include a lot of code snippets, and being able to create highlighted code blocks is much easier in markdown than with other tools I’ve tried (like MS Word or Apple Pages).
 
 There’s no real trick here.
 
 I tend to write like I talk, and I talk very directly. That seems to work well for a lot of people. I’ve also got years of experience writing about technical topics, so I’ve become really efficient at it.
 
 Creating source code from the ebook
 
 Next, I got through every chapter of the ebook and create source code from it.
 
 Every chapter gets its own directory, and every section or snippet gets its own HTML file with the code able to be run live in a browser.
 
 When I’m done, the entire directory gets put up on GitHub for easy access and version control. Yes, that means anyone can view it. I don’t think it’s nearly as useful without the explanations around it, so I’m not particularly worried.
 
 Using the source code to create the video course
 
 The video course versions of my pocket guides are me talking about and explaining the source code.
 
 I copy/paste the text from the ebook into an email and pull it up on my phone as rough notes about the topics I’m supposed to follow. I used to try to follow a very precise script, but found students like it better when I’m a bit more casual in my videos.
 
 So now, I pull up the source code and record myself explaining how it works. I use Screenflow for this.
 
 Occasionally, I’ll have a section that doesn’t have any code.
 
 When that’s the case, I grab a few relevant images from Unsplash or put one or two big, relevant words on a slide in Keynote. Then I use Screenflow to record myself giving a short presentation.
 
 My videos are typically two to five minutes in length.
 
 Creating the book cover and course image
 
 I use the same cover for every book… almost.
 
 Every pocket guide cover has the same exact layout. I use a different background color based on the bundle the book is part of (blue for beginner, greenish for advanced, and purple for expert).
 
 I also include a nautical creature or artifact of some kind on the cover. I try to make it somewhat relevant, when I can. For example, the accessible components guide has a starfish, which looks a little bit like the “arms out person” icon often used as the A11Y logo.
 
 I get the icons from the Noun Project. I purchase the one-time, royalty-free license.
 
 Even though everyone’s moved on to Figma or whatever, I still design my covers in Sketch. I also use Sketch and the cover image to create the graphic that appears at the top of the sales page for the course on the website.
 
 Converting the ebook into PDF, EPUB, MOBI, and HTML formats
 
 So, I’ve got a bunch of markdown files, and I’ve got a cover image. How do I make them into an actual ebook?
 
 I built my own command line tool using Pandoc, wkhtmltopdf, and Calibre. I have an open sourced version of it available here (it’s probably in need of some dependency updates).
 
 The version I use is customized a bit to reuse certain files (like my “about the author” page) across all of the books, and can batch compile multiple books at once.
 
 Uploading the videos to a streaming service and get them captioned
 
 I host my videos on Vimeo Pro.
 
 Vimeo handles bandwidth aware streaming far better than I ever could. They let me customize the appearance of the embedded video player. They let me control where my videos can be embedded. They support closed captions. They support downloading.
 
 Vimeo is one of the easiest no-brainer business expenses I have.
 
 They have a cheaper “Vimeo Plus” plan with many of the same features, but it doesn’t support business users. If you’re selling paid content or creating business content, you need at least a Pro subscription.
 
 I use Rev for my captions.
 
 They’re affordable, have a quick turnaround, and are relatively accurate. Best of all for me, they have Vimeo API integration.
 
 This means I can log in to Rev and just select the videos I want captioned from a list. Rev automatically uploads and activates the captions to the videos for me. I used to have to manually upload each file to the matching video, and that sucked.
 
 Selling the course!
 
 Honestly, for me, the marketing aspect of an education business is far harder than the course creation.
 
 I spend a lot of my time writing articles like this, answering questions on Twitter, chatting with students, and appearing on podcasts. I don’t “buy ads.” I think that’s a waste of money.
 
 Most of my sales come from people who subscribe to my newsletter, like what I have to say, and eventually buy a course or ten.
 ⏰🦉 Early Bird Sale! Today through Monday, get 40% off registration in the next session of the Vanilla JS Academy.</content>
     </entry>
     <entry>
       <title>How To Create A Vanilla JavaScript Gantt Chart: Adding Task Editing Features (Part 2)</title>
         <link href="https://smashingmagazine.com/2022/06/vanilla-javascript-gantt-chart-part-2/"/>
       <updated>2022-06-23T11:30:00.000Z</updated>
       <content type="text">This article is a sponsored by Bryntum
 In Part 1 of this article, we developed a web component for an interactive Gantt Chart. Now we will enhance the Gantt Chart component with some interaction possibilities for editing the jobs: the job bars are made resizable by mouse-dragging, and we also implement an editing dialogue that can be used to modify the start and end dates of a job. In doing so, we will continue to work with Vanilla JS and Web Components. In the end, we will look at some JavaScript libraries that can greatly simplify the effort of developing a fully functional Gantt Chart.
 The following video shows what we are going to build in this article. First, we will add a drag handle on the right-hand side of each job that can be used for resizing the job bar (in the picture, it’s shown as a narrow gray vertical bar). In the next step, we will further extend the behavior of the jobs so that double-clicking on a job bar opens an editing dialogue.
 
 Building on this, the functionality of the chart can be extended further.
 JavaScript Gantt Chart By Bryntum
 Another example that is worth considering is the Bryntum Gantt library, “a super-fast and fully customizable Gantt chart suite.” 
 After downloading a free trial of the library, you will get a build folder with CSS and JavaScript files for creating interactive Gantt charts. You can integrate these files into your web app and then immediately configure your individual chart. A simple getting started guide provides a quick introduction to the component. For example, a basic chart could look like this with the Bryntum Gantt library:
 
 You will learn a lot about the numerous customization options in the full Bryntum Gantt documentation. You will also explore how the tool can be integrated with popular frameworks like Angular, React, Vue, and many more or how to organize the loading and saving of data (CRUD data management). 
 The examples section provides a visual overview of the various features of Bryntum Gantt.
 
 They also offer Bryntum Scheduler — a library for resource planning.
 JavaScript Gantt Chart By Webix
 With Webix Gantt, another commercial Gantt library with rich functionality is available. The uncomplicated steps for installing, creating, and configuring a Gantt chart are documented in detail.
 You can try out the tool in a full-screen interactive demo:
 
 Conclusion
 Gantt Charts are a valuable visualization for project management, planning, and task organization. There are many ways to integrate Gantt Charts into a web application. In the last two articles, we built an interactive Gantt Chart from scratch, and in doing so, we learned a lot about CSS grids, Web Components, and JavaScript events. If you have more complex requirements, it is worth looking at the commercial JS libraries, which are often very powerful.</content>
     </entry>
     <entry>
       <title>The Demo → Demo Loop</title>
         <link href="https://daverupert.com/2022/06/demo-to-demo-loop/"/>
       <updated>2022-06-22T15:12:00.000Z</updated>
       <content type="text">
   
   
 
 I’m 100% convinced that working demo-to-demo is the secret formula to making successful creative products.
 
 Video games have a strong history of ritualizing prototypes, play testing, and following the fun. There’s even some data that suggests prototypes increase your chances of success.
 Mentioned in Creativity, Inc and exemplified in the Frozen II documentary Into the Unknown, Pixar and Disney Animation use “dailies” to demo work in progress. Each animator shares their progress every day on the five seconds of film they’re responsible for.
 Outlined in Ken Kocienda’s book Creative Selection, Apple’s clandestine “Project Purple” that made the iPhone embraced frequent demos when creating iOS, but at the speed at which they were working “hallway demos” would happen hours or minutes after a previous demo.
 A core part of Design Sprints is getting a functioning prototype in front of users so your team can prove and vet an idea in a week rather than months.
 
 Or as IDEO famously put it…
 
 A prototype is worth a thousand meetings
 — IDEO
 
 Demos improve meetings. I’ve found “Demo First” meetings are life-giving because they start with something tangible, as opposed to discussing some hypothetical future scenario until time runs out and you have to schedule more meetings. Start meetings by showing progress. Better yet, a demo doesn’t always need to be a call or a meeting; thirty second screencasts are as good as gold.
 Demos improve workflows. Breaking up large tasks into “what can I demo next” is an exciting way to work. There’s always an immediate goal for me to find a demo-able checkpoint where I can get tactical feedback frequently. It can save a lot of rework and we can catch ourselves going down a bad path early.
 Demos improve project scoping. Demos and prototypes give you a good idea of what’s easy and what’s hard in the underlying system. Demoing often means you and others may be able to spot problems far off or find “fast paths” mid-flight. Often slight corrections or adjustments to the design can save weeks or months of work without sacrificing quality or user experience.
 If I’m honest, “Dailies” are probably overkill, but I wouldn’t hate it. I would certainly prefer daily demos over vague, ritualistic standup-speak. It’s worth noting here that not all days or weeks are bangers. What’s important is establishing an organizational practice of demo’ing and honing your skills and confidence at providing demos.
 The point is, do whatever is a right fit for your organization but never wait too long to demo. Allow room for ad hoc “hallway” demos to happen. And move the needle closer to daily.</content>
     </entry>
     <entry>
       <title>Inclusive design and post-it notes</title>
         <link href="https://gomakethings.com/inclusive-design-and-post-it-notes/"/>
       <updated>2022-06-22T14:30:00.000Z</updated>
       <content type="text">In my last real job before I started teaching JavaScript full time, the company I worked for sold API management software.
 
 Customers would get access to a white-label CMS they could use to document their APIs, register developers, and manage access to different endpoints.
 
 My job was to make that CMS look like their rest of their brand.
 
 Imagine something like WordPress or Squarespace, but with way fewer DIY hooks. I’d add their logo and brand colors and typeface, and create a homepage they were happy with.
 
 But one time, we had a customer who insisted on using a black background with white text and red links.
 
 If you’ve ever seen this combination, the links are hard to read for people with normal vision, and create real problems with people with colorblindness and other vision issues.
 
 I pointed this out to them.
 
 
 We don’t care. Our CEO prefers dark backgrounds.
 
 
 I pulled up their CEO’s own person website, and showed them that it in fact used a white background, not dark. “It doesn’t seem like he minds all that much.”
 
 
 We don’t care. Just do the dark background.
 
 
 I pulled in data about lawsuits related to accessibility issues under the Americans with Disabilities Act (ADA).
 
 
 Just do it for now and we’ll fix it later. We just need the CEO to sign off on it.
 
 
 I ended up fighting with my boss at the time about it (he was an exceedingly kind person, so it wasn’t much of a fight).
 
 He was very much a “just do what the customer asks” type customer-oriented person. I was viewing it as an ethical and professional obligation. As a professional web developer, I’m obligated to build things that are inclusive.
 
 A construction company can’t ignore the fire code just because a CEO doesn’t like the look of fire alarms. Why is what we do any different?
 
 I ultimately lost, and we shipped a dark and inaccessible background with a promise it would be fixed later. It never was.
 
 I wish I’d had this gem from Julianna Roswell in my back pocket at the time…
 
 
 Okay, just so we’re clear let’s write down who your willing to exclude with that decision.
 
 I’ve been using this approach to accessibility and inclusive design. I ask stakeholders to put it down in writing even if it’s on a post-it note which individual, group or community they are willing to exclude based on a decision they are making. I found it has a profound effect.
 
 
 There’s something powerful about making people put in words exactly who they’re willing to say “fuck off” to.
 
 I’ll be holding on to this trick for the next time I run into an issue like this.
 ⏰🦉 Early Bird Sale! Today through Monday, get 40% off registration in the next session of the Vanilla JS Academy.</content>
     </entry>
     <entry>
       <title>Different Ways to Write CSS in React</title>
         <link href="https://css-tricks.com/different-ways-to-write-css-in-react/"/>
       <updated>2022-06-22T14:24:37.000Z</updated>
       <content type="text">We’re all familiar with the standard way of linking up a stylesheet to the &lt;head&gt; of an HTML doc, right? That’s just one of several ways we’re able to write CSS. But what does it look like to style things in a single-page application (SPA), say in a React project?
 
 
 
 Turns out there are several ways to go about styling a React application. Some overlap with traditional styling, others not so much. But let’s count all the ways we can do it.
 
 
 
 
 
 
 
 Importing external stylesheets
 
 
 
 As the name suggests, React can import CSS files. The process is similar to how we link up CSS file in the HTML &lt;head&gt;:
 
 
 
 Create a new CSS file in your project directory.Write CSS.Import it into the React file.
 
 
 
 Like this:
 
 
 
 import &quot;./style.css&quot;;
 
 
 
 That usually goes at the top of the file where other imports happen:
 
 
 
 import { React } from &quot;react&quot;;
 import &quot;./Components/css/App.css&quot;;
 function App() {
   return (
     &lt;div className&#x3D;&quot;main&quot;&gt;
     &lt;/div&gt;
   );
 }
 export default App;
 
 
 
 In this example, a CSS file is imported into an App.js from the /Components/css folder.
 
 
 
 Write inline styles
 
 
 
 You may be used to hearing that inline styling isn’t all that great for maintainability and whatnot, but there are definitely situations (here’s one!) where it makes sense. And maintainability is less of an issue in React, as the CSS often already sits inside the same file anyway.
 
 
 
 This is a super simple example of inline styling in React:
 
 
 
 &lt;div className&#x3D;&quot;main&quot; style&#x3D;{{color:&quot;red&quot;}}&gt;
 
 
 
 A better approach, though, is to use objects:
 
 
 
 First, create an object that contains styles for different elements.Then add it to an element using the style attribute and then select the property to style.
 
 
 
 Let’s see that in context:
 
 
 
 import { React } from &quot;react&quot;;
 function App() {
   const styles &#x3D; {
     main: {
       backgroundColor: &quot;#f1f1f1&quot;,
       width: &quot;100%&quot;,
     },
     inputText: {
       padding: &quot;10px&quot;,
       color: &quot;red&quot;,
     },
   };
   return (
     &lt;div className&#x3D;&quot;main&quot; style&#x3D;{styles.main}&gt;
       &lt;input type&#x3D;&quot;text&quot; style&#x3D;{styles.inputText}&gt;&lt;/input&gt;
     &lt;/div&gt;
   );
 }
 export default App;
 
 
 
 This example contains a styles object containing two more objects, one for the .main class and the other for a text input, which contain style rules similar to what we’d expect to see in an external stylesheet. Those objects are then applied to the style attribute of elements that are in the returned markup.
 
 
 
 Note that curly brackets are used when referencing styles rather than the quotation marks we’d normally use in plain HTML.
 
 
 
 Use CSS Modules
 
 
 
 CSS Modules… what the heck happened to those, right? They have the benefit of locally scoped variables and can be used right alongside React. But what are they, again, exactly?
 
 
 
 Quoting the repo’s documentation:
 
 
 
 CSS Modules works by compiling individual CSS files into both CSS and data. The CSS output is normal, global CSS, which can be injected directly into the browser or concatenated together and written to a file for production use. The data is used to map the human-readable names you’ve used in the files to the globally-safe output CSS.
 
 
 
 In simpler terms, CSS Modules allows us to use the same class name in multiple files without clashes since each class name is given a unique programmatic name. This is especially useful in larger applications. Every class name is scoped locally to the specific component in which it is being imported.
 
 
 
 A CSS Module stylesheet is similar to a regular stylesheet, only with a different extension (e.g. styles.module.css). Here’s how they’re set up:
 
 
 
 Create a file with .module.css as the extension.Import that module into the React app (like we saw earlier)Add a className to an element or component and reference the particular style from the imported styles.
 
 
 
 Super simple example:
 
 
 
 /* styles.module.css */
 .font {
   color: #f00;
   font-size: 20px;
 }
 
 import { React } from &quot;react&quot;;
 import styles from &quot;./styles.module.css&quot;;
 function App() {
   return (
     &lt;h1 className&#x3D;{styles.heading}&gt;Hello World&lt;/h1&gt;
   );
 }
 export default App;
 
 
 
 Use styled-components
 
 
 
 Have you used styled-components? It’s quite popular and allows you to build custom components using actual CSS in your JavaScript. A styled-component is basically a React component with — get ready for it — styles. Some of the features include unique class names, dynamic styling and better management of the CSS as each component has its own separate styles.
 
 
 
 Install the styled-components npm package in the command line:
 
 
 
 npm install styled-components
 
 
 
 Next up, import it into the React app:
 
 
 
 import styled from &#x27;styled-components&#x27;
 
 
 
 Create a component and assign a styled property to it. Note the use of template literals denoted by backticks in the Wrapper object:
 
 
 
 import { React } from &quot;react&quot;;
 import styled from &quot;styled-components&quot;;
 function App() {
   const Wrapper &#x3D; styled.div&#x60;
     width: 100%;
     height: 100px;
     background-color: red;
     display: block;
   &#x60;;
   return &lt;Wrapper /&gt;;
 }
 export default App;
 
 
 
 The above Wrapper component will be rendered as a div that contains those styles.
 
 
 
 Conditional styling
 
 
 
 One of the advantages of styled-components is that the components themselves are functional, as in you can use props within the CSS. This opens the door up to conditional statements and changing styles based on a state or prop.
 
 
 
 Here’s a demo showing that off:
 
 
 
 
 
 
 
 Here, we are manipulating the div’s display property on the display state. This state is controlled by a button that toggles the div’s state when clicked. This, in turn, toggles between the styles of two different states.
 
 
 
 In inline if statements, we use a ? instead of the usual if/else syntax. The else part is after the semicolon. And remember to always call or use the state after it has been initialized. In that last demo, for example, the state should be above the Wrapper component’s styles.
 
 
 
 Happy React styling!
 
 
 
 That’s a wrap, folks! We looked at a handful of different ways to write styles in a React application. And it’s not like one is any better than the rest; the approach you use depends on the situation, of course. Hopefully now you’ve got a good understanding of them and know that you have a bunch of tools in your React styling arsenal.
 
 Different Ways to Write CSS in React originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>How To Test Your MVP Within 2 Weeks (Or Less)</title>
         <link href="https://smashingmagazine.com/2022/06/how-test-mvp-within-weeks/"/>
       <updated>2022-06-22T09:00:00.000Z</updated>
       <content type="text">As both an entrepreneur and designer, I understand the ways startup founders think. Most of them start a project with visions of a perfect product in their head. However, in reality, a well-performing product will likely look way different than an initial concept. Instead of seeking perfection from the outset, beginning with a Minimum Viable Product (MVP) is the smartest route to success. An MVP is a crucial part of the product design process and allows businesses to validate their idea at the minimum expense and time.
 But while a lot of startups already know that an MVP is essential, a majority of the startups that I have mentored over the years have categorically encountered the same problem across all sectors of their process: an MVPs time to market. It simply takes too long for an idea to get into the hands of a consumer.
 Because the truth is, if an MVP testing process takes longer than two weeks, you’re probably doing something wrong. From my experience, most startup product designers make these three common mistakes that lead to longer MVP testing times: 
 
 Using the wrong definition of a product value proposition;
 Underestimating the risks in a design hypothesis;
 Overloading the product with extra features.
 
 Why are startups missing these important steps in the process? I believe there are some tendencies among startups to pursue goals that fundamentally and often accidentally create a blind spot for these common mistakes. 
 Measure And Lean Your MVP
 In Eric Ries’s book The Lean Startup, his definition of the MVP states:
 “The minimum viable product is that version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort.”— Eric Ries
 
 When approaching a new idea, a startup founder might feel compelled to build upon the complexities of the function as a way of seeing their vision come to life rather than compose enough of the fundamental core components to create a hypothesis that they can then test. Ries differentiates between these approaches in his book when he discusses the process of measuring and leaning. 
 
 One of the major benefits of an MVP is lean production. It allows entrepreneurs and companies to produce a product that helps them prove their value proposition while also cutting costs. The clearer the theory, the stronger the MVP, and the more valuable information collected throughout the MVP process.
 An MVP is not only designed to determine the viability of a product’s value proposition but also to test the product’s technical elements and target audiences. Even a simple MVP or acceptance testing that inquires whether the intended audience is interested in using the product in the first place could be more helpful to the progress of a startup’s journey to success than establishing the perfect product right away. 
 The answers that an MVP can prove may be as simple as the MVP itself but highly valuable when continuing down a startup path that includes high-risk and particular investors. In the most general sense, a product is viable when it successfully fulfills a need in the market. And as easy as it seems, the most challenging part of this simple question is determining what exactly is necessary and what is not, as the features that you deem important to your product are, in fact, important. 
 Discernment and clarity are not as easy as it seems, and a large part of being concise when approaching your MVP is keeping the time to market low. This is why I find the importance of testing your MVP within two weeks or less to be a crucial factor in achieving successful results. The key to a highly effective MVP is defining the problem as clearly and specifically as possible.
 Renters Rewards: A Case Study
 Before we dive into some tips, let’s first take a look at a case study that features a mobile app startup. This app is designed to help people find a property to rent while also providing them with opportunities to earn cashback. 
 In this example, a startup saw an opportunity to develop a platform that lets people search for a rental, pay for the application, sign a lease, and continue to pay rent on the property indefinitely — all from one place. To take this product to the next level, we had an idea to offer people who pay on time a 1% cashback match if they pay before their rent due date.
 We wanted to test this hypothesis, so first, we had to develop our problem; in doing this, we could determine the approach and figure out a plan of action to receive valuable results in a couple of weeks. We decided that using the overarching idea of trying to test whether users would pay the rent early if they were offered a 1% cashback was not only too risky but a tough sell to landlords who could find it too costly upfront. 
 Instead, we thought of a way to apply a slightly incremental change to the user’s digital experience that could provide us with preliminary information as to whether or not the users would be interested in this opportunity. We sent out emails to 100 app users stating that if they paid their rent before the end of the month, we would credit $20 to their accounts.
 The test was simple, concise, and succinct enough to give us quick and valuable feedback on our hypothesis, which then could set the app up for a border change to the platform and infrastructure. Over 80% of the app users who received the email opted for the $20 cashback and paid their rent early, determining that if the app were to implement this in a more grand scope, their success would likely be very high. In short, the incentive worked, and it was a quick and low-cost MVP that proved it.
 Five Effective Strategies For Testing Your MVP
 Now that you understand the importance of a short MVP time to market, we can explore some useful tips that will help you put forth the most successful product for your startup.
 1. Properly Formulate A Product Value Proposition
 The first and most important tip for any startup looking to test an MVP is to determine the most effective value proposition for your product. In regards to the aforementioned case study, we talked about how the value proposition began — as a 1% rewards incentive for app users. After understanding how the value prop would be most effectively tested — solidifying a quick and concise testing procedure — we came up with an MVP that highlighted only the most necessary measurable metrics. 
 By determining that the same hypothesis could be discovered by providing 100 users with a $20 incentive as by giving 1% cashback to all users across the platform, we were able to concur that the value proposition for this case study filled a market need. 
 2. Identify The Vital MVP Features And Avoid Any Extra
 You might be noticing a pattern that keeping your MVP simple, clear, and to the point is very important to not only getting the most viable results from testing your MVP but getting you that information quickly. How do you hone in on that particular specificity that will move your MVP to a truly professional level? Start broad and then simplify. Don’t stop until you find the nucleus of the hypothesis that will help you determine the vital features of your product.
 Sure the end goal is to have a wide range of features — all the bells and whistles — that give your product the star power to rise above its competitors. But in the MVP, those extra features are unnecessary and could hinder your goals if you left them in when establishing the MVP for your product. 
 For example, landing pages can be a great MVP when looking for information about a potential product. If a user finds their way to your site because the product is relevant to them, a landing page offers a simple breakdown of the product offers and goals, as well as a sign-up field where the inquirer can add their email to learn more. This can help you determine your target audience without having to invest in a spectrum of functional features. It can also clue you what kinds of buyers are interested in your product and provide information about needs and interests that you didn’t even consider. 
 Instead of throwing up a landing page that reads “website coming soon” or an overly detailed investigation of your potential product, consider using it as a tool to link the vital features of your product with your target audience. It’s a simple way to collect customer feedback and start a killer email list. 
 3. Detect The Risks In Your Assumption
 There are many typical startup risks, and a lot of those flow down into the process of testing an MVP. Yet these risks are very important to take into consideration because if not, they could negatively impact the outcome of your MVP and cost you time and money. Here are a few that we have highlighted as the most significant risks to consider when testing your MVP:
 
 Users might not recognize a product’s valueFor example, if you have added a crypto trading option to your product, your users might not recognise the value in the product because they are already committed to third-party platforms like Coinbase and Revolut.
 Hard to acquire usersIf you don’t take into consideration the scope and breadth of your direct competition or inherently the market that you are pushing a product through, it is likely that you might not realise if your target audience is available or hard to acquire. Take, for instance, an app in Europe that helps you find a healthcare provider in your area. But it turns out that most major cities have one of their own, and they have more advanced knowledge and access to the target audience than you do. This means while your idea might be good, you will have an incredibly hard time converting users to your platform. But using these risks to help you determine your hypothesis and strengthen your MVP is the key to unlocking your product’s potential.
 Difficult to scale the productWhen conceptualizing your product, while you want to narrow in on an MVP that uses minimal product but provides maximum learning, the same isn’t so for understanding the scope of your product idea. Suppose you are setting up a moving company, and once your app receives over 100 requests, it cannot handle any more inquiries. In that case, you have a major salability issue that will never allow your product to meet its full potential. 
 Revenue-generation problemThis is a tough risk to understand while you are in the testing phases of your MVP and your startup. All projects of this nature are inherently risky and don’t often produce early financial results that put you in the green. But when you are looking at the cost-benefit analysis and going over P&amp;L statements from the first waves of use of your product, there should be a lot of green flags that point to profit. If your product P&amp;L sees nothing but red, it might be time to reevaluate your goals. 
 
 4. Pick The One Risk You’re Most Likely To Face And Work It Out
 Pick the one risk that is most likely to align with your MVP and take the time to work through it. Consider it alongside your value proposition and develop a product increment that will take you from hypothesis to MVP in a concise and effective way. Apply the lean approach to ensure you are not over-focusing on unnecessary features and push your MVP into the hands of customers quickly. 
 As in the renters rewards example, it didn’t make sense to take the time to update the app with a new interface or add extra features. Instead, the test was as simple and quick as setting up an email system, and the highest identified risk was easily mitigated by eliminating the need for extra costs or unavailable users. The results were simple but extremely viable, and the MVP time to market was less than two weeks. 
 5. Test Product Branding
 Testing is awesome and essential! But testing features isn’t the only way to effectively test your product and receive useful information. Branding is becoming increasingly important, as most consumers like to be emotionally tied to a product. That emotional element can often be the determining factor of success against a product’s competitors. If your startup is interested in rebranding or gaining more perspective on how their audience connects with their brand storytelling, using MVP testing is a great way to discover valuable data that can inform new and impactful brand strategies. 
 When it comes to branding, we want to assess the user-feedback-related risk of a negative perception of the brand. You could easily drive new branding with limited updates to the logo, landing page, and app images, while it is probably best to hold off on any hard investments like merch, billboards, and deck templates until the new strategy is proven.
 Two things come into play when determining what types of branding elements to test:
 
 Is it easy to produce/costly?
 Is it crucial? 
 
 User experience is becoming the number one factor in branding that determines a wide range of benefits for the product. Pay attention to colors, tone of voice, icons, illustrations, and landings — these are 99% of the things your users will see all the time. These are easy to create and are a touchpoint during every client experience. It is safe to say that these key branding elements play a decisive part in your update.
 How Best To Test Your MVP: The Bottomline
 There is any number of combinations of techniques and strategies that will work best for you and your startup based on the type of MVP you have and the best ways for you to test it. Deciding which ways are best for you, my advice is to start small and grow. It is easier to manage the testing of a hypothesis on a simple email blast or landing page brief than with a whole app features update that affects thousands of users.
  You can always grow and scale your MVP strategy as you gain more insight into your product. There will be many chances to apply MVP testing while you make your way through your startup roadmap.
 The only thing that matters is that you should approach the MVP test as a way of providing you verifiable insights on whether your final product will have a fair, if not successful, share of the market after it finally launches. </content>
     </entry>
     <entry>
       <title>Release Notes for Safari Technology Preview 147 with Safari 16 Features</title>
         <link href="https://webkit.org/blog/12960/release-notes-for-safari-technology-preview-147-with-safari-16-features/"/>
       <updated>2022-06-21T20:40:45.000Z</updated>
       <content type="text">Safari Technology Preview Release 147 is now available for download for macOS Monterey 12.3 or later and macOS Ventura beta. Updates to Safari Technology Preview are no longer available for macOS Big Sur. If you already have Safari Technology Preview installed, you can update in the Software Update pane of System Preferences on macOS Monterey, or System Settings under General → Software Update on macOS Ventura.
 Note: Shared Tab Groups and syncing for Tab Groups, Website Settings, and Web Extensions are not enabled in this release.
 Many of the new Safari 16 features are now available in Safari Technology Preview 147:
 Live Text. Select and interact with text in videos or translate text in images on the web in macOS Ventura beta on Apple Silicon-based Macs.
 Web technologies. Experience and test the HTML, CSS, JavaScript, and other web technologies that are available in Safari 16 Beta and included in previous Safari Technology Preview releases.
 Web Push. Send notifications to people who opt-in on your website or web app with Safari Technology Preview on macOS Ventura beta.
 Passkeys. Preview the new type of phishing-resistant credential that makes signing in to websites safer and easier. Available through Safari’s WebAuthn platform authenticator. To learn more about passkeys, see Meet passkeys.
 Improved Safari Web Extensions. Test out API improvements including the ability to open a Safari Web Extension popover programmatically.
 Web Inspector Extensions. Build custom tooling or convert existing developer tools extensions to use in Web Inspector.
 Flexbox Inspector. Use the new visualization overlay in Web Inspector to help you more quickly and easily understand the layout of elements with Flexbox. It marks both the free space and gaps between flex items to reveal how they affect the result.
 If you see bugs or unexpected behavior with the interface of Safari Technology Preview, please file feedback with Apple’s Feedback Assistant. If you come across an implementation bug in web technology, Web Inspector, or have a request, please file a ticket on the WebKit bug tracker.</content>
     </entry>
     <entry>
       <title>Data and Definitions</title>
         <link href="https://stratechery.com/2022/data-and-definitions/"/>
       <updated>2022-06-21T15:31:43.000Z</updated>
       <content type="text">Last week the German Bundeskartellamt (“Federal Cartel Office”) announced in a press release:
 
   The Bundeskartellamt has initiated a proceeding against the technology company Apple to review under competition law its tracking rules and the App Tracking Transparency Framework. In particular, Apple’s rules have raised the initial suspicion of self-preferencing and/or impediment of other companies, which will be examined in the proceeding.
 
 The press release quoted Andreas Mundt, the President of the Bundeskartellamt, who stated:
 
   We welcome business models which use data carefully and give users choice as to how their data are used. A corporation like Apple which is in a position to unilaterally set rules for its ecosystem, in particular for its app store, should make pro-competitive rules. We have reason to doubt that this is the case when we see that Apple’s rules apply to third parties, but not to Apple itself. This would allow Apple to give preference to its own offers or impede other companies.
 
 The press release continues:
 
   Already based on the applicable legislation, and irrespective of Apple’s App Tracking Transparency Framework, all apps have to ask for their users’ consent to track their data. Apple’s rules now also make tracking conditional on the users’ consent to the use and combination of their data in a dialogue popping up when an app not made by Apple is started for the first time, in addition to the already existing dialogue requesting such consent from users. The Identifier for Advertisers, classified as tracking, which is important to the advertising industry and made available by Apple to identify devices, is also subject to this new rule. These rules apparently do not affect Apple when using and combining user data from its own ecosystem. While users can also restrict Apple from using their data for personalised advertising, the Bundeskartellamt’s preliminary findings indicate that Apple is not subject to the new and additional rules of the App Tracking Transparency Framework.
 
 John Gruber disagrees at Daring Fireball:
 
   I think this is a profound misunderstanding of what Apple is doing, and how Apple is benefiting indirectly from ATT. Apple’s privacy and tracking rules do apply to itself. Apple’s own apps don’t show the track-you-across-other-apps permission alert not because Apple has exempted itself but because Apple’s own apps don’t track you across other apps. Apple’s own apps show privacy report cards in the App Store, too…
   If you want to argue that Apple engaged in this entire ATT endeavor to benefit its own Search Ads platform, that’s plausible too. But if Apple actually cared more about maximizing Search Ads revenue than it does user privacy, wouldn’t they have just engaged in actual user tracking? The Bundeskartellamt perspective here completely disregards the idea that surveillance advertising is inherently unethical and Apple has studiously avoided it for that reason, despite the fact that it has proven to be wildly profitable for large platforms.
 
 This strikes me as a situation where Gruber — my co-host for Dithering — is right on the details, even as the Bundeskartellamt is right on the overall thrust of the argument. The distinction comes down to definitions.
 
 It’s striking in retrospect how little time Apple spent publicly discussing its App Tracking Transparency (ATT) initiative — a mere 20 seconds at WWDC 2020, wedged in between updates about camera-in-use indicators and privacy labels in the App Store:
 
 
   Next, let’s talk about tracking. Safari’s Intelligent Tracking Prevention has been really successful on the web, and this year, we wanted to help you with tracking in apps. We believe tracking should always be transparent, and under your control, so moving forward, App Store policy will require apps to ask before tracking you across apps and websites owned by other companies.
 
 These 20 seconds led, 19 months later, to Meta announcing a $10 billion revenue shortfall, the largest but by no means only significant retrenchment in the online advertising space. Not everyone was hurt, though: Google and Amazon, in particular, have seen their share of digital advertising increase, and, as Gruber admitted, Apple has benefited as well; the Financial Times reported last fall:
 
   Apple’s advertising business has more than tripled its market share in the six months after it introduced privacy changes to iPhones that obstructed rivals, including Facebook, from targeting ads at consumers. The in-house business, called Search Ads, offers sponsored slots in the App Store that appear above search results. Users who search for “Snapchat”, for example, might see TikTok as the first result on their screen. Branch, which measures the effectiveness of mobile marketing, said Apple’s in-house business is now responsible for 58 per cent of all iPhone app downloads that result from clicking on an advert. A year ago, its share was 17 per cent.
 
 These numbers, derived as they are from app analytics companies, are certainly fuzzy, but they are the best we have given that Apple doesn’t break out revenue numbers for its advertising business; they are also from last fall, before ATT really began to bite. They also exclude the revenue Apple earns from Google for being the default search engine for Safari, and while Google’s earnings indicate YouTube has suffered from ATT, search has more than made up for it.
 I explained in depth why these big companies have benefitted from ATT in February’s Digital Advertising in 2022; I wrote in the context of Amazon specifically:
 
   Amazon also has data on its users, and it is free to collect as much of it as it likes, and leverage it however it wishes when it comes to selling ads. This is because all of Amazon’s data collection, ad targeting, and conversion happen on the same platform — Amazon.com, or the Amazon app. ATT only restricts third party data sharing, which means it doesn’t affect Amazon at all…
   That is not to say that ATT didn’t have an effect on Amazon: I noted above that Snap’s business did better than expected in part because its business wasn’t dominated by direct response advertising to the extent that Facebook’s was, and that more advertising money flowed into other types of advertising. This almost certainly made a difference for Amazon as well: one of the most affected areas of Facebook advertising was e-commerce; if you are an e-commerce seller whose Shopify store powered-by Facebook ads was suddenly under-performing thanks to ATT, then the natural response is to shift products and advertising spend to Amazon.
 
 This is where definitions matter. The opening paragraph of Apple’s Advertising &amp; Policy page, housed under the “apple.com/legal” directory, states:
 
   Ads that are delivered by Apple’s advertising platform may appear on the App Store, Apple News, and Stocks. Apple’s advertising platform does not track you, meaning that it does not link user or device data collected from our apps with user or device data collected from third parties for targeted advertising or advertising measurement purposes, and does not share user or device data with data brokers.
 
 I note the URL path for a reason: the second sentence of this paragraph has multiple carefully selected words — and those word choices not only impact the first sentence, but may, soon enough, lead to its expansion. Specifically:
 “Meaning”
 Apple’s advertising platform does not track you, meaning that it does not link user or device data collected from our apps with user or device data collected from third parties for targeted advertising or advertising measurement purposes, and does not share user or device data with data brokers.
 “Tracking” is not a neutral term! My strong suspicion — confirmed by anecdata — is that a lot of the most ardent defenders of Apple’s ATT policy are against targeted advertising as a category, which is to say they are against companies collecting data and using that data to target ads. For these folks I would imagine tracking means exactly that: the collection and use of data to target ads. That certainly seems to align with the definition of “track” from macOS’s built-in dictionary: “Follow the course or trail of (someone or something), typically in order to find them or note their location at various points”.
 However, this is not Apple’s definition: tracking is only when data Apple collects is linked with data from third parties for targeted advertising or measurement, or when data is shared/sold to data brokers. In other words, data that Apple collects and uses for advertising is, according to Apple, not tracking; the privacy policy helpfully lays out exactly what that data is (thanks lawyers!):
 
   We create segments, which are groups of people who share similar characteristics, and use these groups for delivering targeted ads. Information about you may be used to determine which segments you’re assigned to, and thus, which ads you receive. To protect your privacy, targeted ads are delivered only if more than 5,000 people meet the targeting criteria.
   We may use information such as the following to assign you to segments:
 
 Account Information: Your name, address, age, gender, and devices registered to your Apple ID account. Information such as your first name in your Apple ID registration page or salutation in your Apple ID account may be used to derive your gender. You can update your account information on the Apple ID website.
 
 
 Downloads, Purchases &amp; Subscriptions: The music, movies, books, TV shows, and apps you download, as well as any in-app purchases and subscriptions. We don’t allow targeting based on downloads of a specific app or purchases within a specific app (including subscriptions) from the App Store, unless the targeting is done by that app’s developer.
 
 
 Apple News and Stocks: The topics and categories of the stories you read and the publications you follow, subscribe to, or turn on notifications from.
 
 
 Advertising: Your interactions with ads delivered by Apple’s advertising platform.
 
 
 When selecting which ad to display from multiple ads for which you are eligible, we may use some of the above-mentioned information, as well as your App Store searches and browsing activity, to determine which ad is likely to be most relevant to you. App Store browsing activity includes the content and apps you tap and view while browsing the App Store. This information is aggregated across users so that it does not identify you. We may also use local, on-device processing to select which ad to display, using information stored on your device, such as the apps you frequently open.
 
 Just to put a fine point on this: according to Apple’s definition, collecting demographic information, downloads/purchases/subscriptions, and browsing behavior in Apple’s apps, and using that data to deliver targeted ads, is not tracking, because all of the data is Apple’s (and by extension, neither is Google’s collection and use of data from Safari search results, or Amazon’s collection and use of data from its app; however, a developer associating an in-app purchase with a Facebook ad is).
 “And”
 Apple’s advertising platform does not track you, meaning that it does not link user or device data collected from our apps with user or device data collected from third parties for targeted advertising or advertising measurement purposes, and does not share user or device data with data brokers.
 One thing should be made clear: there has been a lot of bad behavior in the digital ad industry. A particularly vivid example was reported by the Wall Street Journal last month:
 
   The precise movements of millions of users of the gay-dating app Grindr were collected from a digital advertising network and made available for sale, according to people familiar with the matter. The information was available for sale since at least 2017, and historical data may still be obtainable, the people said. Grindr two years ago cut off the flow of location data to any ad networks, ending the possibility of such data collection today, the company said.
   The commercial availability of the personal information, which hasn’t been previously reported, illustrates the thriving market for at-times intimate details about users that can be harvested from mobile devices. A U.S. Catholic official last year was outed as a Grindr user in a high-profile incident that involved analysis of similar data. National-security officials have also indicated concern about the issue: The Grindr data were used as part of a demonstration for various U.S. government agencies about the intelligence risks from commercially available information, according to a person who was involved in the presentation.
   Clients of a mobile-advertising company have for years been able to purchase bulk phone-movement data that included many Grindr users, said people familiar with the matter. The data didn’t contain personal information such as names or phone numbers. But the Grindr data were in some cases detailed enough to infer things like romantic encounters between specific users based on their device’s proximity to one another, as well as identify clues to people’s identities such as their workplaces and home addresses based on their patterns, habits and routines, people familiar with the data said.
 
 It’s difficult to defend any aspect of this, and this isn’t even a worst case scenario: there are plenty of unscrupulous apps and ad networks that include explicit Personal Identifiable Information (PII) in these data sales/transfers as well; as Eric Suefert noted in 2020, the industry has had this reckoning coming for a very long time.
 That, though, is why the “and” from Apple is so meaningful; here is the sentence again:
 
   Apple’s advertising platform does not track you, meaning that it does not link user or device data collected from our apps with user or device data collected from third parties for targeted advertising or advertising measurement purposes, and does not share user or device data with data brokers.
 
 This definition conflates two very different things: linking and sharing. The distinction between the two undergirded a regular feature of Meta CEO Mark Zuckerberg’s appearances in Congressional hearings; here is a representative exchange between Senator Edward Markey and Zuckerberg in 2018:
 
 
   Should Facebook get clear permission from users before selling or sharing sensitive information about your health, your finances, your relationships? Should you have to get their permission?…
   Senator…I want to be clear: we don’t sell information. So regardless of whether we get permission to do that, that’s just not a thing we’re going to do.
 
 Meta doesn’t sell data; it collects it, and the third parties that leverage the company’s platforms for advertising very much prefer it that way. PII is like radioactive material: it’s very valuable, and can certainly be leveraged, but it’s also difficult to handle and can be dangerous to not just the users identified but to the companies holding it. The way Meta works is that its collective advertising base has effectively deputized the company to collect data on their behalf; that data is not exposed directly, but is instead used to deliver targeted advertisements that are by-and-large bought not by targeting specific criteria, but rather by specifying desired results: app installs, e-commerce conversions, etc. Everything user-related is, to the companies buying the ads, a complete black box.
 This is where linking comes in: apps or websites that leverage Facebook advertising (or any other relevant advertising platform, like Snap) include a Facebook SDK or Pixel that tracks installs, sales, etc., and sends that data to Meta where it can be linked to an ad that was shown to that user. Again, this is completely invisible to the developer or merchant; technically they are sending data to Meta, since the conversion data was collected in their app or on their website, but in reality it is Meta collecting that data and sending it to themselves.
 The reason why developers and merchants are happy with this arrangement is that advertising is a scale business: you need a lot of data and a lot of customers to make targeted advertising work, and no single developer or website has as much scale as, say, a Google or an Amazon; Meta et al enable all of these smaller developers and merchants to effectively work together without having to know each other, or share data.
 
 You can, to be clear, object to this arrangement, but it’s worth pointing out that this is very different than selling or sharing data with data brokers; all of the data is in one place and one place only, which is broadly similar to the situation with Google or Amazon (or Apple, as I’ll get to in a moment). The big difference is that Meta doesn’t own all of the customer touch points: whereas a Meta advertiser may own their own Shopify website, an Amazon advertiser has to list their goods on Amazon’s site, with all of the loss of control that entails. Apple’s definition, though, lumps Meta’s approach (which again, is representative of other platforms like Snap) in with the worst actors in the space.
 “Our”
 Apple’s advertising platform does not track you, meaning that it does not link user or device data collected from our apps with user or device data collected from third parties for targeted advertising or advertising measurement purposes, and does not share user or device data with data brokers.
 To the extent you think that the Bundeskartellamt is right, then it is this word that is the most problematic definition of all. One would assume that “our” means Apple-created apps, like News or Stocks: just as Amazon collects data from the Amazon app, of course Apple collects data from its own apps. The actual definition, though, is much more expansive; go back to the Epic trial and the exchange I recounted in App Store Arguments:
 
   The argument that Judge Gonzales Rogers seemed the most interested in pursuing was one that Epic de-emphasized: Apple’s anti-steering provisions which prevent an app from telling a customer that they can go elsewhere to make a purchase. Apple’s argument, in this case presented by Cook, goes like this:
   
   This analogy doesn’t work for all kinds of reasons; Apple’s ban is like Best Buy not allowing products in the store to have a website listed in the instruction manual that happens to sell the same products. In fact, as Nilay Patel noted, Apple does exactly this!
   
   The point of this Article, though, is not necessarily to refute arguments, but rather to highlight them, and for me this was the most illuminating part of this case. The only way that this analogy makes sense is if Apple believes that it owns every app on the iPhone, or, to be more precise, that the iPhone is the store, and apps in the store can never leave.
 
 Let me be precise in a different way that is relevant to this Article; Apple doesn’t particularly care about or claim ownership of the content of an app on the iPhone, but:
 
 Apple insists that every app on the iPhone use its payment system for digital content
 Apple treats all transactions made through its payment system as Apple data
 Ergo, all transactions for digital content on the iPhone are Apple data
 
 The end result looks something like this — i.e. strikingly similar to Facebook, but with App Store payments attached:
 
 Here’s the key point: when it comes to digital advertising, particularly for the games that make up the vast majority of the app advertising industry, transaction data is all that matters. All of the data that any platform collects, whether that be Meta, Snap, Google, etc. is insignificant compared to whether or not a specific ad led to a specific purchase, not just in direct response to said ad, but also over the lifetime of the consumer’s usage of said app. That is the data that Apple cut off with ATT (by barring developers from linking it to their ad spend), and it is the same data that Apple has declared is their own first party data, and thus not subject to its ban on “tracking.”
 This, needless to say, is where legitimate questions about self-preferencing come to the forefront. Developers who want to link conversion data with Facebook are banned from doing so, while they have no choice but to share that data with Apple because Apple controls app installation via the App Store; this strikes me as a clear example of the President of the Bundeskartellamt’s claim that “Apple’s rules apply to third parties, but not to Apple itself”.
 
 I have been very clear that I disagree with those who want to ban all targeted advertising; I believe that targeted advertising is an essential ingredient in a new Internet economy that provides opportunities to small businesses serving niches that are only viable when the world is your market. After all, people who might love your product need some way to know that your product exists, and what is compelling about platforms like Facebook is that it completely leveled the advertising playing field: suddenly small businesses had the same tools and opportunities to advertise as the biggest companies in the world. At the same time, I understand and acknowledge those who disagree with the concept on principle.
 What is frustrating about the debate about ATT, though, is that Apple presents itself as a representative of the latter, with its constant declarations that privacy is a human right, and advertisements that lean heavily into the (truly problematic) world of data brokering, even as it builds its own targeting advertising business. Gruber asked me on this morning’s episode of Dithering whether or not I would feel better about ATT if Apple weren’t itself doing targeted advertising, and the answer is yes: I would still be disappointed about the impact on the Internet economy, but at least it wouldn’t be so blatantly anti-competitive.
 Apple, to its credit, has made moves that very much align with its privacy rhetoric by cleaning up some of the worst abuses by apps, including significantly fine-tuning location permissions, providing a new weather framework that makes it significantly cheaper to build a weather app (reducing the incentive to monetize by selling location data), and increasing transparency around data collection. Moreover, at this year’s WWDC the company introduced significant enhancements to SKAdNetwork that should make it easier for developers and platforms like Facebook to re-build their advertising capabilities.
 At the same time, an increasing number of signals suggest that Apple is set to significantly expand their own advertising business; an obvious product to build would be an ad network that runs in apps (given that these apps run on the iPhone, Apple would in this scenario claim that collecting data about who saw what ad would be first party data, just like transactions are). Yes, Apple tried and failed to build an ad network previously, but a big reason that effort failed is because Apple didn’t collect the sort of data necessary to make it succeed.
 What has changed is not just Apple, but also the data that matters: when iAd launched in 2010, digital advertising ran like people still think it does, leveraging relatively broad demographic categories and contextual information to show a hopefully relevant ad;1 what matters today is linking an ad to a transaction, and Apple has positioned itself to have perfect knowledge of both, even as it denies others the same opportunity.
 This is the era when Facebook earned its reputation for being far too cavalier with user data; Facebook was also the company that built the modern advertising approach that depends on linking data instead of sharing it. ↩</content>
     </entry>
     <entry>
       <title>Pirates, projects, and the best damn JavaScript workshop on the interweb</title>
         <link href="https://gomakethings.com/pirates-projects-and-the-best-damn-javascript-workshop-on-the-interweb/"/>
       <updated>2022-06-21T14:30:00.000Z</updated>
       <content type="text">One of the hardest things about learning how to code is being able to actually put all of your learning together to build a thing.
 
 You can memorize JavaScript methods and browser API and understand the fundamentals, but building a project from scratch is hard.
 
 So, a few years ago, I put together the Vanilla JS Academy, an online JavaScript workshop.
 
 A new session starts on July 18, and enrollment opens up on Monday. A lot’s changed with the program over the last year, and today, I wanted to tell you about it.
 
 How it works
 
 Over the course of the 6-week workshop, you’ll get access to over 70 lessons and work on 18 projects.
 
 
 Every other day, you get a project to work on, 1-3 short lessons that provide some background, and a template to help you get started.
 On the in-between days, I share my approach to the project and some of the common challenges and “gotchas” that students often run into.
 
 
 If you get stuck, there’s a private Slack channel exclusively for students. I hold live video office hours every two weeks, where you can ask me questions in real time, share code, and work through bigger challenges.
 
 Two different workshops
 
 When Academy started, it was just a single workshop. Today, there are two different programs.
 
 
 Vanilla JS Essentials. Learn the fundamentals of DOM manipulation and injection, working with APIs and asynchronous JavaScript, managing data with arrays and objects, and saving state with browser storage.
 Structure &amp; Scale. Learn how to structure and organize your code as your projects grow and scale. Dig into concepts like utility libraries, object-oriented programing, JavaScript classes, web components, ES modules, and service workers.
 
 
 The workshops run concurrently, and are designed to be taken one at a time.
 
 I’ve had students sign up for both at the same time and it never works out well. The stuff you do in Structure &amp; Scale assumes you’ve already have the skills you’d learn in Essentials, and the volume of work is just too high to take both together.
 
 Structure &amp; Scale has been completely redesigned
 
 The Structure &amp; Scale workshop is relatively new.
 
 Based on feedback from past participants, I completely redesigned the program for the upcoming session. It has new projects, new lessons, and a stronger focus on how to actually manage code bases as they get bigger.
 
 This session is very pirate themed. You’ll build…
 
 
 A JS library for rolling dice of different sizes
 A Treasure Chest library for managing all of your loot
 A web component for adding digital dice to a UI with just an HTML element
 Seven Seas, an offline-ready travel app for pirates
 
 
 You’ll also learn…
 
 
 How to more easily write libraries with JS classes
 How to modularize a growing code base
 How to add hooks other developers can use to extend the functionality of your code
 How to improve performance and reliability with Service Workers
 How to use build tools to bundle your code
 
 
 If you’re growing into a more senior role (or looking to make the jump), this is an awesome program to level-up your skills!
 
 People seem to like it
 
 Here’s some of the nice things students have said about the program…
 
 
 	
 		
 	
 	
 		Making myself a little web tool and using a whole range of stuff that Chris Ferdinandi’s Vanilla JS Academy taught me.I struggled with JavaScript for a decade so I really would recommend it for anyone who needs a big friendly confidence-booster.- Laura Kalbag
 
 
 	
 		
 	
 	
 		Tired of tutorials? Want to really dig in and work with other developers on how to become a vanilla JS badass? I can&#x27;t tell you how much I&#x27;ve learned from Chris Ferdinandi and his Vanilla JS Academy.- Ben Rudolph
 
 
 	
 		
 	
 	
 		I was confident that I&#x27;d get something valuable from the Academy. The only only hesitation I felt was from knowing how many false starts I had with JS in the past and questioning my own commitment.With Academy, something finally clicked in regards to both my ability to understand and write JavaScript.I now feel pretty confident in reading other&#x27;s scripts and figuring out what&#x27;s going on. That&#x27;s in large part due to your clear bite-sized lessons and the active discussions in Slack that quickly surfaced questions and issues that I was also experiencing. Seeing how the same problem was solved by so many others was also helpful in letting go of the pressure to get things right and shift my focus on getting things to work and adjusting later as needed.I really liked how the lessons were broken up into quickly consumable chunks. I loved having a clean, unopinionated project file to start from that I could wreak design havoc on. :)The Slack community was priceless.I&#x27;ve taken other online classes that include a slack community and never found them to be quite as welcoming and my involvement felt much more transactional. In this community, I enjoyed seeing everyone&#x27;s dedication and investment in each other. It was welcoming, inclusive, helpful, and encouraging. That made a huge difference.- Leticia O&#x27;Neill
 
 
 	
 		
 	
 	
 		Best investment and course I have taken. If you want a bite sized course that will hold you accountable take this course. I have reduced the amount I Google and use Stackexchange by 50% and actually feel like I understand what I am coding.- Walter Jenkins
 
 
 	
 		
 	
 	
 		I&#x27;m not a beginner in JavaScript but I&#x27;m really loving the Academy because I still learn new things and best practices. I think knowing best practices distinguishes a mid-level dev from a senior web dev.- Maria Blair
 
 
 How can you sign up?
 
 Registration opens up on Monday, June 27.
 
 The first week of registration, I run an Early Bird Sale, with 40-percent off registration. If you’re thinking about joining, next week is definitely the week to do it.
 ⏰🦉 Early Bird Sale! Today through Monday, get 40% off registration in the next session of the Vanilla JS Academy.</content>
     </entry>
     <entry>
       <title>Creating a Particles Galaxy with Three.js</title>
         <link href="https://tympanus.net/codrops/2022/06/21/creating-a-particles-galaxy-with-three-js/"/>
       <updated>2022-06-21T14:19:37.000Z</updated>
       <content type="text">
 
 
 
 
 
 In this new ALL YOUR HTML coding session we will look into recreating the particles galaxy from Viverse using Three.js.
 
 
 
 Original: https://www.viverse.com/
 
 
 
 Made by: https://ohzi.io/
 
 
 
 This coding session was streamed live on June 19, 2022.
 
 
 
 Check out the live demo.
 
 
 
 Image credit goes to https://www.instagram.com/tre.zen/
 
 
 
 Support: https://www.patreon.com/allyourhtml 
 
 
 
 Setup: https://gist.github.com/akella/a19954… 
 The post Creating a Particles Galaxy with Three.js appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>Things I Wish I’d Known Earlier In My Career</title>
         <link href="https://smashingmagazine.com/2022/06/things-to-know-earlier-in-your-career/"/>
       <updated>2022-06-21T13:00:00.000Z</updated>
       <content type="text">We often focus on the latest techniques and tooling, trying to optimize our workflows and processes. However, in the end, every single person has their own goals and ambitions, and too often, our individual goals are left far behind the company’s goals and product development roadmaps.
 Over the last few weeks, I’ve been receiving a few emails asking about the right ways to negotiate salary, get promoted and notice red flags early. So I thought that having spent around 20 years in this industry, it might be a good idea to write down a few personal observations that I wish somebody had told me earlier in my career.
 A Bit Of A Backstory
 I started designing and building websites around 1999, just around the glorious era of VRML and Macromedia Shockwave. At the time, I had started my first job in a small digital agency where I worked for a few years all around PHP and shiny new CSS layout — mostly having barely a clue about what I was doing. I was studying computer science, while also earning a bit of money as a freelancer. I did so until early 2006 when I completed my studies and moved on to co-found Smashing Magazine. It seemed to be just a hobby initially, but eventually with the magazine came many products, from books to conferences to the job board and Smashing Membership.
 Throughout the years, I would write and edit articles about the web day and night, and I absolutely loved every single bit of it. When people asked me about how many hours do I work a week, I felt a bit confused because all of it didn&#x27;t feel like work at all. This hasn&#x27;t changed a bit today. Yet around mid 2010s I felt like I&#x27;m missing actual front-end work on actual projects. So around 2014, I slowly started working on projects with companies small and large, mostly as a contractor, and often on long-term projects. Eventually, I launched a video course on interface design, and am currently writing a book that I hope to finish by late 2022.
 Over all these years, I’ve been working with dozens of companies and organizations, in various roles and projects, from front-end optimization to UX and quite a bit in-between. Over all these years, I’ve been reminding myself of a few observations that I made somewhere between chains of emails and Slack conversations, backroom meetings and stories from colleagues and friends whom I had a pleasure to be working.
 We’ll start with one of the most mundane issues that I&#x27;ve been noticing repeating over and over again: telling about salary expectations too early in the interview process.
 
 1. Never Tell Your Salary Expectations First
 Usually it doesn’t take long until a recruiter or HR manager asks you about your salary expectations or estimated total cost in an interview. Of course, you’d like to make a good first impression, and my first instinct has always been to provide a slightly discounted ballpark figure.
 What I didn’t realize for many years is that many recruiters and HR managers have their own KPIs, and often they earn premiums based on how far they can negotiate down the industry’s average salaries or total cost of the engagement. That’s why it’s not uncommon to see experienced professionals being hired for “lower” positions, with a lower salary but similar responsibilities and similar scope of work.
 Salary isn’t everything, but it is important. Be very careful and strategic when negotiating your salary. If you are asked about your salary expectations, politely decline that question and ask for an offer first. You first need to assess the complexity of the project and the inner workings and expertise of the team; otherwise, your ballpark figure is just guesswork and often an inaccurate one. Most importantly, never provide a number right away, and defer it to an email that you’ll be sending later. This gives you a bit of time to think and avoid estimates that you might regret later.
 Some HR managers will insist on some ballpark figures early. That’s why it’s critical to do your research upfront. Don’t focus too much on your current salary as it might not account for inflation, gender gap, and other costs. Instead, explore what a reasonable salary for your role is and for your level of experience in your region — with Glassdoor and similar sites. You might even increase it a bit to leave enough room to negotiate later.
 Even with these preparations in place, though, always avoid exact numbers and provide a range that feels comfortable for you. In fact, a good way of pricing your work is by asking yourself what salary would make you enthusiastic enough to be heavily invested in the product and deliver your best work.
 2. Switching Companies Is How You Make More Money
 Sometimes the same recruiters who signed up with you one year, getting in touch just a year later, with another exciting opportunity from another exciting company. That’s wonderful for climbing the career ladder and salary increases, yet as a result, often you end up without any ownership at all — just because you don&#x27;t get enough time to contribute and see the impact of your contributions in real projects. And to me, that’s always been very important, and more important than the salary.
 In the industry, it’s common to be jumping between companies every 12–18 months, and in fact that’s how you usually would make more money. Sadly, what I see as a result is that when some of my colleagues look back at their career, they realize that it’s difficult for them to feel some sense of significant achievement and pride for the incredible work done — mostly because they never had a change to really finish what they started. Undoubtedly, these achievements reflecting in the incredible wall of incredible companies on your CV, but this often doesn’t turn into some deep feeling of self-realization.
 If you feel valued and appreciated, the team is great and salary is fine, consider staying in the same company for around 2–3 years. Feel free to take job interviews in-between, of course, but you do need a bit of time to become fluent and proficient in a specific company&#x27;s context. Once you are, either grow in your company or do switch to another challenge. Just make sure it doesn’t feel like it’s too early.
 3. Pay Attention To Your Job Title
 We often think that titles don’t matter as much as the work that we are actually doing. Yet throughout the years, I’ve been proved wrong over and over again. Titles do matter. In fact, in many large companies and organizations, titles define salary levels. It might feel acceptable to start from a lower position and grow your way into a senior position, yet in practice, it’s much more difficult than I have anticipated (more on that later).
 Be very careful and strategic about the role that you apply for and the role that you are given in the contract. There are significant differences between UX Designer and Senior UX Designer roles, for example.
 At first, this seems very obvious, but for a long time, I didn’t really pay attention to it until I noticed significant differences in salaries between people doing similar work but hired for different positions. This also applies to responsibilities and, most importantly, your position in the team.
 4. Keep Record Of Your Achievements
 So why is it so difficult to grow from one role into another? Well, we often think that if we just work hard enough, we will get noticed, and we will be promoted to senior positions, with the salary and responsibilities adjusted accordingly.
 However, your growth heavily depends on how attentive and caring your managers are. Sometimes you might indeed get noticed, but more often than not, you won’t. Managers change, companies evolve, teams get restructured, and as these changes are happening, you might not ever get to be promoted. Because everything is shifting all the time, and with all the managers coming and leaving, it’s difficult to stay on track around the right time for your promotion or raise. That’s why many people choose a safe strategy and raise salary by jumping from one company to another every 18-24 months.
 Many companies have feedback loops (“one-on-one” s, 360 reviews, etc.), where you get feedback from your team and from your managers a few times a year. This is a great opportunity to raise the question about your personal growth and what you need to do to move to the next level. Use this opportunity in your favor.
 Maintain an up-to-date record of your achievements, milestones, projects and learnings, workshops and trainings you’ve attended, articles you’ve written and talks you’ve given, your help with onboarding new employees, your contribution to weekly UX meetings, etc. (a Google Doc would do). This will help you argue better during salary negotiations. You have to be proactive about your salary increase — you are unlikely to be promoted on your own, so make sure that your managers provide time and space for you to bring up your question about your career, personal growth, responsibilities and salary.
 5. You Can’t Have It All
 Who doesn’t want to have a lovely combination of ownership in the team, a decent salary with stock options, stability, great managers, fantastic people, and a good work-life balance? It was quite a surprise to me that getting everything neatly packaged in one job opening was quite an unrealistic expectation.
 In a start-up environment, you might have a lot of ownership, but with it often comes slightly chaotic management with last-minute changes and mid-night fixes.
 In large corporations, you would have a reasonable salary and stability, but you probably won’t feel like you make significant contributions to the product. You are likely to be working on tiny adjustments, often not even knowing if your work will ever see the light of day. However, you learn a lot from your team and grow significantly as a professional.
 As a contractor or freelancer, you always need to chase your projects, and with it comes a healthy dose of accounting, estimates, scopes of work, and eventually, last-minute changes and deadlines. Sometimes you might have too many projects, and the next month you might be looking for work. This adds to the pressure and stress levels that you might want to avoid.
 What about agencies and outsourcing companies? Frankly, I wish that in my early 20s, I’d have worked with them more than I did (just around a year, really). Mostly because I’d love to have learned more about different knowledge domains to be able to apply this knowledge to my ongoing projects. Right now, every time I have to deep dive and learn a lot about every single industry, and this takes a remarkable amount of time and effort.
 There is no magic space where you can have it all. And more often than not, it’s not really needed. Figure out what is important to you. Personally, I’d choose to work with good people on a great product over salary and stock options any time of the day. The choice of the company would be influenced by this very decision.
 What’s the best option? There is probably none. To myself, I’d recommend to start out working in an agency or outsourcing company. Learn different knowledge domains, learn how the business works, get to know skilled people, and make connections.
 Then move to a product team to see how products are built and maintained and how teams are working together. Then switch to a larger company to learn from incredible people and understand slightly more complex sides of the product and business.
 Eventually, either become a consultant or build a company of your own or move back to a product team. This might not be for everyone, but it would make me have ownership over some parts of the product, feel stable, learn, and be surrounded by people I can learn from.
 6. Pay Attention To Your Estimates
 As humans, we are incredibly bad in estimating work, and the best way to get better at it is to break down the scope of work into smaller units of work. Many managers assume that just because we have around eight working hours a day, we are actually working productively during that entire timeframe. That, however, doesn’t account for so many things, from routine messaging on Slack and urgent errands to sick days and interruptions.
 When asked to estimate the amount of time you need to deliver, I always try to count on around 6–6.5 productive working hours a day. Feel free to underpromise and overdeliver, but always include the cost of over-delivery in your estimates.
 I definitely spend more time in spreadsheets these days compared to the early days, and that might be one of the bigger changes throughout the years. It pays off to invest enough time into writing detailed scope of work, explaining:
 
 how you understood the problem,
 what the solution requires (with a breakdown of tasks),
 what your assumptions are,
 how you are planning to solve it,
 when customer’s (timely) input will be needed,
 what the milestones and timeline are,
 what delivery date we commit to (for the fixed scope),
 how the pricing and payment are going to work.
 
 Most importantly, make sure that everybody understands that you are estimating delivery for a fixed scope of work, and late changes will be expensive and might delay the delivery. In fact, you might want to repeat that last sentence multiple times in your scope of work and make sure that you get an unambiguous sign-off from your client (preferably with a signature).
 7. Test The Company During Your Probation Period
 We often think about the probation period being a test for us as employees, but you can also see it as an important test for the company, too. Watch out for red flags: do people leave for strange reasons? Do managers change frequently? Are designers and developers being listened to in the company? What are some of the recent changes that were implemented based on users’ feedback?
 Engage in conversations about what the impact of work is, just to make sure that you don’t put your hard work and efforts into something that might not even be worth your energy. You are talented, skilled, and hard-working, and there are plenty of good uses for your skills out there.
 8. Think About Passive Income Early
 When you are in your 20s, it’s easy to dismiss the notion of passive income. After all, you have all the time in the world to make your financial decisions later. But I can’t stress it enough: do think early about your passive income — the earlier you start investing into ETFs or creating digital products, templates and books, the more you can accumulate over the years. That’s a valuable — and the most important — investment of your time for the decades to come.
 Your best asset is the time you have, and the longer you keep investing, the more impactful your interest is going to be after just a few decades. You don’t need much money to start building up your passive income. Even putting aside $100 a month will pay off long term.
 Also, find like-minded people and start cultivating your user base. Once you know what you like doing, try to do as much as possible around that niche to make sure that when the topic comes up, your name, or the resources that you have created, come up along with it.
 This requires visibility — writing, publishing, releasing, and open-sourcing. Set aside a bit of time every week to invest in it — it’s worth every second of your time.
 Wrapping Up
 Of course, everybody has their own experiences, so the things I’ve mentioned here might not be quite what you’d recommend and might not align with your current situation.
 However, I strongly believe that many of these points will be important to consider or think about before switching companies, confirming an offer, or passing the probation period.
 Further Reading on Smashing Magazine
 
 “Rekindling Your Passion For Web Design,” Jeremy Girard
 “Ten Tips For Aspiring Designer Beginners,” Luis Ouriach
 “Why Collaborative Coding Is The Ultimate Career Hack,” Bobby Sebolao
 “Work-Life Balance: Tips From The Community,” Ricky Onsman
 “Launching Your Design Career: Which Type Of Education Is Best For You?,” Alec McGuffey
 </content>
     </entry>
     <entry>
       <title>One line of CSS to add basic dark/light mode</title>
         <link href="https://christianheilmann.com/2022/06/20/one-line-of-css-to-add-basic-dark-light-mode/"/>
       <updated>2022-06-20T22:33:20.000Z</updated>
       <content type="text">When you have your OS set up in dark mode seeing bright form elements is grating. By starting your style sheet with a single line of CSS, you can make sure that people using dark mode get dark form elements and backgrounds and those using light mode, light ones.
 
 	
 
 	You can see it in this codepen
 
 	
 See the Pen 
 color-scheme by Christian Heilmann (@codepo8)
 on CodePen.
 
 
 
 	You can also use the browser developer tools to simulate the different modes to see the difference.
 
 	
 
 	Read more about color-scheme on the MDN web docs.</content>
     </entry>
     <entry>
       <title>It just works</title>
         <link href="https://gomakethings.com/it-just-works/"/>
       <updated>2022-06-20T14:30:00.000Z</updated>
       <content type="text">Last week, Mads Stoumann tweeted…
 
 
 Today I opened a 10-year old vanilla-js app — and it just worked! No node/npm woes …
 
 
 This kind of thing continues to be one of my biggest attractions to a more browser-native experience.
 
 I have so many projects I built just a few years ago using Gulp that are littered with broken dependencies. If I need to make any updates, I first need to patch a bunch of out-of-date NPM dependencies.
 
 Often, that involves replacing at least one or two that have been deprecated, or rewriting my build setup because of a breaking major version change.
 
 That’s not to say I don’t use any build tools. I do. But I built my own to be as close-to-the-metal as possible.
 
 The goal with vanilla JS or the lean web is not to completely shun tooling or write every line of code yourself.
 
 It’s to lean heavily on what the platform gives you out of the box as a strategy for delivery a better user experience with fewer long-term maintenance issues.
 
 Where most people look at robust tooling as an asset, I often see a long-term liability that creates more headaches down the road.
 ⏰🦉 Early Bird Sale! Today through Monday, get 40% off registration in the next session of the Vanilla JS Academy.</content>
     </entry>
     <entry>
       <title>Precise Timing With Web Animations API</title>
         <link href="https://smashingmagazine.com/2022/06/precise-timing-web-animations-api/"/>
       <updated>2022-06-20T09:00:00.000Z</updated>
       <content type="text">I previously viewed animations as something playful. Something that adds fuzziness to interfaces. Apart from that, in good hands, animation can make interfaces clearer. One property of animations on the Web that I didn’t hear much about is their precision. That Web Animations API allows us to drop workarounds concerned with JavaScript timing issues. In this article, you’ll see how not to do animations and how to coordinate animation of several elements.
 When you work on a visual presentation of something that requires to be precise, you quickly learn that you spend too much time working around JavaScript’s inability to be exact about when code will actually execute. Try to implement something that relies on rhythm or shared timing and you will get my point. You might get close in certain cases but it’s never perfect.
 One of the things I find helpful when working on complex problems is to break them down into smaller, simpler problems. It happens that smaller pieces — even if plenty — have something that unifies them. Something that allows you to treat them uniformly. In the case of animations, now that you have many more elements to deal with, you need something that will guarantee a level of timing precision that would exclude the possibility of drift, of elements going “off-beat”.
 First, let’s see what could go wrong with typical tools JavaScript has to offer.
 JavaScript Timing Issues: In Order But Off Beat
 In JavaScript, every task goes through a queue. Your code, user interactions, network events. Each task waits its turn to be performed by an event loop. That way, it guarantees that things happen in order: when you call a function, you can be sure no sudden mouse move would inject itself in the middle. When you need things to happen later, you can register an event listener or a timer.
 When event fires or a timer is due, the task you defined in a callback goes into the queue. Once the event loop gets to it, your code gets executed. It’s a powerful concept that allows us to mostly ignore concurrency. It works well, but you better understand how it works.
 We’ll look into the consequences of this in the context of animations. I encourage you to learn this topic deeper. Understanding the nature of how JavaScript work, will save you hours and will keep color in your hair. Jake Archibald has done a great job of breaking it all down in his “Tasks, Microtasks, Queues and Schedules” article and more recently in his “In The Loop” talk at JSConf.
 Here’s what awaits you once you’ve decided to do animations with setTimeout or setInteval:
 
 Low Precision
 Pile Ups
 Crowded Queue 
 
 Low Precision
 We can define exactly how long a timeout should wait before placing our task in the queue. What we cannot predict is what will be in the queue at the moment. It is possible to implement self-adjusting timers by checking the difference between the planned tick length and the actual moment the code is executed. That difference is applied to the next tick timeout.
 It mostly works, but if the required distance between ticks is measured in two-digit milliseconds or less, it will rarely hit at the right moment. Also, its nature (that it adjusts on execution) makes it hard to visualize something rhythmical. It will show the precise state when it was called, but not the exact moment the state has changed.
 That is because setTimeout guarantees minimum delay before a thing gets put in the queue. But there’s no way to tell what will be in the queue already.
 Pile Ups
 If low precision is fine for you occasionally, you’ll get a pile. Things you’ve meant to be spaced out in time might be executed all at once if there were many tasks for the event loop to work on — or it could all get suspended.
 Advancements in battery life come with better hardware and efficient software. Browser tabs might get suspended to reduce power consumption when not in use. When the tabs are in focus again, the event loop might find itself with a handful of callbacks — some of which are issued by timers — in the queue to process.
 Once I had to implement randomly flipping tiles for a website, and one of the bugs was caused by sleepy tabs. Because each tile maintained its own timer, they all fired simultaneously when the tab became active.
 Notice how the top row blocks are delayed and then flip three at once. (See the Pen CodePen Home Timeouts vs DocumentTimeline by Kirill Myshkin)
 Crowded Queue
 Likely, your code is already constrained by libraries and frameworks. That means callbacks from your timers are more likely to be put in a queue at an unfortunate moment. You might not have much opportunity for optimization, as there is already much code running around.
 The shortcomings above might be resolved in certain cases. You decide for yourself what’s valued more in each particular project. If all your elements could be managed by a single timer, you might be able to make it work.
 Still, I would look at requestAnimationFrame instead of timers to manage animations. The talk by Jake I linked to above illustrates it brilliantly. What it gives you is rhythm. You can be sure that your code will be executed right before the user is able to see anything. Because you have a timestamp of when your function is called, you can use that to calculate the exact state you need to have.
 It’s up to you what’s worth your time to deal with. It might well be that a particular workaround is fine, and you can move on with whatever you’re trying to implement. You are a better judge of what works in your case.
 If the thing you’re trying to implement fits into animations realm, you will benefit from moving it off the queue. Let’s see how we get to a place where time is king.
 Web Animations API: Where Things Are In Sync
 In my previous article, “Orchestrating Complexity With Web Animations API,” we looked at ways to make several animations be controllable as if they were one. Now we’ll look into how to make sure that all your animations start at the right moment.
 Timeline
 Web Animations API introduces a notion of a timeline. By default, all the animations are tied to the timeline of the document. That means animations share the same “inner clock” — a clock that starts at the page load.
 That shared clock is what allows us to coordinate animations. Whether it’s a certain rhythm or a pattern, you don’t need to worry that something will drag or go ahead of itself.
 Start Time
 To make an animation start at a certain moment, you use thestartTime property. The value of startTime is measured in milliseconds from the page load. Animations with a start time set to 1000.5 will start their playback exactly when the document timeline’s currentTime property equals 1000.5.
 Notice the dot in the start time value? Yes, you can use fractions of milliseconds, it’s that precise. However, exact precision depends on browser settings.
 Another useful thing is that start time can be negative as well. You’re free to set it to a moment in the future or a moment in the past. Set the value to -1000, and your animation state would be like it has been played for a second already when the page loads. For the user, it would seem as if the animation had started playing before they even thought to visit your page.
 Note: Beware that timeline and startTime are still experimental technologies.
 Demo
 To demonstrate how you can use it, I’ve set up a demo. I’ve implemented an indicator that, more than any other, depends on time precision — a clock. Well I did two, that way, one would reveal the greatness of the other. Certain things in this demo are simple enough to demonstrate the basics. There’re also some tricky parts that show you where this approach is lacking.
 Digital and analog clock, both implemented digitally. (See the Pen Clock by Kirill Myshkin)  
 The movement of the analog clock is quite simple. Three hands do the same single-turn rotation — quite optimistically — infinite times. Because the clock is a precise instrument, I’ve made the seconds and minutes hands change their position at the exact moment their corresponding values change. That helps to illustrate that they change at the exact moment as their cousins on the digital clock below.
 const clock &#x3D; document.getElementById(&quot;analog-clock&quot;);
 const second &#x3D; clock.querySelector(&quot;.second&quot;);
 const minute &#x3D; clock.querySelector(&quot;.minute&quot;);
 const hour &#x3D; clock.querySelector(&quot;.hour&quot;);
 
 const s &#x3D; 1000;
 const m &#x3D; s * 60;
 const h &#x3D; m * 60;
 const d &#x3D; h * 24;
 
 const hands &#x3D; [second, minute, hour];
 const hand_durations &#x3D; [m, h, d];
 const steps &#x3D; [60, 60, 120];
 
 const movement &#x3D; hands.map(function (hand, index) {
     return hand.animate(
         [
             {transform: &quot;rotate(0turn)&quot;},
             {transform: &quot;rotate(1turn)&quot;}
         ],
         {
             duration: hand_durations[index],
             iterations: Infinity,
             easing: &#x60;steps(${steps[index]}, end)&#x60;
         }
     );
 });
 
 movement.forEach(function (move) {
     move.startTime &#x3D; start_time;
 });
 
 
 Animation for each of the three hands differs in how long they do their rotation and in how many steps it’s divided. Seconds hand does a single revolution in sixty thousand milliseconds. Minutes hand does it sixty times slower than that. Hours hand — because it’s a twenty-four-hour clock — does one in equal time it takes the minutes hand to make twenty four revolutions.
 To tie the clock hands operation to the same notion of time (to make sure the minutes hand updates its position exactly at the moment the seconds hand finishes its rotation), I used the startTime property. All the animations in this demo are set to the same start time. And that’s all you need. Don’t worry about the queue, suspended tabs, or piles of timeouts. Define it once and it’s done.
 The digital clock, on the other hand, is a bit counterintuitive. Each digit is a container with overflow: hidden;. Inside, there’s a row of numbers from zero to one sitting in equal width cells. Each digit is revealed by translating the row horizontally by width of a cell times the digit value. As with the hands on the analog clock, it was a question of setting the right duration for each digit. While all the digits from milliseconds to minutes were easy to do, the hours required some tricks — which I’ll cover below.
 Let’s look at the value of start_time variable:
 const start_time &#x3D; (function () {
     const time &#x3D; new Date();
     const hour_diff &#x3D; time.getHours() - time.getUTCHours();
     const my_current_time &#x3D; (Number(time) % d) + (hour_diff * h);
 
     return document.timeline.currentTime - my_current_time;
 }());
 
 
 To calculate the exact moment all the elements have to be started at (which is past midnight), I took the value of Date.now() (milliseconds since January 1, 1970), stripped full days from it, and adjusted it by the difference with UTC time. That left me with the number of milliseconds that have passed since the start of today. It is the only piece of data my clock needs to show what it is destined to show: hours, minutes and seconds.
 To convert that value to the realm of the document, I need to adjust it based on how much time has passed since the load of this demo’s page until Date.now() call. To do that, I subtracted it from the currentTime of the document. Applying the result to an animation means that this particular animation has been playing since midnight. Apply that to all the animations, and you get a demo that has been playing since midnight.
 Theoretically, we could have a clock that runs since January 1, 1970 (52 years as of this writing), but some browsers have undocumented limits for the value of animation’s duration. It would also feel right to apply some CSS to artificially age such clock — as there won’t be any other distinction from the one that has run since last night. Both of those clocks would be in perfect sync.
 Shortcomings Of This Approach
 It is empowering to be able to implement something of this precision without any sophisticated calculations. But it only works for cases where the things you’re trying to implement could be defined with keyframes. You should decide, based on your particular case, where it would be beneficial and where it would become more cumbersome and costly to deal with shortcomings.
 An alternative to Web Animations API would be to use requestAnimationFrame or performance.now(). With those, you would need to calculate interpolation yourself.
 If you choose to rely on Web Animations API, you would have to face the fact that things fit differently into a keyframe definition. Some things might take practically no work to define because they naturally fit. Others require workarounds. Whether those workarounds add much cost or not to what you’re trying to implement should dictate your approach.
 The clock demo has examples of both cases. The hands themselves were the easiest thing to do. It is a keyframe of one turn rotation with steps easing function to make them tick. In the end, the main movement of the demo clock took almost no work to do. I wish I could say that the digital clock was as easy to implement. But that is due to my own shortcomings, I would say.
 There are three examples of workarounds I had to revert to. I hope they give you an idea of what you might need to do if you go with the animations approach. They aren’t a good representation of Web Animations API limits, they only show how a particular implementation I’ve chosen had to be changed to fit. Let’s see where I had to do additional work.
 Some Properties Won’t Animate As You Want Them To
 If you look closely, each hand on the analog clock has a shadow. They add some depth and make the clock look nicer. Shadows are easily applied using box-shadow or filter CSS properties. It is animatable to a degree, but where it comes short is in the animation of the shadow direction. You don’t set it with angle value but by coordinates.
 I couldn’t find a way to implement a circular movement of a shadow using two coordinates. Instead, I broke down each hand into three elements animated separately (see my previous article “Orchestrating Complexity With Web Animations API” for that technique). The first one is a wrapper that contains the other two parts of a hand: body and shadow. The wrapper is the element to which the main rotation is applied to. The body defines shape, size, and color of a hand, while the shadow copies the body properties (except for the color). Plus, it has its own animation defined — it rotates around the center of its hand.
 Multiplying the number of elements to deal with might seem harder to do. In the case of the shadow, though, the fact that it was eventually separated from the hand gave more flexibility. You could style it using the CSS. And because the timing has already been dealt with, having more elements doesn’t make it harder.
 Division Doesn’t Always Result In Equal Shares
 The second workaround was required for the hours section of the digital clock. The clock is implemented with single digit elements. Three for milliseconds, two for seconds, and two for minutes. Hour digits don’t fit in a logic of looping keyframe.
 The loops aren’t regular because there are only four hours in the twenties. I had to introduce a “wide” digit to tackle this. The wide digit has the same logic as a normal digit would have, only that it supports numbers from zero to ninety-nine — which is plenty for this case. In the end, the digital clock’s hour indicator reused the same timing options as the hours hand on the analog clocks.
 You Never Know How Long The Next Month Would Be Without Checking The Calendar
 The third workaround is for the date complication. Now that I had “wide” digits element, I’ve reused it to show dates and just increased the duration from hours to days.
 The problem with that approach was that the month length didn’t map perfectly with the same length animations used in the demo. You see, the calendar we use today has a messy history and is hard to fit into a simple loop. One would have to define all the exceptions of the Gregorian calendar in a single keyframe. I’ll skip doing that. I’m here to show you a workaround.
 I chose to rely on Date instead of defining yet another flawed calendar. Who knows how many days the months will have in the future, right?
 function month() {
     const now &#x3D; new Date();
     return digits(
         (new Date(now.getFullYear(), now.getMonth() + 1, 0)).getDate(),
         false
     );
 }
 
 function create_animation(digit) {
     const nr_digits &#x3D; digit.firstElementChild.children.length;
     const duration &#x3D; d * nr_digits;
     return digit.firstElementChild.animate(
         [
             {transform: &quot;translateX(0)&quot;},
             {transform: &quot;translateX(calc(var(--len) * -2ch)&quot;}
         ],
         {
             duration,
             easing: &#x60;steps(${nr_digits}, end)&#x60;,
             iterationStart: (d * ((new Date()).getDate() - 1)) / duration
         }
     );
 }
 
 (function set_up_date_complication() {
     const day &#x3D; document.querySelector(&quot;.day&quot;);
 
     const new_day &#x3D; day.cloneNode();
     new_day.append(month());
     day.replaceWith(new_day);
 
     Array.from(new_day.children).forEach(function (digit) {
         const complication &#x3D; create_animation(digit);
         complication.startTime &#x3D; start_time;
         complication.finished.then(set_up_date_complication);
     });
 }());
 
 
 To make the date complication bulletproof, I defined its duration to be the length of the current month. To keep using the same start time and to avoid the limit on duration value, we “rewind” the animation to the correct date using iterationStart property.
 When the month ends, we need to rebuild the date complication for the next month. The right moment to do that would be when the complication animations had finished. Unlike other animations in this demo, the date doesn’t iterate, so we will create a new date using the finished promise of the current month animation.
 That approach suffers from the shortcomings described at the start of this article. But in the case of months, we might close our eyes on slight imprecision.
 You would have to believe my word that it works. Otherwise, return to this article any last day of a month close to midnight. Who knows, you could find me on the same page, with eyes full of hope and fingers crossed.
 Conclusion
 Animations share the same time reference, and by adjusting their startTime property, you can align them to any pattern you need. You can be sure they won’t drift.
 Web Animations API comes with powerful tools that allow you to dramatically reduce the amount of work that your application and you have to do. It also comes with a precision that opens possibilities to implement new kinds of applications.
 That power is contained in the animations realm. Whether it is suitable for your case or not is something you decide based on your needs. I hope the examples I provided in this article will give you a better idea of what path to choose.
 Further Reading On Smashing Magazine
 
 “Orchestrating Complexity With Web Animations API,” Kirill Myshkin
 “Understanding Easing Functions For CSS Animations And Transitions,” Adrian Bece  
 “Practical Techniques On Designing Animation,” Sarah Drasner
 “Designing With Reduced Motion For Motion Sensitivities,” Val Head
 </content>
     </entry>
     <entry>
       <title>Goodbye, IE</title>
         <link href="https://daverupert.com/2022/06/goodbye-ie/"/>
       <updated>2022-06-17T19:29:00.000Z</updated>
       <content type="text">In 1995, I got my first PC with Windows 95 and my first web browser, unbeknownst to me at the time, was Internet Explorer 1.0. I dialed my computer into AOL checked my mail, checked my favorite chat rooms, but I soon learned the Internet I wanted existed outside the in-app silo of America Online. I wanted to surf and create content on the information super highway. Through Angelfire and AOL Pages, I stayed up late creating my own worlds on the World Wide Web.
 In 1998, I went to college and was peer pressured to use Netscape by my roommates. Netscape had an exciting atmosphere about it, and its move to open source turned fellow Computer Science nerds into gleeful converts. That move to open source would be a critical change in the life of Netscape but also in the life of the Web and in the story of Internet Explorer.
 In 2000, I used my student discount to get a copy of FrontPage 98. FrontPage had all these cool DHTML effects like “glow” using proprietary ActiveX controls. I liked the effects so much, I switched back to using Internet Explorer 5. The year 2001 would bring us IE6 on Windows XP with a brand new aesthetic that made Netscape feel ancient. During these years I built websites for college groups like my ultimate frisbee team and was blissfully ignorant of the fact that they didn’t work the same in non-IE browsers. This is some foreshadowing of the problems with IE’s monopoly.
 In 2003, I saved enough money through my first fulltime job to buy my first Macbook Pro. OS X 10.1 was new at the time and Safari didn’t exist yet so I used Internet Explorer 5.5, which took the “Aqua” aesthetic to heart and had a cool feature that would ping a list of your favorite sites to check for updates; a weird precursor to RSS. Later in 2003, Apple released Safari and —Apple fanboy that I was— I switched to that.
 In 2006, I started doing web development as a job. I read Zeldman’s Designing for Web Standards and became passionate about web standards. IE6, in a web of web standards, earned a bit of a bad rap among developers due to its bespoke box model and lack of updates (compared to Firefox, Safari, and soon Chrome). IE7 did come out, then IE8. Each one better than the one before but still with their own quirks that left them with a bad feeling. IE became the butt of every conference joke. “What about IE?” became the bane of every person talking about new web features. We put “Upgrade your browser” or “Try Firefox” banners on our sites. We collectively tried to eradicate it.
 In 2011, Nishant Kothary invited Jason Santa Maria, Frank Chrimero, Naz Hamid, and Paravel to work on a project called “Lost World’s Fairs” to celebrate the launch of IE9 and highlight its newfound support of WOFF web fonts. A celebration of the Web and what browsers can do. IE9 was a massive improvement and it seemed like the gears of change were turning. For sure one of the funnest projects I’ve ever worked on where I got to flex my skills.
 In 2012, in part due to the work on the Lost World’s Fair site, Paravel was able to work on the Microsoft.com homepage, right around the launch of Windows 8 (featuring the short-lived Internet Explorer 10). Gruber liked it and I heard Steve Ballmer himself gave it a thumbs up. Pretty cool.
 In 2015, I switched back to Windows for #davegoeswindows. Part of the unpaid deal was to try and use Edge, the successor to IE, as my daily browser. It’s still impressive what the original Edge team did in an attempt to salvage a browser and its reputation. Pivoting from the old Trident rendering engine to EdgeHTML, rearchitecting its security principles, and building something modern and fast. There were leftover IE bits in Edge (e.g., F12 Developer Tools), but I got used to it and experienced using “not the most cutting edge” browser as my daily driver. The #davegoeswindows experiment was one of the most educational stunts I’ve ever done.
 This week on June 15th, 2022, Internet Explorer reached its end of life and is no longer supported by Microsoft. I think this is something to celebrate. It’s the end of non-evergreen browsers. While “Evergreen” does not mean immediately available and saying “IE is dead” doesn’t mean someone out there isn’t using it, my hope is those people don’t become a gigantic strawman like “the boss who uses IE” did. It’s important to celebrate that outdated software licenses for governments and enterprise IT departments forcing legacy browser usage no longer control our industry or our collective conscious. That is life-changing.
 
 Internet Explorer has been a part of my entire online life; equal parts thrill, shame, joy, and pain… well, a smidge more pain. Over the years, IE crossed the boundaries from being a tech beacon (the first browsers with CSS), to becoming a villain we all raged against, to becoming a technical liability. The full circle. Hearing IE jokes from late night talk show hosts is surreal and noteworthy; IE sits in the cultural seat for a public roasting, the razzing of an old friend, the popularity of a celebrity death.
 Though I may sound sentimental, I’m happy to say goodbye. I already feel the new allocations as sections of my brain used for remembering browser quirks get garbage collected. And in some ironic twist… the browser I’m reading this article on today is a browser from Microsoft. The spiritual successor to that old friend and foe. While IE is dead, its legacy lives on.</content>
     </entry>
     <entry>
       <title>Single Element Loaders: The Dots</title>
         <link href="https://css-tricks.com/single-element-loaders-the-dots/"/>
       <updated>2022-06-17T14:47:55.000Z</updated>
       <content type="text">We’re looking at loaders in this series. More than that, we’re breaking down some common loader patterns and how to re-create them with nothing more than a single div. So far, we’ve picked apart the classic spinning loader. Now, let’s look at another one you’re likely well aware of: the dots.
 
 
 
 Dot loaders are all over the place. They’re neat because they usually consist of three dots that sort of look like a text ellipsis (…) that dances around.
 
 
 
 
 
 
 
 
 Article series
 
 
 
 Single Element Loaders: The SpinnerSingle Element Loaders: The Dots — you are hereSingle Element Loaders: The BarsSingle Element Loaders: Going 3D — coming July 1
 
 
 
 
 Our goal here is to make this same thing out of a single div element. In other words, there is no one div per dot or individual animations for each dot.
 
 
 
 CodePen Embed Fallback
 
 
 
 That example of a loader up above is made with a single div element, a few CSS declarations, and no pseudo-elements. I am combining two techniques using CSS background and mask. And when we’re done, we’ll see how animating a background gradient helps create the illusion of each dot changing colors as they move up and down in succession.
 
 
 
 The background animation
 
 
 
 Let’s start with the background animation:
 
 
 
 .loader {
   width: 180px; /* this controls the size */
   aspect-ratio: 8/5; /* maintain the scale */
   background: 
     conic-gradient(red   50%, blue   0) no-repeat, /* top colors */
     conic-gradient(green 50%, purple 0) no-repeat; /* bottom colors */
   background-size: 200% 50%; 
   animation: back 4s infinite linear; /* applies the animation */
 }
 
 /* define the animation */
 @keyframes back {
   0%,                       /* X   Y , X     Y */
   100% { background-position: 0%   0%, 0%   100%; }
   25%  { background-position: 100% 0%, 0%   100%; }
   50%  { background-position: 100% 0%, 100% 100%; }
   75%  { background-position: 0%   0%, 100% 100%; }
 }
 
 
 
 I hope this looks pretty straightforward. What we’ve got is a 180px-wide .loader element that shows two conic gradients sporting hard color stops between two colors each — the first gradient is red and blue along the top half of the .loader, and the second gradient is green and purple along the bottom half.
 
 
 
 The way the loader’s background is sized (200% wide), we only see one of those colors in each half at a time. Then we have this little animation that pushes the position of those background gradients left, right, and back again forever and ever.
 
 
 
 When dealing with background properties — especially background-position — I always refer to my Stack Overflow answer where I am giving a detailed explanation on how all this works. If you are uncomfortable with CSS background trickery, I highly recommend reading that answer to help with what comes next.
 
 
 
 In the animation, notice that the first layer is Y&#x3D;0% (placed at the top) while X is changes from 0% to 100%. For the second layer, we have the same for X but Y&#x3D;100% (placed at the bottom).
 
 
 
 CodePen Embed Fallback
 
 
 
 Why using a conic-gradient() instead of linear-gradient()?
 
 
 
 Good question! Intuitively, we should use a linear gradient to create a two-color gradients like this:
 
 
 
 linear-gradient(90deg, red 50%, blue 0)
 
 
 
 But we can also reach for the same using a conic-gradient() — and with less of code. We reduce the code and also learn a new trick in the process!
 
 
 
 Sliding the colors left and right is a nice way to make it look like we’re changing colors, but it might be better if we instantly change colors instead — that way, there’s no chance of a loader dot flashing two colors at the same time. To do this, let’s change the animation‘s timing function from linear to steps(1)
 
 
 
 CodePen Embed Fallback
 
 
 
 The loader dots
 
 
 
 If you followed along with the first article in this series, I bet you know what comes next: CSS masks! What makes masks so great is that they let us sort of “cut out” parts of a background in the shape of another element. So, in this case, we want to make a few dots, show the background gradients through the dots, and cut out any parts of the background that are not part of a dot.
 
 
 
 We are going to use radial-gradient() for this:
 
 
 
 .loader {
   width: 180px;
   aspect-ratio: 8/5;
   mask:
     radial-gradient(#000 68%, #0000 71%) no-repeat,
     radial-gradient(#000 68%, #0000 71%) no-repeat,
     radial-gradient(#000 68%, #0000 71%) no-repeat;
   mask-size: 25% 40%; /* the size of our dots */
 }
 
 
 
 There’s some duplicated code in there, so let’s make a CSS variable to slim things down:
 
 
 
 .loader {
   width: 180px;
   aspect-ratio: 8/5;
   --_g: radial-gradient(#000 68%, #0000 71%) no-repeat;
   mask: var(--_g),var(--_g),var(--_g);
   mask-size: 25% 40%;
 }
 
 
 
 Cool cool. But now we need a new animation that helps move the dots up and down between the animated gradients.
 
 
 
 .loader {
   /* same as before */
   animation: load 2s infinite;
 }
 
 @keyframes load {      /* X  Y,     X   Y,    X   Y */
   0%     { mask-position: 0% 0%  , 50% 0%  , 100% 0%; } /* all of them at the top */
   16.67% { mask-position: 0% 100%, 50% 0%  , 100% 0%; }
   33.33% { mask-position: 0% 100%, 50% 100%, 100% 0%; }
   50%    { mask-position: 0% 100%, 50% 100%, 100% 100%; } /* all of them at the bottom */
   66.67% { mask-position: 0% 0%  , 50% 100%, 100% 100%; }
   83.33% { mask-position: 0% 0%  , 50% 0%  , 100% 100%; }
   100%   { mask-position: 0% 0%  , 50% 0%  , 100% 0%; } /* all of them at the top */
 }
 
 
 
 Yes, that’s a total of three radial gradients in there, all with the same configuration and the same size — the animation will update the position of each one. Note that the X coordinate of each dot is fixed. The mask-position is defined such that the first dot is at the left (0%), the second one at the center (50%), and the third one at the right (100%). We only update the Y coordinate from 0% to 100% to make the dots dance.
 
 
 
 
 
 
 
 Here’s what we get:
 
 
 
 CodePen Embed Fallback
 
 
 
 Now, combine this with our gradient animation and magic starts to happen:
 
 
 
 CodePen Embed Fallback
 
 
 
 Dot loader variations
 
 
 
 The CSS variable we made in the last example makes it all that much easier to swap in new colors and create more variations of the same loader. For example, different colors and sizes:
 
 
 
 CodePen Embed Fallback
 
 
 
 What about another movement for our dots?
 
 
 
 CodePen Embed Fallback
 
 
 
 Here, all I did was update the animation to consider different positions, and we get another loader with the same code structure!
 
 
 
 The animation technique I used for the mask layers can also be used with background layers to create a lot of different loaders with a single color. I wrote a detailed article about this. You will see that from the same code structure we can create different variations by simply changing a few values. I am sharing a few examples at the end of the article.
 
 
 
 Why not a loader with one dot?
 
 
 
 CodePen Embed Fallback
 
 
 
 This one should be fairly easy to grok as I am using the same technique but with a more simple logic:
 
 
 
 CodePen Embed Fallback
 
 
 
 Here is another example of loader where I am also animating radial-gradient combined with CSS filters and mix-blend-mode to create a blobby effect:
 
 
 
 CodePen Embed Fallback
 
 
 
 If you check the code, you will see that all I am really doing there is animating the background-position, exactly like we did with the previous loader, but adding a dash of background-size to make it look like the blob gets bigger as it absorbs dots.
 
 
 
 If you want to understand the magic behind that blob effect, you can refer to these interactive slides (Chrome only) by Ana Tudor because she covers the topic so well!
 
 
 
 Here is another dot loader idea, this time using a different technique:
 
 
 
 CodePen Embed Fallback
 
 
 
 This one is only 10 CSS declarations and a keyframe. The main element and its two pseudo-elements have the same background configuration with one radial gradient. Each one creates one dot, for a total of three. The animation moves the gradient from top to bottom by using different delays for each dot..
 
 
 
 Oh, and take note how this demo uses CSS Grid. This allows us to leverage the grid’s default stretch alignment so that both pseudo-elements cover the whole area of their parent. No need for sizing! Push the around a little with translate() and we’re all set.
 
 
 
 More examples!
 
 
 
 Just to drive the point home, I want to leave you with a bunch of additional examples that are really variations of what we’ve looked at. As you view the demos, you’ll see that the approaches we’ve covered here are super flexible and open up tons of design possibilities.
 
 
 
 CodePen Embed Fallback
 
 
 
 CodePen Embed Fallback
 
 
 
 CodePen Embed Fallback
 
 
 
 CodePen Embed Fallback
 
 
 
 CodePen Embed Fallback
 
 
 
 Next up…
 
 
 
 OK, so we covered dot loaders in this article and spinners in the last one. In the next article of this four-part series, we’ll turn our attention to another common type of loader: the bars. We’ll take a lot of what we learned so far and see how we can extend them to create yet another single element loader with as little code and as much flexibility as possible.
 
 
 
 
 Article series
 
 
 
 Single Element Loaders: The SpinnerSingle Element Loaders: The Dots — you are hereSingle Element Loaders: The BarsSingle Element Loaders: Going 3D — coming July 1
 
 
 Single Element Loaders: The Dots originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>How to add interactivity to browser native web components with vanilla JS</title>
         <link href="https://gomakethings.com/how-to-add-interactivity-to-browser-native-web-components-with-vanilla-js/"/>
       <updated>2022-06-17T14:30:00.000Z</updated>
       <content type="text">On Wednesday, we learned what web components are and when and why you might want to use them. Yesterday, we at looked at how to create web components from custom elements.
 
 Today, we’re going to wrap up the series by adding interactivity to our component. Let’s dig in!
 
 Today’s article is an excerpt from my new course and ebook on web components with vanilla JS.
 
 Lifecycle callback hooks
 
 Using web component lifecycle hooks and traditional DOM manipulation techniques, we can add built-in interactivity to our web components.
 
 As a best practice, event listeners for web components should be added inside the connectedCallback() function, since this method runs when the element is actually injected into the DOM.
 
 Inside our connectedCallback() method, let’s use the Element.querySelector() method on the custom element, this, to get the button inside our component.
 
 If the btn exists, we’ll attach a click event listener to it.
 /**
  * Runs each time the element is appended to or moved in the DOM
  */
 connectedCallback () {
 
 	// Attach a click event listener to the button
 	let btn &#x3D; this.querySelector(&#x27;button&#x27;);
 	if (!btn) return;
 	btn.addEventListener(&#x27;click&#x27;, function (event) {
 		// ...
 	});
 
 }
 
 Inside the event listener callback function, this is the element that was clicked, not the custom element.
 
 When the button is clicked, we’ll use the Element.closest() method to get the parent greeting-message element, and assign it to the host variable. Then, we’ll use the Element.querySelector() method to search for the .message element inside it.
 
 If no target element to display the message in is found, we’ll use the return operator to end the callback function early.
 /**
  * Runs each time the element is appended to or moved in the DOM
  */
 connectedCallback () {
 
 	// Attach a click event listener to the button
 	let btn &#x3D; this.querySelector(&#x27;button&#x27;);
 	if (!btn) return;
 	btn.addEventListener(&#x27;click&#x27;, function (event) {
 
 		// Get the host component
 		let host &#x3D; event.target.closest(&#x27;greeting-message&#x27;);
 
 		// Get the message element
 		let target &#x3D; host.querySelector(&#x27;.message&#x27;);
 		if (!target) return;
 
 	});
 
 }
 
 Otherwise, we’ll use the Element.textContent property to show a message in the target element.
 
 Then, we’ll use the setTimeout() method to clear the message out after 5000 milliseconds (or 5 seconds) so that the user can click the button again if they want.
 btn.addEventListener(&#x27;click&#x27;, function (event) {
 
 	// Get the host component
 	let host &#x3D; event.target.closest(&#x27;greeting-message&#x27;);
 
 	// Get the message element
 	let target &#x3D; host.querySelector(&#x27;.message&#x27;);
 	if (!target) return;
 
 	// Inject the message into the UI
 	target.textContent &#x3D; &#x60;Hi there, friend! Hope you&#x27;re having a great day!&#x60;;
 
 	// Clear the message after 5 seconds
 	setTimeout(function () {
 		target.textContent &#x3D; &#x27;&#x27;;
 	}, 5000);
 
 });
 
 Customizing interactivity with HTML attributes
 
 Just like with the button itself, we can use attributes to customize the behavior here.
 
 For example, let’s say we wanted to customize the greeting with the user’s name. We could add a [name] attribute to the greeting-message element.
 &lt;greeting-message name&#x3D;&quot;Merlin&quot;&gt;&lt;/greeting-message&gt;
 Inside the event listener, we can use the Element.getAttribute() method to get the value of the [name] attribute. Then, we can use a ternary operator to use the provided name if there is one, or friend if there’s not.
 btn.addEventListener(&#x27;click&#x27;, function (event) {
 
 	// Get the host component
 	let host &#x3D; event.target.closest(&#x27;greeting-message&#x27;);
 
 	// Get the message element
 	let target &#x3D; host.querySelector(&#x27;.message&#x27;);
 	if (!target) return;
 
 	// Inject the message into the UI
 	let name &#x3D; host.getAttribute(&#x27;name&#x27;);
 	target.textContent &#x3D; &#x60;Hi there, ${name ? name : &#x27;friend&#x27;}! Hope you&#x27;re having a great day!&#x60;;
 
 	// Clear the message after 5 seconds
 	setTimeout(function () {
 		target.textContent &#x3D; &#x27;&#x27;;
 	}, 5000);
 
 });
 
 Removing interactivity when the web component is disconnected
 
 As a best practice, you should remove any attached event listeners whenever the disconnectedCallback() function runs.
 
 This helps prevent events that are no longer needed from taking up space in the browser’s memory.
 
 We can do this using the Element.removeEventListener() method. But to do so, the callback function needs to be a named function in the same scope as both the connectedCallback() and disconnectedCallback() functions.
 
 To start, let’s first move the event listener callback function out as a class function.
 /**
  * Handle click events on the button
  */
 clickHandler (event) {
 
 	// Get the host component
 	let host &#x3D; event.target.closest(&#x27;greeting-message&#x27;);
 
 	// ...
 
 }
 
 Then, inside the connectedCallback() function, we’ll pass this.clickHandler in as the callback argument instead of using our anonymous function.
 /**
  * Runs each time the element is appended to or moved in the DOM
  */
 connectedCallback () {
 
 	// Attach a click event listener to the button
 	let btn &#x3D; this.querySelector(&#x27;button&#x27;);
 	if (!btn) return;
 	btn.addEventListener(&#x27;click&#x27;, this.clickHandler);
 
 }
 
 Finally, inside the disconnectedCallback() function, we can repeat the process.
 
 We’ll use the Element.querySelector() method to get the button. Then, we’ll call the btn.removeEventListener(), again passing in click and this.clickHandler as arguments.
 /**
  * Runs when the element is removed from the DOM
  */
 disconnectedCallback () {
 
 	// Remove the click event listener from the button
 	let btn &#x3D; this.querySelector(&#x27;button&#x27;);
 	if (!btn) return;
 	btn.removeEventListener(&#x27;click&#x27;, this.clickHandler);
 
 }
 
 Now, whenever our web component is injected into the DOM, an event listener will be added, and whenever its removed, the event listener will be, too.
 
 Detecting changes to web component attributes
 
 The web component lifecycle includes an additional function, attributeChangedCallback(), that runs when attributes on a custom element are added, removed, or changed in value.
 
 You can use it to detect attribute changes and run code in response.
 
 For our greeting-message element, let’s detect when the logout attribute is added to the element. When that happens, we want to remove the button from our web component and show a “goodbye” message.
 &lt;!-- The user is logged out --&gt;
 &lt;greeting-message logout&gt;
 	&lt;div class&#x3D;&quot;message&quot; aria-live&#x3D;&quot;polite&quot;&gt;
 		Bye, friend! See you next time.
 	&lt;/div&gt;
 &lt;/greeting-message&gt;
 First, we need to create a static getter method named observedAttributes().
 
 This function needs to return an array of attributes to watch. Only attributes listed in this array will be observed by the attributeChangedCallback() function.
 
 We’ll return an array with the logout attribute.
 /**
  * Create a list of attributes to observe
  */
 static get observedAttributes () {
 	return [&#x27;logout&#x27;];
 }
 
 Next, we’ll add an attributeChangedCallback() function.
 
 It accepts three arguments: the name of the attribute that’s been changed, its oldValue, and its newValue.
 /**
  * Runs when the value of an attribute is changed on the component
  * @param  {String} name     The attribute name
  * @param  {String} oldValue The old attribute value
  * @param  {String} newValue The new attribute value
  */
 attributeChangedCallback (name, oldValue, newValue) {
 	console.log(&#x27;changed&#x27;, name, oldValue, newValue, this);
 }
 
 At this point, if we add the logout attribute to our greeting-message element, the attributeChangedCallback() will log some stuff into the console.
 let greeting &#x3D; document.querySelector(&#x27;greeting-message&#x27;);
 
 // Nothing will happen here, because we&#x27;re not watching this attribute
 greeting.setAttribute(&#x27;hello&#x27;, &#x27;you&#x27;);
 
 // logs &quot;changed&quot;, &quot;logout&quot;, null, &quot;true&quot;
 greeting.setAttribute(&#x27;logout&#x27;, true);
 
 If we were observing more than one attribute, we would want to first check what the name was before doing anything in the attributeChangedCallback() function.
 /**
  * Runs when the value of an attribute is changed on the component
  * @param  {String} name     The attribute name
  * @param  {String} oldValue The old attribute value
  * @param  {String} newValue The new attribute value
  */
 attributeChangedCallback (name, oldValue, newValue) {
 
 	// Of tje logout attribute
 	if (name &#x3D;&#x3D;&#x3D; &#x27;logout&#x27;) {
 		// ...
 	}
 
 }
 
 For this web component, though, the function only runs for the logout attribute, so we don’t have to check the name, nor do we have to worry about the oldValue or newValue.
 
 First, we’ll use the Element.querySelector() method to get the button inside our web component, and assign it to the btn variable.
 
 If a btn is found, we’ll use the Element.removeEventListener() method to remove the click event on the button. Then, we’ll use the Element.remove() method to remove the btn from the DOM.
 /**
  * Runs when the value of an attribute is changed on the component
  */
 attributeChangedCallback () {
 
 	// Remove the button
 	let btn &#x3D; this.querySelector(&#x27;button&#x27;);
 	if (btn) {
 		btn.removeEventListener(&#x27;click&#x27;, this.clickHandler);
 		btn.remove();
 	}
 
 }
 
 Next, we’ll look for the .message element, and assign it to the target variable.
 
 If a target is found, we’ll use the Element.getAttribute() method to get the value of the [name] attribute on the web component. Then, we’ll use the Element.textContent property to display a goodbye message in the target.
 
 If there’s a name, we’ll use it. Otherwise, we’ll use friend.
 /**
  * Runs when the value of an attribute is changed on the component
  */
 attributeChangedCallback () {
 
 	// Remove the button
 	let btn &#x3D; this.querySelector(&#x27;button&#x27;);
 	if (btn) {
 		btn.removeEventListener(&#x27;click&#x27;, this.clickHandler);
 		btn.remove();
 	}
 
 	// Get the message element
 	let target &#x3D; this.querySelector(&#x27;.message&#x27;);
 	if (target) {
 
 		// Inject the message into the UI
 		let name &#x3D; this.getAttribute(&#x27;name&#x27;);
 		target.textContent &#x3D; &#x60;Bye, ${name ? name : &#x27;friend&#x27;}! See you next time.&#x60;;
 
 	}
 
 }
 
 Now, when we add the logout attribute to our custom element, the button is removed and a message is shown in the UI.
 
 Here’s a demo.
 
 What’s next? The shadow DOM.
 
 The Shadow DOM is a special, hidden DOM, separate from the main DOM.
 
 Web components can use the Shadow DOM to encapsulate elements and avoid the naming collisions and unintended side effects that sometimes happen when code is used by teams or across projects.
 
 If you want to keep learning about native web components, click here to check out my new course and ebook.
 ⏰🦉 Early Bird Sale! Today through Monday, get 40% off registration in the next session of the Vanilla JS Academy.</content>
     </entry>
     <entry>
       <title>How To Easily Build And Support Tables In Figma</title>
         <link href="https://smashingmagazine.com/2022/06/easy-build-support-tables-figma/"/>
       <updated>2022-06-17T09:00:00.000Z</updated>
       <content type="text">The table is one of the most painful components designers have to deal with in their daily design lives. The table element is often a complex combination of text components, lines, rectangles, icons, and more. It soon may become a nightmare to work with, especially if you also want to support different screen resolutions, change the order of columns, and use real-life content.
 In my projects, approximately half of the user interface designs I am working on are tables. This is why in this article, I’d like to share my approach to managing tables in Figma in an easier, more streamlined way. 
 I’m not a fan of long reads with too many unnecessary details, so I’ll “jump” into the subject right away. My guide consists of several parts; thus, you can stop reading at any point when you understand that what you have learned so far covers your needs at the moment, and you can go back/or jump forward to any section when you want to refresh your memory or learn about the more complex workflows. Let’s go!
 Table of Contents
 
 Cells and Table Structure
 Real Data and Column Size Corrections
 Responsive Tables
 Basic Table Kit and States
 Figma Design file
 Conclusion
 
 Note: This article is aimed at people with some experience using Figma Design. If you are an absolute beginner in the Figma field, I would suggest first checking some basic Figma tutorials. To make things easier for you, near the end of the article (check Figma Design File section), I have provided my Figma Design which you can use for deconstruction and learning purposes.
 Cells And Table Structure
 I often use the Ant Design System in my projects. Let’s take their table components as an example.
 To start, we need to make only two simple components in Figma:
 
 a head cell,
 a row cell.
 
 Use Left and Center in Constraints to align the text.
 
 Aside on keyboard shortcuts:Ctrl/Cmd — Win/MacAlt/Option — Win/MacShift — Win and MacFigma is a tool that works on both Windows and Mac. For example, the following keyboard shortcut combo Ctrl/Cmd + D means, “Press Ctrl + D on Windows, or press Cmd + D on Mac.”
 
 Then we need to copy the components for our future table:
 
 hold Alt/Option + Shift + left mouse button for copying
 and Ctrl/Cmd + D to repeat the last action in Figma.
 
 
 How to create the table lines? Start here:
 
 press and hold Alt/Option + Shift + mouse left for copying,
 and Ctrl/Cmd+ D to repeat the last action in Figma.
 
 
 How did I do it? Here are the steps:
 
 group the table row elements into a single frame;
 set corners’ radius to 15 px;
 set outline stroke to 1 px, #49E36B;
 set frame fill color to #278EEE.
 
 
 To help you better imagine how it works, here is a quick illustration that I made:
 
 The frame is for coloring the table lines between the rows and the table stroke (the outside table border). And you will need to add “crop frame content” and “corner radius” to shape the table.
 If you add “Auto layout,” it would work like this:
 
 
 As a result, you would get this behavior. Hold Shift and double-click to highlight it, then resize the column.
 
 For fixed-size cells, we apply Fixed horizontally:
 
 For the responsive cells, we need to set Fill horizontally:
 
 Then we turn this table into a Frame, and every row inside the Frame should have horizontal Constraints set as Left and right:
 
 Voilà, we’re fully responsive now!
 
 The kit structure:
 
 icons we use in the table,
 basic header states,
 basic cell states.
 
 Icons
 Using any icon library, you can have a few hundred icons. As a result, this can push you to inconsistency (using different icons for the same goals, for example), especially if you have more than one designer on your team. Table icons as a separate library will help you manage and support consistency on big projects.
 Combinations
 There are a few main combinations we have: 
 
 just text in a cell,
 just an icon or a set of icons in a cell,
 a variety of text, icons, and other objects (checkbox, toggle, action, select, and so on) in a different order within a cell.
 
 Avoid hidden layers! You will know that you used them while building a design system, and you will certainly forget about them later. In addition, people who will use your design system may not know about these hidden layers at all.
 You will have an idea of how to create them based on the illustration above (Building a basic table kit), but I’ll specify a few more complex components for beginner designers.
 
 And we use Auto layout with the following parameters in the second component example:
 
 So, remember the table that we built using only two components? It’s time to update it!
 
 Also, you can use “Figma Properties” to make it compact. All the instructions you can find in the following tutorial video created by the Figma team during Figma Config 2022: “Jumping into component properties.”
 
 It’s only one example of how I structured the basic table kit in this article. You can use a similar workflow or create your own. In my projects, kits are much more complex, so I’ll leave this choice to you. 
 Figma Design File
 I have prepared a Figma Design file that may help you go through some of the steps of my tutorial. If you have questions or need help, do post your questions in the comments section at the end of the article.
 
 Conclusion
 The way I am working with tables in Figma is not as black and white. The approach mainly depends on the product you’re designing and, of course, there are a few possible ways you could achieve the same goals.
 Here are a few general recommendations I can make from my own practice:
 
 Keeping the line components on the design system side provides a chance to update tables for the whole project from one place. But every time you want to make an update, you will need to publish changes on the design system-level.
 If you keep tasks in different documents, don’t forget to disconnect that file from the design system. This would help avoid uncontrolled updates that you will miss.
 At first, using resizable components seems too tempting… until you need to begin supporting different styles in every size. If you have tables with varying line heights, it’s better to create individual components for each one of them.
 There is an approach that consists of using as few components as possible. But most of the time, you don’t look at your components — instead, you use “variants” to switch between them. So, it’s better to have enough separate components and, as a result, “variants” than to use hidden layers, the “Auto layout” option, and components inside other components that would be hard to manage later on.
 Check that all table cells support at least two lines of text. You can use 16 px line spacing to make it happen.  
 I recommend using the minimum width for parent components (minimum width for each column). But these default minimum sizes have to be discussed with the front-end developers as they may sometimes have their own limitations. Therefore you need to ensure that everything in the design can be implemented in the later development stages.
 Create a color palette in your Design System for the tables, so you would be able to control all the colors from one place. Of course, you can use shared colors from the palette, but once you need to change text color in the tables, background, or something else, you will get into trouble. 
 Create different text styles for the tables. For example, we use smaller line spacing in tables than in news feeds or articles. Having separate text settings would help you avoid future conflicts.
 
 Thank you for following me along! As I already said, tables are a complex component, and I can talk about this topic for days. But maybe better to stop here and give you a chance to try this approach for yourself. Then, if you have questions, I’d be happy to reply and help! Or I could write another article: “Working With Tables in Figma: The Pro Level.” ;-)
 Further Reading
 I have collected a few links to resources (tutorials, plugins, discussions, etc.) related to working with tables in Figma:
 
 “Creating Tables In Figma,” Sasha BelichenkoA guide about one possible way of working with tables in Figma: how to create a table using components and Atomic Design methodology, and then how to integrate the table into your design system.
 “Create a Figma Prototype with Data from Google Sheets,” Bryan ElkusThe article covers in detail a plugin for Figma called Google Sheets Sync. It allows a user to pull in content directly from Google Sheets which is super-useful if you want to use this to populate your designs with more realistic data.
 “Creating Tables in Figma with Auto Layout”, Gavin McFarlandIn this tutorial, Gavin explains how to modify tables in Figma which are completely fluid (with the Auto Layout feature). You can inspect the components in Figma Design to see how they were created.
 
 Tweets
 I spend my whole Figma life designing tables. I imagine other designers too. @figma can you please give us more features, like draggable horizontal rows AND vertical columns? Moving data around should be super easy, like Google Sheets.— Joshua Sortino (@sortino) April 11, 2022 
 
 I have a love/hate relationship with tables, so here&#x27;s how I set up my design system to make things easier. Rows vs. columns, cell variants, and a &quot;module&quot; component with a variable toolbar and variable pagination.https://t.co/0MbCROJAmp pic.twitter.com/xztjdwoVeL— Jon Moore (@TheJMoore) April 12, 2022 
 
 We hear tables in @figma are hard, and we agree.Here&#x27;s how we leveraged our internal design tools to create a more seamless workflow for designers across the @DesigningUber team ➡️ pic.twitter.com/R8PwiYdebK— Vincent van der Meulen (@vincentmvdm) April 11, 2022 
 
 A short Twitter thread on this topic, also mentioning the Configurator plugin that Vincent’s team made.
 I found a pretty reliable way to create flexible, responsive custom tables in Figma. I’ll do a video walkthrough at some point, but if you want to play… https://t.co/cibZI3Uk4g— Buzz Usborne (@buzzusborne) April 6, 2022 
 
 Did I make a full video about building tables in Figma? Yes. Do I regret going down this rabbit hole? Also yes. 📺🕳️ https://t.co/JCyLxEBktG— Buzz Usborne (@buzzusborne) April 13, 2022 
 
 Tips time!Using component props, we can create &quot;infinite tables&quot;So we can toggle on however many columns / rows we need in designsThis prevents us maintaining large variant sets for every permutation of table 🍽Community file to play with: https://t.co/WqNM5SMjSE pic.twitter.com/yhefqrNImC— luis. (@disco_lu) May 30, 2022
 
 Note: This technique is interesting if you have just a few tables in the product design. Otherwise it would be a problem to scale the system.
 As you can see, dealing with tables is a “hot topic” 🔥 in the Figma design community! I hope that you could find something useful here, too.</content>
     </entry>
     <entry>
       <title>Collective #716</title>
         <link href="https://tympanus.net/codrops/collective/collective-716/"/>
       <updated>2022-06-16T16:54:31.000Z</updated>
       <content type="text">
 
 
  
 Inspirational Website of the Week: Jorge Toloza
 An excellent design with a unique feel and smooth animations. Our pick this week.
 Get inspired
 
 
 
 
         
 
 
 
 
 
 
 This content is sponsored via BuySellAds
 Build websites faster with Divi Cloud
 Divi Cloud is like Dropbox for your Divi websites: save something to Divi Cloud and it becomes available on all of your and your clients’ websites while you build them.
 Check it out
 
 
 
 
 
  
 Mobile-First CSS: Is It Time for a Rethink?
 Patrick Clancey summarizes the advantages of the mobile first approach and some alternate solutions if mobile-first doesn’t seem to suit a project.
 Read it
 
 
 
 
 
  
 The World’s Most Satisfying Checkbox
 Some great insight into how the (Not Boring) Habits app became Apple Design Award winner with a game feel approach.
 Read it
 
 
 
 
 
  
 Fresh
 Just in time edge rendering, island based interactivity, and no configuration TypeScript support using Deno.
 Check it out
 
 
 
 
 
  
 Untools
 A collection of thinking tools and frameworks to help you solve problems, make decisions and understand systems.
 Check it out
 
 
 
 
 
  
 Software Engineering
 Addy Osmani shares some of the software engineering soft skills he has learned from his first 10 years on Google Chrome.
 Read it
 
 
 
 
 
  
 CSS Shadow Gradients
 A generator for supercool shadow gradients with pure CSS. By Alvaro Trigo.
 Check it out
 
 
 
 
 
  
 In circles and spheres
 In this Offscreen Canvas edition, you’ll learn all about looping over circles, sines, cosines, and spheres.
 Read it
 
 
 
 
 
  
 Simple CSS solutions
 Kevin Powell shows some straigh-forward CSS solutions using pseudo-classes.
 Watch it
 
 
 
 
 
  
 Aspects of Accessibility (a11y) – Semantics, Contrast and… Anxiety?
 An article by Sara J. Wallén that covers some a11y issues, also looking at anxiety-inducing design.
 Read it
 
 
 
 
 
  
 Plain text. With lines.
 Kartik Agaram writes about an app he made that allows to write and draw.
 Check it out
 
 
 
 
 
  
 Stack Tower Game
 Michal Zalobny made this Stack Tower Game with THREE.js and React based on Hunor Márton Borbély’s tutorial.
 Check it out
 
 
 
 
 
  
 The End of Localhost
 Learn why swyx thinks that in the future, most development will not be done on localhost.
 Read it
 
 
 
 
 
  
 Tooll 3 – A realtime animation toolkit
 An open source software for motion design and procedural content generation. Makers are calling for folks to help and try it out!
 Check it out
 
 
 
 
 
  
 Three.js Particle Skull
 Anderson Mancini made this interactive skull in Three.js. Watch the breakdown and check out the code.
 Check it out
 
 
 
 
 
  
 Better scrolling through modern CSS
 Learn all about improving scrolling through CSS in this article by Mayank.
 Check it out
 
 
 
 
 
  
 Useful utilities and toys over DNS
 Free and useful services over DNS accessible on command line.
 Check it out
 
 
 
 
 
  
 How to: Make your multilingual website suitable for RTL
 Some knowledgeable things to make your website suitable for RTL and LTR languages with just HTML and CSS.
 Check it out
 
 
 
 
 
  
 I’ve locked myself out of my digital life
 A very interesting article by Terence Eden on a situation that shows the limits of the “Code Is Law” movement.
 Read it
 
 
 
 
 
  
 Calendar.txt
 Keep your calendar in a plain text file with Calendar.txt. It is versionable, supports all operating systems and easily syncs with Android mobile phone.
 Check it out
 
 
 
 
 
  
 Patterns
 The W3C Web Accessibility Initiative (WAI) shares some essential patterns for understanding when and why to use ARIA.
 Check it out
 
 
 
 
 
  
 Dialogs and shadow DOM: can we make it accessible?
 Nolan Lawson takes another look at getting dialogs to play nicely with shadow DOM.
 Read it
 
 
 
 
 
  
 Sake
 Sake is a command runner for local and remote hosts. You define servers and tasks in a sake.yaml config file and then run the tasks on the servers.
 Check it out
 
 
 The post Collective #716 appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>Hacks Decoded: Bikes and Boomboxes with Samuel Aboagye</title>
         <link href="https://hacks.mozilla.org/2022/06/hacks-decoded-bikes-and-boomboxes-with-samuel-aboagye/"/>
       <updated>2022-06-16T15:00:15.000Z</updated>
       <content type="text">Welcome to our Hacks: Decoded Interview series!
 Once a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.
 Meet Samuel Aboagye!
 Samuel Aboagye is a genius. Aboagye is 17 years old. In those 17 years, he’s crafted more inventions than you have, probably. Among them: a solar-powered bike and a Bluetooth speaker, both using recycled materials. We caught up with Ghanaian inventor Samuel Aboagye over video chat in hopes that he’d talk with us about his creations, and ultimately how he’s way cooler than any of us were at 17.
 
 https://hacks.mozilla.org/files/2022/06/Untitled.mp4
  
 Samuel, you’ve put together lots of inventions like an electric bike and Bluetooth speaker and even a fan. What made you want to make them?
 For the speaker, I thought of how I could minimize the rate at which yellow plastic containers pollute the environment.  I tried to make good use of it after it served its purpose. So, with the little knowledge, I acquired in my science lessons, instead of the empty container just lying down and polluting the environment, I tried to create something useful with it.  
 After the Bluetooth speaker was successful, I realized there was more in me I could show to the universe. More importantly, we live in a very poor ventilated room and we couldn’t afford an electric fan so the room was unbearably hot. As such, this situation triggered and motivated me to manufacture a fan to solve this family problem.
 With the bike, I thought it would be wise to make life easier for the physically challenged because I was always sad to see them go through all these challenges just to live their daily lives. Electric motors are very expensive and not common in my country, so I decided to do something to help. 
 Since solar energy is almost always readily available in my part of the world and able to renew itself, I thought that if I am able to make a bike with it, it would help the physically challenged to move from one destination to another without stress or thinking of how to purchase a battery or fuel.  
 So how did you go about making them? Did you run into any trouble?
 I went around my community gathering used items and old gadgets like radio sets and other electronics and then removed parts that could help in my work. With the electrical energy training given to me by my science teacher after discovering me since JHS1, I was able to apply this and also combined with my God-given talent. 
 Whenever I need some sort of technical guidance, I call on my teacher Sir David. He has also been my financial help for all my projects.  Financing projects has always been my biggest struggle and most times I have to wait on him to raise funds for me to continue.
 The tricycle: Was it much harder to make than a bike?
 ​​Yes, it was a little bit harder to make the tricycle than the bike. It’s time-consuming and also cost more than a bike. It needs extra technical and critical thinking too. 
 You made the bike and speaker out of recycled materials. This answer is probably obvious but I’ve gotta ask: why recycled materials?  Is environment-friendly tech important to you?
 I used recycled materials because they were readily available and comparable to cheap and easy to get. With all my inventions I make sure they are all environmentally friendly so as not to pose any danger now or future to the beings on Earth.  But also, I want the world to be a safe and healthy place to be. 
  
 The post Hacks Decoded: Bikes and Boomboxes with Samuel Aboagye appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>How to create a web component with vanilla JS</title>
         <link href="https://gomakethings.com/how-to-create-a-web-component-with-vanilla-js/"/>
       <updated>2022-06-16T14:30:00.000Z</updated>
       <content type="text">Yesterday, we looked at what web components are. Today, I wanted to share how to actually create one.
 
 Let’s dig in!
 
 Today’s article is an excerpt from my new course and ebook on web components with vanilla JS.
 
 Creating a web component
 
 To help make the concepts in this guide tangible, we’re going to build a really simple web component: greeting-message.
 &lt;greeting-message&gt;&lt;/greeting-message&gt;
 When we’re done, the component will load a button into the UI, and display a welcome message when the button is clicked. If a logout attribute is added to it, the button gets removed from the UI, and a “goodbye message” is shown.
 
 We’ll also include some custom styling, and add some hooks that developers can use to customize it a bit (without breaking things).
 
 Registering a web component
 
 To create a web component, the first thing you have to do is register it with JavaScript.
 
 To do that, we’ll first use a JavaScript class to extend the HTMLElement object. Just like with a traditional constructor pattern, our class name should be in Title Case.
 
 Let’s call this one GreetingMessage.
 // Extend the HTMLElement class to create the web component
 class GreetingMessage extends HTMLElement {
 	// We&#x27;ll create our web component here
 }
 
 After we create our new class, we need to define our component using the CustomElementRegistry.define() method.
 
 The first argument is the name of the element. This is the name of the actual element itself in your HTML. It must include at least one dash (-). Single-word web components are not allowed.
 
 The second argument is the constructor, the new class that you create for your web component.
 
 (As a best practice, we should make sure that customElements exist as an object in the window first.)
 // Extend the HTMLElement class to create the web component
 class GreetingMessage extends HTMLElement {
 	// We&#x27;ll create our web component here
 }
 
 // Define the new web component
 if (&#x27;customElements&#x27; in window) {
 	customElements.define(&#x27;greeting-message&#x27;, GreetingMessage);
 }
 
 Now, we’ve registered a new web component. It doesn’t do anything yet, but it exists!
 
 The web component lifecycle
 
 As the browser parses and renders your web component into the DOM, there are a few lifecycle callback functions that run at various times.
 
 
 The constructor() method is run when the element is created, before its injected into the UI.
 The connectedCallback() method is run when the element is injected into the DOM, and again whenever it’s moved or appended elsewhere.
 The disconnectedCallback() method is run whenever the element is removed from the DOM.
 
 
 We can include functions that run on each of these events inside our web component class.
 
 Because we’re extending an existing class, the constructor() function needs to include the super() method, which provides access to the parent class’s properties and methods.
 // Extend the HTMLElement class to create the web component
 class GreetingMessage extends HTMLElement {
 
 	/**
 	 * The class constructor object
 	 */
 	constructor () {
 
 		// Always call super first in constructor
 		super();
 
 		console.log(&#x27;Constructed&#x27;, this);
 
 	}
 
 	/**
 	 * Runs each time the element is appended to or moved in the DOM
 	 */
 	connectedCallback () {
 		console.log(&#x27;connected!&#x27;, this);
 	}
 
 	/**
 	 * Runs when the element is removed from the DOM
 	 */
 	disconnectedCallback () {
 		console.log(&#x27;disconnected&#x27;, this);
 	}
 
 }
 
 If you include a web component in the UI and do nothing else on the page, the constructor() method will run, followed by the connectedCallback().
 // On page load, the browser would log...
 // &quot;Constructed&quot; &lt;greeting-message&gt;&lt;/greeting-message&gt;
 // &quot;connected!&quot; &lt;greeting-message&gt;&lt;/greeting-message&gt;
 
 If you were to move your element using something like the Element.append() method, the disconnectedCallback() function would run, followed by the connectedCallback() function.
 
 If you removed it with the Element.remove() method, just the disconnectedCallback() function would run.
 let greeting &#x3D; document.querySelector(&#x27;greeting-message&#x27;);
 
 // The console logs...
 // &quot;disconnected&quot; &lt;greeting-message&gt;&lt;/greeting-message&gt;
 // &quot;connected!&quot; &lt;greeting-message&gt;&lt;/greeting-message&gt;
 document.body.append(greeting);
 
 // The console logs...
 // &quot;disconnected&quot; &lt;greeting-message&gt;&lt;/greeting-message&gt;
 greeting.remove();
 
 Generating the web component HTML
 
 Now that we have a registered web component, we need to generate some actual HTML in the UI.
 
 Inside the constructor() function, we can use the Element.innerHTML property to set the HTML inside this, the current instance of the web component element.
 
 In our case, let’s add a p with a button inside it. We’ll also add a .message element, with an ARIA live region on it. We’ll be injecting a greeting in there when the user clicks the button.
 /**
  * The class constructor object
  */
 constructor () {
 
 	// Always call super first in constructor
 	super();
 
 	// Render HTML
 	this.innerHTML &#x3D;
 		&#x60;&lt;p&gt;
 			&lt;button&gt;Hi there!&lt;/button&gt;
 		&lt;/p&gt;
 		&lt;div class&#x3D;&quot;message&quot; aria-live&#x3D;&quot;polite&quot;&gt;&lt;/div&gt;&#x60;;
 
 }
 
 Now, when the web component is loaded, this is what’s displayed in the DOM.
 &lt;greeting-message&gt;
 	&lt;p&gt;
 		&lt;button&gt;Hi there!&lt;/button&gt;
 	&lt;/p&gt;
 	&lt;div class&#x3D;&quot;message&quot; aria-live&#x3D;&quot;polite&quot;&gt;&lt;/div&gt;
 &lt;/greeting-message&gt;
 Now we’ve got a basic web component in place and rendering HTML into the UI. Here’s a demo.
 
 Tomorrow, we’ll look at how to add interactivity.
 ⏰🦉 Early Bird Sale! Today through Monday, get 40% off registration in the next session of the Vanilla JS Academy.</content>
     </entry>
     <entry>
       <title>Web Design Done Well: Delightful Data Visualization Examples</title>
         <link href="https://smashingmagazine.com/2022/06/web-design-done-well-delightful-data-visualization-examples/"/>
       <updated>2022-06-16T09:00:00.000Z</updated>
       <content type="text">They say we are entering the Data Age. There’s certainly enough of the stuff about. Between analytics, public records, and the slow yet steady growth of the Semantic Web, millions of data points are at our fingertips, just waiting to have their stories told.
 Telling captivating stories with data is easier said than done. Spreadsheets don’t exactly get hearts singing. Big pieces of JSON don’t inspire so much as they horrify. It doesn’t need to be that way, though. Data can dance. Data should dance. 
 
 The site is a testament to the importance of publically available data (this will be a running theme in this article). The numbers are pulled through from the National Digital Forecast Database which is updated hourly. As creators Fernanda Viégas and Martin Wattenberg put it, this makes Wind Map a “living portrait” of wind patterns in the contiguous United States.
 DivineComedy.digital
 Data visualization isn’t just about showing information — it’s about showing the connections between information. DivineComedy.digital is a “digital humanities tool” which shows how Dante’s Divine Comedy has manifested itself in the art across the seven centuries since it was published. 
 
 The sections of the book have been broken down into chapters, and the chapters into passages. Each contains a wealth of artworks inspired by the text — from over 70 museums by more than 90 authors. The project is a testament to Dante’s original work, the works it has since inspired, and the power of digital tools to capture the true interconnectedness of things.  
 The Linked Open Data Cloud
 Speaking of the interconnectedness of things, here’s something I became aware of while researching the Semantic Web a couple of years ago. The Linked Open Data Cloud visualizes more that 16,000 links between 1,300 data sources on the Web.  
 
 Maintained by John Philip McCrae for the Insight Centre for Digital Analytics, the graph is built using LOD Cloud Draw. (Though D3’s force-directed graph is perhaps a more mainstream equivalent.) The LODC has come a long way since starting with 12 datasets in 2007.
 
 For those interested in learning more about linked data, DPpedia is a fine place to start. Data visualization tools aren’t much good without data to work their magic on, so networks like this (and the ethos of openness and sharing behind them) are vital.
 United Nations Refugee Project
 I hope you’ve noticed a common theme so far. Data visualization carries extra weight when it has purpose — when it’s more than just something nice to look at. The magic of data visualization is that it can take complex data sets about complex topics and present them in ways that almost anyone can understand. Data visualization can tell stories no other medium can. This stream chart of refugee movement in the 20th and 21st century is a stunning example of that — packed with information yet accessible and clear. 
 
 This was commissioned by the United Nations Refugee Agency and masterminded by the brilliant Curran Kelleher (more on him later). You can almost see the ebb and flow of a crowd of millions. It’s powerful stuff, and just as importantly, it hasn’t watered down the subject matter. Instead, it has brought it to life. 
 Financial Times’ Covid Chart
 Sometimes only a line chart will do. The Financial Times data visualization team knocked it out of the park with their tracking of Covid infections, which became a major point of reference during the early days of the pandemic. It is a true team effort, the culmination of work by developers, designers, and reporters.
 
 Something I especially like about this example of data visualization is the amount of space dedicated to methodology and sources. It takes the time to explain the data and why it’s presented the way it is. Very occasionally these decisions speak for themselves, but it’s best to default to transparency. Give readers the full context.
 
 Data visualization can be beautiful. Well documented data visualization is even better.
 
 Singapore’s Open Data
 Data dashboards have become a lynchpin of the modern web. Singapore’s data.gov.sg is an especially good example of a government making data not just publically available, but readable too. It’s a vast, explorable data dashboard — one we can all learn from, both in terms of design and accessibility. 
 
 With almost 2,000 data sets on subjects ranging from property prices to short story programmes, the site feels like a virtual library. Browsing is easy and intuitive. The data is accessible in every sense of the word, trying to live up to its own data sharing principles: 
 
 data shall be made easily accessible;
 data shall be made available for co-creation;
 data shall be released in a timely manner;
 data shall be shared in machine-readable format;
 data shall be as raw as possible.
 
 Hear, hear.
 The Pudding Explores The Pitch Of Pop Music
 It’s a question that keeps all of us awake at night — are men singing at a higher pitch in pop music than they used to? Luckily, this project by data viz magicians The Pudding gives us the answer. Are Men Singing Higher in Pop Music? uses vocal register data from Pandora to find the average pitch of every song featured in the Billboard Hot 100 since 1958. No, really. They did.
 
 What on the surface may seem a silly (if fun) idea quickly unfolds into a fascinating exploration of music trends, weaving together audio, video, and god honest line graphs into one colorful package. 
 The piece is also an (admittedly inverted) glimpse into the possibilities of data sonification, a concept hauntingly realized by data journalist Simon Huwiler with his covid deaths music box.
 Solar Eclipse Map
 Masterminded by visual journalist Denise Lu, this eclipse map by the Washington Post is both stunning and useful — a winning data viz combination if ever there was one. The article asks for your year of birth, then displays every solar eclipse in your lifetime (provided you live to 100). How does it do this? On the globe, of course.
 
 What by rights should appear extremely complicated is here made simple and intuitive. As is typical of the very best uses of data visualization, you wonder how or why anyone would ever want to present the same information another way. And the globe screenshotted above is just the tip of the iceberg. The article goes on to explore all sorts of fascinating stuff — from degrees of eclipse to the path of totality. 
 IMPF Dashboard
 The seismic impact of the Covid pandemic has lent itself especially well to data visualization over the last couple of years. Infections, hospitalisations, deaths, reinfections, vaccinations… all this and more add up to the story of the pandemic. And that’s to say nothing of its impact on politics, economies, and culture. The amount of data available is overwhelming, but as this dashboard by the German Federal Ministry of Health shows, it can be wrangled into forms we can all understand.
 
 Delving into vaccination data by region, age group, and even manufacturers, the dashboard is its own little masterclass in data visualization. It’s beautifully designed too, with an appealing palette and impressive range of styles giving each visual a distinctive look and feel. Government data doesn’t have to be boring or opaque. And again, it’s all available to download. 
 Lunar Open Architecture
 I don’t think we ever truly lose our childlike wonder for space exploration. Now more than ever humankind is reaching for the stars - but who’s doing what? And how’s it going? Lunar Open Architecture (LOA) seeks to answer those questions and a whole lot more. A collaboration between MIT Media Lab’s Space Exploration Initiative and the Open Lunar Foundation, LOA includes a stunning, outer space-like data visualization of missions since 1958.
 
 By grouping missions together by organizations, type, and status, the constellation of nodes gives you an immediate sense of how space exploration has evolved over time — and how it’s likely to evolve in the future. As MIT put it, “the future of lunar exploration is getting crowded.” Projects like this show that fact is something the masses can and should understand. It’s not rocket science. 
 Nationscape Insights
 There’s an awful lot of bluster around when it comes to the will of the people. Many claim to know what it is, yet seldom do they reach the same conclusions. Probably because they often don’t consult the people themselves. The Nationscape Insights project — a collaboration between the Democracy Fund Voter Study Group, UCLA, and USA Today — is an attempt to strip away the rhetoric from politics and show what voters actually think.  
 
 Containing 80 weeks of survey data collected in the build up to the 2020 US election, the Nationscapes Insights project allows you to explore public opinions on topics ranging from gun control to the Green New Deal, with filters for region, demographics, and political party. It’s a remarkable dataset. The interface is intuitive, clear, and — I think — impressively nonpartisan. The methodology is clear. You can see the questions interviewees were asked. It is data visualization that clarifies rather than clouds, and that’s always a worthy goal. 
 Get Your Data Viz On
 The data visualization examples shared here present but a fraction of what’s possible. If a data set exists, it can be expressed in fascinating, informative ways. It just needs storytellers (who show their methodology).
 In the interests of furthering data visualization everywhere, below is a selection of learning resources, libraries, articles, websites, and  other resources to sink your teeth into. Explore, play, do the unexpected. Go make something beautiful.
 Learning resources
 
 Data Analysis and Visualization Foundations Specialization, a free Coursera course by IBM
 freeCodeCamp D3 basics
 freeCodeCamp’s 17-hour data visualization course taught by Curran Kelleher. D3, React, SVGs — this is the real deal, and you can code along. You’ll be amazed at the stuff you’re making by the end of it.
 
 Libraries
 
 D3
 Three
 P5.js
 WebGL
 Web Audio API
 
 For Your Bookmarks
 
 The Pudding
 The Guardian’s visual journalism
 NYT visuals team
 FiveThirtyEight
 The /r/dataisbeautiful subreddit
 Sigma Awards
 Our World in Data
 La Nacion 
 Texty.org.ua
 
 Further Reading
 
 “From Good To Great In Dashboard Design: Research, Decluttering And Data Viz”, Adam Fard
 “Data Visualization With ApexCharts”, Nefe Emadamerho-Atori
 “Can Data Visualization Improve The Mobile Web Experience?”, Suzanne Scacca
 “A Guide To Building SVG Maps From Natural Earth Data”, Chris Youderian
 “Fun With Physics In Data Visualization”, Antanas Marcelionis
 “Data Visualization and Infographics Resources”, Cameron Chapman
 </content>
     </entry>
     <entry>
       <title>Conditionally Styling Selected Elements in a Grid Container</title>
         <link href="https://css-tricks.com/conditionally-styling-selected-elements-in-a-grid-container/"/>
       <updated>2022-06-15T14:15:50.000Z</updated>
       <content type="text">Calendars, shopping carts, galleries, file explorers, and online libraries are some situations where selectable items are shown in grids (i.e. square lattices). You know, even those security checks that ask you to select all images with crosswalks or whatever.
 
 
 
 
 
 
 
 🧐
 
 
 
 I found a neat way to display selectable options in a grid. No, not recreating that reCAPTCHA, but simply being able to select multiple items. And when two or more adjoining items are selected, we can use clever :nth-of-type combinators, pseudo elements, and the :checked pseudo-class to style them in a way where they look grouped together.
 
 
 
 CodePen Embed Fallback
 
 
 
 The whole idea of combinators and pseudos to get the rounded checkboxes came from a previous article I wrote. It was a simple single-column design:
 
 
 
 CodePen Embed Fallback
 
 
 
 This time, however, the rounding effect is applied to elements along both the vertical and horizontal axes on a grid. You don’t have to have read my last article on checkbox styling for this since I’m going to cover everything you need to know here. But if you’re interested in a slimmed down take on what we’re doing in this article, then that one is worth checking out.
 
 
 
 Before we start…
 
 
 
 It’ll be useful for you to take note of a few things. For example, I’m using static HTML and CSS in my demo for the sake of simplicity. Depending on your application you might have to generate the grid and the items in it dynamically. I’m leaving out practical checks for accessibility in order to focus on the effect, but you would definitely want to consider that sort of thing in a production environment.
 
 
 
 Also, I’m using CSS Grid for the layout. I’d recommend the same but, of course, it’s only a personal preference and your mileage may vary. For me, using grid allows me to easily use sibling-selectors to target an item’s ::before and ::after pseudos.
 
 
 
 Hence, whatever layout standard you might want to use in your application, make sure the pseudos can still be targeted in CSS and ensure the layout stays in tact across different browsers and screens.
 
 
 
 Let’s get started now
 
 
 
 As you may have noticed in the earlier demo, checking and unchecking a checkbox element modifies the design of the boxes, depending on the selection state of the other checkboxes around it. This is possible because I styled each box using the pseudo-elements of its adjacent elements instead of its own element.
 
 
 
 The following figure shows how the ::before pseudo-elements of boxes in each column (except the first column) overlap the boxes to their left, and how the ::after pseudo-elements of boxes in each row (except the first row) overlap the boxes above.
 
 
 
 
 
 
 
 Here’s the base code
 
 
 
 The markup is pretty straightforward:
 
 
 
 &lt;main&gt;
   &lt;input type&#x3D;checkbox&gt; 
   &lt;input type&#x3D;checkbox&gt; 
   &lt;input type&#x3D;checkbox&gt;
   &lt;!-- more boxes --&gt;
 &lt;/main&gt;
 
 
 
 There’s a little more going on in the initial CSS. But, first, the grid itself:
 
 
 
 /* The grid */
 main {
   display: grid;
   grid:  repeat(5, 60px) / repeat(4, 85px);
   align-items: center;
   justify-items: center;
   margin: 0;
 }
 
 
 
 That’s a grid of five rows and four columns that contain checkboxes. I decided to wipe out the default appearance of the checkboxes, then give them my own light gray background and super rounded borders:
 
 
 
 /* all checkboxes */
 input {
   -webkit-appearance: none;
   appearance: none;
   background: #ddd;
   border-radius: 20px;
   cursor: pointer;
   display: grid;
   height: 40px;
   width: 60px;
   margin: 0;
 }
 
 
 
 Notice, too, that the checkboxes themselves are grids. That’s key for placing their ::before and ::after pseudo-elements. Speaking of which, let’s do that now:
 
 
 
 /* pseudo-elements except for the first column and first row */
 input:not(:nth-of-type(4n+1))::before,
 input:nth-of-type(n+5)::after {
   content: &#x27;&#x27;;        
   border-radius: 20px;
   grid-area: 1 / 1;
   pointer-events: none;
 }
 
 
 
 We’re only selecting the pseudo-elements of checkboxes that are not in the first column or the first row of the grid. input:not(:nth-of-type(4n+1)) starts at the first checkbox, then selects the ::before of every fourth item from there. But notice we’re saying :not(), so really what we’re doing is skipping the ::before pseudo-element of every fourth checkbox, starting at the first. Then we’re applying styles to the ::after pseudo of every checkbox from the fifth one.
 
 
 
 Now we can style both the ::before and ::after pseudos for each checkbox that is not in the first column or row of the grid, so that they are moved left or up, respectively, hiding them by default.
 
 
 
 /* pseudo-elements other than the first column */
 input:not(:nth-of-type(4n+1))::before { 
   transform: translatex(-85px);
 }
 
 /* pseudo-elements other than the first row */
 input:nth-of-type(n+5)::after {
  transform: translatey(-60px); 
 }
 
 
 
 Styling the :checked state
 
 
 
 Now comes styling the checkboxes when they are in a :checked state. First, let’s give them a color, say a limegreen background:
 
 
 
 input:checked { background: limegreen; }
 
 
 
 A checked box should be able to re-style all of its adjacent checked boxes. In other words, if we select the eleventh checkbox in the grid, we should also be able to style the boxes surrounding it at the top, bottom, left, and right.
 
 
 
 
 
 
 
 This is done by targeting the correct pseudo-elements. How do we do that? Well, it depends on the actual number of columns in the grid. Here’s the CSS if two adjacent boxes are checked in a 5⨉4 grid:
 
 
 
 /* a checked box&#x27;s right borders (if the element to its right is checked) */
 input:not(:nth-of-type(4n)):checked + input:checked::before { 
   border-top-right-radius: 0; 
   border-bottom-right-radius: 0; 
   background: limegreen;
 }
 /* a checked box&#x27;s bottom borders (if the element below is checked) */
 input:nth-last-of-type(n+5):checked + * + * + * + input:checked::after {
   border-bottom-right-radius: 0;
   border-bottom-left-radius: 0;
   background: limegreen;
 }
 /* a checked box&#x27;s adjacent (right side) checked box&#x27;s left borders */
 input:not(:nth-of-type(4n)):checked + input:checked + input::before {         
   border-top-left-radius: 0; 
   border-bottom-left-radius: 0; 
   background: limegreen;
 }
 /* a checked box&#x27;s adjacent (below) checked box&#x27;s top borders */
 input:not(:nth-of-type(4n)):checked + * + * + * +  input:checked + input::before { 
   border-top-left-radius: 0; 
   border-top-right-radius: 0; 
   background: limegreen;
 }
 
 
 
 If you prefer you can generate the above code dynamically. However, a typical grid, say an image gallery, the number of columns will be small and likely a fixed number of items, whereas the rows might keep increasing. Especially if designed for mobile screens. That’s why this approach is still an efficient way to go. If for some reason your application happens to have limited rows and expanding columns, then consider rotating the grid sideways because, with a stream of items, CSS Grid arranges them left-to-right and top-to-bottom (i.e. row by row).
 
 
 
 We also need to add styling for the last checkboxes in the grid — they’re not all covered by pseudo-elements as they are the last items in each axis.
 
 
 
 /* a checked box&#x27;s (in last column) left borders */
 input:nth-of-type(4n-1):checked + input:checked {
   border-top-left-radius: 0;
   border-bottom-left-radius: 0;
 }
 /* a checked box&#x27;s (in last column) adjacent (below) checked box&#x27;s top borders */
 input:nth-of-type(4n):checked + * + * + * + input:checked {
   border-top-left-radius: 0;
   border-top-right-radius: 0;
 }
 
 
 
 Those are some tricky selectors! The first one…
 
 
 
 input:nth-of-type(4n-1):checked + input:checked
 
 
 
 …is basically saying this:
 
 
 
 A checked &lt;input&gt; element next to a checked &lt;input&gt; in the second last column.
 
 
 
 And the nth-of-type is calculated like this:
 
 
 
 4(0) - 1 &#x3D; no match
 4(1) - 1 &#x3D; 3rd item
 4(2) - 1 &#x3D; 7th item
 4(3) - 1 &#x3D; 11th item
 etc.
 
 
 
 So, we’re starting at the third checkbox and selecting every fourth one from there. And if a checkbox in that sequence is checked, then we style the checkboxes adjacent, too, if they are also checked.
 
 
 
 And this line:
 
 
 
 input:nth-of-type(4n):checked + * + * + * + input:checked
 
 
 
 Is saying this:
 
 
 
 An &lt;input&gt; element provided that is checked, is directly adjacent to an element, which is directly adjacent to another element, which is also directly adjacent to another element, which, in turn, is directly adjacent to an &lt;input&gt; element that is in a checked state.
 
 
 
 What that means is we’re selecting every fourth checkbox that is checked. And if a checkbox in that sequence is checked, then we style the next fourth checkbox from that checkbox if it, too, is checked.
 
 
 
 CodePen Embed Fallback
 
 
 
 Putting it to use
 
 
 
 What we just looked at is the general principle and logic behind the design. Again, how useful it is in your application will depend on the grid design.
 
 
 
 I used rounded borders, but you can try other shapes or even experiment with background effects (Temani has you covered for ideas). Now that you know how the formula works, the rest is totally up to your imagination.
 
 
 
 Here’s an instance of how it might look in a simple calendar:
 
 
 
 CodePen Embed Fallback
 
 
 
 Again, this is merely a rough prototype using static markup. And, there would be lots and lots of accessibility considerations to consider in a calendar feature.
 
 
 
 
 
 
 
 That’s a wrap! Pretty neat, right? I mean, there’s nothing exactly “new” about what’s happening. But it’s a good example of selecting things in CSS. If we have a handle on more advanced selecting techniques that use combinators and pseudos, then our styling powers can reach far beyond the styling one item — as we saw, we can conditionally style items based on the state of another element.
 
 Conditionally Styling Selected Elements in a Grid Container originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>The Guide To Windows High Contrast Mode</title>
         <link href="https://smashingmagazine.com/2022/06/guide-windows-high-contrast-mode/"/>
       <updated>2022-06-15T09:30:00.000Z</updated>
       <content type="text">When we talk about accessibility, we tend to talk about many things — such as dark mode, keyboard navigation, prefers-reduced-motion, and screen readers — but there is one thing that does not receive that much attention: Windows High Contrast Mode (from now on, abbreviated as WHCM). This is a tendency I have seen in some websites at a point where we have normalized some practices that can harm users’ experience in WHCM. In this article, I want to explain what it is and give a good set of practices we can keep in mind to make our sites more usable with this mode.
 About Windows High Contrast Mode
 High Contrast mode is an accessibility feature that changes the look of our website and Windows applications by replacing the color of the different elements (like background, buttons, or text) with some user’s set up colors. This has multiple purposes, like increasing readability, reducing a website’s visual noise by removing certain elements (and by extension, allowing them to have a better focus), and giving users full control of the website’s contrast. You can check out by going to Settings, then clicking on Accessibility, and finally clicking on High Contrast.
 
 To talk about some statistics, according to Melanie Richard in her talk “The tailored web: effectively honoring visual preferences”, around 4% of active devices use Windows High Contrast Mode, and thanks to WebAIM’s Survey of Users with Low Vision we can estimate that around 30% of users with low vision user Windows High Contrast Mode. All this should give you some perspective about the importance of making our website friendly with this mode.
 The name “High Contrast Mode” is a bit misleading because the user can choose their preferred colors, leading to a color palette that has lower contrast than usual — which is not a very odd case. According to WebAIM’s survey, around 3% of users of Windows High Contrast Mode set it up to create a low contrast color pallete. The users with migraines or light sensitivity can do that to mitigate their disabilities’ impact. Just to give you a quick example:
 
 I’m sure you understand the importance of making our website friendly with WHCM, and you might think that due to its nature of replacing a big part of our styles, making a website that works for that mode can be hard. Great news, it’s not! We just need to consider some important issues to ensure the user experience is not harmed.
 Considerations About Windows High Contrast Mode
 Despite how much control we lose when our website is displayed in WHCM, we can make it work without too much effort as long as we keep in mind some considerations. Before I start with that, I’d like you to keep in mind the golden rule with this mode: above all things, High Contrast Mode is about usability, and we need to respect that above any other aesthetics matter. Our biggest priority with this mode is easing readability and not harming the user experience in any way. 
 How can we ensure readability and usability works in WHCM? We can have certain important considerations for that:
 Use Semantic HTML
 This has been a very important topic when we talk about accessibility due to its importance for screen readers, and it’s very important in WHCM as well! Why? Because Windows will add the styles depending on the semantics of an element and not because of how it looks outside WHCM. A link will have the hyperlinks styles, a button will have the Button Text styles, and so on.
 Some devs (for some reason) decide to use aria roles on divs to camouflage them as buttons for assistive technology. However, in WHCM, aria roles are irrelevant for Windows to determine which style to apply, so we depend on semantics to make our website works properly in this mode.
 To validate this point, let’s check how a div that acts as real button and a link would behave in High Contrast Mode using the same styles.
 &lt;div role&#x3D;&quot;button&quot; class&#x3D;&quot;button&quot; tabindex&#x3D;0&gt;
   Not a button
 &lt;/div&gt;
 &lt;button class&#x3D;&quot;button&quot;&gt;
   Definitely a button
 &lt;/button&gt;
 &lt;a href&#x3D;&quot;#&quot; class&#x3D;&quot;button&quot;&gt;
   This is a link
 &lt;/a&gt;
 
 
 
 .button {
   padding: 0.5em 1em;
   border: 2px solid hotpink;
   background-color: hotpink;
   width: fit-content;
   border-radius: 0.5em;
   font-size: 1.5rem;
   font-family: sans-serif;
   text-decoration: none;
   color: black;
 }
 
 
 In default settings, the div and the button will have the same colors but remember: users can change that. Let’s use this color palette, and let’s check the results:
 
 
 Notice that semantics have a significant matter in WHCM for styling. Remember, in this mode, we have to focus on not harming the user’s experience, and choosing the wrong element can confuse users. 
 transparent Properties Are Useful!
 When we style certain interactive components like buttons or links, we tend to remove certain properties with border: none, outline: none, or text-decoration: none because those properties might not match with our design system. Usually, that’s not a bad idea as long as you keep in mind things like hover or focus state for those components. For WHCM, however,it is a serious problem because background elements are completely overwritten, and we’ll depend on borders to differentiate those components from the background.
 Just to give you an example, a very common design pattern I have seen is with the primary and secondary buttons, where the former has a background color and no border, and the latter has just a border and no background. It looks good, but when you see them under High Contrast Mode: 
 &lt;button class&#x3D;&quot;primary&quot;&gt;
   Primary action
 &lt;/button&gt;
 &lt;button class&#x3D;&quot;secondary&quot;&gt;
   Secondary action
 &lt;/button&gt;
   
 
 
     button {
       font-size: 1.3em;
       padding: 0.5em 1em;
       border: none;
       font-family: sans-serif;
       border-radius: 0.4em;
       background-color: transparent;
     }
 
     .primary {
       background-color: hotpink;
     }
 
     .secondary {
       border: 2px solid hotpink
     }
 
 
 
 The primary button can be easily mistaken for a normal text! This is where transparent borders come into play because transparencies will be visible under a High Contrast Mode. So by replacing the border property in the button element with this: 2px solid transparent, we’ll have this result:
 
     button {
       border: 2px solid transparent
     }
 
 
 
 As you can imagine, that also happens with links if you use the property text-decoration-color: transparent, and with outlines if you use outline-color: transparent. Let’s check some quick examples about those properties.
 text-decoration-color: transparent is useful if you’re using another element to represent a link in your site. Just to give an example, you can use background-image to animate the underline, as you can see in this video made by Kevin Powell. However, in WHCM, you’ll only depend on the color the user has in his settings, so if you want to give an additional visual cue, a transparent underline will work great there! 
 
 Outlines are a particularly important topic in this mode because some developers rely on other properties to add focus states to interactive elements — such as changing the background-color or even the box-shadow hack (even if it’s not necessary nowadays because now the outline will follow the element’s border-radius since Chrome 94 and Firefox 88). However, all those things are completely overwritten in this mode, so outline remains as the only reliable way to apply a focus state on an element in WHCM. Always keep that in mind: if you’re going to use something different than an outline to highlight a focus state in an element, add the property outline-color: transparent as a fallback to not missing a focus state in this mode.
 Keep In Mind Scrollbars
 Scrollbars can be styled, but does that mean we should style them? There are some usability and accessibility concerns about this topic. The one I want to bring here is the fact that, depending on how you style it in WHCM, they’ll look clunky in the best of cases, or they won’t be visible at all at worst of scenarios.
 Is there a way to solve that? That depends on how you decide to style a scrollbar. Some people decide to use a solid color to fill the scrollbar’s thumb, and that does have a very easy fix. Let’s suppose we decided to style our scrollbar that way, then you will go for something like:
 
     ::-webkit-scrollbar {
       width: 20px; 
     }
 
     ::-webkit-scrollbar-track {
       background-color: #e4e4e4;
       border-radius: 100px;
     }
 
     ::-webkit-scrollbar-thumb {
       border-radius: 100px;
       background-color: green;
     }
     
 
 As you might guess, the scrollbar won’t be visible in WHCM due to its background-color property being forcedly replaced. The great news is that we have already seen how to remediate this problem!
 Transparent borders can cover this situation. You can use them to cover all the scrollbar’s thumb, and it’ll look like it’ll have a solid color (the one you choose as text color in settings) which will be pretty similar to how it works as a default scrollbar in this mode. To follow our previous example, if we use the property border: 10px solid transparent, it will make it look like it has a solid background in WHCM.
 Be careful using this technique with scrollbar thumbs styled with box-shadow insets, though. If you do that, it’ll make it invisible. Not in WHCM, I mean invisible outside of it. You can check this problem in this scrollbar style made by Ahmad Shadeed, go to the scrollbar thumb styles, and add the same style we added before (border: 10px solid transparent) . You’ll see it’ll become invisible, a good way to make it visible (both regularly and in WHCM) is just using a smaller border (something like 2px instead of 10px) to make it look like this in WHCM:
 
 It looks good! The only problem is that it looks a bit weird outside of WHCM, so keep this in mind if you decide to style a scrollbar using an inset box-shadow.
 Remember that all that applies only to Chromium-based browsers, as Firefox has a different way to style scrollbars using scrollbar-color and scrollbar-width properties. The good news is that in WHCM, you won’t have to do a thing to make it work properly! The colors will be overwritten, and the scrollbar’s thumb will have the same color user has set up as text color.
 Behavior Of Images
 We have different ways to use images on a site: using the tag img, the background-image CSS property, using SVGs, and even CSS art! Let’s dig about those quickly. img tag will behave the same in WHCM, so let’s talk about the other three.
 First, we have the background-image property — and this one will remain the same in WHCM as long as you’re using the url() value. A gradient made with background-image will be overwritten by WHCM’s background color. However, there is only one catch with this. Even though Firefox supports background images in High Contrast Mode since around 2018-2019, it won’t be visible if you put background-image in the body element.
 You can try it out by seeing the CodePen I made to try to open it while using WHCM. So keep that in mind in case you’re using a background image like that.
 
 That’s a bit problematic. Even when we normally want SVG to remain the same, there should be a way to manage those situations for specific scenarios. The good news is that, indeed, there is one! But let’s put a pin on this topic for now.
 Keep in mind that this scenario only happens in Chromium-based browsers — Firefox has its own way to manage this. SVGs inside an anchor that use the currentColor property will receive the same color as the link color user has set up. It’ll even respect whatever color the theme uses as a visited link, as this picture shows:
 
 Finally, we have CSS art. Due to its nature of using elements like box shadows and background gradients, you might guess it won’t look good in WHCM — and you’re absolutely right. Due to its artistic nature, it’s no big deal, so you should be fine. But, if it does have a purpose in your website, we need to look for a way to make it visible. Just a quick note about CSS art: remember you can — and should — make your CSS art accessible!
 As long as you keep in mind those small suggestions, our website will be almost done for WHCM! As you saw, some elements would need some tweaks to make them work to their full extent in this mode, but luckily for us, CSS has a way to help us to get this last part of the job done!
 Media Query Forced-Colors
 Microsoft made an effort to create a standard to support WHCM, and the result of this work was the media query forced-colors, which will help us to detect if the browser or operating system has enabled a mode that limits a website’s styles to a user-chosen color palette. As you might guess, WHCM is the most popular choice among them.
 This media query will act a bit differently due to how WHCM works. Some properties will be limited to certain values, some won’t be able to be overwritten at all, and we have new properties’ values to work with! But before digging into what we can do with this tool, let’s remember that WHCM (and other modes that restrict user’s color palettes) prioritize usability, and this is something we need to respect. So don’t use those properties unless it’s necessary to tweak some elements in your website to give it good usability in this mode.
 With that said, let’s start talking about the media query itself. It has two values: none and active. The former will detect when there is no forced colors mode active, and the second one will detect when there is. Under forced colors mode, the next properties will be replaced with the ones that are set up by the user:
 
 color
 background-color
 text-decoration-color
 text-emphasis-color
 border-color
 outline-color
 column-rule-color
 -webkit-tap-highlight-color
 SVG fill and stroke attributes
 
 Additionally, there are some properties that will have a forced behavior:
 
   
     Property
     Value
   
   
     box-shadow
     none
   
   
     text-shadow
     none
   
   
     background-image
     none (unless it’s url() )
   
   
     color-scheme
     light dark
   
   
     accent-color
     auto
   
   
     scrollbar-color (Firefox)
     auto
   
 
 
 With that explained, let’s dig into two tools we have we can use to enhance the experience in this mode.
 Forced-Color-Adjust
 Now, how can we change how those properties behave? There is a way to avoid WHCM overwrites colors, and this is by using the property forced-color-adjust. This property has two values: auto and none, and it’ll let us decide if we want an element’s colors will be replaced by the user agent’s colors or not, respectively. Let’s check an example of how those work, and there aren’t better examples than the ones we left uncovered in the previous section!
 Let’s check the link with the external link’s SVG we used earlier. Keep in mind that in Chromium-based browsers, this SVG won’t change its color to match the one that is used as a link color because SVGs have a default value of none. So, if we add the property forced-color-adjust: auto to our SVG as follows:
 .inline-icon {
   /* Previous CSS properties */
   forced-color-adjust: auto;
 }
 
 
 This will be our result:
 
 I know this section is about the media query itself, and usually, what you’d do is put that rule inside the media query like this:
 @media screen and (forced-colors: active) {
   .inline-icon {
     forced-color-adjust: auto;
   }
 }
 
 
 That’s a valid approach (and, honestly, the most intuitive one). However, while I did some experiments for this article, I noticed that you can put this property in an element without the need to use the media query, and you’ll get the same result! And because this property will affect only the behavior of this element in a forced colors scenario, it won’t give you any unexpected behavior. 
 Now, with CSS art, we want the opposite to be true (again, as long as this CSS is necessary to give enough context to the user), so we can use forced-color-adjust: none in the art’s parent element, and now all of it will be visible in WHCM.
 You may be thinking that this is not a common use case of forced-color-adjust: none, and you’d be right, so let’s check a more realistic one: showing color palletes on your website! Let’s take a look at any pallete generated by mycolor.space for example:
 
 Those colors are not visible, and it’s an important part of the website, so if we go to the color container element and we add this property, we’ll solve this problem. 
 System Colors
 Now let’s talk about colors. With media query forced-colors we have a handful of system colors we can use. You can see a list of colors in MDN’s documentation, and we can use this list of colors to replace certain properties. Using the property color: LinkText will make it look like a link, for example.
 Just remember: those colors are closely related to HTML semantics, so maybe you’d be better changing an element to its correct tag instead of trying to change how it looks in WHCM. That doesn’t mean it doesn’t have its uses. We just have to be sure we are doing this for the right reasons. Which is a good reason to use this? Well, that depends on the complexity of what you are creating. Let’s take, as an example, this link I created with the help of the clip-path property.
 .link {
   --clip-path: polygon(0% 0%, calc(100% - 0.8em) 0%, 100% 0.8em, 100% 100%, 0.8em 100%, 0% calc(100% - 0.8em));
   font-size: 2rem;
   padding: 0.1em;
   border: none;
   background-color: #0E0054;
   clip-path: var(--clip-path);
   font-family: sans-serif;
 }
 
 .link:focus {
   outline: none;
 }
 
 .link:focus span, .link:hover span {
   outline-offset: -0.5em;
   outline: 3px solid transparent;
   background-color: #0E0054;
   color: white;
   text-decoration-color: white;
 }
 
 .link span {
   display: inline-block;
   padding: 0.5em 1.2em;
   clip-path: var(--clip-path);
   background-color: white;
   color: #0E0054;
   text-decoration: underline #0E0054;
   text-underline-offset: 2px;
 }
 
 .link span {
   display: inline-block;
   padding: 0.5em 1.2em;
   clip-path: var(--clip-path);
   background-color: white;
   color: #0E0054;
   text-decoration: underline #0E0054;
   text-underline-offset: 2px;
 }
 
 
 
 Let’s make a quick check of the problems with this element in WHCM:
 
 I used a background-color to mimic a border with this element, but because it’s a background, it won’t be visible in WHCM.
 Even if I used a transparent outline to make a focus state in this mode, its color would be the one that the system uses as a link color, instead of the one WHCM’s usual outline color.
 
 With this in mind, we can tweak system colors using the media query forced-colors to give enough visual feedback to users by showing them that that is a link.
 @media screen and (forced-colors: active) {
   .link {
     background-color: LinkText;
   }
 
   .link:visited {
     background-color: VisitedText;
   }
 
   .link:focus span {
     outline-color: Highlight;
   }
 }
 
 
 Remember Firefox has a visited state color for links, so to respect that. We should add the VisitedText system color in the visited pseudo-class of our link. With that said, this is our result:
 
 Another simple example of how we can use system colors to tweak the experience is something we saw in the previous section: scrollbars! Let’s suppose that, for some reason, transparent borders are not an option. In this case, we can use system colors to make our scrollbar looks good in this mode! Let’s come back to one of the examples we used previously, and instead of using a transparent border, we’ll use the media query to tweak the scrollbar’s thumb’s color.
 ::-webkit-scrollbar {
   width: 20px;
 }
 
 ::-webkit-scrollbar-track {
   background-color: #e4e4e4;
   border-radius: 100px;
 }
 
 ::-webkit-scrollbar-thumb {
   border-radius: 100px;
   background-color: green;
 }
 
 @media screen and (forced-colors: active) {
   ::-webkit-scrollbar-thumb {
     background-color: CanvasText;
   }
 }
    
 
 Other Uses Of This Media Query
 As you read, forced-color-adjust and system colors are great tools to tweak our design if needed, but that’s not all we can do with this media query. Yes, we saw that some properties are restricted to certain uses, but most of them can be used normally! Remember, this is just to improve usability in WHCM, so there is no need to go too wild with that. Use it just when it’s needed.
 Let’s come back to the clip-path link we used. You could decide that the approach to how it looks in WHCM is to use a simpler design, like maybe just using a regular bordered element. We can do that! Let’s ignore the CSS rules I used in my previous example, and let’s use those instead:
 @media screen and (forced-colors: active) {
   .link {
     --clip-path: none;
     border: 3px solid transparent;
     border-radius: 8px;
   }
 
   .link:focus {
     outline: 3px solid transparent;
     outline-offset: 3px;
   }
 
   .link:focus span {
     outline: none;
   }
 }
 
 
 And this is our result:
 
 With that approach, you still show the user this is a link, and you avoid any possible confusion with this topic. Uses of CSS properties in this media query can open some interesting doors to improve how sites work. You can remove a merely decorative image in this mode — with display: none (if you used an img tag) or background-image: none (if you added it with CSS) — if you consider it can bring a better experience — it might have very bright colors for users with migraine, or it can be a bit distracting, for example.
 As long as you prioritize usability in your website with this mode, it should be good enough. However, most of the times you might not need it as long as you keep into consideration the previous suggestions I mentioned.
 You can also use custom properties in this mode, which will lead to some interesting uses, as you can see in this article by Eric Bailey.
 Other Resources
 It’s important to note that in the case you still need to support Internet Explorer, media query forced-colors won’t work. If you want to know how to give support to High Contrast Mode in this browser, you can read about it in this article written by Greg Whitworth and this one by Adrian Roselli. For the topics covered in this article, you can read the following articles:
 
 “Styling for Windows high contrast with new standards for forced colors” by Microsoft Edge Team.
 “Using JavaScript to detect high contrast and dark modes” by Scott O’Hara.
 “Making Websites Work with Windows High Contrast Mode” by Dietra Rater, a case study of Khan Academy website about this topic.
 
 Wrapping Up
 Windows High Contrast Mode is something I have seen some websites overlook, which can create problems for people who use this accessibility feature. The good news is that we have enough tools to make our website works great in WHCM, even more with Microsoft’s efforts to create the media query forced-colors — which opens new doors to make our sites look better in this mode. Just remember: it’s an accessibility and usability feature so keep this in mind when you want to tweak your project in this mode!
 Further Reading On Smashing Magazine
 
 “Windows High Contrast Mode, Forced Colors Mode And CSS Custom Properties,” Eric Bailey 
 “Manage Accessible Design System Themes With CSS Color-Contrast(),” Daniel Yuschik 
 “When CSS Isn’t Enough: JavaScript Requirements For Accessible Components,” Stephanie Eckles 
 “A Complete Guide To Accessible Front-End Components,” Vitaly Friedman 
 </content>
     </entry>
     <entry>
       <title>Computing in Crip Time</title>
         <link href="https://logicmag.io/clouds/computing-in-crip-time"/>
       <updated>2022-06-14T22:12:28.000Z</updated>
       <content type="text">I walk slower these days. Walking used to be about where I was going next: moving fast and hard through space. I paid less attention to the here and now; here was en route to my tomorrow. My old pace matched my lifestyle—stomping through a long Bay Area commute and a calendar packed with business travel. A life filled with too much working. 
 After my spinal cord injury, that all changed. Now, I walk slower. My engagement with the world is different. I no longer just move through space, I spend time there. I spend time here, now. I use a cane to steady myself. My cane is not only an assistive device, it is a symbolic one. My cane signals to others: the way I am interacting with this space is different, and beware, the way you will interact with me is different, too. Disability has redefined my relationship to many things. Not only with spaces, as I describe, but also with time. I now meet the world differently, with a different body, and this body moves, thinks, and acts at a different pace. This is crip time. 
 “Crip time” describes the alternative relationship many disabled people experience with time. As Ellen Samuels, a disability studies scholar, puts it: “Crip time is time travel.” There are many layers to crip time—from the simple fact that many of us walk or roll through space slowly, to the altered trajectories that unfold over the course of a lifetime. Crip time forces a confrontation with the messy realities of the non-normative, all the ways that our temporal experiences fall outside our expectations. Instead of looking to the norm—the averages, patterns, and trends that fill data-dripping techspeak—crip time provokes us to wonder: What about the peculiar? What about the ad hoc, the irregular, the one-off? What about the unique messiness of the here-and-now?
 Because I now walk slower than before, I notice much more: uneven and poorly maintained sidewalks, worn-out or missing “six feet” social-distancing floor stickers, the monstrosity of stairs. I notice when places are not built for people like me. But I also notice everyday beauty. A slower pace allows me to see the craft tangled in the mundane. I notice the playful swooping geometry of songbirds and the peppery glitter of tree shadows as they jump across the path in front of me. I have to take breaks and let my body rest. I must care for myself in ways I never did before, in ways I was never taught to. Will I be able to find something sturdy to sit awhile and rest? What will I do if I can’t?
 Exiting the Flow
 Where does rest figure in our everyday experiences with technology? I work as a user experience (UX) researcher. As UX professionals, we learn about the psychology of response times and how they relate to human thresholds around human-computer interactions. A machine can move too fast for a person to comprehend, which is not ideal. There is a sweet spot, according to industry expert Jakob Nielsen, to aim for: somewhere between 0.1 and 1.0 seconds is ideal for the system to stay in line with a person’s flow of thought. Ten seconds is the outer limit that a user will wait before getting bored or frustrated in waiting for a system response.
 But why do we get bored or frustrated? In our cult to the gods of relentless busyness, continuous digital activity feels holy. Corporate life pressures us to act quickly. We have become accustomed to high-speed internet connectivity and vast computation processing, all of it occurring with near-simultaneity as our fingers release a tap. We scroll, and scroll, and scroll—our feeds feel endless and our clicks take us seamlessly from one task to the next. 
 This feeling of being “in the zone” is a psychological state called flow, and has been a design concern since the early days of personal computing in the 1980s and 1990s. Then, the concerns were largely focused on designing workplace experiences that integrated PCs with analog-driven workflows in ways that didn’t create too much friction or drag, the opposite of flow. 
 Living in crip time means that flow feels rare. Since my spinal cord injury, I no longer rally for a packed work calendar, where meetings bleed together after hours of social interactions over video teleconferencing. Now, I must be strategic. Crip time means I attend some meetings with my camera on, and some with audio only. Others I watch later as a recording. An audio-only meeting likely means I am attending the meeting while pacing, kneeling, or laying down, alternating positions to help relieve the spinal cord pressure and nerve pain that builds up over the course of the day. Watching recordings of meetings later creates lots of drag, as I move between recording files, PowerPoint decks, meeting chats, and relevant emails to make sense after the fact. 
 But this drag can be meaningful in its own way, too. Just as my slowed-down walking means I notice more in the world around me, my crip meeting practice provides me space where I am able to notice differently. Now that I live in crip time, I am always on the hunt for the red buttons and icons of recordkeeping—digital parallels to the benches and seats I watch for now when I walk. Audio-only meetings mean that I am more comfortable physically—and thus can concentrate more freely on the meeting. I take notes and jam into the conversation when I need to. I focus on the dialogue rather than my appearance via webcam. Watching recordings after the fact, I can approach the meeting well-rested and in a comfortable setting. With the ability to hit pause, I am able to wonder, consider, jot down questions. These are all things that are difficult in the flurry of marathon meetings. 
 It’s not that my crip meeting practice appropriates a kind of alternative productivity tool. Instead, my experiences of disability and of crip time have compelled me to wonder if “flow” is really the metric against which all our journeys of work and technology should be judged. As we enter 2022, year three of the Covid-19 pandemic, this question seems particularly acute. Those of us who are able to work from home have spent month after month booting up, logging on, checking our schedule, and loading our inbox each morning. We are expected to seamlessly enter the flow of remote work at super-highway speed, reproducing pre-pandemic levels of productivity and output, even as we continue to face a world in crisis.
 The lens of disability studies and the experiences of the disabled offer us a different way of thinking about our experience of time. In Feminist, Queer, and Crip, Alison Kafer tells us: “Crip time is flex time not just expanded but exploded; it requires reimagining our notions of what can and should happen in time, or recognizing how expectations of ‘how long things take’ are based on very particular minds and bodies… Rather than bend disabled bodies and minds to meet the clock, crip time bends the clock to meet disabled bodies and minds.” Just as I have come to learn from my own disabled body my new walking pace, the time it takes my body to finish the chores of independent living, I also learn how long it takes my body to bring an article like this together or to finish an important deliverable at work. Where before I let pressures of various sorts build up to optimize my output drive overwork, I now live in crip time. Things get done, just at a different pace. Doing is re-imagined and re-configured, a process driven by my body’s differing, situated abilities, instead of some trend, pattern, or prediction. Achievement is still possible—and I do still achieve—just on my body’s own terms. What are your body’s terms?</content>
     </entry>
     <entry>
       <title>Black Boxes</title>
         <link href="https://logicmag.io/clouds/black-boxes"/>
       <updated>2022-06-14T22:12:15.000Z</updated>
       <content type="text">Adapted from Redacted (Taller California, 2021)
 San Diego is one of the most surveilled places in the United States. Located on the border with Mexico, America’s Finest City is host to one of the largest US Navy bases and a wide array of government agencies that see technology as the key to better managing public infrastructure, tracking the movements of people, and solving crimes. Hidden behind a veil of public safety and national security, the spyware is often invisible to the public.
 For years, officials have been rolling out technologies that they don’t totally understand and with little to no meaningful oversight, turning San Diego into a laboratory for the rest of the country. The residents here did not consent to this development, let alone vote on it—a reality that pits the interests of public planning and law enforcement against the civil liberties and civil rights of everyone else.
 It’s also common to find that technological experimentation in San Diego serves the interests of private companies with something to sell. General Atomics is one of these companies. In 2020, the defense contractor decided to repurpose its Predator and Reaper drones as the benign-sounding SkyGuardian and SeaGuardian. But before General Atomics could test out one of its drones above San Diego, it needed permission from the Federal Aviation Administration (FAA). The airspace above the city is a highly restricted place, where unmanned aircraft cannot typically fly.
 The San Diego test flight was supposed to demonstrate that the new drone, the SkyGuardian—weighing up to 12,500 pounds, with a wingspan of seventy-nine feet—could safely travel over a dense metro area while surveilling infrastructure and the natural landscape. San Diego and its people would be props in General Atomics’ killer demo.
 The company described the drone as a “persistent eye in the sky.” It was an unprecedented project that had the potential to open the airways above major US cities to new forms of surveillance.
 How were regulators evaluating the technology? Had there been any external influence—from, say, the Pentagon or the White House—to force the project through? These were pressing questions, especially because a similar drone from US Customs and Border Protection had experienced mechanical failure and crashed in the waters outside the city in 2014.
 So, in late March 2020, with the test flight about to take place any day, a journalist at the nonprofit news organization Voice of San Diego requested all public records related to the test flight from the FAA, including communications with the defense contractor.
 Complying with the request wasn’t easy. The FAA has five divisions spread across the country. By design, the agency’s public records response is decentralized. Getting the SkyGuardian off the ground required the work of three different units, each of which needed to produce—and redact—the records in its possession. Voice of San Diego filed a lawsuit to speed up the process and, in September 2020, a judge ordered the FAA to hurry up or face penalties. The pressure was on. The agency had six weeks.
 The Minimum Amount of Redactions
 Two months after the original request was filed—well past FOIA’s legal deadline of twenty business days—Rick Perez got the assignment in the FAA’s air certification branch. Perez had been with the FAA since 2011, and his staff was also in the middle of processing more than sixty requests for huge volumes of records related to the certification of the Boeing 737 MAX, two of which had crashed in 2018 and 2019, killing a total of 346 people. Perez’s branch consisted of three federal employees and four contractors. The SkyGuardian was just a blip on their radar.
 Perez’s staff consulted with other experts in the agency, ran an initial query with keywords, and then set about reading through the more than 2,500 pages they had identified. During their line-by-line reviews, it became clear that other organizations might have a stake—and a legal claim—to withhold information about the SkyGuardian, not just the FAA.
 NASA, for instance, had been partnering with General Atomics on the test flight. Both organizations needed to be consulted. Both would drag their feet. And both would then turn around and make cases to the FAA in private that certain documents and communications ought to be treated as confidential. General Atomics argued that some of the information contained in the records was protected by the US Arms Export Control Act, which deals with the sale of weapons overseas. This by itself was revealing: the test flight was not just about showing local officials the ways in which they could monitor infrastructure from above. The test flight was intended as a marketing ploy for international buyers.
 For this reason, Perez discovered, the Department of Defense also needed to weigh in. That slowed the process down even further. In October 2020, he provided the court with an update, arguing for more time—but promised to release everything in his possession “with the minimum amount of redactions.” Here’s a taste of what he produced: 
 Still, there were some revelations visible between the blacked-out lines. What the FAA released showed that regulators did not believe the drone could fly safely above so many people. Regulators had demanded the company use a “chase plane,” manned with a pilot who could monitor the drone in real-time for any problems. Instead of complying with the request—which might have given the international buyers doubts about the technology’s true capabilities—the company rerouted the flight path and pushed the demo into the desert.
 But as it turns out, different units within the FAA had similar records in their possession and redacted them in different ways. The FAA wound up producing the same emails in redacted and unredacted forms, inadvertently revealing how subjective the process really is. What to keep and what to omit appeared to be dependent on the momentary feelings and fleeting judgments of whoever was doing the redactions.
 The email conversations show FAA employees casting doubt on General Atomics’ claim that their technology is safe. In one email, an FAA employee notes that the company is “worried about getting an approval” and that time will run out before the permit process can be completed because, “as usual, they have diplomats from the military attending and want to demonstrate the capability.”
 Secrecy is the way in which government agencies reveal their mistrust of the people. The cloak is needed, you’ll often hear officials say, to keep everyone safe. But safe from what? Or better yet, from whom? Secrecy breeds its own kind of distress, encouraging a fear of the unknown that justifies new levels of authority in a slow-moving but perpetual crisis. 
 Secrecy and surveillance are two sides of the same coin. Both stem from an attitude that the people atop our institutions know better than the rest of us how to govern and structure society. Both engender mistrust and inhibit collective action. They engender mistrust by encouraging us to outsource responsibility for safety. This inhibits collective action, not only by chilling our speech and association but by ceding authority to institutions we mostly do not participate in. 
 Do It Yourself
 Why would I, of all people, want to seek a public record?
 It could be to learn about an issue that affects you or your community. It could be to find more bulletproof evidence of something you know or suspect that will help with your advocacy work. It could be an entry point into the workings of politics or profit by finding the traces left behind by the companies that interact with the government.	
 How do I get started?
 Don’t be intimidated. We had no idea what we were doing when we got started either. No one is born with this knowledge.
 Explore the public records universe to get a sense of the mechanics. Public records are informational arcana produced through the rituals of politicians, functionaries, and bureaucrats. It is hard to know what to ask for if you don’t have a sense of how they organize their world and their work within it. 
 Check online to see if the agency you’re targeting has an open data portal, then start by trawling for topics that you’re interested in. Some public agencies and cities, including Los Angeles, New York City, and San Diego, have websites that allow you to search for the records others have already requested using keywords. You can then dive into the rabbit hole of what’s already been made available. Browsing the records gives you a sense of what kinds of departments exist within those agencies, what kind of records they produce, what kinds of things other people ask for, and how they ask for them.
 What kind of records could address my question?
 Records may not be available to answer your exact question, but there may be records that give you some pieces of an answer. Possible records include email communications, presentations, memoranda of understanding, policy and procedure manuals, government filings, and contracts.
 It can help to speak with people who interact with the government a lot. Activists, lawyers, and policy nerds can help you brainstorm based on their experiences requesting and filing documents. Again, browsing existing open records portals can also give you ideas. There is no shame in starting your request by copying and pasting another request you find and submitting it with the tweaks that get at the specific information you want. We do this all the time. You can also check the agency’s website to get a sense of the kind of forms and records they describe working with publicly. 
 Which public entity has the records I want?
 Determining this is key. After you figure out what you want to know, figure out who has it. The San Diego Police Department? The National Labor Relations Board? The Food and Drug Administration? Cities will often have a clerk who you can call and talk to—an act of bureaucratic goodwill or perhaps a legal requirement, depending on the state. Networking with others who work on the topic you’re interested in can also help. You may encounter hostility from some agencies, but remember, at least California requires that officials help you route your request to the correct departments.
 The entity should have a website explaining their public records procedure, often with an online form. Plan ahead, as it can take months to get the records you request—and that’s if you’re lucky. The agency may also charge you for requests beyond a certain number of pages. You’re also within your rights to request that they waive those fees.
 Transparency is a constant struggle. It helps to have allies. Are there organized groups—unions or community organizations, for example—that have a stake in the issue you’re trying to bring to light? They may have background knowledge about the politics behind the records you seek. They may have access to lawyers who can help you navigate local public records laws. They might want to help you make sense of the records you get because it benefits them too.
 Finally, if you want to know more because you want to change something about the way the world works, you’ll need to join with others to make it happen. Being frustrated and alone is a trash feeling.</content>
     </entry>
     <entry>
       <title>De-Mystifying IndieWeb on a WordPress Site</title>
         <link href="https://css-tricks.com/de-mystifying-indieweb-on-a-wordpress-site/"/>
       <updated>2022-06-14T16:48:34.000Z</updated>
       <content type="text">Well, sheesh. I opened a little can of worms when sharing Miriam’s “Am I on the IndieWeb yet?” with a short post bemoaning my own trouble getting on the IndieWeb train. But it’s a good can of worms.
 
 
 
 I think it was something like the next day after publishing that short post that David Shanske reached out and offered to help wrap my head around IndieWeb and the components that it comprises. And gosh dang if it wasn’t ridiculously helpful! So much so that I’d like to link you up to a new post David wrote after we talked, then summarize things here as best as I can because (1) it’s helpful to write things down and (2) have a reference for later.
 
 
 
 
 
 
 
 Yes, IndieWeb is confusing.
 
 
 
 David had actually helped someone get their WordPress site all set up with IndieWeb powers. That person, too, was struggling to understand how the various pieces fit together. So, David already had this top of mind when Miriam and I were writing.
 
 
 
 “IndieWeb” is a new term for many folks and that’s where a lot of confusion breeds. Is it a framework? A philosophy? A set of standards? Depending on which one it is, the expectations shift as far as what it looks like to be a part of it.
 
 
 
 It’s sort of all of the above. And that needs to be solidified a bit. There’s nothing inherently confusing about IndieWeb itself once you view it through those different lenses. After reading David’s post my understanding is that IndieWeb is more of a set of protocols. Sorta like working with structured data or OpenGraph in markup. There’s nothing to install per se, but there are standards for how to integrate them into your work.
 
 
 
 Your identity powers IndieWeb. In other words, your site establishes your identity and can be used to do lots of things, like:
 
 
 
 Notify other IndieWeb-supported sites when they are mentionedReceive notifications from other IndieWeb sites when you are mentionedFetch information from a mention and format it for displayAuthenticate your identity through your own website (a là a Google sign-in button but connected to your site)…among other things.
 
 
 
 If that sorta sounds like pingbacks, well, it sorta is. But much more robust and maintained.
 
 
 
 It’s different (and perhaps easier) to implement IndieWeb features on WordPress than it is a static site.
 
 
 
 The big difference is that WordPress provides a lot of the requirements needed to do IndieWeb-y things. I like how David explains it:
 
 
 
 The IndieWeb implementation on WordPress is a [series] of building blocks that you can or cannot choose to use, which is what makes it wonderful, but sometimes confusing. WordPress has a philosophy of decisions, not options. But the IndieWeb is all about options…about building the features that are right for you.
 
 
 
 Those building blocks are plugins that you install to add IndieWeb protocols and technologies to WordPress. It’s awesome those are readily available because that takes a a lot of the work out of things. Running a static site, though, you’re on the hook for establishing most of that yourself.
 
 
 
 David’s post is 100% focused on the WordPress implementation. Your mileage may vary, but you will certainly walk away with a better idea of what protocols are available and how they fit together after reading his post — and hopefully this one as well.
 
 
 
 The IndieWeb WordPress plugin establishes your identity.
 
 
 
 
 
 
 
 I thought it was doing so much stuff behind the scenes, but it’s a lot more simple than that:
 
 
 
 The plugin by itself handles establishing your identity as the IndieWeb sees it. It offers an h-card template and widget. H-Card is the markup for marking up information about a person or place. So, this is an element many people opt to put on their site anyway.
 
 
 
 So, really, it’s possible to get the same sort of thing by correctly marking up a WordPress theme. The convenience here is that you get a handy little template that’s marked up to support the h-card open format and a widget to drop it into a theme’s widget area.
 
 
 
 Here’s a super detailed example of the h-card markup pulled from the documentation for Microformats2:
 
 
 
 &lt;div class&#x3D;&quot;h-card&quot;&gt;
   &lt;img class&#x3D;&quot;u-photo&quot; alt&#x3D;&quot;photo of Mitchell&quot;
        src&#x3D;&quot;https://webfwd.org/content/about-experts/300.mitchellbaker/mentor_mbaker.jpg&quot;/&gt;
   &lt;a class&#x3D;&quot;p-name u-url&quot;
      href&#x3D;&quot;http://blog.lizardwrangler.com/&quot; 
     &gt;Mitchell Baker&lt;/a&gt;
  (&lt;a class&#x3D;&quot;u-url&quot; 
      href&#x3D;&quot;https://twitter.com/MitchellBaker&quot;
     &gt;@MitchellBaker&lt;/a&gt;)
   &lt;span class&#x3D;&quot;p-org&quot;&gt;Mozilla Foundation&lt;/span&gt;
   &lt;p class&#x3D;&quot;p-note&quot;&gt;
     Mitchell is responsible for setting the direction and scope of the Mozilla Foundation and its activities.
   &lt;/p&gt;
   &lt;span class&#x3D;&quot;p-category&quot;&gt;Strategy&lt;/span&gt;
   &lt;span class&#x3D;&quot;p-category&quot;&gt;Leadership&lt;/span&gt;
 &lt;/div&gt;
 
 
 
 See those class names? Classes like .h-card, u-photo, p-name, etc. all provide contextual meaning for a person’s identity which it then parsed as JSON:
 
 
 
 {
   &quot;items&quot;: [{ 
     &quot;type&quot;: [&quot;h-card&quot;],
     &quot;properties&quot;: {
       &quot;photo&quot;: [&quot;https://webfwd.org/content/about-experts/300.mitchellbaker/mentor_mbaker.jpg&quot;],
       &quot;name&quot;: [&quot;Mitchell Baker&quot;],
       &quot;url&quot;: [
         &quot;http://blog.lizardwrangler.com/&quot;,
         &quot;https://twitter.com/MitchellBaker&quot;
       ],
       &quot;org&quot;: [&quot;Mozilla Foundation&quot;],
       &quot;note&quot;: [&quot;Mitchell is responsible for setting the direction and scope of the Mozilla Foundation and its activities.&quot;],
       &quot;category&quot;: [
         &quot;Strategy&quot;,
         &quot;Leadership&quot;
       ]
     }
   }]
 }
 
 
 
 The plugin isn’t doing the sending, receiving, or parsing. Instead, it provides a WordPress site with a way to verify your identity in the markup.
 
 
 
 Not all WordPress themes support Microformats
 
 
 
 
 
 
 
 If you scratched your head first time you saw “Microformats” like I did, David defines it nicely:
 
 
 
 […] Microformats…a way of marking up HTML to allow elements to be identified. It is one of several ways of doing this, but is a very simple and readable one, which is why it is popular in the IndieWeb community.
 
 
 
 The problem, as David continues, is that many themes aren’t marked up in a Microformats-friendly way — which is what the Microformats2 plugin is designed to fix.  That said, David is quick to call out that the plugin is extremely limited in how it accomplishes this, and he recommends instead marking up a theme by hand.
 
 
 
 According to David, the next major release of the Webmention plugin will likely include smarter ways of detecting content and images it can use and formatting them for Microformats2 support.
 
 
 
 Webmentions send and receive notifications.
 
 
 
 
 
 
 
 OK, so if you’ve established your identity through your site so you are discoverable, and your site is marked up for h-card support using Microformats2. Great! You still need something in the middle working as an operator that sends and receives notifications. In other words, when another site mentions you — called a Webmention — the site mentioning you needs a way to support sending that mention to you, and your site needs a way to accept it (or vice versa).
 
 
 
 That’s what the Webmention plugin is for. It’s also probably the source of most of my IndieWeb confusion. I thought it was formatting data and needed an additional service to send and receive it. Nope! It’s actually sending and receiving the data rather than creating the mention. Back to David:
 
 
 
 Back when it was built, the plugin handled only the business of receiving and sending webmentions, not handling display to any degree. Semantic Linkbacks, a separate plugin handled that for not only webmentions, but the older pingback and trackback protocols.
 
 
 
 So, the Webmention plugin is communicating notifications. Meanwhile, another plugin called Semantic Linkbacks is what handles the data. And what the heck are Semantic Linkbacks?
 
 
 
 Semantic Linkbacks fetch and handle data.
 
 
 
 
 
 
 
 Semantic Linkbacks is another plugin that handles another piece of the process. There’s no way I can explain it better than David already does:
 
 
 
 Semantic Linkbacks takes a webmention, which is a notification that another site has linked to something on your site, fetches the other site, and tries to render a display of the information. How that is done can vary from just a profile photo (if it can find one), to interpreting it as a full comment.It does this using Microformats.
 
 
 
 I expected that the main IndieWeb plugin was already doing this since it handles other markup. But it only provides the template and widget to get your identity on your site. Once the Semantic Linkbacks plugin fetches an incoming webmention, it takes the data, formats it, then attempts to display it.
 
 
 
 Sounds like the plugin will be somewhat merged with (or replaced by) an upcoming version of the Webmention plugin:
 
 
 
 Since many people are not inclined, or not comfortable modifying a theme, the new version of Webmentions will include several different alternative ways to try to find an image or summary to display…from OpenGraph (which Facebook and Twitter use to display URLs provided to it) to detecting the WordPress REST API version of a page and using that to get the author name and profile image. None of them will provided as much context as Microformats, but the experience will still be something worth installing.
 
 
 
 That’s certainly nice as it taps into the WordPress REST API for the JSON response and formats that for display.
 
 
 
 Brid.gy is a service to help display interactions.
 
 
 
 A Webmention can be an interaction, say someone likes your post on Twitter or retweets it.
 
 
 
 
 
 
 
 Differentiating a like from a repost from a comment from a whatever needs to happen, and you’d need to implement the Twitter (or whatever) API to draw those distinctions.
 
 
 
 That’s something you can certainly do! But if you’d rather plug and play, one of the IndieWeb community members made a service called Brid.gy. You create an account, hook up your site, and give app permissions to the service… then you’re done!
 
 
 
 What Brid.gy has done is essentially implement the APIs for Twitter, Facebook, Instagram, and others, so that when it detects that a post in those services that interacts with your syndicated post, a Webmention is sent to your site and goes through the process of publishing on your own site.
 
 
 
 There’s so much more!
 
 
 
 Quick hits:
 
 
 
 IndieAuth: This is a protocol based on OAuth 2. The plugin establishes an endpoint in the WordPress REST API that can be used to authenticate your identity through your own self-hosted site — essentially your own Google sign-in button but without establishing that endpoint yourself or needing to rely on a separate hosted API.Micropub: For those who use WordPress but prefer a different editor can install the Micropub plugin. This adds an endpoint that allows you to publish content to your site and using a Micropub-supported editor create items in a Microformats2 feed, giving you way more options for writing content outside of the WordPress Block Editor.Simple Location: David wrote this plugin and I was super impressed when he demoed it for me. The idea is it pulls in data from your current location that can used for everything from displaying the weather at the time you wrote a post, to creating an entire archive of posts on an embedded map based on the post location. I’d honestly love to see something like this baked directly into WordPress.
 
 
 
 The updated flow
 
 
 
 I attempted to make an illustration that outlines the various pieces in my last post, but let’s try again with an updated understanding of what’s happening:
 
 
 
 (Full size)
 
 
 
 Is this all making sense?
 
 
 
 High fives to David for both reaching out and taking the time to show me what it looks like to implement IndieWeb on WordPress. I can’t claim I fully understand all the nuances, but I at least feel like I have a decent grasp of the pieces — the philosophy, protocols, and tech — that are required to make it happen.
 
 
 
 I’d like to turn it around to you! Does this help clarify things for you? Is there anything you’re struggling to understand? Think you’re able to configure a WordPress site with IndieWeb features now? Let’s take it to the comments!
 
 De-Mystifying IndieWeb on a WordPress Site originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>Everything Is Broken: Shipping rust-minidump at Mozilla – Part 1</title>
         <link href="https://hacks.mozilla.org/2022/06/everything-is-broken-shipping-rust-minidump-at-mozilla/"/>
       <updated>2022-06-14T15:05:06.000Z</updated>
       <content type="text">Everything Is Broken: Shipping rust-minidump at Mozilla
 For the last year I’ve been leading the development of rust-minidump, a pure-Rust replacement for the minidump-processing half of google-breakpad.
 Well actually in some sense I finished that work, because Mozilla already deployed it as the crash processing backend for Firefox 6 months ago, it runs in half the time, and seems to be more reliable. (And you know, isn’t a terrifying ball of C++ that parses and evaluates arbitrary input from the internet. We did our best to isolate Breakpad, but still… yikes.)
 This is a pretty fantastic result, but there’s always more work to do because Minidumps are an inky abyss that grows deeper the further you delve… wait no I’m getting ahead of myself. First the light, then the abyss. Yes. Light first.
 What I can say is that we have a very solid implementation of the core functionality of minidump parsing+analysis for the biggest platforms (x86, x64, ARM, ARM64; Windows, MacOS, Linux, Android). But if you want to read minidumps generated on a PlayStation 3 or process a Full Memory dump, you won’t be served quite as well.
 We’ve put a lot of effort into documenting and testing this thing, so I’m pretty confident in it!
 Unfortunately! Confidence! Is! Worth! Nothing!
 Which is why this is the story of how we did our best to make this nightmare as robust as we could and still got 360 dunked on from space by the sudden and incredible fuzzing efforts of @5225225.
 This article is broken into two parts:
 
 what minidumps are, and how we made rust-minidump
 how we got absolutely owned by simple fuzzing
 
 You are reading part 1, wherein we build up our hubris.
 Background: What’s A Minidump, and Why Write rust-minidump?
 Your program crashes. You want to know why your program crashed, but it happened on a user’s machine on the other side of the world. A full coredump (all memory allocated by the program) is enormous — we can’t have users sending us 4GB files! Ok let’s just collect up the most important regions of memory like the stacks and where the program crashed. Oh and I guess if we’re taking the time, let’s stuff some metadata about the system and process in there too.
 Congratulations you have invented Minidumps. Now you can turn a 100-thread coredump that would otherwise be 4GB into a nice little 2MB file that you can send over the internet and do postmortem analysis on.
 Or more specifically, Microsoft did. So long ago that their docs don’t even discuss platform support. MiniDumpWriteDump’s supported versions are simply “Windows”. Microsoft Research has presumably developed a time machine to guarantee this.
 Then Google came along (circa 2006-2007) and said “wouldn’t it be nice if we could make minidumps on any platform”? Thankfully Microsoft had actually built the format pretty extensibly, so it wasn’t too bad to extend the format for Linux, MacOS, BSD, Solaris, and so on. Those extensions became google-breakpad (or just Breakpad) which included a ton of different tools for generating, parsing, and analyzing their extended minidump format (and native Microsoft ones).
 Mozilla helped out with this a lot because apparently, our crash reporting infrastructure (“Talkback”) was miserable circa 2007, and this seemed like a nice improvement. Needless to say, we’re pretty invested in breakpad’s minidumps at this point.
 Fast forward to the present day and in a hilarious twist of fate, products like VSCode mean that Microsoft now supports applications that run on Linux and MacOS so it runs breakpad in production and has to handle non-Microsoft minidumps somewhere in its crash reporting infra, so someone else’s extension of their own format is somehow their problem now!
 Meanwhile, Google has kind-of moved on to Crashpad. I say kind-of because there’s still a lot of Breakpad in there, but they’re more interested in building out tooling on top of it than improving Breakpad itself. Having made a few changes to Breakpad: honestly fair, I don’t want to work on it either. Still, this was a bit of a problem for us, because it meant the project became increasingly under-staffed.
 By the time I started working on crash reporting, Mozilla had basically given up on upstreaming fixes/improvements to Breakpad, and was just using its own patched fork. But even without the need for upstreaming patches, every change to Breakpad filled us with dread: many proposed improvements to our crash reporting infrastructure stalled out at “time to implement this in Breakpad”.
 Why is working on Breakpad so miserable, you ask?
 Parsing and analyzing minidumps is basically an exercise in writing a fractal parser of platform-specific formats nested in formats nested in formats. For many operating systems. For many hardware architectures. And all the inputs you’re parsing and analyzing are terrible and buggy so you have to write a really permissive parser and crawl forward however you can.
 Some specific MSVC toolchain that was part of Windows XP had a bug in its debuginfo format? Too bad, symbolicate that stack frame anyway!
 The program crashed because it horribly corrupted its own stack? Too bad, produce a backtrace anyway!
 The minidump writer itself completely freaked out and wrote a bunch of garbage to one stream? Too bad, produce whatever output you can anyway!
 Hey, you know who has a lot of experience dealing with really complicated permissive parsers written in C++? Mozilla! That’s like the core functionality of a web browser.
 Do you know Mozilla’s secret solution to writing really complicated permissive parsers in C++?
 We stopped doing it.
 We developed Rust and ported our nastiest parsers to it.
 We’ve done it a lot, and when we do we’re always like “wow this is so much more reliable and easy to maintain and it’s even faster now”. Rust is a really good language for writing parsers. C++ really isn’t.
 So we Rewrote It In Rust (or as the kids call it, “Oxidized It”). Breakpad is big, so we haven’t actually covered all of its features. We’ve specifically written and deployed:
 
 dump_syms which processes native build artifacts into symbol files.
 rust-minidump which is a collection of crates that parse and analyze minidumps. Or more specifically, we deployed minidump-stackwalk, which is the high-level cli interface to all of rust-minidump.
 
 Notably missing from this picture is minidump writing, or what google-breakpad calls a client (because it runs on the client’s machine). We are working on a rust-based minidump writer, but it’s not something we can recommend using quite yet (although it has sped up a lot thanks to help from Embark Studios).
 This is arguably the messiest and hardest work because it has a horrible job: use a bunch of native system APIs to gather up a bunch of OS-specific and Hardware-specific information about the crash AND do it for a program that just crashed, on a machine that caused the program to crash.
 We have a long road ahead but every time we get to the other side of one of these projects it’s wonderful.
  
 Background: Stackwalking and Calling Conventions
 One of rust-minidump’s (minidump-stackwalk’s) most important jobs is to take the state for a thread (general purpose registers and stack memory) and create a backtrace for that thread (unwind/stackwalk). This is a surprisingly complicated and messy job, made only more complicated by the fact that we are trying to analyze the memory of a process that got messed up enough to crash.
 This means our stackwalkers are inherently working with dubious data, and all of our stackwalking techniques are based on heuristics that can go wrong and we can very easily find ourselves in situations where the stackwalk goes backwards or sideways or infinite and we just have to try to deal with it!
 It’s also pretty common to see a stackwalker start hallucinating, which is my term for “the stackwalker found something that looked plausible enough and went on a wacky adventure through the stack and made up a whole pile of useless garbage frames”. Hallucination is most common near the bottom of the stack where it’s also least offensive. This is because each frame you walk is another chance for something to go wrong, but also increasingly uninteresting because you’re rarely interested in confirming that a thread started in The Same Function All Threads Start In.
 All of these problems would basically go away if everyone agreed to properly preserve their cpu’s PERFECTLY GOOD DEDICATED FRAME POINTER REGISTER. Just kidding, turning on frame pointers doesn’t really work either because Microsoft invented chaos frame pointers that can’t be used for unwinding! I assume this happened because they accidentally stepped on the wrong butterfly while they were traveling back in time to invent minidumps. (I’m sure it was a decision that made more sense 20 years ago, but it has not aged well.)
 If you would like to learn more about the different techniques for unwinding, I wrote about them over here in my article on Apple’s Compact Unwind Info. I’ve also attempted to document breakpad’s STACK WIN and STACK CFI unwind info formats here, which are more similar to the  DWARF and PE32 unwind tables (which are basically tiny programming languages).
 If you would like to learn more about ABIs in general, I wrote an entire article about them here. The end of that article also includes an introduction to how calling conventions work. Understanding calling conventions is key to implementing unwinders.
  
 How Hard Did You Really Test Things?
 Hopefully you now have a bit of a glimpse into why analyzing minidumps is an enormous headache. And of course you know how the story ends: that fuzzer kicks our butts! But of course to really savor our defeat, you have to see how hard we tried to do a good job! It’s time to build up our hubris and pat ourselves on the back.
 So how much work actually went into making rust-minidump robust before the fuzzer went to work on it?
 Quite a bit!
 I’ll never argue all the work we did was perfect but we definitely did some good work here, both for synthetic inputs and real world ones. Probably the biggest “flaw” in our methodology was the fact that we were only focused on getting Firefox’s usecase to work. Firefox runs on a lot of platforms and sees a lot of messed up stuff, but it’s still a fairly coherent product that only uses so many features of minidumps.
 This is one of the nice benefits of our recent work with Sentry, which is basically a Crash Reporting As A Service company. They are way more liable to stress test all kinds of weird corners of the format that Firefox doesn’t, and they have definitely found (and fixed!) some places where something is wrong or missing! (And they recently deployed it into production too! )
 But hey don’t take my word for it, check out all the different testing we did:
 Synthetic Minidumps for Unit Tests
 rust-minidump includes a synthetic minidump generator which lets you come up with a high-level description of the contents of a minidump, and then produces an actual minidump binary that we can feed it into the full parser:
 // Let’s make a synth minidump with this particular Crashpad Info…
 let module &#x3D; ModuleCrashpadInfo::new(42, Endian::Little)
     .add_list_annotation(&quot;annotation&quot;)
     .add_simple_annotation(&quot;simple&quot;, &quot;module&quot;)
     .add_annotation_object(&quot;string&quot;, AnnotationValue::String(&quot;value&quot;.to_owned()))
     .add_annotation_object(&quot;invalid&quot;, AnnotationValue::Invalid)
     .add_annotation_object(&quot;custom&quot;, AnnotationValue::Custom(0x8001, vec![42]));
 
 let crashpad_info &#x3D; CrashpadInfo::new(Endian::Little)
     .add_module(module)
     .add_simple_annotation(&quot;simple&quot;, &quot;info&quot;);
 
 let dump &#x3D; SynthMinidump::with_endian(Endian::Little).add_crashpad_info(crashpad_info);
 
 // convert the synth minidump to binary and read it like a normal minidump
 let dump &#x3D; read_synth_dump(dump).unwrap();
 // Now check that the minidump reports the values we expect…
 minidump-synth intentionally avoids sharing layout code with the actual implementation so that incorrect changes to layouts won’t “accidentally” pass tests.
 A brief aside for some history: this testing framework was started by the original lead on this project, Ted Mielczarek. He started rust-minidump as a side project to learn Rust when 1.0 was released and just never had the time to finish it. Back then he was working at Mozilla and also a major contributor to Breakpad, which is why rust-minidump has a lot of similar design choices and terminology.
 This case is no exception: our minidump-synth is a shameless copy of the synth-minidump utility in breakpad’s code, which was originally written by our other coworker Jim Blandy. Jim is one of the only people in the world that I will actually admit writes really good tests and docs, so I am totally happy to blatantly copy his work here.
 Since this was all a learning experiment, Ted was understandably less rigorous about testing than usual. This meant a lot of minidump-synth was unimplemented when I came along, which also meant lots of minidump features were completely untested. (He built an absolutely great skeleton, just hadn’t had the time to fill it all in!)
 We spent a lot of time filling in more of minidump-synth’s implementation so we could write more tests and catch more issues, but this is definitely the weakest part of our tests. Some stuff was implemented before I got here, so I don’t even know what tests are missing!
 This is a good argument for some code coverage checks, but it would probably come back with “wow you should write a lot more tests” and we would all look at it and go “wow we sure should” and then we would probably never get around to it, because there are many things we should do.
 On the other hand, Sentry has been very useful in this regard because they already have a mature suite of tests full of weird corner cases they’ve built up over time, so they can easily identify things that really matter, know what the fix should roughly be, and can contribute pre-existing test cases!
 Integration and Snapshot Tests
 We tried our best to shore up coverage issues in our unit tests by adding more holistic tests. There’s a few checked in Real Minidumps that we have some integration tests for to make sure we handle Real Inputs properly.
 We even wrote a bunch of integration tests for the CLI application that snapshot its output to confirm that we never accidentally change the results.
 Part of the motivation for this is to ensure we don’t break the JSON output, which we also wrote a very detailed schema document for and are trying to keep stable so people can actually rely on it while the actual implementation details are still in flux.
 Yes, minidump-stackwalk is supposed to be stable and reasonable to use in production!
 For our snapshot tests we use insta, which I think is fantastic and more people should use. All you need to do is assert_snapshot! any output you want to keep track of and it will magically take care of the storing, loading, and diffing.
 Here’s one of the snapshot tests where we invoke the CLI interface and snapshot stdout:
 #[test]
 fn test_evil_json() {
     // For a while this didn&#x27;t parse right
     let bin &#x3D; env!(&quot;CARGO_BIN_EXE_minidump-stackwalk&quot;);
     let output &#x3D; Command::new(bin)
         .arg(&quot;--json&quot;)
         .arg(&quot;--pretty&quot;)
         .arg(&quot;--raw-json&quot;)
         .arg(&quot;../testdata/evil.json&quot;)
         .arg(&quot;../testdata/test.dmp&quot;)
         .arg(&quot;../testdata/symbols/&quot;)
         .stdout(Stdio::piped())
         .stderr(Stdio::piped())
         .output()
         .unwrap();
 
     let stdout &#x3D; String::from_utf8(output.stdout).unwrap();
     let stderr &#x3D; String::from_utf8(output.stderr).unwrap();
 
     assert!(output.status.success());
     insta::assert_snapshot!(&quot;json-pretty-evil-symbols&quot;, stdout);
     assert_eq!(stderr, &quot;&quot;);
 }
 
 
 Stackwalker Unit Testing
 The stackwalker is easily the most complicated and subtle part of the new implementation, because every platform can have slight quirks and you need to implement several different unwinding strategies and carefully tune everything to work well in practice.
 The scariest part of this was the call frame information (CFI) unwinders, because they are basically little virtual machines we need to parse and execute at runtime. Thankfully breakpad had long ago smoothed over this issue by defining a simplified and unified CFI format, STACK CFI (well, nearly unified, x86 Windows was still a special case as STACK WIN). So even if DWARF CFI has a ton of complex features, we mostly need to implement a Reverse Polish Notation Calculator except it can read registers and load memory from addresses it computes (and for STACK WIN it has access to named variables it can declare and mutate).
 Unfortunately, Breakpad’s description for this format is pretty underspecified so I had to basically pick some semantics I thought made sense and go with that. This made me extremely paranoid about the implementation. (And yes I will be more first-person for this part, because this part was genuinely where I personally spent most of my time and did a lot of stuff from scratch. All the blame belongs to me here!)
 The STACK WIN / STACK CFI parser+evaluator is 1700 lines. 500 of those lines are a detailed documentation and discussion of the format, and 700 of those lines are an enormous pile of ~80 test cases where I tried to come up with every corner case I could think of.
 I even checked in two tests I knew were failing just to be honest that there were a couple cases to fix! One of them is a corner case involving dividing by a negative number that almost certainly just doesn’t matter. The other is a buggy input that old x86 Microsoft toolchains actually produce and parsers need to deal with. The latter was fixed before the fuzzing started.
 And 5225225 still found an integer overflow in the STACK WIN preprocessing step! (Not actually that surprising, it’s a hacky mess that tries to cover up for how messed up x86 Windows unwinding tables were.)
 (The code isn’t terribly interesting here, it’s just a ton of assertions that a given input string produces a given output/error.)
 Of course, I wasn’t satisfied with just coming up with my own semantics and testing them: I also ported most of breakpad’s own stackwalker tests to rust-minidump! This definitely found a bunch of bugs I had, but also taught me some weird quirks in Breakpad’s stackwalkers that I’m not sure I actually agree with. But in this case I was flying so blind that even being bug-compatible with Breakpad was some kind of relief.
 Those tests also included several tests for the non-CFI paths, which were similarly wobbly and quirky. I still really hate a lot of the weird platform-specific rules they have for stack scanning, but I’m forced to work on the assumption that they might be load-bearing. (I definitely had several cases where I disabled a breakpad test because it was “obviously nonsense” and then hit it in the wild while testing. I quickly learned to accept that Nonsense Happens And Cannot Be Ignored.)
 One major thing I didn’t replicate was some of the really hairy hacks for STACK WIN. Like there are several places where they introduce extra stack-scanning to try to deal with the fact that stack frames can have mysterious extra alignment that the windows unwinding tables just don’t tell you about? I guess?
 There’s almost certainly some exotic situations that rust-minidump does worse on because of this, but it probably also means we do better in some random other situations too. I never got the two to perfectly agree, but at some point the divergences were all in weird enough situations, and as far as I was concerned both stackwalkers were producing equally bad results in a bad situation. Absent any reason to prefer one over the other, divergence seemed acceptable to keep the implementation cleaner.
 Here’s a simplified version of one of the ported breakpad tests, if you’re curious (thankfully minidump-synth is based off of the same binary data mocking framework these tests use):
 #[test]
 fn test_x86_frame_pointer() {
     let mut f &#x3D; TestFixture::new();
     let frame0_ebp &#x3D; Label::new();
     let frame1_ebp &#x3D; Label::new();
     let mut stack &#x3D; Section::new();
 
     // Setup the stack and registers so frame pointers will work
     stack.start().set_const(0x80000000);
     stack &#x3D; stack
         .append_repeated(12, 0) // frame 0: space
         .mark(&amp;frame0_ebp)      // frame 0 %ebp points here
         .D32(&amp;frame1_ebp)       // frame 0: saved %ebp
         .D32(0x40008679)        // frame 0: return address
         .append_repeated(8, 0)  // frame 1: space
         .mark(&amp;frame1_ebp)      // frame 1 %ebp points here
         .D32(0)                 // frame 1: saved %ebp (stack end)
         .D32(0);                // frame 1: return address (stack end)
     f.raw.eip &#x3D; 0x4000c7a5;
     f.raw.esp &#x3D; stack.start().value().unwrap() as u32;
     f.raw.ebp &#x3D; frame0_ebp.value().unwrap() as u32;
 
     // Check the stackwalker&#x27;s output:
     let s &#x3D; f.walk_stack(stack).await;
     assert_eq!(s.frames.len(), 2);
     {
         let f0 &#x3D; &amp;s.frames[0];
         assert_eq!(f0.trust, FrameTrust::Context);
         assert_eq!(f0.context.valid, MinidumpContextValidity::All);
         assert_eq!(f0.instruction, 0x4000c7a5);
     }
     {
         let f1 &#x3D; &amp;s.frames[1];
         assert_eq!(f1.trust, FrameTrust::FramePointer);
         assert_eq!(f1.instruction, 0x40008678);
     }
 }
 A Dedicated Production Diffing, Simulating, and Debugging Tool
 Because minidumps are so horribly fractal and corner-casey, I spent a lot of time terrified of subtle issues that would become huge disasters if we ever actually tried to deploy to production. So I also spent a bunch of time building socc-pair, which takes the id of a crash report from Mozilla’s crash reporting system and pulls down the minidump, the old breakpad-based implementation’s output, and extra metadata.
 It then runs a local rust-minidump (minidump-stackwalk) implementation on the minidump and does a domain-specific diff over the two inputs. The most substantial part of this is a fuzzy diff on the stackwalks that tries to better handle situations like when one implementation adds an extra frame but the two otherwise agree. It also uses the reported techniques each implementation used to try to identify whose output is more trustworthy when they totally diverge.
 I also ended up adding a bunch of mocking and benchmarking functionality to it as well, as I found more and more places where I just wanted to simulate a production environment.
 Oh also I added really detailed trace-logging for the stackwalker so that I could easily post-mortem debug why it made the decisions it made.
 This tool found so many issues and more importantly has helped me quickly isolate their causes. I am so happy I made it. Because of it, we know we actually fixed several issues that happened with the old breakpad implementation, which is great!
 Here’s a trimmed down version of the kind of report socc-pair would produce (yeah I abused diff syntax to get error highlighting. It’s a great hack, and I love it like a child):
 comparing json...
 
 : {
     crash_info: {
         address: 0x7fff1760aca0
         crashing_thread: 8
         type: EXCEPTION_BREAKPOINT
     }
     crashing_thread: {
         frames: [
             0: {
                 file: wrappers.cpp:1750da2d7f9db490b9d15b3ee696e89e6aa68cb7
                 frame: 0
                 function: RustMozCrash(char const*, int, char const*)
                 function_offset: 0x00000010
 -               did not match
 +               line: 17
 -               line: 20
                 module: xul.dll
 
 .....
 
     unloaded_modules: [
         0: {
             base_addr: 0x7fff48290000
 -           local val was null instead of:
             code_id: 68798D2F9000
             end_addr: 0x7fff48299000
             filename: KBDUS.DLL
         }
         1: {
             base_addr: 0x7fff56020000
             code_id: DFD6E84B14000
             end_addr: 0x7fff56034000
             filename: resourcepolicyclient.dll
         }
     ]
 ~   ignoring field write_combine_size: &quot;0&quot;
 }
 
 - Total errors: 288, warnings: 39
 
 benchmark results (ms):
     2388, 1986, 2268, 1989, 2353, 
     average runtime: 00m:02s:196ms (2196ms)
     median runtime: 00m:02s:268ms (2268ms)
     min runtime: 00m:01s:986ms (1986ms)
     max runtime: 00m:02s:388ms (2388ms)
 
 max memory (rss) results (bytes):
     267755520, 261152768, 272441344, 276131840, 279134208, 
     average max-memory: 258MB (271323136 bytes)
     median max-memory: 259MB (272441344 bytes)
     min max-memory: 249MB (261152768 bytes)
     max max-memory: 266MB (279134208 bytes)
 
 Output Files: 
     * (download) Minidump: b4f58e9f-49be-4ba5-a203-8ef160211027.dmp
     * (download) Socorro Processed Crash: b4f58e9f-49be-4ba5-a203-8ef160211027.json
     * (download) Raw JSON: b4f58e9f-49be-4ba5-a203-8ef160211027.raw.json
     * Local minidump-stackwalk Output: b4f58e9f-49be-4ba5-a203-8ef160211027.local.json
     * Local minidump-stackwalk Logs: b4f58e9f-49be-4ba5-a203-8ef160211027.log.txt
 Staging and Deploying to Production
 Once we were confident enough in the implementation, a lot of the remaining testing was taken over by Will Kahn-Greene, who’s responsible for a lot of the server-side details of our crash-reporting infrastructure.
 Will spent a bunch of time getting a bunch of machinery setup to manage the deployment and monitoring of rust-minidump. He also did a lot of the hard work of cleaning up all our server-side configuration scripts to handle any differences between the two implementations. (Although I spent a lot of time on compatibility, we both agreed this was a good opportunity to clean up old cruft and mistakes.)
 Once all of this was set up, he turned it on in staging and we got our first look at how rust-minidump actually worked in ~production:
 Terribly!
 Our staging servers take in about 10% of the inputs that also go to our production servers, but even at that reduced scale we very quickly found several new corner cases and we were getting tons of crashes, which is mildly embarrassing for the thing that handles other people’s crashes.
 Will did a great job here in monitoring and reporting the issues. Thankfully they were all fairly easy for us to fix. Eventually, everything smoothed out and things seemed to be working just as reliably as the old implementation on the production server. The only places where we were completely failing to produce any output were for horribly truncated minidumps that may as well have been empty files.
 We originally did have some grand ambitions of running socc-pair on everything the staging servers processed or something to get really confident in the results. But by the time we got to that point, we were completely exhausted and feeling pretty confident in the new implementation.
 Eventually Will just said “let’s turn it on in production” and I said “AAAAAAAAAAAAAAA”.
 This moment was pure terror. There had always been more corner cases. There’s no way we could just be done. This will probably set all of Mozilla on fire and delete Firefox from the internet!
 But Will convinced me. We wrote up some docs detailing all the subtle differences and sent them to everyone we could. Then the moment of truth finally came: Will turned it on in production, and I got to really see how well it worked in production:
 *dramatic drum roll*
 It worked fine.
 After all that stress and anxiety, we turned it on and it was fine.
 Heck, I’ll say it: it ran well.
 It was faster, it crashed less, and we even knew it fixed some issues.
 I was in a bit of a stupor for the rest of that week, because I kept waiting for the other shoe to drop. I kept waiting for someone to emerge from the mist and explain that I had somehow bricked Thunderbird or something. But no, it just worked.
 So we left for the holidays, and I kept waiting for it to break, but it was still fine.
 I am honestly still shocked about this!
 But hey, as it turns out we really did put a lot of careful work into testing the implementation. At every step we found new problems but that was good, because once we got to the final step there were no more problems to surprise us.
 And the fuzzer still kicked our butts afterwards.
 But that’s part 2! Thanks for reading!
  
 The post Everything Is Broken: Shipping rust-minidump at Mozilla – Part 1 appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>Adding Search To Your Site In 15 Minutes</title>
         <link href="https://smashingmagazine.com/2022/06/adding-search-website-sitesearch360/"/>
       <updated>2022-06-14T09:00:00.000Z</updated>
       <content type="text">This article is a sponsored by Site Search 360
 My site has been created using a static site generator and deployed to a CDN, so I’m super happy with how fast it is. But there has been a functionality that I’ve been missing all along: search.
 As it’s been mentioned many times, Jamstack doesn’t mean “static” — we can perfectly create fully dynamic sites powered by JavaScript on the client-side interacting with (mostly third-party) APIs. Search falls within this category.
 All this time, I hadn’t added search to my site because I was weary it would prove to be a difficult task. I’m not exceptionally skilled in JavaScript or CSS, so I feared that creating an elegant (or, at least, respectable) search input that suggests results as the user types, and integrating it into my site, would fall outside of my abilities.
 And then, all of a sudden, in actually less than 15 minutes, I had search on my site!
 
 After this, the service will spend a minute or two indexing your site (or maybe a few more, depending on how much content your site has), and then inform you that it’s all ready.
 
 Click on “Let’s Get Started!” to go to the application dashboard, where an onboarding process will guide you in setting-up search for your site.
 
 Since your site’s content has been indexed by now, when clicking on “Test now!” the preview of the search UI will already use your site’s actual content. Notice how, even before you start customizing the look and feel, it looks great!
 
 Click on “More results” in the suggestions dropdown to visualize what the search results will look like. (Below, we’ll see how this list, initially displaying the item’s thumbnail, title, URL, and excerpt with the matching term in bold, can be modified.)
 
 Click on the “Search Designer” button to customize the search UI, and replicate the style on your site. This step is not a one-time off: you can come back to it at any moment (even after your custom configuration has been published and search is already installed on your site).
 
 The welcome screen asks how we’d like to start. Since we are new users, we must start from scratch and click on “Start now”.
 
 We are now using the Search Designer which will help you edit the visual appearance of the search input, dropdown, and results.
 
 As you edit the styles, these are immediately reflected on the right-side pane.
 
 In the Search Designer, you can configure the following elements (among many more):
 
 The primary, secondary, background, text, icon and border colors, text size, rounded corners size, and other styles;
 The language (from among 19 supported languages to date);
 Using a search input you already have on your site or the one by Site Search 360;
 Enabling autocomplete suggestions (displayed as the user is typing);
 Connect to Google Analytics and Google Tag Manager;
 Enabling voice search;
 Layouts to display results on desktop and mobile.
 
 The search dropdown can be further configured, allowing to display the previously-searched queries and also predefined queries. In my case, I’ve decided to already suggest those search queries that make my site rank high on Google.
 
 Once done with the configuration, let’s go back to the onboarding process.
 The next step is to tweak the search results, indicating how to extract the data (for the title, images, and excerpt) from the webpage. The default automatic configuration already works very well (extracting the data from the &lt;meta&gt; attributes), so you can quite likely skip this step.
 
 You’re pretty much done by now! All there’s left to do is to publish the configuration.
 Go back to the onboarding process, and click on “Install now!”. This will open a modal window; copy the HTML code and paste it into your site’s source code (before the closing &lt;/body&gt;), and click on “Publish.”
 
 Re-deploy your site, refresh the browser, and what do you have? Your site now has search!
 
 Updating The Visual Appearance
 As I mentioned earlier on, you can keep configuring the search UI even after search is live. From now on, you can easily access the Search Designer from the application menu on the left (under the item “Design &amp; Publish”). After doing some modification, clicking on “Publish” will already apply the new style on your site without having to copy/paste any new code or re-deploy the site.
 
 In my case, I tweaked the configuration a bit more. I changed my mind about the position of the search input, preferring to display it on top of the navigation. The solution was to wrap the navigation menu in a new &lt;div id&#x3D;&quot;nav-wrapper&quot;&gt;  element and then inject the input “Before” the #nav-wrapper selector.
 
 I also noticed that the layout didn’t look right on mobile because the search input was being aligned to the center, while the logo and navigation menu were aligned to the left.
 
 The Search Designer accepts custom CSS, so I could provide a snippet of additional CSS code to override the default style of the search input, aligning it to the left.
 
 I clicked on “Publish”, refreshed the browser, and now the style in mobile looks right.
 
 Finally, I also decided to enable the “Result Groups” feature, which splits the results into different categories of our own choosing. (What pages are contained in a group is defined via a URL pattern; for instance, I’ve set group “Blog” to contain URLs of type /blog/....) In my case, I want to display results from groups “Blog,” “Guides,” and “Meta” and hide results from “Tags.”
 
 Now, when the visitor inputs a search query, the results in the dropdown are organized by the chosen groups, which makes it much easier to find the desired information. For instance, when searching for “namespacing”, “Meta” will contain a page describing the feature; “Guides” will contain pages explaining how to use and configure this feature; and “Blog” will display those blog posts announcing the feature.
 
 Likewise, when clicking on “Show all results”, the modal window is organized into tabs (at one tab per group), which makes it easy to scroll down and browse the results within each group.
 
 I’m super happy with these results! It took just 15 min to go from 0 to 100, the user interface looks really good, and the search is super fast. Overall, the experience for my visitors is compelling. Head over to my site and play with the search input to understand why I’m so satisfied with it.
 Refining Search
 What I showed above is barely scratching the surface, as it’s what’s included in the free plan! Check out the pricing page to see all the other features available in the service for each of the different tiers.
 For instance, if you have a content-rich site, such as a blog or an online store, and would like to filter your search results (by category, date, price, and so on), you can do so:
 
 And then, Site Search 360 has a superpower that makes it stand out from its competitors: its AI-powered semantic product search engine continuously optimizes search result rankings, promoting the most popular results to the top. If you have eCommerce search enabled, just indicate what’s important for you to sell in your online store, and the engine will automatically re-arrange the results based on multiple ranking factors.
 Wrapping Up
 I was personally quite impressed by Site Search 360’s powerful search offer. In only 15 minutes, my site had a search functionality which I had been postponing for such a long time. Problem solved!
 If you too want to effortlessly add search to your site, then go check it out.</content>
     </entry>
     <entry>
       <title>Useful Tools for Visualizing Databases on a Budget</title>
         <link href="https://css-tricks.com/useful-tools-for-visualizing-databases-on-a-budget/"/>
       <updated>2022-06-13T19:15:26.000Z</updated>
       <content type="text">A diagram is a graphical representation of information that depicts the structure, relationship, or operation of anything. Diagrams enable your audience to visually grasp hidden information and engage with them in ways that words alone cannot. Depending on the type of project, there are numerous ways to use diagrams. For example, if you want to depict the relationship between distinct pieces, we usually use an Entity Relationship Diagram (ERD). There are many great tools that can help you sketch out your database designs beautifully.
 
 
 
 In this article, I will be sharing some of my favorite tools that I use to curate my data structures and bring my ideas to life.
 
 
 
 
 
 
 
 Google Docs Drawing
 
 
 
 The drawing function in Google Docs allows you to add illustrations to your pages. You can add custom shapes, charts, graphs, infographics, and text boxes to your document with the built-in drawing tool.
 
 
 
 
 
 
 
 Sketching with Google Docs
 
 
 
 Although it is simple to add a graphic to your Google Docs, the procedure is not totally visible. Here’s how:
 
 
 
 1 . Open a new document on Google Docs.
 
 
 
 
 
 
 
 2 . Click on the insert button and select Drawing . Then, from the drop-down option, choose New to open the drawing screen.
 
 
 
 
 
 
 
 3 . You can use the toolbox on this screen to add text boxes, select lines, and shapes, and modify the colors of your drawing.
 
 
 
 
 
 
 
 4 . You may also use the cursor to adjust the size of your drawings and the color of your designs by using the toolbox at the top of your screen.
 
 
 
 
 
 
 
 5 . When finished, click the Save and close button. You can click on the “File” toolbar displayed on the top of your screen to download your document.
 
 
 
 Features
 
 
 
 CostFree.CLI? GUI? Online?Online.Requires an Account?Yes, a Google account is required.Collaborative Editing?Yes, with Google Drive sharing.Import SQLNot Applicable.Export SQLNot Applicable.Export Formats.doc, .pdf, .rtf, .odt, .txt, .html, .epubGenerate Shareable URLYes.
 
 
 
 Google Docs offers amazing convenience. However, diagramming databases is not something it was intended for. You may find yourself frustrated with redrawing arrows and relationships if you are making frequent edits to your model.
 
 
 
 Graphviz
 
 
 
 Graphviz is a free graph visualization software that allows us to express information diagrammatically.
 
 
 
 
 
 
 
 Graphviz implements the DOT language. The DOT language is an abstract grammar that makes use of terminals, non terminals, parentheses, square brackets, and vertical bars. More information about the DOT language can be found in its documentation.
 
 
 
 Features
 
 
 
 CostFree.CLI? GUI? Online?CLI.Visual Studio Code, Eclipse, and Notepad++.Graphical Interfaces.Requires an Account?No.Collaborative Editing?Not Applicable.Import SQLYes, using SQL Graphviz.Export SQLYes, using SQL Graphviz.Export Formats.gif, .png, .jpeg, .json, .pdf and moreGenerate Shareable URLNot Applicable.
 
 
 
 Graphviz has an impressive and supportive community. However, a high level of SQL support is only available when you install additional third-party software. This overhead may make it less approachable to users that are not comfortable setting up their computer to support these tools.
 
 
 
 ERDPlus
 
 
 
 ERDPlus is a database modeling tool that allows you to create Entity Relationship Diagrams, Relational Schemas, Star Schemas, and SQL DDL statements.
 
 
 
 
 
 
 
 It includes a brief guide on how to create your ER diagrams, which is especially useful for beginners. You can also easily convert your created ER diagrams to relation schemas.
 
 
 
 Features
 
 
 
 CostFree.CLI? GUI? Online?Online.Requires an Account?Not required, but recommended for saving.Collaborative Editing?Not Applicable.Import SQLNo.Export SQLYes, with the support of SQL DDL statements.Export Formats.pngGenerate Shareable URLNot Applicable.
 
 
 
 ERDPlus is suited for SQL. It does lack additional export formats and ability to share with teams, but these features are not necessary with import and export.
 
 
 
 Diagrams.net
 
 
 
 Diagrams.net (previously Draw.io) is a free online diagramming tool that can be used to create flowcharts, UML diagrams, database models, and other types of diagrams.
 
 
 
 
 
 
 
 Features
 
 
 
 CostFree.CLI? GUI? Online?Desktop and Online.Requires an Account?Not required, but recommended for saving.Collaborative Editing?Sharing requires Google Drive or OneDrive.Import SQLYes.Export SQLNo.Export Formats.png, .jpeg, .svg, .pdf, .html and more.Generate Shareable URLYes, export as URL an option.
 
 
 
 Diagrams.net is designed to support many different workflows. Its ability to easily integrate with third-party integrations such as Trello, Quip, Notion, and others distinguishes it from the other options. The ability to share and collaborate may make it work well for collaborative teams.
 
 
 
 Conclusion
 
 
 
 This article is based on using free database tools that could help visualize your ideas and their capabilities with limitations to great details on how to use these tools.
 
 
 
 In my research, I also came across other excellent tools with free trials available for creating database diagrams like Lucidchart, EDrawMax, and, DrawSQL. However, these free trials have limitations which may make them less suited for developers working on multiple projects.
 
 
 
 I strongly recommend that you read the documentation for each of these tools to determine what works best for you and, most importantly, to avoid any difficulties in using these tools.
 
 
 
 Thank you for taking the time to read this far, and I hope you found what you were looking for. Have a wonderful day!
 
 Useful Tools for Visualizing Databases on a Budget originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>Harbour Kitchen</title>
         <link href="https://www.logodesignlove.com/harbour-kitchen"/>
       <updated>2022-06-13T13:43:41.000Z</updated>
       <content type="text">
 
 Located on the harbour-front in Stornoway, Scotland, Harbour Kitchen prides itself in its fresh selection of seafood. Designer Pearse O’Halloran was tasked with creating an identity that told the story of their culinary pride in a contemporary context.
 “We created a suite of brand assets and we were delighted to work with local printmaker Alice Macmillan on the main visual elements. The project includes a custom display typeface based on Hebridean culture.”
 
 
 
 
 
 
 
 
 
 
 
 
 You’ll undoubtedly be surprised to note that the HK monogram is the star of the project for me.
 The intricate linocut work in Alice’s portfolio is lovely, too. And, a bit off-topic, I enjoyed reading a 2019 blog post of hers about moving back to the Hebrides after a decade in South England. “…I’d come home and slump on the sofa like an irritable cabbage…”
 The level of detail in Alice’s linocut (below). Splendid.
 
 
 
 
 
 
 
 Reminded me of another fish kitchen logo I enjoyed.
 More from Pearse and Alice:
 www.loomgraphics.work
 www.alicemacprints.co.uk</content>
     </entry>
     <entry>
       <title>Designing Cross-Cultural Websites</title>
         <link href="https://tympanus.net/codrops/2022/06/13/designing-cross-cultural-websites/"/>
       <updated>2022-06-13T13:17:20.000Z</updated>
       <content type="text">As the world continues to become more interconnected, so too grows the need for designers to consider how their designs impact the digital experience of those who engage with it. Take buying a phone for example. Buying an iPhone 13 will always result in the same end product; an iPhone 13. However, the digital journey that a user in England experiences in comparison with a user in China may very well differ substantially. The content will be translated into the appropriate language, colours and copy justification will need to be considered for cultural nuance, and the site itself will need to rank on SERPs (Search Engine Results Pages) across different countries.When researching options to ensure your business can receive the most inclusive website possible, looking into how cross-cultures are also implemented can feel like an intimidating feat. Creative Brand Design, a leading web design agency, has curated a few tools and resources that you can use to help you understand what localisation is and how it’s used in the context of your web design.“Localization is more than just writing your content in multiple languages. You need a strategy to determine what localization to send, and code to do it.” – Sam RichardCreating a culturally inclusive site goes far beyond changing the language used.
 
 
 
 What is localisation?
 
 
 
 Localisation is the practice of adapting a website to meet the needs and expectations of multiple target cultures such as English and Chinese. When undertaking the localisation of any given website, some key things need to be considered before you put your fingers to the keyboard:
 
 
 
 Cultural nuance and client segmentsColour psychology in relation to the cultures you’re targetingLinguistic differencesImageryTypography 
 
 
 
 Cultural Nuance and Client Segments
 
 
 
 Photography by Haseeb Jamil
 
 
 
 Considering cultural nuance when designing a localised website is key. Understanding the markets that you are looking to target is the first step to understanding how they behave and what they’re looking for.If you already have an established product, marketing strategy and design in an existing country, use your current marketing and design strategies as a base to build from.In the UK and United States, they don’t typically put an enormous amount of pressure on getting married, settling down, having children, or living with/away from parents. However, they do encourage establishing a good education, exploring the world, travelling, socialising and nights out with friends.Other cultures will value settling down, getting married, having a stable and secure career and looking after parents as soon as possible. You can use this research to inform the type of imagery and marketing you will use throughout your website design to encourage action from your users. A picture of marital, familial bliss may not be so attractive to a young, university-leaving customer in the UK or the US, but it might be the cultural and societal ideal for a university leaver in other cultures. 
 
 
 
 Cultural Colour Psychology 
 
 
 
 Photography by Mg Cthu
 
 
 
 This is a bigger player than you might think. In Europe for example they see yellow to be a happy, exciting and joyful colour. It evokes feelings of happiness and pictures of sunshiny days and flowering daffodils. However, if your website and branding is heavily yellow leaning and you are looking to break through into the Latin market, you may want to reconsider or adapt how you use that yellow throughout your branding. Latin cultures may associate yellow with mourning, loss and times of grieving. In Eastern and Asian cultures, yellow is often worn by, and reserved for, members of the ruling class; it is believed to be sacred.Offending your prospective clients before you’ve even had the chance to say ‘hello, there!’ does not make for a successful design strategy.
 
 
 
 Language &amp; Linguistics
 
 
 
 Image by Eugenia Shustikova
 
 
 
 The translation is one aspect of localisation and probably one of the most fundamental – without being fully understood by the audience, you stand no chance of succeeding in any international market.Believe it or not, Google translate is not your friend. We all had it hammered into us in language classes in school, and I’m here to tell you that your teacher was right – sorry! Google translate is not a reliable source of translation and can often result in causing accidental offences!Hiring a reputable translator, preferably with your target language as their mother tongue, is an excellent place to start when it comes to translation. Why is this third on the list if it’s so important? Well, because very simply, hiring a good translator isn’t always very cheap. Understanding your client base and how they want to be spoken to and how they want to engage with your content will help to keep revisions of your content – and therefore translations – to a minimum.More commonly, English speaking companies are opting for using “they/them” pronouns in their marketing and advertising materials to avoid offending or excluding anyone who might be reading their content. Your product may be specifically female/male targeted and this may not be appropriate in your target market’s culture. You will need to consider the genders of certain words (for example German, French, Italian and Spanish languages all use gendered pronouns for objects and people alike) and make sure that they’re appropriately used throughout your content.Ensuring that your language is appropriate for your audience, correctly written, and well structured will also help to boost your SEO rankings in your new market’s SERPs. 
 
 
 
 Visual Elements
 
 
 
 Image by Igor Miske
 
 
 
 They say a picture is worth a thousand words, and as it tends to be one of the first things people look at when a page loads it’s essential to make sure that it sets the right tone and thoughts for the site.For example, a single slice of bacon could be mouthwatering for some, but highly offensive and derogatory to those where eating pork is prohibited.Also, it’s worth considering how a person is represented in the image when designing a product for a specific culture. Although it may seem obvious to some – make sure that the image you include accurately represents the people of that specific culture.The last thing you would want to end up doing is accidentally offending someone due to poor research or consideration! 
 
 
 
 Different Fonts
 
 
 
 Image by Mika Baumeister
 
 
 
 ‘It’s not what you say it’s how you say it’ – especially when it comes to font styles. Languages such as Chinese, Japanese, and Hebrew or just some of the more visually detailed languages with varying ligatures often use more pixels than the Latin alphabet we are used to.When translating from one language to another you could encounter a variety of different issues such as text expansion or decrease! By this, we refer to the amount of space different languages take up.Take Creative Brand Design’s name for example. In English it is a three-word name, however when translated into languages such as Polish, Chinese, or German you can see there are varying amounts of space required or even different ligatures. –Polish: Kreatywne projektowanie markiChinese: 創意品牌設計German: Kreatives markendesignItalian: Design creativo del marchio“Without the right font for the right language, the design of an app can quickly be destroyed.” – Google on Noto fonts
 
 
 
 If you’re looking for a font that would harmonise across any written language Google has answered your question! Noto was released in over 1000 different languages to help.
 
 
 
 Localising your text elements
 
 
 
 Photography by Marco Zuppone
 
 
 
 One thing that is worth bearing in mind is how each text appears on the page. For the majority of western languages such as Spanish, English and French, we read from left to right. However, there are many languages that are written from right to left and even further still some languages such as Japanese read vertically from top-down.As you can imagine – changing the layout to account for this can have extreme implications! So if you are looking at localising your design, it’s worth keeping in mind how the design itself will be affected by the change of language!
 
 
 
 Conclusion
 
 
 
 No matter how you look at it – culture is complex. It differs from one part of the world to the next and even sometimes within the same country, and just like language it’s ever-evolving and changing.At its core, it boils down to scalability. Designing and building features with scalability in mind can help navigate some of the hurdles when looking at localising your website and products.
 The post Designing Cross-Cultural Websites appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>How To Apply UX Principles To Embedded Systems: Learnings From The Field</title>
         <link href="https://smashingmagazine.com/2022/06/user-experience-principles-embedded-systems/"/>
       <updated>2022-06-13T11:00:00.000Z</updated>
       <content type="text">Embedded systems mean different things to different people; they can be standalone and independent, working by themselves, or be a part of a larger system. They are purpose-built for a particular application, designed to perform a specific function or set of tasks. Complexities of embedded systems range from very simple to highly sophisticated implementations, depending on the functions and features that need to be performed and its interactions and connections with other systems. Some examples are autonomous driving, artificial intelligence, and the internet of things. 
 To provide some context, embedded systems have been around for a long-time. Two examples are the difference engine devised by Charles Babbage in the 1830s and the Apollo Guidance Computer built in the 1960s and considered to be the first modern embedded system. A key component of an embedded system is its software. Embedded software brings a system to life, performing tasks on your behalf. Software is where most of the design effort and complexity of embedded systems lie. 
 
 In this article, I share three key learnings I have gained from applying UX and human-centered approaches when working with embedded software. These three takeaways include the complexity and advantage of addressing the needs of multiple stakeholders, how to benefit from understanding the dependencies between different components (by definition, embedded means integrated onto something), and how to overcome the challenge of communicating the value of technology that’s often invisible.
 
 Embrace Complexity In Your Projects And Designs
 Find Ways To Learn Fast And Don’t Go Solo
 Communicate The Value Of Embedded Software
 
 Learning #1: Embrace Complexity In Your Projects And Designs
 Different stakeholders will have different sets of challenges and goals interacting with your product.
 
 Let’s take a typical car as an example and imagine you develop software for its advanced driver-assistance (ADAS) system. There are three stakeholders in this example:
 
 the driver of the car, who is the end-user; 
 the person that integrates the software into the car is the user;
 the person in charge of purchasing what goes into the final car is the customer.
 
 Even then, this is a bit of a simplification. In real-life projects, there are even more dependencies and stakeholders involved. 
 Thus, who should you take into account when building and designing your solution? More than who to design software for, it is about thinking of what’s important for each stakeholder. In this example, the end-user cares about the experience and that everything works as expected or even better; the user is after a complete feature set and possibilities; the customer tends to care about cost-efficiency. In the end, you need to meet end-user expectations (who don’t even know they are interacting with embedded software) while making your product easy to integrate for the user and ensuring the customer that they are getting the best solution for an optimal price. 
 Additionally, embedded systems are, in many cases, resource-constrained in power processing or memory, yet the expectation is that the whole system works seamlessly and in a sophisticated way. Therefore, you need to find a balance between performance and reliability without hurting the experience. Optimizing this interaction requires a great level of expertise and specialized knowledge, which comes at a price. 
 When designing embedded software, it’s critical to spend time researching and understanding the problem space, the goals, and jobs to be done by the different stakeholders in real life. The better you understand each stakeholder’s expectation, the less risk of spending time building features that would have no use in a real environment or falling short of features for the future. Performing software upgrades to support new use cases in embedded systems can be very challenging because sometimes these devices are located in places that are hard to reach or have no connectivity. 
 Problem space exploration is closely related to user research. As such, there is plenty of literature, case studies, materials, frameworks and tools that you can use to collect data and gain insights into users’ needs and challenges. These activities usually include surveys, interviews, questionnaires, or market research. 
 Here are some examples:
 
 “Map the Problem Space”, Carissa Carter, Megan Stariha, Mark Grundberg
 The Problem Space Workshop, Christy Cattin
 “Product Strategy — Insights”, Marty Cagan
 
 A key aspect of understanding the problem space is to be able to map the different journeys for each of your stakeholders and their touchpoints and where in those journeys you could implement those research tools and feedback mechanisms to help you with your planning of user research and problem discovery.
 The template below from Columbia Road is a great starting point. I have simply added a new dimension — Feedback tools — to be able to link feedback activities to the journey stages. There are also other activities that you don’t necessarily think of as feedback or research tools. Yet, there are incredibly rich practices for gathering information, and I will cover some of those in the next point. 
 
 Learning #2: Find Ways To Learn Fast And Don’t Go Solo
 As mentioned, embedded systems have many dependencies. Something as trivial as a fridge can have dozens of interrelated embedded systems.
 
 The software runs on the hardware. Multiple systems need to interact in consonance with one another to deliver the expected functionality to the end-user. In practice, this dependency means that no matter how good of an idea you have, you will have to collaborate with someone to make it happen. Collaborations and joint efforts can take different shapes and forms, and here are a couple of tools and practices that I’ve found helpful:
 
 Workshops and roadmap alignment sessions involving customers and partners (handled as interactive events, not one-way presentations)These sessions give you the opportunity to explain how you perceive the market and what end-users are doing based on data and experiences you’ve collected, and check with customers and partners whether they see the same problems in the field and align on a future direction. There isn’t a universal way of doing this. Still, there are multiple templates and resources available for building roadmaps that allow you to group features under the key categories — themes — that are relevant to your markets and customers and that also let you prioritize your initiatives in a way that is flexible to account for changes and potential scenarios for the product:
 Roadmapping From A to Z (pdf)
 Agile Roadmap Template
 “What is a Theme-Based Roadmap and Why Is it Important?”
 “Outcome Roadmaps”, Sean Sullivan
 
 
 
 
 An initiative, theme-based roadmap with broad timelines (now/next/later) helps you drive a discussion where to align your current focus, explore together ideas for the future, and discuss expectations. In other words, it helps to validate that you and the customer have the same understanding of the current landscape, a willingness to collaborate in the future and that your efforts are going into addressing the right problems. The cadence will vary depending on your business and your product release planning.
 
 Joint demos and joint prototyping can also be used to validate assumptions, increase your knowledge, and test potential solutions. Research shows that inter-organizational product development initiatives have been growing and that co-development positively affects innovation. In embedded fairs and events, it is common to see demos of a company on display at other vendors’ booths. Hardware vendors and manufacturers tend to lack software skills in-house, and software companies can better tune and optimize their products if they have a deeper access to the underlying hardware. 
 
 Talent is highly specialized, and hiring prices in the industry can be relatively costly. Collaboration is a cost-effective way to work on proof of concepts for addressing emerging demands, finding new revenue streams, or reaching new markets. Joint prototyping can help you identify potential challenges and gather early feedback, as well as speed up the development process, spread out risks, and support your time-to-market strategy.
 In embedded, the saying “the whole is more/something else than the sum of its parts” is accurate. Being able to show how systems “talk” to each other and what is the jointly created value is very useful, especially when it comes to complex systems that can have a lifecycle of 5 to +10 years. In these cases, it is essential to get things right or rather spot design mistakes early on and validate your concepts whenever possible.
 
 Take all the chances you get for sharing your insights and ideas in panel discussions, contributing to communities specialized in a particular domain, or working on collaborative content like blog posts, whitepapers, and videos with other companies. This is a great way to gauge the interest in a given topic and get reactions and comments on your hypotheses.
 
 The embedded systems industry is highly competitive, with multiple players developing products in the same category. However, paradoxically, there’s still a reluctance to publicly engage in dialogues that may challenge the status quo and provoke new ways of thinking.
 I have found that the companies that end up being trusted (and building trust is one of the central goals of user experience and goes beyond the bare visuals of a product ) and respected are those who show up and spark discussion, and share their points of view and ideas without fear of others taking credit. To quote a sentence from the TV series Billions, “A lot of people watch Bruce Lee movies. It doesn’t mean they can do karate.”
 Learning #3: Communicate The Value Of Embedded Software
 When software is hidden and goes unnoticed, find ways to communicate its value by pointing out the needs it solves and the impact it has on the end-user in real life.
 
 You cannot differentiate your product with graphical interfaces, visual design elements, or aesthetics for software that performs tasks in the background and controls systems behind the scenes. Therefore, you need to find different ways to convey its value and relevance. Even if you are not able to directly interact with it, this type of software plays a key role in the end-user experience, impacting, for example, the performance of a device, its battery life, power consumption, and its overall behavior. Let’s see some examples:
 
 Your Smart TV And Power OutageThere was a power outage at home, and you were recording a program on it. What would you expect after the power comes back? Your TV is able to recover. You can turn it on again and boot successfully; all its settings are there, and even the program you were recording when losing power is there. To support this experience, the system needs to be designed to support power losses and not corrupt the file system or lose data — the software embedded in the TV should be reliable enough to take care of that.
 Smartwatches And Battery LifeSmartwatches are notoriously famous for their challenges with battery life. The need to regularly charge the battery affects the user experience, and it’s something that can be affected by optimizing memory and CPU consumption at the software level.
 
 
 
 Navigation Systems In Your Car And Network CommunicationsWhen driving, you expect your maps and traffic info to be updated in real-time with minimal latency), yet on the road, it is easy to lose network connectivity. Embedded software can help smooth that experience, gracefully behaving when the system is offline and caching the data until the system is back online and ensuring no data is missing. 
 
 Thinking of the scenarios and systems holistically and then being able to explain how you help in the field and the impact you can make for the final user is an effective way to communicate the value of software and the impact it has in providing seamless experiences and meeting expectations. Find a story that resonates with your audience first, and from there, you can always go into more details for those interested in the hows and why their devices operate.
 Conclusion
 We interact with embedded systems all the time by unawarely experiencing how they make our lives easier and enjoying all the features and benefits they bring. Quietly in the background, performing the tasks they were meant to, embedded systems are ubiquitous and have different levels of complexity, such as smartphones, traffic lights, manufacturing appliances, satellites, vehicles, medical equipment, and countless other devices. They are present in nearly every aspect of our lives.
  No matter how advanced or simple your product is and what is its place in the system, embedded systems will certainly play a role in the final user experience.
 Understanding the multiple dependencies to other components, the different needs of all the stakeholders you need to consider, and being able to validate your concepts and communicate your ideas early is crucial. It will help you design your embedded product in a way that lets you differentiate yourself through a better overall user experience and, silently behind the scenes, improve people’s lives.</content>
     </entry>
     <entry>
       <title>Zero-COVID and Free Speech</title>
         <link href="https://stratechery.com/2022/zero-covid-and-free-speech/"/>
       <updated>2022-06-13T09:39:07.000Z</updated>
       <content type="text">From the Financial Times, last Thursday:
 
   Barely a week after the Chinese Communist party declared victory in its struggle to protect Shanghai from coronavirus, half of the financial hub’s districts will be shuttered this weekend to test millions of residents after signs emerged of renewed community transmission of the virus. China’s most populous city, which was only released from a two-month lockdown last week, detected 11 new infections on Thursday, six outside the city’s mass quarantine centres. The measures will affect eight of the financial hub’s 16 districts, including Pudong, one of the worst-hit areas at the start of the lockdown.
   Three cases were detected in the Red Rose beauty parlour in the city centre, prompting health authorities to test more than 90,000 people close to the salon. Only a few days previously, the Xuhui local party body wrote a celebratory post on the microblogging platform Weibo hailing the salon’s reopening on June 1 for clients who had gone weeks without a professional haircut. It said the state-run salon’s resumption of business reflected how the city’s “pandemic situation improved”. The post has since been taken down.
 
 The mass testing ended up finding 5 cases; Chaoyang district in eastern Beijing, meanwhile, is undergoing mass testing of its own, and schools are closed.
 
 One of the common responses to China’s draconian efforts to control COVID’s spread (which, notably, do not include forced vaccination, or the use of Western vaccines), is that it doesn’t work: SARS-CoV-2, particularly the Omicron variant, is simply too viral. It’s worth pointing out that this response is incorrect: China not only eventually controlled the Wuhan outbreak, and not only kept SARS-CoV-2 out for most of 2021, but also ultimately controlled the Shanghai outbreak as well. The fact there were only 5 community cases over the weekend is proof that China’s approach works!
 What I think people saying this mean is something different: either they believe the trade-offs entailed in this effort are not worth it, or they simply can’t imagine a government locking people in their homes for months, hauling citizens off to centralized quarantine, separating parents and children, entering and spraying their homes, and killing their pets. I suspect the latter is more common, at least amongst most Westerners: people are so used to a baseline of individual freedom and autonomy that the very possibility of the reality of COVID in China simply does not compute.
 Taiwan and Zero-COVID
 Perhaps it is not only my knowledge of China, but also my experience living in Taiwan for nearly two decades, or more pertinently, my experience of living in Taiwan the last two years, that makes me much more willing to believe in the effectiveness of China’s approach.
 For most of the last two-and-a-half years Taiwan was COVID-free; for most of 2020 that meant life went on as normal, with no masks, everything open, etc.; the one abnormality was that every person entering Taiwan had to quarantine (at home or in a hotel) for 14 days. Things changed in 2021, when the Alpha variant broke through, leading to a soft lockdown: restaurants and schools were closed, and workplaces were strongly encouraged to work from home; masks were instituted everywhere, including outside, and quarantine was hotel only. What is less known is that quarantine went beyond travelers: anyone who was a close contact of an infected person, including family members and co-workers, but also people who might have had the misfortune of being in the same restaurant at the same time as a positive case, were quarantined as well (your location in said restaurant was ascertained by reviewing your cellular location data).
 It is this last point that, in my estimation, stopped the 2021 spread in its tracks, and kept Taiwan COVID-free until earlier this year (I myself endured an 18-day centralized quarantine due to testing positive at the airport). It is also, for nearly every Westerner I have relayed this fact to, a startling abridgment of civil liberties. The very idea that you can be locked up for simply being in the wrong place at the wrong time is inconceivable; that, though, is much less stringent than China’s approach in Shanghai, including the requirement that you need a PCR-test within the last 72 hours to even grocery shop.
 Here’s the thing: that relative reduction in stringency relative to China is precisely why Taiwan’s containment eventually failed; Taiwan, for most of the last month, has had the highest case rate in the world. From the New York Times COVID tracker:1
 
 Taiwan, to its credit, did not lockdown in the face of this outbreak; I suspect the horrors of the Shanghai lockdown served as a deterrent, particularly given Taiwan’s ongoing struggle for international recognition and desire to distinguish itself from China. It’s also worth noting that at the critical moment — late March and early April — it wasn’t clear if China’s lockdowns would work; still, even if the outcome was clear, Taiwan — despite its willingness to violate civil liberties to a considerably greater degree than most Western democracies — was never willing to go as far as China. And so, while the Chinese approach worked, it almost certainly would not have worked in Taiwan simply because the latter wasn’t willing to be as brutal as the former.
 I am being, as best as I can, impartial about the choices here: the important takeaway is not simply that China’s approach did in fact work to arrest the spread of SARS-CoV-2, but also that it was the only approach that worked; even Taiwan’s approach, which was far more stringent than any Western country would tolerate, eventually failed. Of course there were benefits, particularly in terms of getting time to administer vaccines, but it’s certainly worth wondering if it was all worth it.2
 The opposite side of the spectrum were areas of America that, after enduring a few months of (very) soft lockdown at the beginning of the pandemic, were mostly open from the summer of 2020 on; I have friends in parts of Wisconsin, for example, whose kids have been in school since the fall of 2020. The price of this approach was far more deaths, particularly amongst the elderly who have always been at far higher risk: over 1 million Americans have died of COVID.
 This isn’t the complete COVID story, though, and not simply because there can be no honest accounting of the pandemic until it finally sweeps China; the most effective vaccines in the world were developed in the West, and the U.S. produced and distributed the largest number of them. How many lives were saved, and how much economic upheaval — which isn’t about simply dollars and cents, but people’s livelihoods, sense of worth, and even sanity — was avoided or reduced because of vaccines? That must be recorded in the ledger as well, and in this accounting the West comes out looking far stronger.
 The Great Firewall
 The reason to audit this accounting is that I think there is an analogy to be drawn between COVID and the debates around free speech that have sprung up over the last six years. Before then, there wasn’t much of a debate about free speech: just as the W.H.O. and C.D.C. used to maintain that lockdowns don’t work, it used to be widely accepted that free speech was a good thing. Moreover, it was also accepted that free speech was not simply a legalistic limitation on government power, but was a cultural value. I pointed out earlier this year that this was no longer the case in elite culture; the debate around Elon Musk buying Twitter confirmed exactly that.
 To summarize, the “sophisticated” view on free speech is that the First Amendment both restricts the government and also protects companies who make their own moderation decisions; this is of course correct legally, but the idea that this distinction should be both celebrated and pushed to its limit is new. That, by extension, means that the “rube” view on free speech is that said principle ought to apply broadly: not only should the government not be able to limit your speech, but neither should Facebook or Twitter or Google. Again, this was a widely held view not too long ago: much of the debate around net neutrality, for example, centered on the importance of private corporations not being allowed to treat different bits of data differently based on what type of content they represented.
 There are, of course, philosophical arguments to be made as to why either view is better or worse than the other; to return to the COVID analogy, one can debate whether or not the sacrifice of civil liberties is worth whatever deaths might be prevented (again, with the caveat that the final accounting is not yet complete). What I think is missing in both debates, though, is the question of what was possible.
 Go back to my point above: I strongly suspect that most people in the West are convinced that China’s approach will not work — even though it is! — because they simply cannot imagine enduring or tolerating or even encountering the level of brutality necessary for success; that is certainly true of COVID dead-enders who still bemoan that the West isn’t doing enough to control the spread of COVID. It is, from my perspective, hard to imagine any of these folks accepting non-negotiable centralized quarantine simply for being in the wrong restaurant at the wrong time — and again, this is the Taiwan approach that ultimately failed! They are complaining about something that simply isn’t possible, not because their political enemies are unwilling to do what is necessary, but because they themselves would never tolerate it.
 This, I should note, is why I have long been in strong favor of fully opening up: while there was an argument to be made that it was worth trying to delay outbreaks until vaccines were widely available, by the summer of 2021 (in the U.S.) the only possible outcome of restrictions was to make people miserable at best, and cause economic, socio-political, and developmental damage at worst; spread, absent a China-style approach, was inevitable, so why invite bad outcomes when there are no benefits?3
 I have the same questions about free speech. Once again, I must acknowledge that China’s approach to free speech works, at least in terms of its leaders’ immediate goals. In other words, it doesn’t exist, even — especially! — on the Internet. This — like China’s insistence on zero-COVID — was something that Westerners scoffed at as being unrealistic; then-President Bill Clinton said upon the establishment of Permanent Normal Trade Relations with China:
 
   In the new century, liberty will spread by cell phone and cable modem. In the past year, the number of Internet addresses in China has more than quadrupled, from 2 million to 9 million. This year the number is expected to grow to over 20 million. When China joins the W.T.O., by 2005 it will eliminate tariffs on information technology products, making the tools of communication even cheaper, better, and more widely available. We know how much the Internet has changed America, and we are already an open society. Imagine how much it could change China.
   Now there’s no question China has been trying to crack down on the Internet. Good luck! That’s sort of like trying to nail jello to the wall. But I would argue to you that their effort to do that just proves how real these changes are and how much they threaten the status quo. It’s not an argument for slowing down the effort to bring China into the world, it’s an argument for accelerating that effort. In the knowledge economy, economic innovation and political empowerment, whether anyone likes it or not, will inevitably go hand in hand.
 
 Clinton, along with nearly all of the Western intelligentsia, underrated China’s willingness to do whatever it took to build a mold around that jello, from building the Great Firewall to employing countless numbers of censors to tanking its entire IT sector once it felt it was becoming too politically powerful. The end result is a populace that not only has little idea about today’s reality — i.e. that most people have had COVID, and are fine, and are living normally — but even less idea about the past.
 Tank Cake
 Last February Time Magazine named Li Jiaqi one of its “Next Top 100 Most Influential People”. Li’s nickname was the “lipstick king”, which refers to the time in 2018 when the live-streaming e-commerce peddler sold 15,000 lipsticks in 5 minutes; last fall Li sold $1.7 billion worth of goods in 12 hours. Ten days ago, on June 3, Li was doing what he does best — selling goods via live-streaming — when his stream suddenly went off the air; Li, within a matter of hours, was suddenly off of the Internet, no longer appearing on Taobao, Alibaba’s e-commerce platform that streamed his show. The BBC explained what happened:
 
   Last Friday night, Li was mid-way through his popular livestream show when it ended abruptly. The 30-year-old, known for his smooth voice and K-pop idol looks, had just shown his audience a vanilla log cake while selling snacks. The cake resembled a tank: it had Oreos for wheels and a wafer pipe resembling a cannon. And Li’s show was on 3 June, the eve of the 33rd anniversary of the Tiananmen Square massacre…
   
   Generations of Chinese have grown up without learning of the massacre – and many of those millennials and Gen Z-ers appeared to be among Li’s audience on Friday and in the days after. Li failed to return to his livestreaming show after the transmission was cut. Shortly after, he posted on his Weibo account saying he had merely faced technical issues. But his continued absence – he has missed three shows so far during one of the year’s biggest online shopping festivals – has only fuelled more questions and debate. Some have cottoned on quickly as to why he was censored, while others are having a revelation. “What does the tank mean?” a confused viewer asked. Another said: “What could possibly be the wrong thing to say while selling snacks?”
 
 That’s not all, though: it seems almost certain that Li had no idea he did anything wrong, or why.
 
   Few online believe that Li was trying to make a political statement. Given his celebrity status, he knew how to navigate political sensitivities and to steer clear of minefields, they said. And he had never expressed political beliefs before. Some even argued that he was possibly among those who didn’t know about the Tiananmen Square massacre.
   Many of his loyal fans also wondered if the top livestreamer had been set up by competitors to take a political fall, and perhaps the cake was sneaked into the line-up of his show on Friday. A clip circulating on social media, apparently of the moment before the cake is brought out, also shows Li expressing surprise over the announcement of a tank product. A male assistant announces in the background that the team has a tank-shaped good to sell. Li laughs and says: “What? A tank?” His co-presenter then says: “Let’s see if Li Jiaqi and I will still be here at 11pm.” They were taken off air shortly after 9pm.
 
 Many fans suspect purposeful sabotage; perhaps that is a conspiracy theory, but said theory is undergirded by the reality that it is not just possible but even probable that a 30 year-old in China has no idea that selling a tank-shaped cake on June 3rd is grounds for being disappeared. To put it another way, China’s control of information is not unlike its control of COVID: it seems impossible, and the means intolerable, but that is simply because we in the West can’t imagine the limitations on personal freedom necessary to make it viable.
 Acceptance and Competition
 To further expand on this point: if people in the West would not accept truly strict lockdowns, then they certainly wouldn’t accept centralized quarantine (which didn’t work), which means they absolutely wouldn’t accept forced testing and the inability to leave your house for months. Ergo, people in the West would never accept the reality of zero-COVID, which is why it makes sense to go in the opposite direction: open up, and forgo the massive costs of zero-COVID as well. Don’t get stuck in the middle, enduring the worst outcomes of both.
 Similarly, if people in the U.S. would not accept any government infringement on speech, then they certainly wouldn’t accept ISP-level censorship like the Great Firewall, which means they absolutely wouldn’t accept forced disappearances for selling the wrong cake. Ergo, people in the U.S. would never accept the reality of true control of speech, which is why it makes sense to go in the opposite direction: embrace free speech not just as a law but as a cultural more, and forgo the massive costs of half-ass speech restrictions as well. Don’t get stuck in the middle, enduring the worst outcomes of both.
 COVID, alas, seems to have been a worst case scenario in terms of both points: we suffered the aforementioned economic, socio-political, and development damage associated with strict control, while controlling nothing; meanwhile private platforms went overboard in controlling information, and ended up only deepening the suspicion of skeptics about COVID and its vaccines, leading to many more deaths, but also increased skepticism about vaccines generally.
 The worry is that this middling approach, where we get the worst of both worlds, impacts innovation generally; China is increasingly focused on a top-down approach to technological innovation in particular, placing heavy emphasis and tons of money on catching up in areas like semiconductors and AI. The best response is to go in the opposite direction, and let a thousand flowers bloom, trusting that innovation by definition arises in places we least expect it.
 To put it another way, if we could accurately eliminate bad ideas, then there would, by definition, be no more good ideas to discover; the way to compete with China is to lean into the fact that there remains so much we don’t yet know.
 
 You likely have, by this point, heard the story of Katalin Karikó; from Stat News in 2020:
 
   Before messenger RNA was a multibillion-dollar idea, it was a scientific backwater. And for the Hungarian-born scientist behind a key mRNA discovery, it was a career dead-end. Katalin Karikó spent the 1990s collecting rejections. Her work, attempting to harness the power of mRNA to fight disease, was too far-fetched for government grants, corporate funding, and even support from her own colleagues…By 1995, after six years on the faculty at the University of Pennsylvania, Karikó got demoted. She had been on the path to full professorship, but with no money coming in to support her work on mRNA, her bosses saw no point in pressing on.
 
 Karikó would eventually figure out how to stop the body from rejecting mRNA, an essential discovery on the way to today’s vaccines. Along the way, though, she was nearly defeated by an academic system that increasingly relies on money from the powers that be, who think they know everything; fortunately said powers couldn’t actually stop her work, even though the consensus was that said work was a bad idea.
 Only with time did it reveal itself as a good idea, which is the story of almost everything in life: we live, we learn, we discover new things, not just those of us alive in 2022, but all of humanity for our entire existence. That is how we beat COVID: not by destroying our liberties and lives, but by invention and information. It turns out that free speech isn’t just an analogy to COVID: it’s an essential part of getting past it. And, critically, it’s the only approach that nearly all of us reading this article — particularly those of us in the U.S., no matter our political affiliation — would actually tolerate.
 In short, we live in the U.S., not China, and it’s high time all of us — including tech companies — started acting like it, instead of LARPing the most pathetic imitation possible.
 This case rate is likely significantly underreported, I would add: given that positive cases are not allowed to leave their house for 7 days — again, tracked by cellphone — there is a very strong incentive to simply not report a positive case; anecdotally speaking the majority of people I know in Taiwan have gotten COVID over the last month or so. ↩My aforementioned 18-day quarantine in April certainly seemed like a needless waste of my life — as do ongoing traveler quarantines whose only purpose is to protect travelers from what is again, the highest case rate in the world. ↩I do recognize that people wished to wait for a children’s vaccine; given the relative risk for children I disagreed, but I acknowledge the argument ↩</content>
     </entry>
     <entry>
       <title>Single Element Loaders: The Spinner</title>
         <link href="https://css-tricks.com/single-element-loaders-the-spinner/"/>
       <updated>2022-06-10T14:26:06.000Z</updated>
       <content type="text">Making CSS-only loaders is one of my favorite tasks. It’s always satisfying to look at those infinite animations. And, of course, there are lots of techniques and approaches to make them — no need to look further than CodePen to see just how many. In this article, though, we will see how to make a single element loader writing as little code as possible.
 
 
 
 
 
 
 
 I have made a collection of more than 500 single div loaders and in this four-part series, I am going to share the tricks I used to create many of them. We will cover a huge number of examples, showing how small adjustments can lead to fun variations, and how little code we need to write to make it all happen!
 
 
 
 
 Single-Element Loaders series:
 
 
 
 Single Element Loaders: The Spinner — you are hereSingle Element Loaders: The DotsSingle Element Loaders: The BarsSingle Element Loaders: Going 3D — coming July 1
 
 
 
 
 For this first article, we are going to create a one of the more common loader patterns: spinning bars:
 
 
 
 CodePen Embed Fallback
 
 
 
 Here’s the approach
 
 
 
 A trivial implementation for this loader is to create one element for each bar wrapped inside a parent element (for nine total elements), then play with opacity and transform to get the spinning effect.
 
 
 
 My implementation, though, requires only one element:
 
 
 
 &lt;div class&#x3D;&quot;loader&quot;&gt;&lt;/div&gt;
 
 
 
 …and 10 CSS declarations:
 
 
 
 .loader {
   width: 150px; /* control the size */
   aspect-ratio: 1;
   display: grid;
   mask: conic-gradient(from 22deg, #0003, #000);
   animation: load 1s steps(8) infinite;
 }
 .loader,
 .loader:before {
   --_g: linear-gradient(#17177c 0 0) 50%; /* update the color here */
   background: 
     var(--_g)/34% 8%  space no-repeat,
     var(--_g)/8%  34% no-repeat space;
 }
 .loader:before {
   content: &quot;&quot;;
   transform: rotate(45deg);
 }
 @keyframes load {
   to { transform: rotate(1turn); }
 }
 
 
 
 Let’s break that down
 
 
 
 At first glance, the code may look strange but you will see that it’s more simple than what you might think. The first step is to define the dimension of the element. In our case, it’s a 150px square. We can put aspect-ratio to use so the element stays square no matter what.
 
 
 
 .loader {
   width: 150px; /* control the size */
   aspect-ratio: 1; /* make height equal to width */
 }
 
 
 
 When building CSS loaders, I always try to have one value for controlling the overall size. In this case, it’s the width and all the calculations we cover will refer to that value. This allows me to change a single value to control the loader. It’s always important to be able to easily adjust the size of our loaders without the need to adjust a lot of additional values.
 
 
 
 Next, we will use gradients to create the bars. This is the trickiest part! Let’s use one gradient to create two bars like the below:
 
 
 
 background: linear-gradient(#17177c 0 0) 50%/34% 8% space no-repeat;
 
 
 
 
 
 
 
 Our gradient is defined with one color and two color stops. The result is a solid color with no fading or transitions. The size is equal to 34% wide and 8% tall. It’s also placed in the center (50%). The trick is the use of the keyword value space — this duplicates the gradient, giving us two total bars.
 
 
 
 From the specification:
 
 
 
 The image is repeated as often as will fit within the background positioning area without being clipped and then the images are spaced out to fill the area. The first and last images touch the edges of the area.
 
 
 
 I am using a width equal to 34% which means we cannot have more than two bars (3*34% is greater than 100%) but with two bars we will have empty spaces (100% - 2 * 34% &#x3D; 32%). That space is placed in the center between the two bars. In other words, we use a width for the gradient that is between 33% and 50% to make sure we have at least two bars with a little bit of space between them. The value space is what correctly places them for us.
 
 
 
 We do the same and make a second similar gradient to get two more bars at the top and bottom, which give us a background property value of:
 
 
 
 background: 
  linear-gradient(#17177c 0 0) 50%/34% 8%  space no-repeat,
  linear-gradient(#17177c 0 0) 50%/8%  34% no-repeat space;
 
 
 
 We can optimize that using a CSS variable to avoid repetition:
 
 
 
 --_g: linear-gradient(#17177c 0 0) 50%; /* update the color here */
 background: 
  var(--_g)/34% 8%  space no-repeat,
  var(--_g)/8%  34% no-repeat space;
 
 
 
 So, now we have four bars and, thanks to CSS variables, we can write the color value once which makes it easy to update later (like we did with the size of the loader).
 
 
 
 To create the remaining bars, let’s tap into the .loader element and its ::before pseudo-element to get four more bars for a grand total of eight in all.
 
 
 
 .loader {
   width: 150px; /* control the size */
   aspect-ratio: 1;
   display: grid;
 }
 .loader,
 .loader::before {
   --_g: linear-gradient(#17177c 0 0) 50%; /* update the color here */
   background: 
     var(--_g)/34% 8%  space no-repeat,
     var(--_g)/8%  34% no-repeat space;
 }
 .loader::before {
   content: &quot;&quot;;
   transform: rotate(45deg);
 }
 
 
 
 Note the use of display: grid. This allows us to rely on the grid’s default stretch alignment to make the pseudo-element cover the whole area of its parent; thus there’s no need to specify a dimension on it — another trick that reduces the code and avoid us to deal with a lot of values!
 
 
 
 Now let’s rotate the pseudo-element by 45deg to position the remaining bars. Hover the following demo to see the trick:
 
 
 
 CodePen Embed Fallback
 
 
 
 Setting opacity
 
 
 
 What we’re trying to do is create the impression that there is one bar that leaves a trail of fading bars behind it as it travels a circular path. What we need now is to play with the transparency of our bars to make that trail, which we are going to do with CSS mask combined with a conic-gradient as follows:
 
 
 
 mask: conic-gradient(from 22deg,#0003,#000);
 
 
 
 To better see the trick, let’s apply this to a full-colored box:
 
 
 
 CodePen Embed Fallback
 
 
 
 The transparency of the red color is gradually increasing clockwise. We apply this to our loader and we have the bars with different opacity:
 
 
 
 
 
 
 
 In reality, each bar appears to fade because it’s masked by a gradient and falls between two semi-transparent colors. It’s hardly noticeable when this runs, so it’s sort of like being able to say that all the bars have the same color with a different level of opacity.
 
 
 
 The rotation
 
 
 
 Let’s apply a rotation animation to get our loader. Note, that we need a stepped animation and not a continuous one that’s why I am using steps(8). 8 is nothing but the number of the bars, so that value can be changed depending on how many bars are in use.
 
 
 
 .loader {
   animation: load 3s steps(8) infinite;
 }
 
 /* Same as before: */
 @keyframes load {
   to { transform: rotate(1turn) }
 }
 
 
 
 CodePen Embed Fallback
 
 
 
 That’s it! We have our loader with only one element and a few lines of CSS. We can easily control its size and color by adjusting one value.
 
 
 
 CodePen Embed Fallback
 
 
 
 Since we only used the ::before pseudo-element, we can add four more bars by using ::after to end with 12 bars in total and almost the same code:
 
 
 
 CodePen Embed Fallback
 
 
 
 We update the rotation of our pseudo-elements to consider 30deg and 60deg instead of 45deg while using an twelve-step animation, rather than eight. I also decreased the height to 5% instead of 8% to make the bars a little thinner.
 
 
 
 Notice, too, that we have grid-area: 1/1 on the pseudo-elements. This allows us to place them in the same area as one another, stacked on top of each other.
 
 
 
 Guess what? We can reach for the same loader using another implementation:
 
 
 
 CodePen Embed Fallback
 
 
 
 Can you figure out the logic behind the code? Here is a hint: the opacity is no longer handled with a CSS mask but inside the gradient and is also using the opacity property.
 
 
 
 Why not dots instead?
 
 
 
 We can totally do that:
 
 
 
 CodePen Embed Fallback
 
 
 
 If you check the code, you will see that we’re now working with a radial gradient instead of a linear one. Otherwise, the concept is exactly the same where the mask creates the impression of opacity, but we made the shapes as circles instead of lines.
 
 
 
 Below is a figure to illustrate the new gradient configuration:
 
 
 
 
 
 
 
 If you’re using Safari, note that the demo may be buggy. That’s because Safari currently lacks support for the at syntax in radial gradients. But we can reconfigure the gradient a bit to overcome that:
 
 
 
 .loader,
 .loader:before,
 .loader:after {
   background:
     radial-gradient(
       circle closest-side,
       currentColor 90%,
       #0000 98%
     ) 
     50% -150%/20% 80% repeat-y,
     radial-gradient(
       circle closest-side,
       currentColor 90%,
       #0000 98%
     ) 
     -150% 50%/80% 20% repeat-x;
 }
 
 
 
 CodePen Embed Fallback
 
 
 
 More loader examples
 
 
 
 Here is another idea for a spinner loader similar to the previous one.
 
 
 
 CodePen Embed Fallback
 
 
 
 For this one, I am only relying on background and mask to create the shape (no pseudo-elements needed). I am also defining the configuration with CSS variables to be able to create a lot of variations from the same code — another example of just the powers of CSS variables. I wrote another article about this technique if you want to more details.
 
 
 
 Note that some browsers still rely on a -webkit- prefix for mask-composite with its own set of values, and will not display the spinner in the demo. Here is a way to do it without mast-composite for more browser support.
 
 
 
 I have another one for you:
 
 
 
 CodePen Embed Fallback
 
 
 
 For this one, I am using a background-color to control the color, and use mask and mask-composite to create the final shape:
 
 
 
 
 
 
 
 Before we end, here are some more spinning loaders I made a while back. I am relying on different techniques but still using gradients, masks, pseudo-element, etc. It could be a good exercise to figure out the logic of each one and learn new tricks at the same time. This said, if you have any question about them, the comment section is down below.
 
 
 
 CodePen Embed Fallback
 
 
 
 CodePen Embed Fallback
 
 
 
 CodePen Embed Fallback
 
 
 
 Wrapping up
 
 
 
 See, there’s so much we can do in CSS with nothing but a single div, a couple of gradients, pseudo-elements, variables. It seems like we created a whole bunch of different spinning loaders, but they’re all basically the same thing with slight modifications.
 
 
 
 This is only the the beginning. In this series, we will be looking at more ideas and advanced concepts for creating CSS loaders.
 
 
 
 
 Single-Element Loaders series:
 
 
 
 Single Element Loaders: The Spinner — you are hereSingle Element Loaders: The DotsSingle Element Loaders: The BarsSingle Element Loaders: Going 3D — coming July 1
 
 
 Single Element Loaders: The Spinner originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>DevFest For Ukraine, A Charity Conference On The Future Of Tech 🇺🇦</title>
         <link href="https://smashingmagazine.com/2022/06/devfest-for-ukraine/"/>
       <updated>2022-06-10T13:59:00.000Z</updated>
       <content type="text">Every day, millions of Ukrainians show incredible courage and strength resisting Russian aggression. Volunteers, individuals and organizations are working together to provide support and raise funds for those in need.
 
 In times like these, uniting efforts and working together as a community matters more than ever. That’s why we’d like to highlight a wonderful initiative today. DevFest for Ukraine, a conference with world-class technical speakers and an important mission: to raise funds for Ukraine.
 
 DevFest for Ukraine is a charitable tech conference that will bring together 20 industry-leading speakers over two days (June 14–15), featuring live streams from London and Lviv. It will address key topics for the future of tech, including trends in Android, web, and AI.
 
 
 What Can You Expect At DevFest For Ukraine?
 
 The conference will take place online June 14–15, featuring both live and recorded sessions from London and Lviv, along with live Q&amp;As. From tech sessions and inspirational keynotes to networking and overviews of the latest developer tools, DevFest for Ukraine brings together people who shape the future of Android, web, and AI technologies. 
 
 Learn from industry-leading speakers like Romain Guy and Chet Haase who have been building Android since 1.0, Jhey Tompkins and Una Kravets from the Google Chrome team, or Robert Crowe from the TensorFlow team who will talk about trends in ML development. The participation is donation-based: your donation will give you access to the live stream and recordings after the event. For the detailed schedule, visit the DevFest for Ukraine website.
 
 Where Will Donations Go?
 
 All funds raised will support different causes in a transparent, public way. If you want to learn more about the organizations, the DevFest for Ukraine team summarized why they chose them.
 
 The initial goal was to raise $50,000. Thanks to multiple donations from individuals and partners, they’ve already reached their $50,000 and $75,000 milestones. And now they set a new goal — raising $100,000. With these funds, they can buy about 1,000 first aid kits, or 320 bulletproof vests, or 60 tons of humanitarian aid, which can save the lives of thousands of our Ukrainian friends. Let’s do it together!
 
 How To Join
 
 Join the conference to support Ukraine and gain knowledge from the speakers who are shaping the future of tech!
 
 Thank you for your kind support. 💙💛</content>
     </entry>
     <entry>
       <title>Measuring The Performance Of Typefaces For Users (Part 2)</title>
         <link href="https://smashingmagazine.com/2022/06/measuring-performance-typefaces-users-part2/"/>
       <updated>2022-06-10T08:00:00.000Z</updated>
       <content type="text">In the first part of this article, we saw that measuring and comparing typefaces is not a simple task. Testing it (subjectively or objectively) also depends on the context — which can be very tricky. We saw how important it is to keep the typographic design parameters and variables the same to get a more accurate result while testing different typefaces.
  Did you miss out on it? Don’t worry! Take a look at the first part of this article here so you can get all the context you need to fully enjoy it.
 
 Measuring, comparing, and testing typefaces may guide your project decisions towards a more accessible and highly legible typeface. Let’s dive into the specifics of typeface aspects so you can get the best out of your tests. So what aspects of typefaces could we measure?
 Aspects Specific To The Typeface Itself For Extended Reading Typefaces
 Ascender And Descender Height In Relation To X-height
 
 Validation:There seems to be some truth in a larger x-height and medium-length ascenders and descenders being ideal.
 Description:The most efficient typefaces with the best ratio of x-height and cap-height seem to be Wayfinding Sans Pro and Johnston Underground with an x-height between 67% to 69% of the cap height. Learn more about it in Ralf Hermann’s (director of Typography.Guru paper, “Does A Large X-Height Make Fonts More Legible?.”
 Measurement:Millimeters.
 Measure quality type:Strong (objective).
 
 Aesthetic Quality In Relation To Other Similar Typefaces
 
 Validation:There would probably be other typefaces that the typeface would be similar to or would fit with, so how well does it compare?
 Description:Expert review or opinion.
 Measurement:Not good, okay, very good.
 Measure quality type:Weak (subjective).
 
 Aesthetic Quality In Relation To Historical Revival Or Similarity
 
 Validation:There would probably be other typefaces that the typeface would be similar to or fit with. Therefore, how well does it perform when compared to other typefaces that are considered to be historically well done or have been revived well?
 Description:Expert review or opinion.
 Measurement:Not good, okay, very good.
 Measure quality type:Weak (subjective).
 
 Character, Symbol, And Language Support
 
 Validation:To find out how usable the typeface is, for different characters, symbols, languages, and information types.
 Description:We know character and symbol support, like for maths and different languages, is desirable and needed.
 Measurement:Numerical score or tick list against features and languages.
 Measure quality type:Strong (objective).  
 
 Kerning
 
 Validation:To find out how well the typeface has been kerned — because kerning leads to better and more legible typography.
 Description:A kerning test based on the example below, from Veronika Burian and José Scaglione’s — directors of the font foundry TypeTogether — article “Quality type: How to spot fonts worth your money.”
 
 
 
 Measurement:Not good, okay, very good. Percentage value, maybe. Precise numerical score based on a standard test.
 Measure quality type:Strong (objective).
 
 Accessibility (Children)
 
 Validation:We know that accessible characters and symbols lead to better, more legible, and easier-to-read typography.
 Description:Infant characters as in my paper “Letter and symbol misrecognition in highly legible typefaces for general, children, dyslexic, visually impaired and aging readers — 2019 fourth edition.”
 
 
 
 Measurement:Numerical score based on the number of characters and symbols. Research and design effort based on not good, okay, very good.
 Measure quality type:Strong (objective).
 
 Accessibility (Dyslexia)
 
 Validation:Dyslexic characters as in my paper “Letter and symbol misrecognition in highly legible typefaces for general, children, dyslexic, visually impaired and aging readers — 2019 fourth edition” and Robert Hillier’s (typeface designer and academic) PhD “A typeface for the adult dyslexic reader.”
 
 
 
 Description:Numerical score based on the number of characters and symbols. 
 Measurement:Research and design effort based on not good, okay, very good.
 Measure quality type:Strong (objective).
 
 Accessibility (Vision Impairment)
 
 Validation:Visually impaired characters as in my paper “Letter and symbol misrecognition in highly legible typefaces for general, children, dyslexic, visually impaired and aging readers — 2019 fourth edition.”
 
 
 
 Measurement:Numerical score based on the number of characters and symbols.
 Description:Research and design effort based on not good, okay, very good.
 Measure quality type:Strong (objective).
 
 Aspects Specific To The Reader/User For Extended Reading Typefaces
 Comprehension
 
 Validation:To find out how much and how well information is absorbed, retained, and recalled from a typeface or different typefaces.
 Description:This is a very difficult area to measure, and I would like to explain why:
 Not everyone will be able to recall accurately everything they know or do not know in an exam or in test questions;
 What people say they know, fail to communicate, and what they actually do in the real world are very different things. Just because some cannot recall something or write it in an exam paper, it does not mean they do not know it. 
 
 
 
 “Many previous reading studies investigated the effect of typography on reading speed. But we know that faster speed does not always equate to better comprehension. In fact, better comprehension is often associated with slower reading speed.”— Sofie Beier (legibility expert), “Bringing together science and typography”
 
 
 Measurement:A paragraph of text, or pages of text and information, followed by questions or set the users tasks to do based on the information.
 Measure quality type:Weak (subjective).
 
 Speed
 
 Validation:To find out how much, or how much more, they can read compared to what is considered normal/average from a typical typeface.
 Description:This is another very difficult area to measure, let me explain why. I could quickly scan a single page of a book and, in theory, have read all the content in about 6 seconds (because I have scanned my eyes across all the text quickly). Although, just because I have, in theory, read (or scanned) the text, it does not necessarily mean I have understood or absorbed it. However, the result would be taken into consideration — and there would probably exist clear and strong differences in performance between comparing a script typeface (like Snell Roundhand) against a highly legible typeface (like the Unit typeface). So the Unit typeface would be much easier and quicker to read than Snell Roundhand.
 Measurement:Eye-tracking (time, speed, and behavior) recording and data collection.
 Measure quality type:Strong (objective).
 
 Facial Muscle Activation
 
 Validation:The zygomatic muscle activity (which controls smiling) is positively associated with positive emotional stimuli and a positive mood state.
 Description:By placing tiny sensors over certain facial muscles, one can measure the minute changes in the muscles’ electrical activity, which reflects changes in muscle tension. Facial EMG (electromyography) studies have found that activity of the corrugator muscle (which lowers the eyebrow and is involved in producing frowns) varies inversely with the emotional valence of the presented stimuli and reports of mood state. You can see in the academics John Cacioppo, Lauren Bush, and Louis Tassinary’ paper “Microexpressive facial actions as a function of affective stimuli: Replication and extension” and in the academic Ulf Dimberg’s writing “Facial electromyography and emotional reactions.”
 Measurement:Use an electromyography (EMG) sensor placed on top of a muscle to measure the amount of electrical current in the muscle. You get frequency readings.
 Measure quality type:Weak (subjective) and maybe strong (objective).
 
 Character, Symbol, Or Word-finding Test
 
 Validation:To find out how quickly they can find information.
 Description:The participants were asked to locate a specific character in a text with a color pen. The specific character was shown at the bottom of the sheet for easy referral. The response times were recorded. Find out more about this method in the academic Brian Sze-Hang Kwok’s paper “Legibility of medicine labels.”
 Measurement:A numerical score of correct and incorrect.
 Measure quality type:Strong (objective).
 
 Searching a Phrase Test
 
 Validation:To find out how quickly readers can find information.
 Description:The participants were required to locate a phrase in the context of a medicine label. The specific phrase was shown at the bottom of the sheet for easy referral. The response times were recorded. Find more about this method in the academic Brian Sze-Hang Kwok’s paper “Legibility of medicine labels.”
 Measurement:A numerical score of correct and incorrect.
 Measure quality type:Strong (objective).
 
 Read-aloud
 
 Validation:To find out if there are any issues or obvious difficulties between different typefaces, or different classifications of typefaces, and maybe weights (like thin, extra bold, italic or condensed).
 Description:The subject producing a reader protocol is requested to read the text aloud and to immediately express any thoughts about the document. More about this method can be seen in the academics Leo Lentz’s and Henk Pander Maat’s paper “Reading aloud and the delay of feedback.”
 Measurement:Notes and recordings based on not good, okay, very good, and notes of specific problems.
 Measure quality type:Weak (subjective).
 
 Think-aloud
 
 Validation:To find out if there are any issues and if any issues can be highlighted or discovered.
 Description:Get people to perform certain specific tasks while using the document to vocalize the person’s thinking. From academics Leo Lentz’s and Henk Pandar Maat’s paper “Reading aloud and the delay of feedback.”
 Measurement:Notes and recording based on not good, okay, very good, and notes of specific problems.
 Measure quality type:Weak (subjective).
 
 At-a-glance
 
 Validation:To find out if they can correctly identify a word, letter, or symbol and not misread it as another word, letter, or symbol in a quick response environment.
 Description:Typefaces were individually sized to a height of 4 mm using the letter “H” as the reference. Participants viewed the monitor at a distance of approximately 70 cm. Participants’ distance to the screen was measured at the start of the session using a tape measure. Each individual trial followed the same sequence of presentation: a large fixation rectangle signifying the start of the new trial (400 ms), a masking stimulus composed of non-letter characters (200 ms), the stimulus of interest (variable timing, according to staircase rules as described above), a second masking stimulus of non-letter characters (200 ms), and then a response prompt (up to 5000ms). You can see more about this method in the paper “The great typography bake-off: comparing legibility at-a-glance,” by Ben Sawyer’s (academic), Jonathan Dobres (academic), Nadine Chahine (typeface designer), and academic Bryan Reimer’s.
 Measurement:A numerical score based on the number of characters or symbols. The time measure is also checked.
 Measure quality type:Strong (objective).
 
 Questionnaire
 
 Validation:People’s opinions, preferences, thoughts, concerns, views, likes, and dislikes.
 Description:Question asking, interviews, and real-time observations.
 Measurement:Notes and recordings.
 Measure quality type:Weak (subjective).
 
 The Radner Reading Charts
 
 Validation:Test the person’s vision accuracy and acuity with a typeface.
 Description:The Radner reading chart is a highly standardized multilingual reading test system. The result of the collaboration is a standardized, valid, and reliable reading test system available in numerous languages. The reading chart consists of sentence optotypes, which are optimized reading test items (standardized by construction), and statistical selection. Sentence optotypes consist of short sentences that are highly comparable in terms of the number of words (14 words), the word length, the position of words, the lexical difficulty, and the syntactic complexity. Language-specific characteristics were considered, as were the number of letters and syllables per word, line, and sentence. Get to know more from the legibility experts Sofie Beier and Kevin Larson’s paper “How does typeface familiarity affect reading performance and reader preference?”
 
 
 
 Measurement:Correct or incorrect response. Note the point of failure or incapability to proceed anymore.
 Measure quality type:Strong (objective).
 
 Legibility (Misrecognition)
 
 Validation:To find out if they can correctly identify a letter, number, word, or symbol and not misread it as another letter, word, or symbol.
 Description:As in my paper, “Letter and symbol misrecognition in highly legible typefaces for general, children, dyslexic, visually impaired and aging readers — 2019 fourth edition.”
 Measurement:Score for correct or incorrect identification. Time measure check also.
 Measure quality type:Strong (objective).
 
 Legibility (At a Very Small Typeface Size)
 
 Validation:To push a person’s eyesight to the maximum and see what happens at a very small size.
 Description:At small sizes, less than 8pt, for instance.
 Measurement:x-height size measurement preferred over pt size. Also, maybe a rating of difficulty and time to read like: easy, reasonable, and hard.
 Measure quality type:Strong (objective).
 
 Legibility (Distance)
 
 Validation:To see when a letter, symbol, or a word becomes unreadable and how far away it can be read or not recognized anymore.
 Description:In Robert Waller’s (information designer) article “Comparing typefaces for airport signs” he says that you could use a screen, physical sign, or printed paper to display a word, letter, or symbol. A person needs to stand far away and then get closer to the display until they can correctly identify the word, letter, or symbol. If a screen is being used, the person can also be at a fixed distance from the screen and then you can make the word, letter, or symbol bigger on the screen, until they can correctly identify it. This would give us a legibility score and distance measurement in relation to the correct identification. The first presented character was the letter “d.” As identified in Miles Tinker’s book Legibility of print, this character is one of the most easily recognizable letters. The purpose of this first exposure was to locate the individual vision threshold. The participant was placed at a distance of 10 meters from the screen and asked to move slowly forward until the presented letter was at the threshold of being identifiable. This was the distance at which the individual participant was tested — varying from 4.5–9 meters (with an average of 6 meters) from the screen. You can read more about this method in Sofie Beier (legibility expert) and Kevin Larson’s (legibility expert) paper “Design improvements for frequently misrecognised letters.”
 Measurement:Measurement in mm, cm or m.
 Measure quality type:Strong (objective).
 
 Legibility (Rotated Information)
 
 Validation:To push a typeface and person’s eyesight to the maximum and see what happens at these extreme angles. Also, these angles are common in VR (virtual reality) software and products.
 Description:At an angle: -45 degrees horizontally left and +45 degrees horizontally right, -45 degrees vertically up and +45 degrees vertically down.
 
 
 
 Measurement:Legibility test (character, symbol, word) test.
 Measure quality type:Strong (objective).
 
 Legibility (Degrading, Distortion, and Blurring)
 
 Validation:To push a typeface and person’s eyesight to the maximum and see what happens under these extreme conditions.
 Description:Legibility degrading test as in Ralf Hermann’s (director of Typography.Guru) paper “Designing the ultimate wayfinding typeface.”
 
 
 
 Measurement:The score for correct or incorrect identification. Time measure was also checked.
 Measure quality type:Strong (objective).
 
 Appeal (Typeface Fitting Subject and Content)
 
 Validation:How well does it fit and suit the content?
 Description:An appeal concerning the content, as an example, for content on gardening, when a slightly more organic, chiseled, and wavy typeface might communicate and fit the content better.
 Measurement:Score based not good, okay, very good.
 Measure quality type:Strong (objective).
 
 Appeal (User Feedback and Responses In Relation to Other Typefaces They May Know)
 
 Validation:How well does it fit and suit the content?
 Description:Appeal in relation to the typeface itself. What does the user say they like or dislike about this typeface in relation to other typefaces they use and know about? This method could produce interesting observations and data, albeit highly subjective.
 Measurement:Notes and recordings.
 Measure quality type:Weak (subjective).
 
 Fixation Duration
 
 Validation:How quick or lengthy does the eye have to fixate to understand the information?
 Description:Fixation duration is a period of time when the focus of the participant’s gaze is relatively still on an area and taking in information about that which is looked at, as in the academics Ivan Burmistrov, Tatiana Zlokazova, Iuliia Ishmuratova, and Maria Semenova’s paper “Legibility of light and ultra-light fonts: eyetracking study.”
 Measurement:Milliseconds (ms).
 Measure quality type:Weak (subjective) and maybe strong (objective).
 
 Saccadic Amplitude
 
 Validation:To find out what behavior, movements, or patterns are happening.
 Description:Saccadic amplitudes are a quick simultaneous movement of both eyes, like when you are reading a line of text. In this method, we monitor what happens with saccadic eye movements when reading, as in the academics Ivan Burmistrov, Tatiana Zlokazova, Iuliia Ishmuratova, and Maria Semenova’s paper “Legibility of light and ultra-light fonts: eyetracking study.”
 Measurement:Degrees (°).
 Measure quality type:Weak (subjective) and maybe strong (objective).
 
 Aspects Specific to The Users’ Environment and Situation
 Light
 
 Validation:To see how a typeface performs in lighting conditions and see how people respond in the lighting conditions.
 Description:Low light or good light condition and see how it affects the information and performance.
 Measurement:The score for correct or incorrect identification. Time measure is also checked. The light strength is measured in lumens (lm).
 Measure quality type:Strong (objective).
 
 Stress
 
 Validation:To find out how typefaces would work better or worse, under stress and high-pressure situations, with quick stressed eye movements.
 Description:Setup situations such as: booking a ticket and going through an airport, doing tasks after they have finished a 6-hour working day, or reading or doing tasks late at night — when it is more likely that they will be more tired.
 Measurement:Accuracy and efficiency of users’ actions. Maybe blood pressure or heart rate testing.
 Measure quality type:Weak (subjective).
 
 OpenType variable fonts and real-time responsive features
 
 Validation:Whereas a single non-variable font does not essentially change or alter itself, variable fonts can, and can be given settings to instruct them to work in different technological environments.
 
 Notes and Description:Variable fonts have the ability to alter and modify themselves in real-time to different screen sizes, media query responses, user customisations and environments. Variable fonts can utilise different aspects such as: weights, width, italic, slant, optical size and grade, to change and adapt, to help them work better in different and changing situations.  
 
 Measurement:Assessment (legibility, readability, comprehension, user preference) in relation to the changing circumstances. [Many measurement types in this area, too many to mention, but see other areas in this paper for aspects applyable.]
 
 Measure quality type:Weak (subjective) and maybe strong (objective).
 
 
 Time Pressure
 
 Validation:To find out how typefaces would work better (or worse) under time pressure and quick stressed eye movements.
 Description:Setup situations like finding information within a certain timeframe, booking a taxi very quickly, or finding something in a telephone directory.
 Measurement:Time measuring. Maybe a blood pressure test.
 Measure quality type:Weak (subjective).
 
 In Diverse Situations (Driving In a Car At Distance On a Road Sign Or Airport Sign)
 
 Validation:To push a person’s vision (length of view) and agility and see how typefaces respond.
 Description:To test the extremes of people’s vision and ability, like driving in a car and reading a road sign where distance and orientation are factors. How does weather affect information and communication?
 Measurement:In meters, centimeters, or millimeters.
 Measure quality type:Strong (objective).
 
 Aspects Specific to Technology for Extended Reading Typefaces
 Range of Weights
 
 Validation:It is always appreciated and helpful to use a typeface with a range of weights.
 Notes and description of measuring type:Range of weights offered.
 Measurement:Numerical score in the amount of weights.
 Measure quality type:Strong (objective).
 
 On-screen Rendering and Hinting
 
 Validation:Bad hinting and screen rendering leads to hard-to-read on-screen typography and illegibility.
 Description:Analyze by taking a screengrab (then zooming-in), or by using a zoom-in device (magnifying glass), then analyzing the hinting.
 Measurement:A score based on not good, okay, and very good. Also, use 3 different types of screens (low-resolution, HD and 4k+).
 Measure quality type:Strong (objective).
 
 Font File Size
 
 Validation:Larger font sizes can take up more bandwidth, especially across larger websites, and be slower to load initially in a webpage’s first content paint.
 Description:Look at the file size of the font.
 Measurement:The file size (kb) would give a score, although this measure is certainly not very useful, as there is no escape from a typeface with a large symbol and language support, which cannot really be made any smaller in file size.
 Measure quality type:Strong (objective).
 
 OpenType Features/Variable?
 
 Validation:If a typeface has more desirable features (such as small caps, different number styles, ligatures, and so on), it makes the typography better and typographically more usable.
 Description:A typeface is better if it has the features required by users and information.
 Measurement:Numerical score or tick list of features.
 Measure quality type:Strong (objective).
 
 Specific Typographic Design Variables Affecting Performance
 Typographic Design
 
 Validation:They affect typeface and typographic communication.
 Description:Tracking, leading, kerning, typeface weight, line length, word spacing, condensed weight, typeface size, typeface color, OpenType features.
 Measurement:Various possible (must be controlled and precise, as mentioned in Ralf Hermann (director of Typography.Guru) paper “What makes letters legible?”).
 Measure quality type:Weak (subjective) hard to measure accurately.
 
 What Would Typeface Performance Measurements, Results, and Scores Potentially Look Like?
 A data table, infographic, or some kind of graph could be used?
 Scientists And Designers Needing to Work Better Together
 Sofie Beier (legibility expert) in her paper “Letterform research: An academic orphan” touches upon the different issues and constraints designers and academics have faced in the past:
 “To produce findings that are relevant for the practicing designer, scientists benefit from consulting designers in the development of the experiments. While designers can contribute with design skills, they cannot always contribute with scientific rigor. Hence, researchers will profit from adopting a methodological approach that ensures both control of critical typographical variables and scientific validation. An interdisciplinary collaboration where scientists provide valid test methods and analysis and designers identify relevant research questions and develop test materials, will enable a project to reach more informed findings than what the two fields would be able to produce in isolation.”— Sofie Beier in Letterform Research: An Academic Orphan
 
 
 To recap, designers have tended to produce information lacking scientific rigor in the past. In contrast, scientists produce information that is hard to understand — with equations and lacks practical application. So both sides, whichever you are on, have their weaknesses and lack expertise.
 Am I Making Typeface Designers’ Job Harder?
 It is not my aim to make a typeface designer’s job any harder. It is commonly known that any typeface takes at least one year of hard work. The typeface designer Martin Majoor states that he designed the Questa typeface at irregular intervals over the course of 7 years. I have nothing but respect for typeface designers and the amazingly hard job they do. In fact, I have so much respect for the time and difficulty of designing a typeface, that I refuse even to try to attempt the task.
 What Now?
 
 Research into what is legible and what characteristics make letters and symbols more legible, go to the library and research online. For example, the academic journal Visible Language has all journals available for free on their website. There is some incredible research and work done, that was done more than 50 years ago; 
 Speak with people and speak with other typeface designers; 
 Avoid designing and releasing typefaces done, expressed, and designed on your own;
 Test typefaces, try to do the test accurately and try to compare what you are designing with another typeface, to see where there are weaknesses and strengths in testing results with people and in different contexts and environments. How is it working (or not working) better than another typeface and in different contexts and environments?
 Test typefaces with different categories of people in different contexts and environments;
 Make your findings, design intentions, and tactical fixes available for free, as part of the typeface release, as a publication, or as some kind of central public list (like on GitHub), so we can start better and get to where we need to be quicker;
 Maybe a completely new typeface might not be as good of an idea as you think. Maybe extension, improvement, or modification to an existing typeface might be smarter. New is not automatically better.
 
 Conclusions
 Do we, as users and designers, really need to assess typefaces and find out how they perform? Is it necessary? Well, whatever your thoughts are, in 2022, with a mass of typefaces available and 100s of years of designing and manufacturing typefaces, it is time to consider this topic. I think the time has come, and we are there. This is especially true for highly legible typefaces, some kind of measure or measures — even if new typefaces got released and they only had one performance measure (or say three), that would be a start.  
 We may also need some cross-measurable tests that are used, so everyone tests against the same (or as near as) thing. Because, as previously mentioned, if someone tests their typeface against a sans serif (like Arial), then another person tests their typeface using another typeface (like Helvetica), the data will not be cross-comparable. And furthermore, what typographic design and typesetting values the two people use, would most definitely be different, but actually, it would be highly desirable and more accurate if they were the same.
 I hope I have made your life more difficult and more confusing! Maybe I have asked a lot of pointless questions? Furthermore, in theory, just because a typeface does not score well (or score just as well as another typeface) does not necessarily mean it is an ineffective or bad typeface. It just means that, in theory, it may not score as well as another typeface in the same context. Nor does it mean that it would not perform well in reality. What did I say? I told you this was a difficult area!
 To confuse things even more, legibility expert Kevin Larson, academic Richard Hazlett, usability expert Barbara Chaparro and academic Rosalind Picard in their paper “Measuring the aesthetics of reading,” found that, when the typeface was set with no OpenType features in a normal body text paragraph (as typically found in a book), the users could read faster and understood more of the text. In other words, there were no ligatures, no small caps, no old-style figures, no real fractions, and no real superscripts and subscripts. I am not saying you should start typographically undesigning, disregarding years of best practice knowledge, but it goes to show that few things are certain in graphic communication.  
 Well whatever your thoughts are, in 2022 with a mass of typefaces available and 100s of years of designing and manufacturing typefaces, it is time to consider this topic, I think the time has come and we are there. This is especially true for highly legible typefaces, some kind of measure or measures, even if new typefaces got released and they only had one performance measure, or say three, this would be a start.
 Acknowledgements
 Alma Hoffmann (editing and feedback), Kevin Larson (feedback), Karel van der Waarde (extensive comments and feedback), and Erik Spiekermann (feedback).
 Further Reading on Smashing Magazine
 
 “Micro-Typography: How To Space And Kern Punctuation Marks And Other Symbols,” Thomas Bohm
 “A Reference Guide For Typography In Mobile Web Design,” Suzanne Scacca
 “7 Gorgeous Free And Open-Source Typefaces And When To Use Them,” Noemi Stauffer
 “Exo 2.0, A Contemporary Geometric Sans Serif Font (Freebie),” Vitaly Friedman
 </content>
     </entry>
     <entry>
       <title>WDRL — Edition 300: Threehundred and the web still evolves fast</title>
         <link href="https://wdrl.info/archive/300"/>
       <updated>2022-06-10T06:00:00.000Z</updated>
       <content type="text">Hey,
 
 this is number 300 of my article recommendation summaries. It’s crazy to see where this project is now compared to when I started: I always read a lot of resources and missed some sort of summary. Back then, I was blogging more often myself and so I just published my first summary on my blog. Quickly after, I realized that this makes more sense for many people to get it via email and I started using TinyLetter for my first 12 subscribers. It grew quickly to about 100 subscribers and then onwards to 500. At some point Vitaly Friedman from Smashing Magazine asked me whether I wanted to publish the summaries on their site as well and I agreed. This gave the project a huge boost and I had to switch to the self-hosted Mailcoach + Amazon SES sending service which served me well until this year. Today I send out the mails via Mailcoach and SES to 19k subscribers. I’d never thought this would reach so many people and am humbled by the feedback and reviews I get. Over time, my own focus changed a bit but writing this newsletter is still fun and I want to do this for many more editions. 
 One last thing: I rely fully on your financial support with this project so if you can give something back from time to time, I appreciate it a lot.
 I hope you enjoy my work on this and now will give you another few articles I found interesting to read the past weeks:
 News
 
 	It feels like there was a shift in the development of WebKit and Safari. The engine and browser are getting better fast. At WWDC22, Apple released even more cool features: Web inspector extensions, container queries, web push for macOS, subgrid, and more.
 
 Generic
 
 	Jack Ellis elaborates on the question whether Laravel or PHP can scale. Everyone who heard or worked with PHP surely heard how awful PHP is from other people. But then, why is most of the web powered by PHP, even some of the largest platforms like Facebook?     It’s a very powerful, professional language with a huge ecosystem and it can be used to serve fast, efficient, reliable and readable codebases. I’ve seen a couple of Python services, node.js services and others that were quite inefficient and I’ve myself built a lot of websites and even some platforms with PHP that were blazing fast while serving a lot of customers. It’s always about how you use a language and how you plan (architect) a platform.
 	I couldn’t agree more to Bramus here that dark mode toggles should be a feature of the browser, not by the website. It’s a lot of effort to build them right and then the user still has to find them on the web page and either needs a login for saving the setting or redo the setting from time to time.
 
 UI/UX
 
 	Oliver Schöndorfer shares why he loves the concept of a Font Matrix.
 	Vitaly Friedman on designing better language selectors for websites that let the user choose easily while anticipating the best for them.
 
 Tooling
 
 	When learning web development, you do not only have to learn about the development part itself. You also have to learn a lot about the tooling around it, for example, the command-line interface (CLI). I have seen a lot of new developers struggling using the CLI. Josh Comeau wrote a simple front-end developer&#x27;s guide to the terminal to help them get started.
 
 CSS
 
 	Adam Argyle shares what’s new and upcoming in CSS this year and it’s amazing to see how powerful the web is becoming. Alone color-contrast() is such an amazing feature that helps building better and more accessible products, but there are so many more cool things like subgrid, @layer, relative colors, accent-color, inert, :has()…
 	Ahmad Shadeed shares a learning resource on CSS Subgrid, a great addition to Grid that lets us inherit the parent’s grid settings to child elements.
 	We can use multi-colored fonts in browsers since 2018 already but here’s an explainer how to apply COLR   fonts nicely with CSS @font-palette-values.
 	New selectors in CSS can make our life much easier and code more readable. Here’s a step by step when to use :where(), :is() and :has() for what purposes.
 	This article made me think about the complexity of web development. How hard should it be to build a universal button for a web application? Adam Argyle uses the latest technologies like modern selectors that unify a lot but then this article is still complex, long and not easy to replicate. I think it should be easier than that to style button-like elements (something like a :button / :form-action universal selector?).
 	This is a short and nice example of CSS evolving over the years: A short code for a centered, flexible container where used to write media queries and hacky code before.
 	This article gives us another example of all the helpful CSS stuff which comes to web browsers very soon. With the new object-view-box property, we can define the view box of an image directly.
 
 Work &amp; Life
 
 	While there are plenty of open job offers for web developers, it is not always easy to find the right job for yourself. Leo Liou made the experience that big tech companies paid him a bigger salary, but he never found fulfillment there. It is a good read for everyone thinking about a job change. Although I think the article ends way to negative and definitive (“[…] because I learnt nothing there”) but it has some very valid points I can confirm from a couple of projects and companies as well. Choose well and if you’re not happy it’s time to change.
 	Sometimes we can achieve more by doing less. In this vein, Thomas Scott gives us five simple productivity tips.
 	I was invited by Wes Bos and Scott Tolinski to their syntax.fm podcast to share my story of becoming a part-time market gardener. This was much fun so if you’re into podcasts, enjoy this one hour work-life happiness discussion.
 
 
 If you like this newsletter, you can contribute here. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Am I on the IndieWeb yet?</title>
         <link href="https://css-tricks.com/am-i-on-the-indieweb-yet/"/>
       <updated>2022-06-09T15:32:50.000Z</updated>
       <content type="text">Can’t smash the Like button hard enough for what Miriam Suzanne has to say on the challenging technical hurdles of implementing Webmentions:
 
 
 
 The first round required several online services along with HTML &amp; JS changes to my static site, just to verify my indieweb identity. Then more changes to the site and more online services to help fetch any mentions (so far, nothing to see, but that’s probably expected). It seems the only way to test the setup is to launch all those changes publicly, and then ask for other devs to send you mentions.[…]I’m an experienced web developer, and I can figure it out. But the steps aren’t simple, and most of my friends are not web developers. So, to me, this all feels like the prototype of an idea – a proof of concept.
 
 
 
 A proof of concept for sure. And one that has been around for quite some time. The IndieWeb idea of owning your own data and using your website as a social hub is right up my alley — and likely yours, too, as someone working on the front end.
 
 
 
 
 
 
 
 Update! David Shanske is one of the developers of the WordPress plugins that support IndieWeb features and he not only published a wonderful explanation of how everything fits together, but also reached out directly and helped me wrap it all around my head.
 
 
 
 Yet, I’ve tinkered on and off with it — specifically Webmentions — over the past like three years with little to show for it. The problem isn’t so much the documentation of getting started because it’s all there. It’s more a combination of things…
 
 
 
 The wiki is confusing. Wikis are super cool in general, but the non-linear nature of it makes it tough to know where to start and where to end.The plugin ecosystem is complex. My personal site is on WordPress and there’s a plugin designed to make it easy to integrate IndieWeb features on it. Except that it’s really one plugin that steers you to install several others, each one introducing a technology that I honestly struggle to understand.There’s a bunch of terms to learn. I mean, “IndieWeb” and “Webmention” are already difficult to grok. Toss in things like “Micropub,” “Microformats,” “IndieAuth,” and “Semantic Linkbacks,” and suddenly it feels like a bunch of puzzle pieces from different puzzles trying to fit together.Some middleware seems necessary? For example, I had to give a service called Bridgy access to my Twitter to get that activity going. It apparently has something to do with Twitter’s shortened t.co URLs and making them play well with microformats.
 
 
 
 But, like Miriam, I struggled my way through it and got something working in the end. This is the sort of visual I wish I had when I was first getting started, and maybe it’ll help you too.
 
 
 
 
 
 
 
 Feels like a lot to get Webmentions going, but maybe that’s only because I have such a light grasp of the tech and how it all fits together. All the pieces are there, though, and even with the initial struggle, I love the IndieWeb concept, er prototype.
 To Shared Link — Permalink on CSS-Tricks
 Am I on the IndieWeb yet? originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>Collective #715</title>
         <link href="https://tympanus.net/codrops/collective/collective-715/"/>
       <updated>2022-06-09T11:07:22.000Z</updated>
       <content type="text">
 
 
  
 Inspirational Website of the Week: RRE
 This retro-modern design mix is an absolute hit. The scroll interactions and details make it a great web experience.
 Get inspired
 
 
 
 
 
         
 This content is sponsored via BuySellAds
 Northwestern’s Online MS in Information Design and Strategy
 Prepare for a range of dynamic communication roles and build the skills needed to lead communication strategy, translate complex data, and drive user interactions. Choose from specializations in content strategy, UX/UI, communication with data, and learning design.
         Find out more
 
 
 
 
 
  
 A beginner’s guide to CI/CD and automation on GitHub
 CI/CD and workflow automation are native capabilities on GitHub platform. Learn how to start using them and speed up your workflows in this article by Rizel Scarlett.
 Read it
 
 
 
 
 
  
 Hamburger Footer: Reaching the Bottom of Infinite Scroll
 Graeme Fulton shares some examples and techniques on how to make footers reachable on infinite scrolling sites.
 Check it out
 
 
 
 
 
  
 Meet Web Push
 WebKit now supports the W3C standards for Push API, Notifications API, and Service Workers to enable Web Push.
 Read it
 
 
 
 
 
  
 How to pick the least wrong colors
 Matthew Ström shares an algorithm for creating color palettes for data visualization.
 Check it out
 
 
 
 
 
  
 Plasmo Framework
 The Plasmo Framework is a battery-packed browser extension framework made by hackers for hackers.
 Check it out
 
 
 
 
 
  
 Rulex
 Rulex is a new, portable, regular expression language.
 Check it out
 
 
 
 
 
  
 GitNoter
 GitNoter is a web application that allows users to store notes in their git repository.
 Check it out
 
 
 
 
 
  
 Orbit Gallery
 Infinite orbit gallery made with THREE.js by Michal Zalobny, based on Luis Bizarro’s Awwwards course. Code can be found .
 Check it out
 
 
 
 
 
  
 Monorepos in JavaScript &amp; TypeScript
 A tutorial on how to use a monorepo architecture in frontend JavaScript and TypeScript with tools like npm/yarn/pnpm workspaces, Turborepo/NX/Lerna, Git Submodules and more.
 Read it
 
 
 
 
 
  
 Re-evaluating technology
 Jeremy Keith writes about the importance of revisiting past decisions. Especially when it comes to the web.
 Read it
 
 
 
 
 
  
 Interactive Typography Tutorial
 An interactive tutorial on typography, including how to pick great fonts, styling text, and typographical design patterns in UI design.
 Check it out
 
 
 
 
 
  
 Optical Size And Variable Fonts
 Robin Rendle gives some insight on optical sizing in variable fonts that helps determine how a font should look at certain sizes.
 Read it
 
 
 
 
 
  
 System Font Stack
 A quick reference for all the basic system font stacks.
 Check it out
 
 
 
 
 
  
 Google Fonts Pairings
 Sarah Daily shares some handpicked Google Fonts pairings that are ready to be used in Figma.
 Check it out
 
 
 
 
 
  
 A Short History of iOS App Icons
 Jim Nielsen shares a chapter he wrote for “The iOS App Icon Book”.
 Check it out
 
 
 
 
 
  
 Obscure CSS: Implicit List-Item Counter
 Learn about the built-in &#x60;list-item&#x60; counter for ordered lists in this article by Roman Komarov.
 Read it
 
 
 
 
 
  
 ffmpeg buddy
 A great tool to build commands if you use ffmpeg.
 Check it out
 
 
 
 
 
  
 GraphQL client in the terminal
 Build and execute GraphQL queries in the terminal. A project inspired by https://graphiql-online.com.
 Check it out
 
 
 
 
 
  
 On Creativity: My modest guide to being more creative
 Jeff Zych shares some advice on a solid creative process.
 Read it
 
 
 
 
 
  
 Inspiral Web
 The web version of the Inspiral app. Written in TypeScript, using D3.js by Nathan Friend.
 Check it out
 
 
 
 
 
  
 Redactle
 A daily puzzle game where you have to find the title of a random Wikipedia article by guessing words to reveal them on the page.
 Check it out
 
 
 
 
 
  
 From Our Blog
 How to Animate SVG Shapes on Scroll
 A short tutorial on how to animate SVG paths while smooth scrolling a web page using Lenis and GSAP’s ScrollTrigger plugin.
 Check it out
 
 
 The post Collective #715 appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>The Future Of Frontend Build Tools</title>
         <link href="https://smashingmagazine.com/2022/06/future-frontend-build-tools/"/>
       <updated>2022-06-09T09:00:00.000Z</updated>
       <content type="text">Frontend build tooling is crucial to the workflow of the modern frontend developer for a host of reasons classified under improved developer and user experiences. From the developer’s perspective, frontend tooling gives us: the ability to author modules, a dev server for local development, Hot Module Replacement (HMR) for a shorter feedback loop in development mode, the ability to target legacy browsers with polyfills, processing a host of filetypes apart from JavaScript, the list goes on. 
 As a result, users can enjoy more capable and feature-rich applications that remain performant through techniques like code-splitting, caching, prefetching, and other resource optimization techniques — with some applications that are even able to work offline.
 Frontend tooling gives us so much today that it is hard to imagine that there was a time when it was not even needed at all. A trip down memory lane could help us understand how we got here.
 The Past
 Before frontend applications became as complex as they are today, all JavaScript was used for was to add basic interactivity to otherwise simple HTML documents — similar to the way Adobe’s Flash was used. 
 There were no complex “JavaScript-heavy” applications, so, no need for any tooling to make authoring and shipping JavaScript better, but that would not remain the case. 
 As time went on and we started to create more involved user experiences on the web, we shifted from more static web pages to highly dynamic web applications serving user-specific data. These applications required more JavaScript than their traditional counterparts, and the limits of working with JavaScript became a lot more apparent.
 There are two major ways to load JavaScript in the browser. One is with a script tag referencing a JavaScript file, and the other is to write your JavaScript directly in the HTML inside script tags.
 &lt;script src&#x3D;&quot;my-javascript-file.js&quot;&gt;&lt;/script&gt;
 
 &lt;script&gt;
     var a &#x3D; 1;
     var b &#x3D; 2;
 
     var result &#x3D; a + b;
 &lt;/script&gt;
 
 
 This limitation on loading JavaScript becomes a bottleneck when you have lots of JavaScript to load. There are browser limitations to loading many JavaScript files concurrently and maintainability issues with having one huge JavaScript file (like file size, scoping issues, namespace collision, and so on). 
 We came up with solutions like Immediately Invoked Function Expressions (IIFEs) to help with encapsulation and some of the scoping issues after which, we gained the ability to write our JavaScript in many different files. Then, we needed a way for these many files to be combined into
 one file to be served in the browser
 The present
 Being able to split our JavaScript into different files with IIFEs, it seemed like all we needed was a way to concatenate these files and ship a single file to the browser. This need saw the rise of tools like Gulp, Grunt, Brocolli, and so forth. However, we soon realized that our thinking might have been a little too simplistic. 
 As our applications got even more complex, matters like lack of dead code elimination, full rebuilds for small changes, and other performance issues made us realize that we needed something more than just concatenation. That gave rise to the more modern bundlers like Webpack, Parcel, and others.
 With the pace of advancement in the frontend space not slowing down, we have started to observe gaps and issues with the modern build tools.  
 Some of the major limitations include:  
 
 Complex setup and configuration of some of these existing bundlers;
 Increase in build times as applications get larger;
 Suboptimal performance in development mode.
 
 The rate at which things change in the JavaScript ecosystem is often fatiguing, but the upside is that the community quickly identifies problems and gets to work on potential solutions. With our sights set on improving the performance of frontend tooling, a new generation of build tools is being developed.
 The Future
 The limitations of the mainstream build tools of the day have led to several attempts to reimagine what a frontend build tool should be and do, and there are quite a few new build tools in the wild today. 
 Closer inspection will reveal that these new tools seem to be taking two major approaches to solving the problem (not necessarily mutually exclusive): a change in paradigm and a change in platform — both powered by new advancements in the web development ecosystem. 
 A Replatform
 Frontend build tools have traditionally been built with JavaScript and, more recently, Typescript. This made sense, as JavaScript is the language of the web, and authoring build tools for the web in the same language makes it easier for more people to contribute to the effort and build a community around these tools. Still, there are inherent problems with this approach. 
 As a high-level language, JavaScript cannot reach native levels of performance. This means that tools built on top of this platform have a cap on how performant they can be. So, to break out of this limitation, newer build tools are being built on lower-level, inherently more performant languages like Rust. 
 Languages like Rust and Go have become popular options for authoring the next generation of build tools with a strong emphasis on performance. Rust, in particular, is popular not only for its performance but also for its impressive developer experience — voted the &quot;most-loved&quot; programming language six years in a row in the Stack Overflow Developer Survey. 
 In speaking about the decision to build Rome (the build tool and not the city) with Rust, Jamie Kyle says:
 “Many others have communicated the performance, memory, and safety benefits of Rust before us — let’s just say everyone who has ever said Rust is good is correct. However, our biggest concern was our own productivity.
 
 [...]
 After some prototyping, however, we quickly realized we might actually be more productive in Rust”— Jamie Kyle in Rome Will Be Written In Rust
 
 The project SWC is at the forefront of this idea of using Rust for frontend build tools. It is now powering projects like Next.js’s new compiler, Deno, Parcel, and others — with a performance that is many orders of magnitude above other existing build tools. 
 Projects like SWC prove that with a change of the underlying platform, the performance of build tools can be significantly improved.
 A paradigm shift
 The way a typical frontend build pipeline works today is you author JavaScript modules in many different files, run a command, the build tool picks up these modules, bundles them into a single module, converts them to a format understood by browsers, and serves that file in the browser.
 To improve the performance in development mode, a lot of the optimizations that would take more time to complete are left out and are, instead run when we are bundling our production application. This ensures that it takes as little time as possible to spin up a dev server, run our application in development mode and get productive.
 The bundling process still takes quite some time, though and as a project grows, build times (even in development) only get longer and longer. Wouldn’t it be great if we could somehow skip bundling altogether while still being able to write modules as usual and have the browser understand how to work with them? A new set of build tools is taking this approach, known as Unbundled Development.
 Unbundled development is great. It solves a major issue with existing build tools: they often need to rebuild entire sections of your application for even trivial code changes, and build times get longer as the application grows. We lose the rapid feedback — essential for a pleasant development experience. 
 One might wonder, if unbundled development is so great, why isn’t it the norm today? There are two major reasons why unbundled development is only starting to gain traction: browser compatibility for cutting edge features and processing node module imports.
 1. Browser compatibility for cutting edge features
 Unbundled Development is powered by ES Modules (ESM), which brings a standardized module system to JavaScript — supported natively across multiple runtimes, including the browser. With this new capability, we can mark our script tags as modules and can consequently use the import and export keywords that we are all familiar with;
 &lt;script type&#x3D;&quot;module&quot; src&#x3D;&quot;main.js&quot;&gt;&lt;/script&gt;
 
 &lt;script type&#x3D;&quot;module&quot;&gt;
   /** JavaScript module code goes here */
 &lt;/script&gt;
 
 
 ES modules have been around for quite some time. Still, we are only able to start taking advantage of it for things like unbundled development, mostly because of how long its standardization took across all the players in the web ecosystem. 
 In her article about ES Modules, on Mozilla Hacks, Lin Clark says:
 “ES modules bring an official, standardized module system to JavaScript. It took a while to get here, though — nearly 10 years of standardization work.”— Lin Clark in ES Modules: A Cartoon Deep-Dive
 
 The issue of browser support or lack thereof has plagued frontend development for a long time. This is why we have vendor prefixing our CSS, sometimes the reason for polyfills, why we spend time ensuring cross-platform support for our web applications, and why it sometimes takes quite some time before we can take advantage of the latest and greatest web features in our day to day work. 
 Try to visit a StackBlitz project using Safari, and you will be greeted with the following screen communicating the lack of support for WebContainers in non-Chromium-based browsers.
 
 The pace of feature adoption is not the same across browser providers, and there are often variations in how different vendors implement certain features. However, the future looks bright with initiatives like Interop 2022.
 2. Processing node module imports
 Most, if not all, frontend applications we write today depend on external libraries from NPM. For a typical react application, We would import react at the top of our component files like so:
 import React from &#x27;react&#x27;
 
 /** The rest of your component code */
 
 
 Trying to load this directly in the browser, as we would need to do for unbundled development, will lead to two issues. First, the browser does not know how to resolve the path to find react and secondly, the react library is published as a Common JS (CJS) module — which cannot run natively in the browser without some pre-processing.
 The latter is the bigger issue here, as it is possible, and even trivial, to simply replace our node module imports with relative paths to specific files. Still, the fact that most NPM packages are written in a module format more suitable for Node JS than the browser requires that our NPM dependencies are treated specially to facilitate unbundled development. 
 Snowpack, in particular, handles this by processing application dependencies into separate Javascript files that can then be used directly in the browser. More on how Snowpack does this can be found here.
 With ES Modules now being mainstream in most modern browsers, and clever workarounds for NPM dependencies, build tools like Vite and Snowpack can offer the option of unbundled development with drastically improved performance, snappy builds, besides super fast HMR.
 Final Thoughts
 Frontend development has come a long way, and our needs are constantly evolving and increasing in complexity. Build tools are an essential part of how we build frontend applications, and existing tools are falling short of the mark sparking the development of new tools that reimagine how build tools should be designed.
 With a huge focus on performance, ease of use, and less complex configuration, the next generation of build tools are poised to power ambitious frontend applications for some time to come.
 Are you excited about the recent developments in this space? Let me know in the comments what you think about the upcoming innovations and the current landscape.
 Further Reading on Smashing Magazine
 
 “How To Maintain A Large Next.js Application,” Nirmalya Ghosh 
 “Breaking Down Bulky Builds With Netlify And Next.js,” Átila Fassina
 “Getting Started With Webpack,” Nwani Victory
 “What’s That (Dev) Tool?,” Patrick Brosset
 </content>
     </entry>
     <entry>
       <title>Let’s Make a QR Code Generator With a Serverless Function!</title>
         <link href="https://css-tricks.com/lets-make-a-qr-code-generator-with-a-serverless-function/"/>
       <updated>2022-06-08T14:30:53.000Z</updated>
       <content type="text">QR codes are funny, right? We love them, then hate them, then love them again. Anyways, they’ve lately been popping up again and it got me thinking about how they’re made. There are like a gazillion QR code generators out there, but say it’s something you need to do on your own website. This package can do that. But it’s also weighs in at a hefty 180 KB for everything it needs to generate stuff. You wouldn’t want to serve all that along with the rest of your scripts.
 
 
 
 Now, I’m relatively new to the concept of cloud functions, but I hear that’s the bee’s knees for something just like this. That way, the function lives somewhere on a server that can be called when it’s needed. Sorta like a little API to run the function.
 
 
 
 Some hosts offer some sort of cloud function feature. DigitalOcean happens to be one of them! And, like Droplets, functions are pretty easy to deploy.
 
 
 
 
 
 
 
 Create a functions folder locally
 
 
 
 DigitalOcean has a CLI that with a command that’ll scaffold things for us, so cd wherever you want to set things up and run:
 
 
 
 doctl serverless init --language js qr-generator
 
 
 
 Notice the language is explicitly declared. DigitalOcean functions also support PHP and Python.
 
 
 
 We get a nice clean project called qr-generator with a /packages folder that holds all the project’s functions. There’s a sample function in there, but we can overlook it for now and create a qr folder right next to it:
 
 
 
 
 
 
 
 That folder is where both the qrcode package and our qr.js function are going to live. So, let’s cd into packages/sample/qr and install the package:
 
 
 
 npm install --save qrcode
 
 
 
 Now we can write the function in a new qr.js file:
 
 
 
 const qrcode &#x3D; require(&#x27;qrcode&#x27;)
 
 exports.main &#x3D; (args) &#x3D;&gt; {
   return qrcode.toDataURL(args.text).then(res &#x3D;&gt; ({
     headers:  { &#x27;content-type&#x27;: &#x27;text/html; charset&#x3D;UTF-8&#x27; },
     body: args.img &#x3D;&#x3D; undefined ? res : &#x60;&lt;img src&#x3D;&quot;${res}&quot;&gt;&#x60;
   }))
 }
 
 if (process.env.TEST) exports.main({text:&quot;hello&quot;}).then(console.log)
 
 
 
 All that’s doing is requiring the the qrcode package and exporting a function that basically generates an &lt;img&gt; tag with the a base64 PNG for the source. We can even test it out in the terminal:
 
 
 
 doctl serverless functions invoke sample/qr -p &quot;text:css-tricks.com&quot;
 
 
 
 Check the config file
 
 
 
 There is one extra step we need here. When the project was scaffolded, we got this little project.yml file and it configures the function with some information about it. This is what’s in there by default:
 
 
 
 targetNamespace: &#x27;&#x27;
 parameters: {}
 packages:
   - name: sample
     environment: {}
     parameters: {}
     annotations: {}
     actions:
       - name: hello
         binary: false
         main: &#x27;&#x27;
         runtime: &#x27;nodejs:default&#x27;
         web: true
         parameters: {}
         environment: {}
         annotations: {}
         limits: {}
 
 
 
 See those highlighted lines? The packages: name property is where in the packages folder the function lives, which is a folder called sample in this case. The actions/ name property is the name of the function itself, which is the name of the file. It’s hello by default when we spin up the project, but we named ours qr.js, so we oughta change that line from hello to qr before moving on.
 
 
 
 Deploy the function
 
 
 
 We can do it straight from the command line! First, we connect to the DigitalOcean sandbox environment so we have a live URL for testing:
 
 
 
 ## You will need an DO API key handy
 doctl sandbox connect
 
 
 
 Now we can deploy the function:
 
 
 
 doctl sandbox deploy qr-generator
 
 
 
 Once deployed, we can access the function at a URL. What’s the URL? There’s a command for that:
 
 
 
 doctl sbx fn get sample/qr --url
 https://faas-nyc1-2ef2e6cc.doserverless.co/api/v1/web/fn-10a937cb-1f12-427b-aadd-f43d0b08d64a/sample/qr
 
 
 
 Heck yeah! No more need to ship that entire package with the rest of the scripts! We can hit that URL and generate the QR code from there.
 
 
 
 Demo
 
 
 
 We fetch the API and that’s really all there is to it!
 
 
 
 CodePen Embed Fallback
 
 Let’s Make a QR Code Generator With a Serverless Function! originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>How to Animate SVG Shapes on Scroll</title>
         <link href="https://tympanus.net/codrops/2022/06/08/how-to-animate-svg-shapes-on-scroll/"/>
       <updated>2022-06-08T11:25:49.000Z</updated>
       <content type="text">Scrolling can be so much fun! Let’s have a look at how to make SVG shapes and clip-paths animate on scroll to add a bit of drama and waviness to a design. We can morph one path into another and we’ll do that once a shape enters the viewport. 
 
 
 
 Let’s get started!
 
 
 
 Path Animation on Separators
 
 
 
 Animating a path on scroll can be particularly interesting for separators and borders to full screen images. So, let’s have a look at this first example, where we simply animate the path of an SVG that has the same fill color as the background of the page:
 
 
 
 As we scroll, we’ll animate an SVG path from a rectangle to a wave shape.
 
 
 
 For this to work, we need two paths: one initial path that is a rectangle and a final path that is the wavy shape. When we create paths that will be animated, we have to keep in mind that all points present in the final path, also need to be there in the initial shape. So the best way to make sure our path animation doesn’t turn out funky, is to start crafting the most complex shape which in our case is the final path with the curve. 
 
 
 
 Creating the SVG paths
 
 
 
 Unfortunately, graphic design softwares might not be the best choice for making proper, optimized paths. I usually start making the shapes in Sketch and then I optimize them using SVGOMG. Then, I copy the path and paste it into the SvgPathEditor. The optimization step is not always needed as the path editor offers rounding, which is great. I use it for paths or groups that had transforms applied to them by Sketch. SVGOMG can remove those.
 
 
 
 SVGOMG cleans up SVGs and removes transforms
 
 
 
 Once we have our optimized SVG with a “clean” path, we can use the editor to create the initial shape out of the more complex one:
 
 
 
 Although this shape is not really visible, we need all the points of the final path to be present in the initial one.
 
 
 
 When doing this, it’s as well a great way to roughly visualize how the animation will look and feel like (in reverse, of course). Once we have both paths, we can use them in our HTML.
 
 
 
 Markup and Styling
 
 
 
 &lt;svg class&#x3D;&quot;separator separator--up&quot; width&#x3D;&quot;100%&quot; height&#x3D;&quot;100%&quot; viewBox&#x3D;&quot;0 0 100 10&quot; preserveAspectRatio&#x3D;&quot;none&quot;&gt;
 	&lt;path 
 		class&#x3D;&quot;separator__path path-anim&quot; 
 		d&#x3D;&quot;M 0 0 C 40 0 60 0 100 0 L 0 0 Z&quot; 
 		data-path-to&#x3D;&quot;M 0 0 C 40 10 60 10 100 0 L 0 0 Z&quot; 
 		vector-effect&#x3D;&quot;non-scaling-stroke&quot; 
 	/&gt;
 &lt;/svg&gt;
 
 
 
 Using a data-attribute, we define the final path that we want the initial one to animate to. A bit of CSS will make sure that our SVG is placed in full width, at the top of the large background image:
 
 
 
 .separator {
 	display: block;
 	position: absolute;
 	z-index: 1000;
 	pointer-events: none;
 	width: 100%;
 	height: 150px;
 	fill: var(--color-bg);
 }
 
 
 
 Note that you can stretch your SVG to your desired height, depending on how dramatic you want the wave to look when scrolling.
 
 
 
 The JavaScript
 
 
 
 For the smooth scrolling, we’ll use the new Lenis library by Studio Freight. GSAP’s ScrollTrigger plugin will allow us to animate an element when it enters or exits the viewport. 
 
 
 
 Let’s import the scripts that we need:
 
 
 
 import Lenis from &#x27;@studio-freight/lenis&#x27;
 import { gsap } from &#x27;gsap&#x27;;
 import { preloader } from &#x27;./preloader&#x27;;
 import { ScrollTrigger } from &#x27;gsap/ScrollTrigger&#x27;;
 gsap.registerPlugin(ScrollTrigger);
 
 
 
 Let’s preload the images:
 
 
 
 preloader();
 
 
 
 Now we need all the path elements that we want animated (they have the class “path-anim”): 
 
 
 
 const paths &#x3D; [...document.querySelectorAll(&#x27;path.path-anim&#x27;)];
 
 
 
 Next, we initialize smooth scrolling:
 
 
 
 const lenis &#x3D; new Lenis({
     lerp: 0.1,
     smooth: true,
 });
 const scrollFn &#x3D; () &#x3D;&gt; {
     lenis.raf();
     requestAnimationFrame(scrollFn);
 };
 requestAnimationFrame(scrollFn);
 
 
 
 And finally, we animate our paths when they enter the viewport. The final path is defined in the data-attribute “data-path-to” in the path element, as we have seen previously. 
 
 
 
 The start and end is defined by the SVG’s element top reaching the bottom of the viewport, and its bottom reaching the top of the viewport:
 
 
 
 paths.forEach(el &#x3D;&gt; {
     const svgEl &#x3D; el.closest(&#x27;svg&#x27;);
     const pathTo &#x3D; el.dataset.pathTo;
 
     gsap.timeline({
         scrollTrigger: {
             trigger: svgEl,
             start: &quot;top bottom&quot;,
             end: &quot;bottom top&quot;,
             scrub: true
         }
     })
     .to(el, {
         ease: &#x27;none&#x27;,
         attr: { d: pathTo }
     });
 });
 
 
 
 Now, we have our morphing SVG paths while scrolling the page!
 
 
 
 While we can use this animation technique on paths and “separators” that simply cover some big background image, we can also animate clip-paths on images, like in this example:
 
 
 
 &lt;svg class&#x3D;&quot;image-clip&quot; width&#x3D;&quot;500px&quot; height&#x3D;&quot;750px&quot; viewBox&#x3D;&quot;0 0 500 750&quot;&gt;
 	&lt;defs&gt;
 		&lt;clipPath id&#x3D;&quot;shape1&quot;&gt;
 			&lt;path 
 				class&#x3D;&quot;path-anim&quot; 
 				d&#x3D;&quot;M 0 0 L 500 0 C 500 599.6 500 677.1 500 750 L 0 750 C 0 205 0 105 0 0 Z&quot;
 				data-path-to&#x3D;&quot;M 0 0 L 500 0 C 331 608 485 551 500 750 L 0 750 C 120 281 7 296 0 0 Z&quot; 
 			/&gt;
 		&lt;/clipPath&gt;
 	&lt;/defs&gt;
 	&lt;image 
 		clip-path&#x3D;&quot;url(#shape1)&quot; 
 		xlink:href&#x3D;&quot;img/2.jpg&quot; 
 		x&#x3D;&quot;0&quot; y&#x3D;&quot;0&quot; 
 		width&#x3D;&quot;500&quot; 
 		height&#x3D;&quot;750&quot;
 	/&gt;
 &lt;/svg&gt;
 
 
 
 We can animate clip-paths on SVG images
 
 
 
 Final Result
 
 
 
 Combining smooth scrolling with SVG path animations can add an extra level of morph-coolness to a design. It’s not very complicated but the result can look very dramatic. This technique gives an organic touch to a scroll experience and might spare the need to use WebGL for certain simple distortion effects.
 
 
 
 
 
 
 
 I really hope you enjoyed this little tutorial and find it useful for creating your own animations!
 The post How to Animate SVG Shapes on Scroll appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>Simplify Your Color Palette With CSS Color-Mix()</title>
         <link href="https://smashingmagazine.com/2022/06/simplify-color-palette-css-color-mix/"/>
       <updated>2022-06-08T09:00:00.000Z</updated>
       <content type="text">There’s a reason for all the new, experimental color features CSS is introducing. And there’s a reason for all the excitement they’re stirring up.
 Colors are hard. Defining a base color palette can be time-consuming and involve quite a few stakeholders. And that’s not even considering contextual colors, like hover, active and inactive states. Defining these values requires even more time and more attention to accessibility. This can result in a bloated palette and an even more bloated set of design tokens.
 It can be a lot to juggle. 🤹 
 While the CSS color-mix() function may only blend two colors together, could it be used to simplify color palettes and streamline contextual values across themes?
 The CSS Color-Mix() Function
 The CSS color-mix() function is an experimental feature that is currently a part of the Color Module 5. True to its name, the function will accept any two colors, mix them together and return a little color Frankenstein.
 
 For the sake of this article, let’s define how these arguments will be called while using this example.
 
 Color Space would refer to HSL;
 Base Color would refer to red;
 Base Percent would refer to 50%;
 Blend Color would refer to white;
 Blend Percent, not shown in this example, will refer to a value covered later.
 
 There are quite a few moving pieces here, so let’s have a quick interactive visual to simulate the base color, base percent, and blend color.
 
 Building the linear color wheel was a lot of fun and a great dive into using color-mix(). It often helps when experimenting with a new feature to already know what the visual outcome should be. 
 So how does this work?
 First: Define the base primary colors.
 --primary-1: #ff0;
 --primary-2: #f00;
 --primary-3: #00f;
 
 
 Next: Mix the primary colors to create the secondary colors.
 --secondary-1: color-mix(in srgb, var(--primary-1) 50%, var(--primary-2));
 --secondary-2: color-mix(in srgb, var(--primary-2) 50%, var(--primary-3));
 --secondary-3: color-mix(in srgb, var(--primary-3) 50%, var(--primary-1));
 
 
 Last: Mix the primary and secondary colors to create the tertiary colors.
 --tertiary-1: color-mix(in srgb, var(--primary-1) 50%, var(--secondary-1));
 --tertiary-2: color-mix(in srgb, var(--secondary-1) 50%, var(--primary-2));
 --tertiary-3: color-mix(in srgb, var(--primary-2) 50%, var(--secondary-2));
 --tertiary-4: color-mix(in srgb, var(--secondary-2) 50%, var(--primary-3));
 --tertiary-5: color-mix(in srgb, var(--primary-3) 50%, var(--secondary-3));
 --tertiary-6: color-mix(in srgb, var(--secondary-3) 50%, var(--primary-1));
 
 
 Of course, when I was in art class, there was only one set of paints. So if you wanted yellow, there was only one yellow. Red? There was only one red. Blue? Well, you get the idea. 
 But the web and CSS offer a much wider selection of colors in the way of ‘color spaces.’ Some of these color spaces may already be familiar, but there were quite a few I hadn’t used before, including four new CSS color features which are gradually gaining support.
 Color spaces can calculate their colors differently from one another. Newer color spaces provide wider palettes with more vivid shades to maximize the latest screen technologies — like ultra-high-definition retina displays. It means that a single color may appear differently across each color space.
 Knowing the CSS color-mix() function supports using different color spaces, let’s experiment with color spaces by replacing the use of srgb from the previous example with a custom property to see how the color wheel changes.
 
 While the W3 docs explain the calculations behind this functionality quite well, the math is a tad beyond my abilities to explain clearly — this is art class, after all. But, as best as I can put it:
 --math-bg: color-mix(in srgb, red 20%, white 60%);
 
 
 In this example, the base percentage is 20 while the blend percent is 60 creating a total of 80. This gives us, what’s called, an alpha multiplier of 0.8 where 1 &#x3D; 100 and 0.8 &#x3D; 80%.
 To fill in the gaps, the function will multiply the base and blend percentages by this alpha multiplier to scale them up to 100% while remaining relative to their original weights.
 20% * 100/80 &#x3D; 25%
 60% * 100/80 &#x3D; 75%
 
 --math-bg: color-mix(in srgb, red 25%, white 75%);
 
 
 If the base and blend percentages total more than 100, the inverse of this approach would be taken to round down to 100. Again, the math behind the scaling of these values, along with the general mixing calculations, is beyond my depth. For those interested in digging deeper into the technicalities of color-mix(), I would point to the W3 docs.
 However, that mathematical understanding isn’t required for the below demo, where both the base and blend percentages can be adjusted to view the result.
 
 Because the --background-color property is technically defined, the fallback won’t trigger.
 However, that’s not to say color-mix() can’t be used progressively, though. It can be paired with the @supports() function, but be mindful if you decide to do so. As exciting as it may be, with such limited support and potential for syntax and/or functionality changes, it may be best to hold off on mixing this little gem into an entire codebase.
 @supports (background: color-mix(in srgb, red 50%, blue)) {
   --background-color: color-mix(in srgb, red 50%, blue);
 }
 
 
 CurrentColor Is Not Supported
 A powerful little piece of CSS is being able to use currentColor as a value, keeping styles relative to their element. Unfortunately, this relative variable cannot be used with color-mix().
 button {
   background: color-mix(in srgb, currentColor 50%, white);
 }
 
 
 The hope was to have ever greater control over relative colors, but unfortunately, using currentColor in this way will not work. While color-mix() can’t achieve relative colors to this degree, the new relative color syntax is also coming to CSS. Read about CSS relative color syntax with Stefan Judis.
 Wrapping Up
 While color-mix() may not be as powerful as something like color-contrast(), there is definitely a place for it in a CSS tool belt — or kitchen cabinet. Wherever.
 The use cases for contextual colors are intriguing, while the integration into design systems and themes (to potentially simplify color palettes while retaining great flexibility) is where I want the most to experiment with in the feature. However, those experiments are likely still a ways off due to the current browser support. 
 Personally, combining color-mix() with color-contrast() is an area that seems particularly exciting, but without proper browser support, it will still be difficult to fully explore.
 Where would you first implement color-mix()? 🤔 
 Maybe it could be used as a mixin to roughly replicate the lighten() and darken() SCSS functions. Could there be greater potential in the realm of user-generated themes? Or even web-based graphic editors and tools? Maybe it could be used as a simple color format converter based on device capabilities.
 Nevertheless, CSS is providing the web with plenty of new and exciting ingredients. It’s only a matter of time before we start mixing up some incredible recipes.
 Further Reading On Smashing Magazine
 
 “Manage Accessible Design System Themes With CSS Color-Contrast(),” Daniel Yuschick 
 “A Recipe For A Good Design System,” Átila Fassina
 “A Guide To Modern CSS Colors With RGB, HSL, HWB, LAB And LCH,” Michelle Barker
 “Color Tools And Resources,” Cosima Mielke
 </content>
     </entry>
     <entry>
       <title>GitHub Copilot explaining code and translating it from one language to another</title>
         <link href="https://christianheilmann.com/2022/06/07/github-copilot-explaining-code-and-translating-it-from-one-language-to-another/"/>
       <updated>2022-06-07T20:57:10.000Z</updated>
       <content type="text">GitHub Copilot is already the dog’s bollocks, but I just installed the GitHub Labs extension and my jaw is still open.
 
 	Code explanation
 
 	This one allows you to highlight a piece of code and get explanations of what it does:
 
 	
 
 	For example:
 
 	
 async function downloadZipFile(downloadUrl) {
 await fs.remove(‘out/edge’);
 
 	const file &#x3D; fs.createWriteStream(‘edge.zip’);
 https.get(downloadUrl, function(response) {
 response.pipe(file);
 response.on(‘end’, async ()&#x3D;&gt;{
 await fs.createReadStream(‘edge.zip’).pipe(unzipper.Extract({path: ‘out/edge/’}));
 await fs.unlink(‘edge.zip’, () &#x3D;&gt; {} );
 const outdir &#x3D; path.join(__dirname, ‘out’, ‘edge’);
 console.log(&#x60;Edge files extracted to: ${outdir}&#x60;);
 });
 });
 }
 	
 
 	Turns into:
 
 	
 		We first delete the ‘out/edge’ folder.
 			We downoad the Edge.zip file from the GitHub release page.
 			We unzip the Edge zip file to the ‘out/edge’ folder.
 			We print a message to the console.
 	
 
 	Code translation
 
 	You can also highlight a piece of code and it gets translated to another language. For example, JavaScript to PHP.
 
 	
 
 	I am so looking forward to where this is all going!</content>
     </entry>
     <entry>
       <title>Training efficient neural network models for Firefox Translations</title>
         <link href="https://hacks.mozilla.org/2022/06/training-efficient-neural-network-models-for-firefox-translations/"/>
       <updated>2022-06-07T15:25:47.000Z</updated>
       <content type="text">Machine Translation is an important tool for expanding the accessibility of web content. Usually, people use cloud providers to translate web pages. State-of-the-art Neural Machine Translation (NMT) models are large and often require specialized hardware like GPUs to run inference in real-time.
 If people were able to run a compact Machine Translation (MT) model on their local machine CPU without sacrificing translation accuracy it would help to preserve privacy and reduce costs.
 The Bergamot project is a collaboration between Mozilla, the University of Edinburgh, Charles University in Prague, the University of Sheffield, and the University of Tartu with funding from the European Union’s Horizon 2020 research and innovation programme. It brings MT to the local environment, providing small, high-quality, CPU optimized NMT models. The Firefox Translations web extension utilizes proceedings of project Bergamot and brings local translations to Firefox.
 In this article, we will discuss the components used to train our efficient NMT models. The project is open-source, so you can give it a try and train your model too!
 Architecture
 NMT models are trained as language pairs, translating from language A to language B. The training pipeline was designed to train translation models for a language pair end-to-end, from environment configuration to exporting the ready-to-use models. The pipeline run is completely reproducible given the same code, hardware and configuration files.
 The complexity of the pipeline comes from the requirement to produce an efficient model. We use Teacher-Student distillation to compress a high-quality but resource-intensive teacher model into an efficient CPU-optimized student model that still has good translation quality. We explain this further in the Compression section.
 The pipeline includes many steps: compiling of components, downloading and cleaning datasets, training teacher, student and backward models, decoding, quantization, evaluation etc (more details below). The pipeline can be represented as a Directly Acyclic Graph (DAG).
  
 
 The workflow is file-based and employs self-sufficient scripts that use data on disk as input, and write intermediate and output results back to disk.
 We use the Marian Neural Machine Translation engine. It is written in C++ and designed to be fast. The engine is open-sourced and used by many universities and companies, including Microsoft.
 Training a quality model
 The first task of the pipeline is to train a high-quality model that will be compressed later. The main challenge at this stage is to find a good parallel corpus that contains translations of the same sentences in both source and target languages and then apply appropriate cleaning procedures.
 Datasets
 It turned out there are many open-source parallel datasets for machine translation available on the internet. The most interesting project that aggregates such datasets is OPUS. The Annual Conference on Machine Translation also collects and distributes some datasets for competitions, for example, WMT21 Machine Translation of News. Another great source of MT corpus is the Paracrawl project.
 OPUS dataset search interface:
 
 It is possible to use any dataset on disk, but automating dataset downloading from Open source resources makes adding new language pairs easy, and whenever the data set is expanded we can then easily retrain the model to take advantage of the additional data. Make sure to check the licenses of the open-source datasets before usage.
 Data cleaning
 Most open-source datasets are somewhat noisy. Good examples are crawled websites and translation of subtitles. Texts from websites can be poor-quality automatic translations or contain unexpected HTML, and subtitles are often free-form translations that change the meaning of the text.
 It is well known in the world of Machine Learning (ML) that if we feed garbage into the model we get garbage as a result. Dataset cleaning is probably the most crucial step in the pipeline to achieving good quality.
 We employ some basic cleaning techniques that work for most datasets like removing too short or too long sentences and filtering the ones with an unrealistic source to target length ratio. We also use bicleaner, a pre-trained ML classifier that attempts to indicate whether the training example in a dataset is a reversible translation. We can then remove low-scoring translation pairs that may be incorrect or otherwise add unwanted noise.
 Automation is necessary when your training set is large. However, it is always recommended to look at your data manually in order to tune the cleaning thresholds and add dataset-specific fixes to get the best quality.
 Data augmentation
 There are more than 7000 languages spoken in the world and most of them are classified as low-resource for our purposes, meaning there is little parallel corpus data available for training. In these cases, we use a popular data augmentation strategy called back-translation.
 Back-translation is a technique to increase the amount of training data available by adding synthetic translations. We get these synthetic examples by training a translation model from the target language to the source language. Then we use it to translate monolingual data from the target language into the source language, creating synthetic examples that are added to the training data for the model we actually want, from the source language to the target language.
 The model
 Finally, when we have a clean parallel corpus we train a big transformer model to reach the best quality we can.
 Once the model converges on the augmented dataset, we fine-tune it on the original parallel corpus that doesn’t include synthetic examples from back-translation to further improve quality.
 Compression
 The trained model can be 800Mb or more in size depending on configuration and requires significant computing power to perform translation (decoding). At this point, it’s generally executed on GPUs and not practical to run on most consumer laptops. In the next steps we will prepare a model that works efficiently on consumer CPUs.
 Knowledge distillation
 The main technique we use for compression is Teacher-Student Knowledge Distillation. The idea is to decode a lot of text from the source language into the target language using the heavy model we trained (Teacher) and then train a much smaller model with fewer parameters (Student) on these synthetic translations. The student is supposed to imitate the teacher’s behavior and demonstrate similar translation quality despite being significantly faster and more compact.
 We also augment the parallel corpus data with monolingual data in the source language for decoding. This improves the student by providing additional training examples of the teacher’s behavior.
 Ensemble
 Another trick is to use not just one teacher but an ensemble of 2-4 teachers independently trained on the same parallel corpus. It can boost quality a little bit at the cost of having to train more teachers. The pipeline supports training and decoding with an ensemble of teachers.
 Quantization
 One more popular technique for model compression is quantization. We use 8-bit quantization which essentially means that we store weights of the neural net as int8 instead of float32. It saves space and speeds up matrix multiplication on inference.
 Other tricks
 Other features worth mentioning but beyond the scope of this already lengthy article are the specialized Neural Network architecture of the student model, half-precision decoding by the teacher model to speed it up, lexical shortlists, training of word alignments, and finetuning of the quantized student.
 Yes, it’s a lot! Now you can see why we wanted to have an end-to-end pipeline.
 How to learn more
 This work is based on a lot of research. If you are interested in the science behind the training pipeline, check out reference publications listed in the training pipeline repository README and across the wider Bergamot project. Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task is a good academic starting article. Check this tutorial by Nikolay Bogoychev for a more practical and operational explanation of the steps.
 Results
 The final student model is 47 times smaller and 37 times faster than the original teacher model and has only a small quality decrease!
 Benchmarks for en-pt model and Flores dataset:
 
 
 
 Model
 Size
 Total number of parameters
 Dataset decoding time on 1 CPU core
 Quality, BLEU
 
 
 Teacher
 798Mb
 192.75M
 631s
 52.5
 
 
 Student quantized
 17Mb
 15.7M
 17.9s
 50.7
 
 
 
 We evaluate results using MT standard BLEU scores that essentially represent how similar translated and reference texts are. This method is not perfect but it has been shown that BLEU scores correlate well with human judgment of translation quality.
 We have a GitHub repository with all the trained models and evaluation results where we compare the accuracy of our models to popular APIs of cloud providers. We can see that some models perform similarly, or even outperform, the cloud providers which is a great result taking into account our model’s efficiency, reproducibility and open-source nature.
 For example, here you can see evaluation results for the English to Portuguese model trained by Mozilla using open-source data only.
 
 Anyone can train models and contribute them to our repo. Those contributions can be used in the Firefox Translations web extension and other places (see below).
 Scaling
 It is of course possible to run the whole pipeline on one machine, though it may take a while. Some steps of the pipeline are CPU bound and difficult to parallelize, while other steps can be offloaded to multiple GPUs. Most of the official models in the repository were trained on machines with 8 GPUs. A few steps, like teacher decoding during knowledge distillation, can take days even on well-resourced single machines. So to speed things up, we added cluster support to be able to spread different steps of the pipeline over multiple nodes.
 Workflow manager
 To manage this complexity we chose Snakemake which is very popular in the bioinformatics community. It uses file-based workflows, allows specifying step dependencies in Python, supports containerization and integration with different cluster software. We considered alternative solutions that focus on job scheduling, but ultimately chose Snakemake because it was more ergonomic for one-run experimentation workflows.
 Example of a Snakemake rule (dependencies between rules are inferred implicitly):
 rule train_teacher:
     message: &quot;Training teacher on all data&quot;
     log: f&quot;{log_dir}/train_teacher{{ens}}.log&quot;
     conda: &quot;envs/base.yml&quot;
     threads: gpus_num*2
     resources: gpu&#x3D;gpus_num
     input:
         rules.merge_devset.output, 
         train_src&#x3D;f&#x27;{teacher_corpus}.{src}.gz&#x27;,
         train_trg&#x3D;f&#x27;{teacher_corpus}.{trg}.gz&#x27;,
         bin&#x3D;ancient(trainer), 
         vocab&#x3D;vocab_path
     output: model&#x3D;f&#x27;{teacher_base_dir}{{ens}}/{best_model}&#x27;
     params: 
         prefix_train&#x3D;teacher_corpus, 
         prefix_test&#x3D;f&quot;{original}/devset&quot;, 
         dir&#x3D;directory(f&#x27;{teacher_base_dir}{{ens}}&#x27;),
         args&#x3D;get_args(&quot;training-teacher-base&quot;)
     shell: &#x27;&#x27;&#x27;bash pipeline/train/train.sh \
                 teacher train {src} {trg} &quot;{params.prefix_train}&quot; \
                 &quot;{params.prefix_test}&quot; &quot;{params.dir}&quot; \
                 &quot;{input.vocab}&quot; {params.args} &gt;&gt; {log} 2&gt;&amp;1&#x27;&#x27;&#x27;
 Cluster support
 To parallelize workflow steps across cluster nodes we use Slurm resource manager. It is relatively simple to operate, fits well for high-performance experimentation workflows, and supports Singularity containers for easier reproducibility. Slurm is also the most popular cluster manager for High-Performance Computers (HPC) used for model training in academia, and most of the consortium partners were already using or familiar with it.
 How to start training
 The workflow is quite resource-intensive, so you’ll need a pretty good server machine or even a cluster. We recommend using 4-8 Nvidia 2080-equivalent or better GPUs per machine.
 Clone https://github.com/mozilla/firefox-translations-training and follow the instructions in the readme for configuration.
 The most important part is to find parallel datasets and properly configure settings based on your available data and hardware. You can learn more about this in the readme.
 How to use the existing models
 The existing models are shipped with the Firefox Translations web extension, enabling users to translate web pages in Firefox. The models are downloaded to a local machine on demand. The web extension uses these models with the bergamot-translator Marian wrapper compiled to Web Assembly.
 Also, there is a playground website at https://mozilla.github.io/translate where you can input text and translate it right away, also locally but served as a static website instead of a browser extension.
 If you are interested in an efficient NMT inference on the server, you can try a prototype HTTP service that uses bergamot-translator natively compiled, instead of compiled to WASM.
 Or follow the build instructions in the bergamot-translator readme to directly use the C++, JavaScript WASM, or Python bindings.
 Conclusion
 It is fascinating how far Machine Translation research has come in recent years. Local high-quality translations are the future and it’s becoming more and more practical for companies and researchers to train such models even without access to proprietary data or large-scale computing power.
 We hope that Firefox Translations will set a new standard of privacy-preserving, efficient, open-source machine translation accessible for all.
 Acknowledgements
 I would like to thank all the participants of the Bergamot Project for making this technology possible, my teammates Andre Natal and Abhishek Aggarwal for the incredible work they have done bringing Firefox Translations to life, Lonnen for managing the project and editing this blog post and of course awesome Mozilla community for helping with localization of the web-extension and testing its early builds.
 This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 825303 
 The post Training efficient neural network models for Firefox Translations appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>Meet Web Push</title>
         <link href="https://webkit.org/blog/12945/meet-web-push/"/>
       <updated>2022-06-07T15:00:55.000Z</updated>
       <content type="text">
 Websites have many reasons to notify their users of time-sensitive or high-priority events, even if the user does not currently have the site open. This feature is called Web Push, and is enabled by the W3C standards for Push API,  Notifications API, and Service Workers, all working together. WebKit now supports the relevant parts of those standards to enable Web Push.
 Apple has made changes to macOS that deeply integrate with WebKit’s support to provide a great user experience, and we’re excited to announce that Web Push is supported in Safari 16 on macOS Ventura.
 Keep an eye out for Web Push on iOS and iPadOS in 2023.
 As long as you’ve coded your web application to the standards you will be able to reach Safari 16 users on macOS Ventura. You don’t need to join the Apple Developer Program to send Web Push notifications.
 If you exclude Safari through browser detection, now would be a great time to switch to feature detection, which lets you take advantage of new features as soon as they’re supported. Additionally, if you tightly manage push end points on your server, be sure to allow URLs from any subdomain of push.apple.com.
 All of this and more is covered in Meet Web Push (15 minute video) at WWDC22.
 Standards overview
 Most features of the web platform are described in a single web standard. Web Push is an exception, with multiple standards describing implementation requirements.
 There are many resources on the web to help web application authors get up and running with these standards. But to further cover how WebKit’s support works, it is useful to cover the web standards at a high level.
 The Push API standard is the most directly relevant to start with. It describes the JavaScript interface that allows a website to register a push subscription. That subscription enables sending push messages to your user’s browser using a push service.
 The ServiceWorker API is extended to support these push messages. Once a push message is received from a domain, that domain’s registered service worker script receives an event representing the push message.
 The Notifications API is extended to allow service worker scripts to post a notification even without an open browser tab.
 When a web application registers a push subscription, they promise that pushes will always be user visible. When the service worker handles a push message, it is required to use the Notifications API to display a user visible notification. Finally, when the user activates that notification, the service worker is sent an event representing the notification activation.
 Power and privacy
 Both the WebKit open source project and Apple treat privacy as a fundamental human right. As with other privileged features of the web platform, requesting a push subscription requires an explicit user gesture. It also requires you set the userVisibleOnly flag to true, and fulfill that promise by always showing a notification in response to a push message.
 The Web Push API is not an invitation for silent background runtime, as that would both violate a user’s trust and impact a user’s battery life.
 Violations of the userVisibleOnly promise will result in a push subscription being revoked.
 A little bit about WebKit
 Some of you are interested in the implementation details of Web Push in WebKit.
 One goal of the WebKit open source project is to make it easy to deliver a modern browser engine that integrates well with any modern platform.
 Many web-facing features are implemented entirely within WebKit, and the maintainers of a given WebKit port do not have to do any additional work to add support on their platforms.
 Occasionally features require relatively deep integration with a platform. That means a WebKit port needs to write a lot of custom code inside WebKit or integrate with platform specific libraries. For example, to support the HTML &lt;audio&gt; and &lt;video&gt; elements, Apple’s port leverages Apple’s Core Media framework, whereas the GTK port uses the GStreamer project.
 A feature might also require deep enough customization on a per-Application basis that WebKit can’t do the work itself.
 For example web content might call window.alert(). In a general purpose web browser like Safari, the browser wants to control the presentation of the alert itself. But an e-book reader that displays web content might want to suppress alerts altogether.
 From WebKit’s perspective, supporting Web Push requires deep per-platform and per-application customization.
 Web Push in Apple’s WebKit port
 Apple’s WebKit port includes a new daemon called webpushd. It is installed as a LaunchAgent in macOS Ventura to support Web Push. This daemon takes push subscription requests from webpages in Safari 16 and turns them into actual push subscriptions with the Apple Push Notification service.
 Incoming pushes to the system are delivered to webpushd, which then wakes the appropriate application to hand off any pending push messages to a service worker.
 The promise of Web Push is that you can reach your users even if they don’t have your website open in a browser tab. Because of how we integrated webpushd with built-in push support in macOS Ventura, Safari doesn’t even need to be running for a push message to be delivered.
 The requirement to display user visible notifications is another platform specific point. Different browsers might implement Notifications API support in different ways. Safari has always supported local notifications by relying on the macOS Notification Center and has made additional changes to handle activating these notifications when Safari is not running.
 Integrating Apple Push Notification service’s new Web Push support with webpushd and supporting notifications while Safari isn’t running are both system-level changes, making our implementation require macOS Ventura and later.
 More resources
 Apple has a few more resources to learn more about Web Push support in Safari 16 on macOS Ventura:
 
 Read Sending web push notifications in Safari and other browsers
 Watch Meet Web Push for Safari (15 minutes)
 
 MDN has some great resources on Web Push. You should start out with Web Push API Notifications best practices.
 And of course you can always reference the W3C standards directly:
 
 Push API
 Notifications API
 ServiceWorker API
 </content>
     </entry>
     <entry>
       <title>Building Interoperable Web Components That Even Work With React</title>
         <link href="https://css-tricks.com/building-interoperable-web-components-react/"/>
       <updated>2022-06-07T13:57:57.000Z</updated>
       <content type="text">Those of us who’ve been web developers more than a few years have probably written code using more than one JavaScript framework. With all the choices out there — React, Svelte, Vue, Angular, Solid — it’s all but inevitable. One of the more frustrating things we have to deal with when working across frameworks is re-creating all those low-level UI components: buttons, tabs, dropdowns, etc. What’s particularly frustrating is that we’ll typically have them defined in one framework, say React, but then need to rewrite them if we want to build something in Svelte. Or Vue. Or Solid. And so on.
 
 
 
 Wouldn’t it be better if we could define these low-level UI components once, in a framework-agnostic way, and then re-use them between frameworks? Of course it would! And we can; web components are the way. This post will show you how.
 
 
 
 
 
 
 
 As of now, the SSR story for web components is a bit lacking. Declarative shadow DOM (DSD) is how a web component is server-side rendered, but, as of this writing, it’s not integrated with your favorite application frameworks like Next, Remix or SvelteKit. If that’s a requirement for you, be sure to check the latest status of DSD. But otherwise, if SSR isn’t something you’re using, read on.
 
 
 
 First, some context
 
 
 
 Web Components are essentially HTML elements that you define yourself, like &lt;yummy-pizza&gt; or whatever, from the ground up. They’re covered all over here at CSS-Tricks (including an extensive series by Caleb Williams and one by John Rhea) but we’ll briefly walk through the process. Essentially, you define a JavaScript class, inherit it from HTMLElement, and then define whatever properties, attributes and styles the web component has and, of course, the markup it will ultimately render to your users.
 
 
 
 Being able to define custom HTML elements that aren’t bound to any particular component is exciting. But this freedom is also a limitation. Existing independently of any JavaScript framework means you can’t really interact with those JavaScript frameworks. Think of a React component which fetches some data and then renders some other React component, passing along the data. This wouldn’t really work as a web component, since a web component doesn’t know how to render a React component.
 
 
 
 Web components particularly excel as leaf components. Leaf components are the last thing to be rendered in a component tree. These are the components which receive some props, and render some UI. These are not the components sitting in the middle of your component tree, passing data along, setting context, etc. — just pure pieces of UI that will look the same, no matter which JavaScript framework is powering the rest of the app.
 
 
 
 The web component we’re building
 
 
 
 Rather than build something boring (and common), like a button, let’s build something a little bit different. In my last post we looked at using blurry image previews to prevent content reflow, and provide a decent UI for users while our images load. We looked at base64 encoding a blurry, degraded versions of our images, and showing that in our UI while the real image loaded. We also looked at generating incredibly compact, blurry previews using a tool called Blurhash.
 
 
 
 That post showed you how to generate those previews and use them in a React project. This post will show you how to use those previews from a web component so they can be used by any JavaScript framework.
 
 
 
 But we need to walk before we can run, so we’ll walk through something trivial and silly first to see exactly how web components work.
 
 
 
 Everything in this post will build vanilla web components without any tooling. That means the code will have a bit of boilerplate, but should be relatively easy to follow. Tools like Lit or Stencil are designed for building web components and can be used to remove much of this boilerplate. I urge you to check them out! But for this post, I’ll prefer a little more boilerplate in exchange for not having to introduce and teach another dependency.
 
 
 
 A simple counter component
 
 
 
 Let’s build the classic “Hello World” of JavaScript components: a counter. We’ll render a value, and a button that increments that value. Simple and boring, but it’ll let us look at the simplest possible web component.
 
 
 
 In order to build a web component, the first step is to make a JavaScript class, which inherits from HTMLElement:
 
 
 
 class Counter extends HTMLElement {}
 
 
 
 The last step is to register the web component, but only if we haven’t registered it already:
 
 
 
 if (!customElements.get(&quot;counter-wc&quot;)) {
   customElements.define(&quot;counter-wc&quot;, Counter);
 }
 
 
 
 And, of course, render it:
 
 
 
 &lt;counter-wc&gt;&lt;/counter-wc&gt;
 
 
 
 And everything in between is us making the web component do whatever we want it to. One common lifecycle method is connectedCallback, which fires when our web component is added to the DOM. We could use that method to render whatever content we’d like. Remember, this is a JS class inheriting from HTMLElement, which means our this value is the web component element itself, with all the normal DOM manipulation methods you already know and love.
 
 
 
 At it’s most simple, we could do this:
 
 
 
 class Counter extends HTMLElement {
   connectedCallback() {
     this.innerHTML &#x3D; &quot;&lt;div style&#x3D;&#x27;color: green&#x27;&gt;Hey&lt;/div&gt;&quot;;
   }
 }
 
 if (!customElements.get(&quot;counter-wc&quot;)) {
   customElements.define(&quot;counter-wc&quot;, Counter);
 }
 
 
 
 …which will work just fine.
 
 
 
 
 
 
 
 Adding real content
 
 
 
 Let’s add some useful, interactive content. We need a &lt;span&gt; to hold the current number value and a &lt;button&gt; to increment the counter. For now, we’ll create this content in our constructor and append it when the web component is actually in the DOM:
 
 
 
 constructor() {
   super();
   const container &#x3D; document.createElement(&#x27;div&#x27;);
 
   this.valSpan &#x3D; document.createElement(&#x27;span&#x27;);
 
   const increment &#x3D; document.createElement(&#x27;button&#x27;);
   increment.innerText &#x3D; &#x27;Increment&#x27;;
   increment.addEventListener(&#x27;click&#x27;, () &#x3D;&gt; {
     this.#value &#x3D; this.#currentValue + 1;
   });
 
   container.appendChild(this.valSpan);
   container.appendChild(document.createElement(&#x27;br&#x27;));
   container.appendChild(increment);
 
   this.container &#x3D; container;
 }
 
 connectedCallback() {
   this.appendChild(this.container);
   this.update();
 }
 
 
 
 If you’re really grossed out by the manual DOM creation, remember you can set innerHTML, or even create a template element once as a static property of your web component class, clone it, and insert the contents for new web component instances. There’s probably some other options I’m not thinking of, or you can always use a web component framework like Lit or Stencil. But for this post, we’ll continue to keep it simple.
 
 
 
 Moving on, we need a settable JavaScript class property named value
 
 
 
 #currentValue &#x3D; 0;
 
 set #value(val) {
   this.#currentValue &#x3D; val;
   this.update();
 }
 
 
 
 It’s just a standard class property with a setter, along with a second property to hold the value. One fun twist is that I’m using the private JavaScript class property syntax for these values. That means nobody outside our web component can ever touch these values. This is standard JavaScript that’s supported in all modern browsers, so don’t be afraid to use it.
 
 
 
 Or feel free to call it _value if you prefer. And, lastly, our update method:
 
 
 
 update() {
   this.valSpan.innerText &#x3D; this.#currentValue;
 }
 
 
 
 It works!
 
 
 
 
 
 
 
 Obviously this is not code you’d want to maintain at scale. Here’s a full working example if you’d like a closer look. As I’ve said, tools like Lit and Stencil are designed to make this simpler.
 
 
 
 Adding some more functionality
 
 
 
 This post is not a deep dive into web components. We won’t cover all the APIs and lifecycles; we won’t even cover shadow roots or slots. There’s endless content on those topics. My goal here is to provide a decent enough introduction to spark some interest, along with some useful guidance on actually using web components with the popular JavaScript frameworks you already know and love.
 
 
 
 To that end, let’s enhance our counter web component a bit. Let’s have it accept a color attribute, to control the color of the value that’s displayed. And let’s also have it accept an increment property, so consumers of this web component can have it increment by 2, 3, 4 at a time. And to drive these state changes, let’s use our new counter in a Svelte sandbox — we’ll get to React in a bit.
 
 
 
 We’ll start with the same web component as before and add a color attribute. To configure our web component to accept and respond to an attribute, we add a static observedAttributes property that returns the attributes that our web component listens for.
 
 
 
 static observedAttributes &#x3D; [&quot;color&quot;];
 
 
 
 With that in place, we can add a attributeChangedCallback lifecycle method, which will run whenever any of the attributes listed in observedAttributes are set, or updated.
 
 
 
 attributeChangedCallback(name, oldValue, newValue) {
   if (name &#x3D;&#x3D;&#x3D; &quot;color&quot;) {
     this.update();
   }
 }
 
 
 
 Now we update our update method to actually use it:
 
 
 
 update() {
   this.valSpan.innerText &#x3D; this._currentValue;
   this.valSpan.style.color &#x3D; this.getAttribute(&quot;color&quot;) || &quot;black&quot;;
 }
 
 
 
 Lastly, let’s add our increment property:
 
 
 
 increment &#x3D; 1;
 
 
 
 Simple and humble.
 
 
 
 Using the counter component in Svelte
 
 
 
 Let’s use what we just made. We’ll go into our Svelte app component and add something like this:
 
 
 
 &lt;script&gt;
   let color &#x3D; &quot;red&quot;;
 &lt;/script&gt;
 
 &lt;style&gt;
   main {
     text-align: center;
   }
 &lt;/style&gt;
 
 &lt;main&gt;
   &lt;select bind:value&#x3D;{color}&gt;
     &lt;option value&#x3D;&quot;red&quot;&gt;Red&lt;/option&gt;
     &lt;option value&#x3D;&quot;green&quot;&gt;Green&lt;/option&gt;
     &lt;option value&#x3D;&quot;blue&quot;&gt;Blue&lt;/option&gt;
   &lt;/select&gt;
 
   &lt;counter-wc color&#x3D;{color}&gt;&lt;/counter-wc&gt;
 &lt;/main&gt;
 
 
 
 And it works! Our counter renders, increments, and the dropdown updates the color. As you can see, we render the color attribute in our Svelte template and, when the value changes, Svelte handles the legwork of calling setAttribute on our underlying web component instance. There’s nothing special here: this is the same thing it already does for the attributes of any HTML element.
 
 
 
 Things get a little bit interesting with the increment prop. This is not an attribute on our web component; it’s a prop on the web component’s class. That means it needs to be set on the web component’s instance. Bear with me, as things will wind up much simpler in a bit.
 
 
 
 First, we’ll add some variables to our Svelte component:
 
 
 
 let increment &#x3D; 1;
 let wcInstance;
 
 
 
 Our powerhouse of a counter component will let you increment by 1, or by 2:
 
 
 
 &lt;button on:click&#x3D;{() &#x3D;&gt; increment &#x3D; 1}&gt;Increment 1&lt;/button&gt;
 &lt;button on:click&#x3D;{() &#x3D;&gt; increment &#x3D; 2}&gt;Increment 2&lt;/button&gt;
 
 
 
 But, in theory, we need to get the actual instance of our web component. This is the same thing we always do anytime we add a ref with React. With Svelte, it’s a simple bind:this directive:
 
 
 
 &lt;counter-wc bind:this&#x3D;{wcInstance} color&#x3D;{color}&gt;&lt;/counter-wc&gt;
 
 
 
 Now, in our Svelte template, we listen for changes to our component’s increment variable and set the underlying web component property.
 
 
 
 $: {
   if (wcInstance) {
     wcInstance.increment &#x3D; increment;
   }
 }
 
 
 
 You can test it out over at this live demo.
 
 
 
 We obviously don’t want to do this for every web component or prop we need to manage. Wouldn’t it be nice if we could just set increment right on our web component, in markup, like we normally do for component props, and have it, you know, just work? In other words, it’d be nice if we could delete all usages of wcInstance and use this simpler code instead:
 
 
 
 &lt;counter-wc increment&#x3D;{increment} color&#x3D;{color}&gt;&lt;/counter-wc&gt;
 
 
 
 It turns out we can. This code works; Svelte handles all that legwork for us. Check it out in this demo. This is standard behavior for pretty much all JavaScript frameworks.
 
 
 
 So why did I show you the manual way of setting the web component’s prop? Two reasons: it’s useful to understand how these things work and, a moment ago, I said this works for “pretty much” all JavaScript frameworks. But there’s one framework which, maddeningly, does not support web component prop setting like we just saw.
 
 
 
 React is a different beast
 
 
 
 
 
 
 
 React. The most popular JavaScript framework on the planet does not support basic interop with web components. This is a well-known problem that’s unique to React. Interestingly, this is actually fixed in React’s experimental branch, but for some reason wasn’t merged into version 18. That said, we can still track the progress of it. And you can try this yourself with a live demo.
 
 
 
 The solution, of course, is to use a ref, grab the web component instance, and manually set increment when that value changes. It looks like this:
 
 
 
 import React, { useState, useRef, useEffect } from &#x27;react&#x27;;
 import &#x27;./counter-wc&#x27;;
 
 export default function App() {
   const [increment, setIncrement] &#x3D; useState(1);
   const [color, setColor] &#x3D; useState(&#x27;red&#x27;);
   const wcRef &#x3D; useRef(null);
 
   useEffect(() &#x3D;&gt; {
     wcRef.current.increment &#x3D; increment;
   }, [increment]);
 
   return (
     &lt;div&gt;
       &lt;div className&#x3D;&quot;increment-container&quot;&gt;
         &lt;button onClick&#x3D;{() &#x3D;&gt; setIncrement(1)}&gt;Increment by 1&lt;/button&gt;
         &lt;button onClick&#x3D;{() &#x3D;&gt; setIncrement(2)}&gt;Increment by 2&lt;/button&gt;
       &lt;/div&gt;
 
       &lt;select value&#x3D;{color} onChange&#x3D;{(e) &#x3D;&gt; setColor(e.target.value)}&gt;
         &lt;option value&#x3D;&quot;red&quot;&gt;Red&lt;/option&gt;
         &lt;option value&#x3D;&quot;green&quot;&gt;Green&lt;/option&gt;
         &lt;option value&#x3D;&quot;blue&quot;&gt;Blue&lt;/option&gt;
       &lt;/select&gt;
 
       &lt;counter-wc ref&#x3D;{wcRef} increment&#x3D;{increment} color&#x3D;{color}&gt;&lt;/counter-wc&gt;
     &lt;/div&gt;
   );
 }
 
 
 
 
 Live demo
 
 
 
 
 As we discussed, coding this up manually for every web component property is simply not scalable. But all is not lost because we have a couple of options.
 
 
 
 Option 1: Use attributes everywhere
 
 
 
 We have attributes. If you clicked the React demo above, the increment prop wasn’t working, but the color correctly changed. Can’t we code everything with attributes? Sadly, no. Attribute values can only be strings. That’s good enough here, and we’d be able to get somewhat far with this approach. Numbers like increment can be converted to and from strings. We could even JSON stringify/parse objects. But eventually we’ll need to pass a function into a web component, and at that point we’d be out of options.
 
 
 
 Option 2: Wrap it
 
 
 
 There’s an old saying that you can solve any problem in computer science by adding a level of indirection (except the problem of too many levels of indirection). The code to set these props is pretty predictable and simple. What if we hide it in a library? The smart folks behind Lit have one solution. This library creates a new React component for you after you give it a web component, and list out the properties it needs. While clever, I’m not a fan of this approach.
 
 
 
 Rather than have a one-to-one mapping of web components to manually-created React components, what I prefer is just one React component that we pass our web component tag name to (counter-wc in our case) — along with all the attributes and properties — and for this component to render our web component, add the ref, then figure out what is a prop and what is an attribute. That’s the ideal solution in my opinion. I don’t know of a library that does this, but it should be straightforward to create. Let’s give it a shot!
 
 
 
 This is the usage we’re looking for:
 
 
 
 &lt;WcWrapper wcTag&#x3D;&quot;counter-wc&quot; increment&#x3D;{increment} color&#x3D;{color} /&gt;
 
 
 
 wcTag is the web component tag name; the rest are the properties and attributes we want passed along.
 
 
 
 Here’s what my implementation looks like:
 
 
 
 import React, { createElement, useRef, useLayoutEffect, memo } from &#x27;react&#x27;;
 
 const _WcWrapper &#x3D; (props) &#x3D;&gt; {
   const { wcTag, children, ...restProps } &#x3D; props;
   const wcRef &#x3D; useRef(null);
 
   useLayoutEffect(() &#x3D;&gt; {
     const wc &#x3D; wcRef.current;
 
     for (const [key, value] of Object.entries(restProps)) {
       if (key in wc) {
         if (wc[key] !&#x3D;&#x3D; value) {
           wc[key] &#x3D; value;
         }
       } else {
         if (wc.getAttribute(key) !&#x3D;&#x3D; value) {
           wc.setAttribute(key, value);
         }
       }
     }
   });
 
   return createElement(wcTag, { ref: wcRef });
 };
 
 export const WcWrapper &#x3D; memo(_WcWrapper);
 
 
 
 The most interesting line is at the end:
 
 
 
 return createElement(wcTag, { ref: wcRef });
 
 
 
 This is how we create an element in React with a dynamic name. In fact, this is what React normally transpiles JSX into. All our divs are converted to createElement(&quot;div&quot;) calls. We don’t normally need to call this API directly but it’s there when we need it.
 
 
 
 Beyond that, we want to run a layout effect and loop through every prop that we’ve passed to our component. We loop through all of them and check to see if it’s a property with an in check that checks the web component instance object as well as its prototype chain, which will catch any getters/setters that wind up on the class prototype. If no such property exists, it’s assumed to be an attribute. In either case, we only set it if the value has actually changed.
 
 
 
 If you’re wondering why we use useLayoutEffect instead of useEffect, it’s because we want to immediately run these updates before our content is rendered. Also, note that we have no dependency array to our useLayoutEffect; this means we want to run this update on every render. This can be risky since React tends to re-render a lot. I ameliorate this by wrapping the whole thing in React.memo. This is essentially the modern version of React.PureComponent, which means the component will only re-render if any of its actual props have changed — and it checks whether that’s happened via a simple equality check.
 
 
 
 The only risk here is that if you’re passing an object prop that you’re mutating directly without re-assigning, then you won’t see the updates. But this is highly discouraged, especially in the React community, so I wouldn’t worry about it.
 
 
 
 Before moving on, I’d like to call out one last thing. You might not be happy with how the usage looks. Again, this component is used like this:
 
 
 
 &lt;WcWrapper wcTag&#x3D;&quot;counter-wc&quot; increment&#x3D;{increment} color&#x3D;{color} /&gt;
 
 
 
 Specifically, you might not like passing the web component tag name to the &lt;WcWrapper&gt; component and prefer instead the @lit-labs/react package above, which creates a new individual React component for each web component. That’s totally fair and I’d encourage you to use whatever you’re most comfortable with. But for me, one advantage with this approach is that it’s easy to delete. If by some miracle React merges proper web component handling from their experimental branch into main tomorrow, you’d be able to change the above code from this:
 
 
 
 &lt;WcWrapper wcTag&#x3D;&quot;counter-wc&quot; increment&#x3D;{increment} color&#x3D;{color} /&gt;
 
 
 
 …to this:
 
 
 
 &lt;counter-wc ref&#x3D;{wcRef} increment&#x3D;{increment} color&#x3D;{color} /&gt;
 
 
 
 You could probably even write a single codemod to do that everywhere, and then delete &lt;WcWrapper&gt; altogether. Actually, scratch that: a global search and replace with a RegEx would probably work.
 
 
 
 The implementation
 
 
 
 I know, it seems like it took a journey to get here. If you recall, our original goal was to take the image preview code we looked at in my last post, and move it to a web component so it can be used in any JavaScript framework. React’s lack of proper interop added a lot of detail to the mix. But now that we have a decent handle on how to create a web component, and use it, the implementation will almost be anti-climactic.
 
 
 
 I’ll drop the entire web component here and call out some of the interesting bits. If you’d like to see it in action, here’s a working demo. It’ll switch between my three favorite books on my three favorite programming languages. The URL for each book will be unique each time, so you can see the preview, though you’ll likely want to throttle things in your DevTools Network tab to really see things taking place.
 
 
 
 
   
           View entire code      
   
 
 class BookCover extends HTMLElement {
   static observedAttributes &#x3D; [&#x27;url&#x27;];
 
   attributeChangedCallback(name, oldValue, newValue) {
     if (name &#x3D;&#x3D;&#x3D; &#x27;url&#x27;) {
       this.createMainImage(newValue);
     }
   }
 
   set preview(val) {
     this.previewEl &#x3D; this.createPreview(val);
     this.render();
   }
 
   createPreview(val) {
     if (typeof val &#x3D;&#x3D;&#x3D; &#x27;string&#x27;) {
       return base64Preview(val);
     } else {
       return blurHashPreview(val);
     }
   }
 
   createMainImage(url) {
     this.loaded &#x3D; false;
     const img &#x3D; document.createElement(&#x27;img&#x27;);
     img.alt &#x3D; &#x27;Book cover&#x27;;
     img.addEventListener(&#x27;load&#x27;, () &#x3D;&amp;gt; {
       if (img &#x3D;&#x3D;&#x3D; this.imageEl) {
         this.loaded &#x3D; true;
         this.render();
       }
     });
     img.src &#x3D; url;
     this.imageEl &#x3D; img;
   }
 
   connectedCallback() {
     this.render();
   }
 
   render() {
     const elementMaybe &#x3D; this.loaded ? this.imageEl : this.previewEl;
     syncSingleChild(this, elementMaybe);
   }
 }
 
 
 
 
 
 First, we register the attribute we’re interested in and react when it changes:
 
 
 
 static observedAttributes &#x3D; [&#x27;url&#x27;];
 
 attributeChangedCallback(name, oldValue, newValue) {
   if (name &#x3D;&#x3D;&#x3D; &#x27;url&#x27;) {
     this.createMainImage(newValue);
   }
 }
 
 
 
 This causes our image component to be created, which will show only when loaded:
 
 
 
 createMainImage(url) {
   this.loaded &#x3D; false;
   const img &#x3D; document.createElement(&#x27;img&#x27;);
   img.alt &#x3D; &#x27;Book cover&#x27;;
   img.addEventListener(&#x27;load&#x27;, () &#x3D;&gt; {
     if (img &#x3D;&#x3D;&#x3D; this.imageEl) {
       this.loaded &#x3D; true;
       this.render();
     }
   });
   img.src &#x3D; url;
   this.imageEl &#x3D; img;
 }
 
 
 
 Next we have our preview property, which can either be our base64 preview string, or our blurhash packet:
 
 
 
 set preview(val) {
   this.previewEl &#x3D; this.createPreview(val);
   this.render();
 }
 
 createPreview(val) {
   if (typeof val &#x3D;&#x3D;&#x3D; &#x27;string&#x27;) {
     return base64Preview(val);
   } else {
     return blurHashPreview(val);
   }
 }
 
 
 
 This defers to whichever helper function we need:
 
 
 
 function base64Preview(val) {
   const img &#x3D; document.createElement(&#x27;img&#x27;);
   img.src &#x3D; val;
   return img;
 }
 
 function blurHashPreview(preview) {
   const canvasEl &#x3D; document.createElement(&#x27;canvas&#x27;);
   const { w: width, h: height } &#x3D; preview;
 
   canvasEl.width &#x3D; width;
   canvasEl.height &#x3D; height;
 
   const pixels &#x3D; decode(preview.blurhash, width, height);
   const ctx &#x3D; canvasEl.getContext(&#x27;2d&#x27;);
   const imageData &#x3D; ctx.createImageData(width, height);
   imageData.data.set(pixels);
   ctx.putImageData(imageData, 0, 0);
 
   return canvasEl;
 }
 
 
 
 And, lastly, our render method:
 
 
 
 connectedCallback() {
   this.render();
 }
 
 render() {
   const elementMaybe &#x3D; this.loaded ? this.imageEl : this.previewEl;
   syncSingleChild(this, elementMaybe);
 }
 
 
 
 And a few helpers methods to tie everything together:
 
 
 
 export function syncSingleChild(container, child) {
   const currentChild &#x3D; container.firstElementChild;
   if (currentChild !&#x3D;&#x3D; child) {
     clearContainer(container);
     if (child) {
       container.appendChild(child);
     }
   }
 }
 
 export function clearContainer(el) {
   let child;
 
   while ((child &#x3D; el.firstElementChild)) {
     el.removeChild(child);
   }
 }
 
 
 
 It’s a little bit more boilerplate than we’d need if we build this in a framework, but the upside is that we can re-use this in any framework we’d like — although React will need a wrapper for now, as we discussed.
 
 
 
 Odds and ends
 
 
 
 I’ve already mentioned Lit’s React wrapper. But if you find yourself using Stencil, it actually supports a separate output pipeline just for React. And the good folks at Microsoft have also created something similar to Lit’s wrapper, attached to the Fast web component library.
 
 
 
 As I mentioned, all frameworks not named React will handle setting web component properties for you. Just note that some have some special flavors of syntax. For example, with Solid.js, &lt;your-wc value&#x3D;{12}&gt; always assumes that value is a property, which you can override with an attr prefix, like &lt;your-wc attr:value&#x3D;{12}&gt;.
 
 
 
 Wrapping up
 
 
 
 Web components are an interesting, often underused part of the web development landscape. They can help reduce your dependence on any single JavaScript framework by managing your UI, or “leaf” components. While creating these as web components — as opposed to Svelte or React components — won’t be as ergonomic, the upside is that they’ll be widely reusable.
 
 Building Interoperable Web Components That Even Work With React originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>Digital Museums For Digital History</title>
         <link href="https://smashingmagazine.com/2022/06/digital-museums-digital-history/"/>
       <updated>2022-06-07T09:30:00.000Z</updated>
       <content type="text">Technological development is an iterative process. One might assume that any engineer has at least a rough idea of how we got from the first wheel to self-driving cars or from the abacus to fintech applications, but this is a risky thing to take for granted. Even digital heritage needs museums to be preserved. Without them, the history of the Internet and the evolution of computers and software could be lost. 
 Artefacts from the electronic era, together with information stored on diskettes, CDs, and DVDs, as well as magnetic or punched tapes, will soon disappear — much more quickly than canvas, paper, parchment or papyrus. Computer hardware loses its value quickly, and obsolete equipment is discarded. Entire generations of hardware that played an integral role in technological development and had an enormous impact on our society are destroyed. 
 The desire to preserve these assets is what fueled our own efforts with the DataArt IT Museum — digital heritage preserved digitally for anyone and everyone to see, hear, and watch. In this article, we explore the evolution of the museum, how recent innovations informed our own approach, and what you can do to help preserve IT history.
 Rise Of The Museum
 The idea of a public museum is relatively new, dating to the seventeenth century, and fundamentally differs from private collections. Beginning with the Kunstmuseum Basel (founded in 1661) and Oxford’s Ashmolean Museum (1683), university and municipal chambers of curiosities were intended to be not just an attraction but an introduction to natural diversity or a brief history of human thought. With the rise of industrial exhibitions in the nineteenth century, this idea was taken to a new level.
 The Crystal Palace Exhibition held in London in 1851 showcased the achievements of the industrial revolution and established a tradition of craftsmanship museums. After the exhibition finished, many of the displays became part of the founding collections of the Victoria and Albert Museum and London’s Science Museum. Similarly, exhibits from the 1873 Vienna World’s Fair formed the core of Vienna’s Museum of Applied Arts. The 1881 International Exposition of Electricity served as an inspiration for Munich’s Deutsches Museum.
 
 A similar trend was witnessed in the United States. The Smithsonian Institution owes much to the Centennial International Exhibition of 1876 held in Philadelphia, while Chicago’s Science and Industry Museum opened following the World’s Columbian Exposition in 1893. The scientist George Brown Goode, who oversaw many early exhibitions at the Smithsonian, believed that museums should serve as a vehicle for adult education, reminding people of the value of civilization. They should, in his own words, “be a house full of ideas.”
 Machinery, unique items made by the best craftsmen, mass-produced goods, and even the pavilions themselves formed the core of future technical museum collections. These museums inspired engineers, just as art galleries inspire new generations of artists.
 The Museum As A Data Bank
 Although a boon for creativity and opportunities to share knowledge accumulated in industrial (and art) museums was (and to an extent, remains) limited because physical presence was necessary. A tantalizing possibility of digital collections is that they can be viewed anywhere by anyone. 
 The first digitization projects were started in the 1960s, and in 1967 the Metropolitan Museum of Art initiated the Museum Computer Network (MCN). In the beginning, it included 15 museums, and the number of participants grew rapidly. Now, the MCN’s stated mission is “to grow the digital capacity of museum professionals by connecting them to ideas, information, opportunities, proven practices, and each other.”
 Museums, archives, and libraries have experimented with digital alternatives but have principally been concerned with preserving physical objects that could be damaged over time. Digital copies have been available to a relatively small number of scholars. In 1991, the American Association of Museums named Treasures of the Smithsonian, an interactive program on CD, its Muse Award winner. You can watch footage of it in action on YouTube.
 
 Mass scanning and modeling started only in the 1990s when the necessary hardware became less expensive, and broadband Internet created a new kind of consumer for digital materials. Users could enjoy a collection or conduct research from any location. Subsequently, museums enjoyed a new marketing tool to grow loyal audiences.
 Digital collections also made it possible to crosslink artefacts, taking knowledge exchange to a new level by allowing any item to be put into cultural context with a focus on time, tradition, mutual influence, or ideological kinship. The opportunity to upload videos, audio recordings, and hi-res pictures make these collections more engaging to a general audience, more useful for researchers, and more attractive for designers.
 It was at this point in the evolution of the museum that we began developing our own.
 DataArt Museum Project
 Our project started with a collection of old hardware that had accumulated in one of our offices. Among the motley laptops, joysticks, terminals, and beepers, there were some very interesting things, although the collection hardly rivaled those of the Computer History Museum in Silicon Valley or The National Museum of Computing in Bletchley, England. 
 Apart from these institutions, there are plenty of other computer museums in the world. Most focused on electronic curiosities interesting only to connoisseurs who can truly appreciate a Hewlett-Packard mainframe or a 1990s orange-screen laptop. For us, this was definitely not the way to go.
 What turns a collection into a museum? We think it’s the context. Put old dusty hardware into historical, social, or cultural surroundings to explain their historical significance, and the stories come alive.
 We realized we needed to focus on the specific story behind these relics, a story relevant to us and one only we could tell. Our collection might not be the most complete, but it would be perfect for the story we want to tell. For us, as a company founded by Eastern Europeans and at the very beginning hiring predominantly from this region, the choice was obvious. We decided to start with the IT story of the former Soviet bloc — from East Germany to Armenia.
 The first generations of Eastern European computer engineers grew up behind the Iron Curtain — penniless, without access to modern technology, the latest scientific publications, quality components, or home computers. In the face of such challenges, how did this region give birth to a professional culture that still produces brilliant IT specialists?
 In 1962, Hrachya Hovsepyan, an engineer from Yerevan, received a commission to clone the French CAB 500 computer. It seemed to be cutting-edge but used a bulky magnetic drum memory. Hrachya recalls:
 “Our technologies did not allow us to reproduce the CAB 500, and my idea was completely different. I wanted to build a parallel-action machine with microprogrammed control. That’s why I kept a low profile and did my work on the sly.”
 
 This is a typical situation that illustrates relations between engineers and their commissioners (only Soviet officials could place an order for any developers). Hovsepyan’s pilot project resulted in three generations of “Nairi” computers, and his team came closer to creating a machine for personal use than anyone else in the Eastern bloc. At the same time, there was little wonder that he was later fired from his position and spent years fighting for the right to leave the USSR.
 
 We wanted to share the untold history and highlight the forgotten heroes of East and Central European IT, as well as learn about the details of their everyday jobs in the context of the industry’s strategic plans, along with the official and unofficial cultural scenes in the background. 
 This includes scientists who worked in the first computer labs, young men and women responsible for the maintenance of the first computers, inventors creating unique platforms on ternary logic, military engineers developing the Soviet proto-Internet, and thousands of enthusiasts soldering together rudimentary computers in their kitchens. That reality — DIY by necessity — can still be seen behind the approaches taken by Eastern European developers.
 Vera Glushkova, an engineer, historian, and daughter of the cybernetics pioneer Victor Glushkov, told us about engineers in Kyiv:
 “Business and commercial orders were a serious crime, but some people working in Kyiv assembled such incredible things for personal use and sometimes even on demand! One engineer, Evgeny Bondarenko, who worked on the pre-personal MIR-computer project, had all the desks in his office covered with tiny parts and circuits. He could design and assemble anything from a radio up to a pipeline processor.”
 
 Local peculiarities of this kind make face-to-face interviews with active participants of any East European computer project especially important. Luckily, modern means of communication make it easier to contact them and do such recordings, no matter where they live now. We store these audio and video files in our collection alongside hardware, books, documents, etc., and publish their transcribed versions on our website and other media, either fully or in parts. We also try to support texts with auxiliary materials, scanning private archives’ photos and documents, digitizing videos, or simply linking our stories to relevant pieces we can find on the web.
 Thus we get together exclusive findings and public domain information, making new sources accessible to the audience and suggesting another view on some well-known facts. Such a combination seems to be essential to a digital museum that can partly separate itself from its physical collection and transform to follow its curators’ ideas. At the same time, it still needs to keep its role as an artefact storage not to lose its museum roots. Otherwise, it takes the risk of becoming an internet blog.
 Our museum is a balance between CSR and marketing projects. It’s consistent with the DataArt corporate culture, as we generally look upon ourselves as geeks, people interested in digging deeper than they must. At the same time, we’re glad to use our resources to help potential researchers. That’s why we add original audio, and sometimes video, to our projects — we would be glad to share full versions of our recordings with historians, social anthropologists, or scholars of any kind.
 
 A comedy film called “The Heist at Midnight”, directed by engineer Radik Ananyan, is an example of how different aspects of history can be linked by an artefact’s preservation. Ananyan founded an amateur studio in the Yerevan Institute of Mathematical Machines, where he and most of his actors and film crew worked. The movie was shot in the institute, and we can see several of the first-generation machines from the 1950s. At the same time, it’s evidence of a multidimensional cultural life circulating around the engineering community in Yerevan. Later, Radik digitalized the movie and let DataArt’s IT museum use it in our project relating to Armenian computing, which later became a book.
 We also try to look at IT history from different angles to stimulate discussion.
 Donate Your Two Cents To Protect Civilization
 Throughout history, a variety of catastrophes not only took human lives but destroyed a vast number of historical artefacts. Thousands of books and historical documents, art, design objects, and more were lost to the ages, destroying traditions and threatening identities. As recent events have shown, such destruction is not a thing of the past. 
 The preservation of knowledge is among the key skills demanded by any field of science, humanities, or fine art. There are ways to contribute to this succession, which is the core of our civilization. It’s not only about returning books to the library but also about preserving your personal history and helping store data collected on a much larger scale.
 Each of us collects photos and videos from our lives, documenting special occasions, our daily operations at work, vacations, and more. We scan our documents and complete online forms, write texts, create spreadsheets, and together generate billions of media files every day. We choose which files to keep and what should go into the bin. We store our archives in the way modern digital devices allow us, using metadata and giving names to every item. 
 Providing descriptions for stored files is a good idea — you’ll appreciate it later. We also shouldn’t underestimate how fascinating such information and visuals (sometimes accidentally preserved in family files) could be for a researcher in the future.
 Several ideas for private data archiving:
 
 Look at your data and attempt a first glance analysis to get an idea of how you might prioritize and arrange your files.
 Divide your data into sections by origin or topic.
 Create folders based on the divisions and sort your files into them.
 Describe the data in the file name and add metadata where it is possible (e.g., geotags, or just tags, comments, and so on).
 Avoid copies or similar files that can consume archival space.
 Make regular backups of significant data and files and store them in the cloud and offline.
 
 Private archives are something we take care of ourselves, and you never know what information our heirs may find interesting one day. Take care of your own little pieces of history.
 It’s significantly more complicated to maintain the history of an organization, a professional community, a technology, an industry, a region, or even a war. These types of projects are partly supported by governments but can’t be totally covered by national institutions. At the same time, such data could be easily manipulated or erased from their archives. This is possible with digital artefacts, just as it is with paper, parchment, or clay tablets.
 
 There are many non-profit organizations and individual enthusiasts who preserve copies of hard-printed or even digitally created materials. But they need our help, either in the form of a donation or through direct volunteer assistance.
 If you’re unsure where to start, here are some projects to support:
 
 Internet Archive
 Syrian Archive
 The HistoryMakers
 Digital Country
 
 Our digital heritage can be an incredible source of inspiration and enlightenment. Computer hardware is just as important as antique writing implements we see displayed in museums. It deserves the same respect and care. There are ways we can all take part in this mission, should we choose to accept it.</content>
     </entry>
     <entry>
       <title>Web technology sessions at WWDC22</title>
         <link href="https://webkit.org/blog/12840/web-platform-and-web-extensions-features-highlighted-at-wwdc22/"/>
       <updated>2022-06-06T21:30:55.000Z</updated>
       <content type="text">WWDC22 is here, and with it, a host of announcements of new web technology shipping in WebKit on macOS, iOS and iPadOS, including advancements in privacy and security – plus new features for Safari, Web Inspector and Safari Web Extensions. Much of the news was announced on Monday during this year’s keynote, is listed in the Safari 16 Beta Release Notes, and is described in News from WWDC: WebKit Features in Safari 16 Beta. But that’s not all.
 Ten sessions at WWDC22 go into greater detail, demonstrating new technology directly relevant to web developers. New videos will be released each day this week. You can watch them on the WWDC22 website, or in the Apple Developer app for macOS, iOS, iPadOS, and tvOS.
 Be part of the conversation during WWDC on the Apple Developer Forums, or share your thoughts with @WebKit on Twitter.
 Tuesday, June 7
 
 What’s new in Safari and WebKit
 Explore the latest features in Safari and WebKit and learn how you can make better and more powerful websites. We’ll take you on a tour through the latest updates to HTML, CSS enhancements, Web Inspector tooling, Web APIs, and more.
 Watch What’s new in Safari and WebKit starting on Tuesday, June 7.
 
 
 Meet Web Push for Safari
 Bring better notifications to your websites and web apps in Safari on macOS with Web Push. We’ll show you how you can remotely send notifications to people through the web standards-based combination of Push API, Notifications API, and Service Workers.
 Watch Meet Web Push for Safari starting on Tuesday, June 7.
 
 
 Meet Passkeys
 It’s time for a security upgrade: Learn how to add support for passkeys to create a quick and easy sign in experience for people, all while offering a radical increase to account security. Passkeys are simple and strong credentials built to eliminate phishing attacks. We’ll share how passkeys are designed with security in mind, show you how people will use them, go over how to integrate passkeys in your log in flow, and explore the platform and web APIs you need to adopt this feature.
 Watch Meet Passkeys starting on Tuesday, June 7.
 
 Wednesday, June 8
 
 What’s new in Safari Web Extensions
 Learn how you can use the latest improvements to Safari Web Extensions to create even better experiences for people browsing the web. We’ll show you how to upgrade to manifest version 3, adopt the latest APIs for Web Extensions, and sync extensions across devices.
 Watch What’s new in Safari Web Extensions starting on Wednesday, June 8.
 
 
 Replace CAPTCHAs with Private Access Tokens
 Don’t be captured by CAPTCHAs! Private Access Tokens are a powerful alternative that help you identify HTTP requests from legitimate devices and people without compromising their identity or personal information. We’ll show you how your app and server can take advantage of this tool to add confidence to your online transactions and preserve privacy.
 Watch Replace CAPTCHAs with Private Access Tokens starting on Wednesday, June 8.
 
 Thursday, June 9
 
 Create Safari Web Inspector Extensions
 Learn how to add your own tools directly into Web Inspector using the latest Web Extensions APIs. We’ll show you how to create your own tab in Web Inspector, evaluate JavaScript in the inspected page, and use the result to help you troubleshoot and identify potential problems.
 Watch Create Safari Web Inspector Extensions starting on Thursday, June 9.
 
 
 What’s new in web accessibility
 Discover techniques for building rich, accessible web apps with custom controls, SSML, and the dialog element. We’ll discuss different assistive technologies and help you learn how to use them when testing the accessibility of your web apps.
 Watch What’s new in web accessibility starting on Thursday, June 9.
 
 
 Enhance your Sign in with Apple experience
 Learn how you can provide safe and fast authentication in your app using Sign in with Apple. We’ll show you how you can upgrade password-based accounts into secure, single-tap login credentials, and explore how you can seamlessly handle changes to user sessions in your app. We’ll also help you take advantage of Sign In with Apple across the web and on other platforms. To get the most out of this session, we recommend having familiarity with Sign In with Apple and REST API. We’d also recommend having a basic understanding of JavaScript.
 Watch Enhance your Sign in with Apple experience starting on Thursday, June 9.
 
 Friday, June 10
 
 What’s new in WKWebView
 Explore the latest updates to WKWebView, our framework for incorporating web content into your app’s interface. We’ll show you how to use the JavaScript fullscreen API, explore CSS viewport units, and learn more about find interactions. We’ll also take you through refinements to content blocking controls, embedding encrypted media, and using the Web Inspector.
 Watch What’s new in WKWebView starting on Friday, June 10.
 
 
 Improve DNS security for apps and servers
 Discover the latest ways to ensure that DNS — the foundation of internet addressing — is secure within your app. Learn how to authenticate DNS responses in your app with DNSSEC and enable DNS encryption automatically with Discovery of Designated Resolvers (DDR).
 Watch Improve DNS security for apps and servers starting on Friday, June 10.</content>
     </entry>
     <entry>
       <title>News from WWDC22: WebKit Features in Safari 16 Beta</title>
         <link href="https://webkit.org/blog/12824/news-from-wwdc-webkit-features-in-safari-16-beta/"/>
       <updated>2022-06-06T19:00:23.000Z</updated>
       <content type="text">WebKit has had a big year, with over 162 new features and improvements shipping in WebKit browsers — including Safari 15.2, Safari 15.4, and Safari 15.5. Features from earlier this year include dialog element, lazy loading, inert, :has() pseudo-class, new viewport units, Cascade Layers, focus visible, accent color, appearance, font palettes for color fonts, BroadcastChannel, Web Locks API, File System Access API, enhancements to WebAssembly, support for Display-P3 in canvas, additions to COOP and COEP, improved CSS autocompletion and new CSS variable tooling in Web Inspector, and much, much more.
 We’re excited to announce today the major web technologies shipping in Safari 16 beta.
 If you are an Apple Developer program member, you can test Safari 16 today by installing the developer betas of macOS Ventura, iOS or iPadOS 16.
 Web Inspector Extensions
 Safari 16 brings support for Web Inspector Extensions, so you can enhance Safari’s built-in browser developer tools. This can be especially helpful when using powerful third-party frameworks and services — perhaps your team uses React, Angular, Vue, or Ember; or maybe a popular test suite or another developer service. Now with Safari Web Inspector Extensions, you’ll be able install developer tools extensions from those frameworks and services to make your job developing with them faster and easier. Look for such extensions in the App Store this fall.
 Extensions for popular third-party frameworks and services aren’t the only exciting use of Web Inspector Extensions. Often, a small enhancement to developer tools can make a huge difference in workflow. You might be the best person to imagine and create such an extension. Web extensions are made from HTML, CSS, and JS — a perfect project for web developers. To learn the basics of building a Safari Web Extension, either from a quick-start template or by converting an existing extension to work with Safari, along with how to package it for the App Store, watch the Tech Talk Build and deploy Safari Extensions.
 Safari Web Inspector Extensions are made with the same JavaScript APIs  as the developer tools extensions in other browsers. This makes it possible for the creators of your favorite developer tools extensions to easily port them to Safari.
 Web Inspector Extensions join other improvements to Safari Web Extensions, including the ability to sync which extensions are enabled across iOS, iPadOS, and macOS.
 Container Queries
 
 After years of collaboration by engineers working on various browsers to figure out whether or not they would even be possible, Container Queries are finally here. Similar to Media Queries, Container Queries allow you to adjust the layout or styling of a particular item on your web page based on the size of its container rather than the size of the viewport. They’ll be an invaluable tool for creating reusable components in a design system.
 Safari 16 supports size queries and container query units. “Size queries” are what web developers imagine when they talk about container queries — the opportunity to write CSS that only applies if a container is a certain size. Other ideas for style queries are also being discussed as part of Container Queries as something for the future.
 Container query units are similar to viewport units, but they specify a length relative to the dimensions of a query container instead of the viewport.
 
 
 
 unit
 relative to
 
 
 
 
 cqw
 1% of a query container’s width
 
 
 cqh
 1% of a query container’s height
 
 
 cqi
 1% of a query container’s inline size
 
 
 cqb
 1% of a query container’s block size
 
 
 cqmin
 The smaller value of cqi or cqb
 
 
 cqmax
 The larger value of cqi or cqb
 
 
 
 Web Push for macOS
 
 Web Push is coming to Safari 16 on macOS Ventura. This lets you remotely send notifications to users of your websites and web apps — and deliver those notifications even when Safari isn’t running. It uses the same combination of web standards you may be familiar with from other browsers: Push API and Notifications API, along with Service Worker.
 Users opt into notifications by first indicating interest through a user gesture — such as clicking a button. Then, they’ll be prompted to give permission for your site or app to send notifications. Users will be able to view and manage notifications in Notifications Center, and customize styles and turn notifications off per website in Notifications Settings.
 If you’ve already implemented Web Push for your web app or website using industry best practices, it will automatically work in Safari. Although, if you’ve excluded Safari through browser detection, you’ll need to switch to feature detection to get it working.
 Web Push in Safari uses the same Apple Push Notification service that powers native push on all Macs and iOS devices. If you tightly manage push endpoints on your server, be sure you allow URLs from any subdomain of push.apple.com. You do not need to be an Apple Developer Program member.
 And look for Web Push for iOS and iPadOS in 2023.
 Subgrid
 
 CSS Grid shipped over five years ago, in March 2017, revolutionizing what’s possible in layout design on the web. Subgrid takes Grid to another level, providing an easy way to put grandchildren of a grid container on that grid. It makes it possible to line up items across complex layouts without being constrained by the HTML structure. And Safari’s Grid Inspector lets you turn on the overlays for as many grids as you want — which is especially helpful when coding subgrid.
 Flexbox Inspector
 
 Following last year’s Grid Inspector, Safari 16 adds a Flexbox Inspector. It pairs perfectly with the addition of the Alignment Editor in Safari 15.4.
 Overlays for Flexbox containers make it easier to visualize the effects your CSS has on Flexbox containers. The new overlay helps you visually distinguish between free space and gaps. It also shows the bounds of items revealing how they are distributed both on the main axis and cross axis of your Flexbox containers. The toggle-able “Order Numbers” option helps show the layout order of elements in the container, which can be helpful when using the order CSS property for items. And, just like our overlays for Grid last year, you can turn on as many Flexbox overlays as you need, without impacting performance.
 Accessibility Improvements
 Safari 16 introduces a re-architecture of WebKit’s accessibility support on macOS that delivers improved performance and increased responsiveness. This change allows WebKit to service more accessibility requests from clients like VoiceOver in less time than before. On some complex webpages, we’ve measured twice the number of accessibility requests served in twenty-five percent less time.
 This release also greatly improves accessibility support for elements with display:contents by ensuring they are properly represented in the accessibility tree.
 Animation Improvements
 CSS Offset Path (also known as Motion Path) provides web developers a way to animate things along a custom path of any shape. The offset-path property let’s you define a geometrical path along which to animate. The offset-anchor, offset-distance, offset-position, and offset-rotate properties give you additional abilities to refine the exact movement of the object being animated. While the offset property acts as a shorthand for combining these properties.
 With Safari 16, you can now animate a CSS Grid. That means changes in the size of rows and/or columns can be animated, opening up a whole new set of possibilities for movement on a page.
 Safari 16 also adds support for composite operations, resolving how an element’s animation impacts its underlying property values. And it adds support for discrete animation to thirty-nine CSS properties — see the full list in the Safari Technology Preview 143 release notes.
 Overscroll Behavior
 CSS Overscroll Behavior determines what happens when a user scrolls and reaches the boundary of a scrolling area. It’s useful when you want to stop scroll chaining — when a user scrolls inside a box and hits the end, you now have control over stopping or allowing scrolling on the rest of the page.
 Shared Worker
 Just when you thought there weren’t enough different kinds of workers, there’s a new type of worker in Safari — Shared Worker.  Like Service Worker, a Shared Worker runs JavaScript in the background, but its lifetime is slightly different.  Your Shared Worker runs as long as the user has any tab open to your domain, and all the tabs open to the same domain can share the same Shared Worker.  So, if you want to do something like have one WebSocket connection open to a server that communicates on behalf of multiple tabs, try out Shared Worker.
 And more
 There’s much more, including fixes and improvements to form controls as well as support for &lt;form&gt;.requestSubmit() and the showPicker() method for HTML input elements. Plus support for Shadow Realms, as well as support for the worker-src Content Security Policy directive.
 To learn more about what’s in Safari 16 for web developers, including a list of bug fixes, read the Safari 16 beta release notes.
 Feedback
 We love hearing from you. Send a tweet to @webkit, @jensimmons, or @jonathandavis to share your thoughts on this release. What technology from Safari 16 are you most excited about? What features or fixes do you want to see next? If you run into any issues, we welcome your feedback on Safari UI, or your WebKit bug report about web technology or Web Inspector. Filing issues really does make a difference.
 Download the latest Safari Technology Preview to stay at the forefront of the web platform and to use the latest Web Inspector features. You can also use the WebKit Feature Status page to watch for new information about the web features that interest you the most.</content>
     </entry>
     <entry>
       <title>The Case For Prisma In The Jamstack</title>
         <link href="https://smashingmagazine.com/2022/06/case-prisma-jamstack/"/>
       <updated>2022-06-06T09:00:00.000Z</updated>
       <content type="text">The Jamstack approach originated from a speech given by Netlify’s CEO Matt Biilmann at Smashing Magazine’s very own Smashing Conf in 2016.
 Jamstack sites serve static pre-rendered content through a CDN and generate dynamic content through microservices, APIs &amp; serverless functions. They are commonly created using JavaScript frameworks, such as Next.js or Gatsby, and static site generators — Hugo or Jekyll, for example. Jamstack sites often use a Git-based deployment workflow through tools, such as Vercel and Netlify. These deployment services can be used in tandem with a headless CMS, such as Strapi. 
 The goal of using Jamstack to build a site is to create a site that is high performant and economical to run. These sites achieve high speeds by pre-rendering as much content as possible and by caching responses on “the edge” (A.K.A. executing on servers as close to the user as possible, e.g. serving a Mumbai-based user from a server in Singapore instead of San Francisco). 
 Jamstack sites are more economical to run, as they don’t require using a dedicated server as a host. Instead, they can provision usage from cloud services (PAASs) / hosts / CDNs for a lower price. These services are also set up to scale in a cost-efficient manner, without developers changing their infrastructure and reducing their workload. 
 The other tool that makes up this combination is Prisma — an open source ORM (object relational mapping) built for TypeScript &amp; JavaScript. 
 Prisma is a JavaScript / TypeScript tool that interpretes a schema written in Prisma’s standards and generates a type-safe module that provides methods to create records, read records, update records, and delete records (CRUD).
 
 Prisma handles connections to the database (including pooling) and database migrations. It can connect with databases that use PostgreSQL, MySQL, SQL Server or SQLite (additionally MongoDB support is in preview).
 To help you get a sense of Prisma, here’s the some basic example code to handle the CRUD of users:
 import { PrismaClient } from &#x27;@prisma/client&#x27;
 
 const prisma &#x3D; new PrismaClient()
 
 const user &#x3D; await prisma.user.create({
   data: {
     name: Sam,
     email: &#x27;sam@sampoder.com&#x27;,
   },
 })
 
 const users &#x3D; await prisma.user.findMany()
 
 const updateUser &#x3D; await prisma.user.update({
   where: {
     email: &#x27;sam@sampoder.com&#x27;,
   },
   data: {
     email: &#x27;deleteme@sampoder.com&#x27;,
   },
 })
 
 const deleteUser &#x3D; await prisma.user.delete({
   where: {
     email: &#x27;deleteme@sampoder.com&#x27;,
   },
 })
 
 The associated project’s Prisma schema would look like:
 datasource db {
   url      &#x3D; env(&quot;DATABASE_URL&quot;)
   provider &#x3D; &quot;postgresql&quot;
 }
 
 generator client {
   provider &#x3D; &quot;prisma-client-js&quot;
 }
 
 model User {
   id        Int      @id @default(autoincrement())
   email     String   @unique
   name      String?
 }
 
 
 
 The Use Cases for Prisma
 Armed with a knowledge of how Prisma operates, let’s now explore where we can use it within Jamstack projects. Data is important in two aspects of the Jamstack: whilst pre-rendering static pages and on API routes. These are tasks often achieved using JavaScript tools, such as Next.js for static pages and Cloudfare Workers for API routes. Admitally, these aren’t always achieved with JavaScript — Jekyll, for example, uses Ruby! So, maybe I should amend the title for the case of Prisma in JavaScript-based Jamstack. Anyhow, onwards!
 A very common use-case for the Jamstack is a blog, where Prisma will come in handy for a blog to create a reactions system. You’d use it in API routes with one that would fetch and return the reaction count and another that could register a new reaction. To achieve this, you could use the create and findMany methods of Prisma!
 Another common use-case for the Jamstack is a landing page, and there’s nothing better than a landing with some awesome stats! In the Jamstack, we can pre-render these pages with stats pulled from our databases which we can achieve using Prisma’s reading methods.
 Sometimes, however, Prisma can be slightly overkill for certain tasks. I’d recommend avoiding using Prisma and relational databases in general for solutions that need only a single database table, as it adds additional and often unnecessary development complexity in these cases. For example, it’d be overkill to use Prisma for an email newsletter signup box or a contact form. 
 Alternatives to Prisma
 So, we could use Prisma for these tasks, but we could use a plethora of other tools to achieve them. So, why Prisma? Let’s go through three Prisma alternatives, and I’ll try to convince you that Prisma is preferable.
 Cloud Databases / Services
 Services like Airtable are incredibly popular in the Jamstack space (I myself have used it a ton), they provide you with a database (like platform) that you can access through a REST API. They’re good fun to use and prototype with, however, Prisma is arguably a better choice for Jamstack projects.
 Firstly, with cost being a major factor in Jamstack’s appeal, you may want to avoid some of these services. For example, at Hack Club, we spent $671.54 on an Airtable Pro subscription last month for our small team (yikes!).
 On the other hand, hosting an equivalent PostgreSQL database on Heroku’s platform costs $9 a month. There certainly is an argument to make for these cloud services based on their UI and API, but I would respond by pointing you to Prisma’s Studio and aforementioned JavaScript / TypeScript client.
 Cloud services also suffer from a performance-issue, especially considering that you, as the user, have no ability to change / improve the performance. The cloud services providing the database put a middleman in between your program and the database they’re using, slowing down how fast you can get to the database. However, with Prisma you’re making direct calls to your database from your program which reduces the time to query / modify the database.
 Writing Pure SQL
 So, if we’re going to access our PostgreSQL database directly, why not just use the node-postgres module or — for many other databases — their equivalent drivers? I’d argue that the developer experience of using Prisma’s client makes it worth the slightly increased load.
 Where Prisma shines is with its typings. The module generated for you by Prisma is fully type-safe — it interprets the types from your Prisma schema — which helps you prevent type errors with your database. Furthermore, for projects using TypeScript, Prisma auto-generates type definitions that reflect the structure of your model. Prisma uses these types to validate database queries at compile-time to ensure they are type-safe.
 Even if you aren’t using TypeScript, Prisma also offers autocomplete / Intelli-sense, linting, and formatting through its Visual Studio Code extension. There are also community built / maintained plugins for Emacs (emacs-prisma-mode), neovim (coc-prisma), Jetbrains IDE (Prisma Support), and nova (the Prisma plugin) that implement the Prisma Language Server to achieve code validation. Syntax highlighting is also available for a wide array of editors through plugins.
 Other ORMs
 Prisma is, of course, not the only ORM available for JavaScript / TypeScript. For example, TypeORM is another high quality ORM for JavaScript projects. And in this case, it is going to come down to personal preference, and I encourage you to try a range of ORMs to find your favourite. I personally choose Prisma to use for my project for three reasons: the extensive documentation (especially this CRUD page, which is a lifesaver), the additional tooling within the Prisma ecosystem (e.g. Prisma Migrate and Prisma Studio), and the active community around the tool (e.g. Prisma Day and the Prisma Slack). 
 Using Prisma in Jamstack Projects
 So, if I’m looking to use Prisma in a Jamstack project, how do I do that?
 Next.js
 Next.js is growing to be a very popular framework in the Jamstack space, and Prisma is a perfect fit for it. The examples below will serve as pretty standard examples that you can transfer into other projects using different JavaScript / TypeScript Jamstack tools.
 The main rule of using Prisma within Next.js is that it must be used in a server-side setting, this means that it can be used in getStaticProps, getServerSideProps, and in API routes (e.g. api/emojis.js).
 In code, it looks like this (example taken from a demo app I made for a talk at Prisma Day 2021 which was a virtual sticker wall):
 import prisma from &#x27;../../../lib/prisma&#x27;
 import { getSession } from &#x27;next-auth/client&#x27;
 
 function getRandomNum(min, max) {
   return Math.random() * (max - min) + min
 }
 
 export async function getRedemptions(username) {
   let allRedemptions &#x3D; await prisma.user.findMany({
     where: {
       name: username,
     },
     select: {
       Redemptions: {
         select: {
           id: true,
           Stickers: {
             select: { nickname: true, imageurl: true, infourl: true },
           },
         },
         distinct: [&#x27;stickerId&#x27;],
       },
     },
   })
   allRedemptions &#x3D; allRedemptions[0].Redemptions.map(x &#x3D;&gt; ({
     number: getRandomNum(-30, 30),
     ...x.Stickers,
   }))
   return allRedemptions
 }
 
 export default async function RedeemCodeReq(req, res) {
   let data &#x3D; await getRedemptions(req.query.username)
   res.send(data)
 }
 
 As you can see, it integrates really well into a Next.js project. But you may notice something interesting: &#x27;../../../lib/prisma&#x27;. Previously, we imported Prisma like this:
 import { PrismaClient } from &#x27;@prisma/client&#x27;
 
 const prisma &#x3D; new PrismaClient()
 
 Unfortunately, this is due to a quirk in Next.js’ live refresh system. So, Prisma recommends you paste this code snippet into a file and import the code into each file.
 Redwood
 Redwood is a bit of an anomaly in this section, as it isn’t necessarily a Jamstack framework. It began under the banner of bringing full stack to the Jamstack but has transitioned to being inspired by Jamstack. I’ve chosen to include it here, however, as it takes an interesting approach of including Prisma within the framework. 
 It starts, as always, with creating a Prisma schema, this time in api/db/schema.prisma (Redwood adds this to every new project). However, to query and modify the database, you don’t use Prisma’s default client. Instead, in Redwood, GraphQL mutations and queries are used. For example, in Redwood’s example todo app, this is the GraphQL mutation used to create a new todo:
 const CREATE_TODO &#x3D; gql&#x60;
   mutation AddTodo_CreateTodo($body: String!) {
     createTodo(body: $body) {
       id
       __typename
       body
       status
     }
   }
 &#x60;
 
 And in this case, the Prisma model for a todo is:
 model Todo {
   id     Int    @id @default(autoincrement())
   body   String
   status String @default(&quot;off&quot;)
 }
 
 To trigger the GraphQL mutation, we use the useMutation function which is based on Apollo’s GraphQL client imported from @redwoodjs/web:
 
 const [createTodo] &#x3D; useMutation(CREATE_TODO, {
     //  Updates Apollo&#x27;s cache, re-rendering affected components
     update: (cache, { data: { createTodo } }) &#x3D;&gt; {
       const { todos } &#x3D; cache.readQuery({ query: TODOS })
       cache.writeQuery({
         query: TODOS,
         data: { todos: todos.concat([createTodo]) },
       })
     },
   })
 
   const submitTodo &#x3D; (body) &#x3D;&gt; {
     createTodo({
       variables: { body },
       optimisticResponse: {
         __typename: &#x27;Mutation&#x27;,
         createTodo: { __typename: &#x27;Todo&#x27;, id: 0, body, status: &#x27;loading&#x27; },
       },
     })
   }
 
 
 With Redwood, you don’t need to worry about setting up the GraphQL schema / SDLs after creating your Prisma schema, as you can use Redwood’s scaffold command to convert the Prisma schema into GraphQL SDLs and services — yarn rw g sdl Todo, for example.
 Cloudfare Workers
 Cloudfare Workers is a popular platform for hosting Jamstack APIs, as it puts your code on the “edge”. However, the platform has its limitations, including a lack of TCP support, which the traditional Prisma Client uses. Though now, through Prisma Data Proxy, it is possible.
 To use it, you’ll need a Prisma Cloud Platform account which is currently free. Once you’ve followed the setup process (make sure to enable Prisma Data Proxy), you’ll be provided with a connection string that begins with prisma://. You can use that Prisma connection string in your .env file in place of the traditional database URL:
 DATABASE_URL&#x3D;&quot;prisma://aws-us-east-1.prisma-data.com/?api_key&#x3D;•••••••••••••••••&quot;
 
 And then, instead of using npx prisma generate, use this command to generate a Prisma client:
 PRISMA_CLIENT_ENGINE_TYPE&#x3D;dataproxy npx prisma generate
 
 Your database requests will be proxied through, and you can use the Prisma Client as usual. It isn’t a perfect set-up, but for those looking for database connections on Cloudfare Workers, it’s a relatively good solution.
 Conclusion
 To wrap up, if you’re looking for a way to connect Jamstack applications with a database, I wouldn’t look further than Prisma. Its developer experience, extensive tooling, and performance make it the perfect choice. Next.js, Redwood, and Cloudfare Workers — each of them has a unique way of using Prisma, but it still works very well in all of them. 
 I hope you’ve enjoyed exploring Prisma with me. Thank you!
 Further Reading on Smashing Magazine
 
 “Jamstack Rendering Patterns: The Evolution,” Ekene Eze
 “Interactive Learning Tools For Front-End Developers,” Louis Lazaris
 “Jamstack CMS: The Past, The Present and The Future,” Mike Neumegen
 “Smashing Podcast Episode 29 With Leslie Cohn-Wein: How Does Netlify Dogfood The Jamstack?,” Drew McLellan
 </content>
     </entry>
     <entry>
       <title>Vibe Check №17</title>
         <link href="https://daverupert.com/2022/06/vibe-check-17/"/>
       <updated>2022-06-05T18:44:00.000Z</updated>
       <content type="text">May is over already which means the kids are now out of school, summer has begun, and work is ramping up at Luro. Here is the retelling of vibes…
 
 The little league championship 🏆
 In my last vibe check I mentioned my son’s little league experience and how he and his teammates talked about the playoffs, but I wasn’t sure if they existed or not. Well… the playoffs were real! A single elimination playoff bracket. Eight teams enter, one team leaves…
 Otis’s team, the Athletics, squeaked their way through to the finals and ended up winning the Northwest Little League championship. 🎉
 A happy, unexpected end to our first full season of baseball. Seeing the growth of every kid on every team over the course of a season was a treat. At the beginning of the season no one can pitch and it’s a walk-a-thon, but then the pitchers start throwing strikes, then the kids realize they can’t stand there and need to start hitting, then the kids realize they need to start fielding the ball, then they start making plays and forcing outs. It’s a how-to play baseball montage that happens over eight weeks. The coaches did a phenomenal job and fostered a great experience.
 The road to productivity hell is paved with context switching
 In mid-April my wife got a job at the kids’ elementary school. It’s awesome and I think the job suits her unique skills; namely she can remember everyone’s name. One side effect of this lifestyle change is we destroyed the balance of household duties we’ve established over the last ~8 years since having kids.
 We expected the change but a lot of May was about adjusting to this new balance. A day went like this: Wake up make breakfasts and lunch → Get the kids off to school → Work for an hour or two → Interrupt my morning flow with a meeting → Work a bit and get lunch → Sit down at my desk for an hour → Get the kids from school → Get the kids to swim → The kids ask me for snacks until their mom gets home → Try to get another hour of work done…
 Darn near impossible to establish any sort of deep work rhythm with that schedule. As a result I’m not sure I was as productive as I hoped to be last month.
 The family context switching was extra, but even work had a considerable bit of context switching. A lot of switching back and forth between backend work → frontend work but also switching from building the product → building the business → building the engineering team. In a day I might cycle between product dev work → documentation → testing → meetings → podcasts → research → learning → side projects → open source. Each of those contexts have a Brain RAM cost. Mix in my (undiagnosed) ADHD and what a wonderful tapestry of distraction and experiences.
 For some reason I thought this would be a good month to try live streaming some side project work on Twitch. After three streams I think my streaming career is kaput. In a different multiverse where I have no kids, I’m not starting a company, or if my work duties radically change… it could potentially work.
 The tl;dr here is I need to eliminate some extra contexts from my life. That hurts my heart but the road to burnout is paved with context switching and we don’t want that. My new goal in life —after getting a breakfast platter named after me— is to be so bored I have time to read a magazine subscription.
 New hires and new features
 An exciting month at the job. We managed to hire two talented folks who are going to help us build the future of Luro. One person is already onboarded and one arrives next week.
 Hiring people is a lot of work. Who knew. All worth it, of course. First off, as a business, you need to know how you’re going to assess the skills and “fit” of a person. In Good to Great, Jim Collins calls this “getting the right people on the bus”, but ethically you also want to do that in a way that’s as fair and unbiased as possible. To me, this means establishing a standard interview template (with some flexibility) and devising a grading system. This complete guide to job interviewing was helpful but you need to mix in some engineering specifics.
 Then you need to comb backlogs, perform mini-explorations of near-future features, speculate far off features; all to figure out your exact skill gaps and what types of roles might fill those gaps.
 Then you need to determine if a job candidate’s skills match those gaps. This is hard for me, because I optimistically believe everyone has potential to figure out everything, but in startup land, that time and runway is somewhat precious.
 Then, if you make a hire, before they show up you have to:
 
 Make sure all your documentation is up-to-snuff
 Make sure the local dev environment stands up in minutes and not days
 Create some Good First Issue tickets
 Devise some low- to high-challenge tasks for ramping up, hopefully providing exposure to critical parts of the application
 Establish clear and concrete goals and objectives
 
 I’ve also decided that for every new hire I’m going make sure all module dependencies and Node versions are as updated as possible, so those problems don’t create a bad first install experience. Thinking about the “user experience” of employee onboarding is possibly overkill but it’s something I’d like to do well if possible.
 In a couple weeks everyone will be fully onboarded and back from vacation. I’m excited to see the lights turn on in the engineering department. In fact, with the help of our newest hire we were able to build out a new feature that we think is going to make a big impact and become a centerpiece to the application. Good vibes all around. It feels like the rocket ship is about to take off.
 Uvalde
 It’s hard to reflect on May without acknowledging the horrible, preventable tragedy that occurred at Robb Elementary School in Uvalde, TX. Kids my own son’s age. Mass shootings are shockingly not uncommon in America, but having an insincere state government that blusters and passes laws to make the situation worse while kids are dying en masse is heartbreaking. My heart goes out to all the families dealing with the unimaginable pain, loss, and grief.
 Texans, please register and vote accordingly this fall.
 Core Dave Vitals
 What you stat nerds all come here for….
 
 📖 Reading - All my reading notes are on my bookshelf.
 
 What a Body Can Do? by Sara Hendren [🌟 Staff pick]
 Drive by Daniel Pink
 The First 90 Days by Michael Watkins
 The Rema Chronicles by Amy Kim Kibushi
 
 
 🧠 Learning - Educational resources
 
 Frontend Masters Web Components - My Frontend Masters course on web components is now LIVE!
 Mastering Nuxt - I use Nuxt everyday, but working through this course to round out my knowledge and shore up my understanding of the idioms of Nuxt/Vue, rather than solely relying on my current on-demand learning approach.
 
 
 🧶 Crafting - No crafts. But I got some new Gundam HGs for my birthday.
 🎙 Podcasts, YouTubes, etc.
 
 ShopTalk YouTubes
 
 Enhancing Details with Details-Utils
 A Shared Header via Edge Functions
 
 
 Recommendations
 
 Things Fell Apart by Jon Ronson (BBC) - I love Jon Ronson’s ability to talk to people and get them to talk. This audio essay looks at the culture wars happening in American society. The first episode about Francis Schaeffer (a minor character in my evangelical past) hooked me into an exploration of decades of American division. [🌟 Staff pick]
 The Trojan Horse Affair by Brian Reed and Hamza Syed (Serial Productions) - An anonymous letter exposing the “Islamification” of school in Birmingham, a headmaster fired, a school board acting sus, yet no one knows who wrote the letter. Equal parts mystery and outrage.
 Crystal Blue by 7lamb productions. - A sci-fi audiodrama set on a prison planet on the outer edge of the federation. I could nit-pick some of the dialog but the premise is great and I think they’re doing a great job spacing out the story beats.
 
 
 
 
 ⌨️ Open source - A little bit of work in #tabvengers this month.
 
 Brian Kardell summarizes some of the progress (?) on &lt;spicy-sections&gt;
 I failed at updating the dependencies in dependency project that uses Typescript.
 
 
 👾 Video games - My Playdate arrived. I’ve had a lot of fun with it so far. More to report soon.
 📝 Blogging - Could have been better but with all the context switching.
 
 Notes from a gopher:// site - A software inspiration post
 Thoughts on spectrums - A post about dichotomies, gradients, plots, and 3D spaces
 The recycling backlog - A post about backlogs
 
 
 </content>
     </entry>
     <entry>
       <title>Please Give Me Some Space</title>
         <link href="https://css-tricks.com/please-give-me-some-space/"/>
       <updated>2022-06-03T14:39:11.000Z</updated>
       <content type="text">There’s all kinds of ways to do that. Some more advisable and better-suited for certain situations than others, of course.
 
 
 
 
 
 
 
 We could do it directly in HTML:
 
 
 
 &lt;p&gt;We go from one line...&lt;br&gt;&lt;br&gt; down a couple more.&lt;/p&gt;
 
 
 
 But that’s what CSS is really for:
 
 
 
 &lt;p&gt;We go from one line...&lt;span&gt;down a couple more.&lt;/span&gt;&lt;/p&gt;
 
 
 
 span {
   display: block;
   margin-block-start: 1.5rem;
 }
 
 
 
 Line height can also give us extra breathing room between lines of text:
 
 
 
 p {
   line-height: 1.35;
 }
 
 
 
 Since we’re talking text, there’s also letter-spacing and word-spacing, not to mention text-indent:
 
 
 
 CodePen Embed Fallback
 
 
 
 But let’s talk boxes instead of text. Say we have two simple divs:
 
 
 
 &lt;div&gt;Twiddle Dee&lt;/div&gt;
 &lt;div&gt;Twiddle Dum&lt;/div&gt;
 
 
 
 Those are block-level so they’re already on different lines. We can reach for margin again. Or we could create the impression of space with padding. I suppose we could translate those suckers in either direction:
 
 
 
 div:nth-child(2) {
   transform: translateY(100px);
 }
 
 
 
 But maybe those elements are absolutely positioned so we can use physical offsets:
 
 
 
 div {
   position: absolute;
 }
 div:nth-child(1) {
   inset: 0;
 }
 div:nth-child(2) {
   inset-inline-start: 100px; /* or top: 100px; */
 }
 
 
 
 If we’re working in a grid container, then we get gap-age:
 
 
 
 &lt;section&gt;
   &lt;div&gt;Twiddle Dee&lt;/div&gt;
   &lt;div&gt;Twiddle Dum&lt;/div&gt;
 &lt;/section&gt;
 
 
 
 section {
   display: grid;
   grid-template-columns: 1fr 1fr;
   gap: 100px;
 }
 
 
 
 Same deal with a flexible container:
 
 
 
 section {
   display: flex;
   gap: 100px;
 }
 
 
 
 While we’re working in grid and flexible containers, we could call on any alignment property to generate space.
 
 
 
 section {
   display: flex;
   align-items: space-between;
   justify-content: space-between;
 }
 
 
 
 There are tables, of course:
 
 
 
 &lt;table cellspacing&#x3D;&quot;100&quot;&gt;
   &lt;!-- etc.  --&gt;
   &lt;tbody&gt;
     &lt;tr&gt;
       &lt;td&gt;Twiddle Dee&lt;/td&gt;
       &lt;td&gt;Twiddle Dum&lt;/td&gt;
     &lt;/tr&gt;
   &lt;/tbody&gt;
 &lt;/table&gt;
 
 
 
 Or the CSS-y approach:
 
 
 
 /* We could use &#x60;display: table&#x60; if we&#x27;re not working in a table element. */
 table {
   border-spacing: 100px;
 }
 
 
 
 Let’s go deeper into left field. We can make one element look like two using a linear gradient with a hard color stop:
 
 
 
 div {
   background-image:
     linear-gradient(
       to right,
       rgb(255 105 0 / 1) 50%,
       rgb(207 46 46 / 1) 50%,
       rgb(207 46 46 / 1) 100%
     );
 }
 
 
 
 Then we do a head fake and insert a hard transparent color stop between the two colors:
 
 
 
 CodePen Embed Fallback
 
 
 
 As long as we’re fakin’ bacon here, might as well toss in the ol’ “transparent” border trick:
 
 
 
 CodePen Embed Fallback
 
 
 
 Let’s go back to text for a moment. Maybe we’re floating an element and want text to wrap around it… in the shape of the floated element while leaving some space between the two. We have shape-margin for that:
 
 
 
 CodePen Embed Fallback
 
 
 
 Dare I even mention the spacer.gif days?
 
 
 
 &lt;div&gt;Twiddle Dee&lt;/div&gt;
 &lt;img src&#x3D;&quot;spacer.gif&quot;&gt; &lt;!-- 🤢 --&gt;
 &lt;div&gt;Twiddle Dum&lt;/div&gt;
 
 
 
 There’s gotta be more
 
 
 
 You’re all a smart bunch with great ideas. Have at it!
 
 Please Give Me Some Space originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>Measuring The Performance Of Typefaces For Users (Part 1)</title>
         <link href="https://smashingmagazine.com/2022/06/measuring-performance-typefaces-users-part1/"/>
       <updated>2022-06-03T10:00:00.000Z</updated>
       <content type="text">Our focus is on typefaces for reading large amounts of text and information in the most efficient, legible, pleasurable, comprehensible, and effective way possible. For instance, typefaces used for a novel, an academic paper in a journal, or a lengthy online article like this one that uses the Elena typeface, that you are reading now on this webpage. The questions that we will explore are:
 
 How well do typefaces for extended reading actually work?
 How well does a typeface work and perform against another similar typeface?
 How would we test to see if there is any difference between a good sans serif and a serif typeface with users?
 What would the world’s most ideal, best practice and design research-driven highly legible serif, sans serif, and slab serif possibly be like? What characteristics and themes would be most advisable, and do we need a central public list of aspects and features?
 There is both the aesthetic and functional aspect to a typeface, but what is the functional aspect, and how can it be investigated and measured?
 How good is a new typeface, and how good is it compared to a similar typeface designed in previous years?
 
 Should typefaces be measured? There is no simple answer. The short answer: yes. The long answer: it is a difficult and imprecise task. We will discuss the pros and the cons, and I will show you what things are involved and how we could go about doing it.
 
 A Very Short Introduction To Typefaces
 For 100s of years, we have enjoyed using typefaces. These compiled systems for letters and symbols, which are representations of sounds and information, get a lot of use and are a large part of graphic communication.
 The first movable type machine, and therefore the first printing press, was created by a man named Bi Sheng, who lived in Yingshan, China, from what we believe to be 970–1051 AD — over four full centuries before Johannes Gutenberg was even born. The moveable type, sculptured in a lead-based alloy — which is essentially metal blocks of letters and symbols that can be moved, arranged, and used for mass printing — was Johannes Gutenberg’s contribution. Fast forward to the early 1960s, phototypesetting systems appeared. These devices consisted of glass disks (one per typeface) that spun in front of a light source, which exposed characters onto light-sensitive paper. Later on, in the 1980s, type started to be used in a digital context in computers. And today, we still have type in a digital context, but it travels through cables, wirelessly on smartphones, and in virtual reality glasses in 3D.
 There are many different classifications of typefaces. To name a few: sans serif, serif, slab serif, script, handwritten, display, ornamental, stencil, and monospace. In a way, technology also created new typeface classifications. Today, we even have mixed typefaces with elements of serif and sans serif, such as the Luc(as) de Groot’s typeface TheMix. This diversity adds to the difficulty and complexity of defining and testing typefaces.
 Reasons To Measure The Performance Of Typefaces?
 Because technology has made it possible to design typefaces easier than ever before, we seem to be reinventing “different types of wheels” that already get the job done. However, rather than reinventing these typefaces, maybe we can get some objective measures, learn from what works and what does not work, and then design objectively better wheels (typefaces). 
 If your aim is to produce a new typeface based on historical exemplars, tradition, or design, then fine, this is what you will be aiming for. Alternatively, if you want to do something new and expressive, or that has never been done before, then fine, of course. However, some contexts, situations, and users need and demand highly functional typefaces. 
 As I briefly mentioned, measuring a typeface’s effectiveness is difficult. Since many new typefaces are not supplied with any objective concrete testing data, how do we determine how well they work and where they succeed or fail?
 Should We Measure The Typeface Alone, And/Or The Context And Environment That The Typeface Is Used In?
 When considering the questions above, we can see that this is a large and complex issue. There are many different types of information, situations in which information is used, types of environments, and there are many different categories of people. Here are some extreme examples:
 
 A person who is elderly trying to read road signs, driving home at night;
 An accountant doing a large amount of numerical calculations for a million-pound/dollar company, needing to turn around the work in 30 minutes;
 A young person learning to read for the first time, sitting in the back of a car full of people on bumpy roads;
 A person with dyslexia trying to read and complete their evening class assignment.
 
 Measuring Typefaces And The Resulting Performance Data
 One of the reasons why measuring a typeface’s effectiveness is difficult is that we cannot accurately measure what goes on in people’s minds. Many factors are invisible, as Paul Luna — a professor at the University of Reading’s Department of Typography &amp; Graphic Communication — mentions in this video Paul Luna on the typographer’s task. In addition, Robert Waller, information designer at the Simplification Centre states:
 “Legibility research has a long history (going back to the 1870s). A wide range of issues has been studied, including type size, line spacing, line length, typestyle, serifs, and more. However, as Buckingham in New data on the typography of textbooks pointed out relatively early on, these factors interact in complex ways, apparently unrecognizable by many researchers. Indeed, in recent times a consensus has grown that the interaction of variables in type design is so complex that few generalizable findings can be found (see a longer review in Robert Waller’s “Typography and discourse”).”— Robert Waller in Comparing Typefaces For Airport Signs 
 
 
 Furthermore, Ralf Hermann, director of Typography.Guru in his article says:
 “Doing scientific studies to test which typefaces work best in this regard, is almost impossible to do. For a proper test setup you would need to modify one parameter while keeping every other parameter unchanged. But setting a letter or word in different typefaces can not be considered as “changing one parameter”, because a typeface consists of dozens of relevant parameters like x-height, weight, contrast, width — just to name a few. So scientific tests for typeface legibility are often full of flaws. Very often the typefaces are set at the same point size, but as every graphic designer should know, the point size does not reflect the actual size of the letters in print or on-screen. So if you come across a scientific legibility study that compares typefaces set at the same point size, don’t even bother to read on!”— Ralf Hermann at What Makes Letters Legible? 
 
 
 The observations expressed in these quotes demonstrate that testing typefaces involves many complex factors. Because of this complexity, it has to be carefully controlled and modified, but it may not even be worth the effort.
 Consistency And Variables
 When testing typefaces or a selection of typefaces against another, we need to keep the typographic design parameters and variables the same, so we do not introduce or change the previously tested type settings. One example is the difference between the typefaces’ x-height’s (the height of a lowercase x) of any two typefaces we are testing. It is unlikely that they will be the same, as x-heights differ greatly. Thus, one of the two typeface x-height’s will seem to be larger in size, although it may be the same point size in the software. I will show you more about typographic variables under the section “Specific Typographic Design Variables Affecting Performance” in the second part of this article.
 Robert Waller mentions in “The Clear Print standard: arguments for a flexible approach” that “although both point size and x-height are specified, it is the point size (pt) that is most commonly quoted — and point size is a notoriously imprecise measure.” It is, however, more effective and accurate to set an x-height measurement and set the typefaces being compared to that same x-height measurement. The x-height using point sizes actually results in different sizes — and does not look inconsistent between different typefaces.
 
 Notice on the 1st line that we can see that both typefaces are set to 26 pt in Adobe InDesign. However, if you look at the tops of the “erdana” you can see that they go slightly above the line, so the Verdana typeface is, in essence, larger than the Info Display typeface, even when they are both typeset at 26 points. On the 2nd line, both typefaces have been typeset to a consistent and accurate measurement of an x-height of 5.5 mm. Notice that while the x-height is the same for both typefaces on the 2nd line, it gives a different point size for each typeface. This is why point size is not an accurate way to measure typeface size and for testing and comparing two or more typefaces.
 Additionally, how you use and typeset the typeface in the actual typographic design and layout (line length, typeface size, color, spacing, leading, and so on) is probably more important than the actual typeface used. Thus, you could use one of the world’s most legible typefaces, but if you typeset it with a leading of -7 points and a line length of 100 characters, it would be rendered nearly useless.
 
 As you can see, we can’t use a singular factor to measure typefaces. Instead, we need to address multiple factors within the design system. They all have to work well together to bring an ideal and effective final presentation.
 Do We Need To Decide On A Base Default Typeface To Standardized Test Typefaces Against?
 I would like to make things more complicated. (Remember when I told you this article had some difficult and complex issues?) So as an example, let’s say that we want to test a serif typeface against another serif and then again a sans serif against another sans serif. One would think that one of the two serifs or one of the sans serifs would perform better than the other, right? Well maybe, but not quite. Now, let’s say that we have the previous person testing two serif typefaces and two sans serif typefaces. What would happen if someone else did the same test but then tested their serif and sans serif against a different serif and sans serif typefaces that the 1st person used. Well, the result is simply that two people tested a serif and a sans serif typeface against different serif and sans serif typefaces, and they are not cross comparable.
 So, the question is: should we, as a community, decide on base typefaces to test against? So, for a serif, it is quite popular and common in academic journals to test against Times New Roman. So, for sans serif, Arial is again another popular base typeface typically used to test another sans serif against. Then for monospace, Courier?
 Last but not least, we have 2 people previously testing typefaces, but what typographic design and typesetting settings and variables did they use? Once again, even more inconsistency is introduced because they would most definitely test their typefaces with different typographic designs and typesetting settings. Do we need to set a base/default typographic design and typesetting, so everyone tests and measures against the same thing?
 
 The Difference Between Near-identical Typefaces: Two Brief Discussions
 There are many typefaces, and many of them are very similar or are nearly identical to previous or contemporary versions available. Here are some examples:
 
 Neue Haas Grotesk (1956), Helvetica (1957), Arial (1982), Bau (2002), Akkurat (2004), Aktiv Grotesk (2010), Acumin (2015), Real (2015);
 Frutiger (1976), Myriad (1992), Monotype SST (2017), Squad (2018), Silta (2018);
 Collis (1993), Novel (2008), Elena (2010), Permian (2011), Lava (2013).
 
 Note: For more information, see my article “No more new similar typefaces for extended reading, please!”
 If we look at a typeface like Garamond, we can see that there are many versions of Garamond — all with slightly different interpretations of what the ultimate or most accurate version of Garamond is. Furthermore, they are all designed for slightly different uses, contexts, and technological choices:
 
 Typeface designers and foundries supplying these versions of Garamond say theirs is the best, but which one is right? They were all designed for slightly different contexts and technological times. It would be interesting to find out what the performance differences are between these very similar typefaces.
 
 Furthermore, if we compare a typeface like Minion Pro (which is quite robust and sturdy) against a typeface like Monotype Baskerville, we can observe that Minion Pro has more consistent stroke widths and slightly less personality than Monotype Baskerville. In contrast, Monotype Baskerville has more variance in stroke width, with more of a posh and sophisticated personality than Minion Pro. Therefore, how would these differences affect their performance? Perhaps there is no one right answer to these questions.
 I certainly feel, in 2022, that we, as designers, researchers, and academics, have certainly built up some fairly sensible and reasonable conclusions based on previous research and previous arguments over the last years. Nevertheless, the questions seem to remain around — what are the characteristics that make typefaces work a little bit better, and what would be “more advisable?”
 Kai Bernau’s Neutral Typeface
 Neutral, a typeface designed by Kai Bernau, is an interesting example of how an ideal utopian legible typeface might look and be like. We can see in the image below that Bernau has analyzed the fine and very subtle characteristics that are common in neutral typefaces, such Univers, Helvetica, TheSans, and so on.
 The term “neutral” basically refers to a typeface design that does not say much or does not say anything and that has a very “no style/anonymous” feel and voice — like the color grey. Think of it like speaking to someone with little personality or who has a not-obvious personality. In his design, Bernau is attempting to find out what an almost merged letter skeleton of all these neutral typefaces would look like when comparing all these typefaces.
 
 Kai Bernau’s Neutral typeface began as a graduation project at KABK (the Academy of Art, The Hague), taking inspiration from typefaces that seem ageless, remain fresh and relevant even decades after they were designed. His project was constructed based on a set of parameters derived by measuring and averaging a number of popular 20th-century sans serif fonts (like Helvetica, Univers, Frutiger, Meta, and TheSans), trying to design the ultimate “neutral” font. It is a very interesting idea that builds on previous best practices to find an optimal solution. This is much more like the conceptual typeface design that we need to see in the future. For more, see the “An Idea of a typeface” article by Kai Bernau.
 Can A Utopian Highly Legible Typeface Exist?
 Bernau’s typeface aims for neutrality and utopian legibility. However, if I asked the question: what would the world’s most legible sans serif, serif, or slab serif typeface look like? How much better than a typically good highly legible sans serif, serif, or slab serif typeface would it be? Furthermore, is it even worth the effort? 
 Whatever your thoughts are, it is the designer’s job to design things that work and read well. Adding to this conversation, Sofie Beier (a legibility expert) says:
 “In the history of design, there are many examples of designers proposing an “ideal typeface”. The fact is that there is no optimal typeface style. A thorough literature review shows that typeface legibility varies significantly depending on the reading situation.”— Sofie Beier in Bringing Together Science And Typography
 
 Perhaps, we should consider developing a central list of the elements, research, data, sources, and aspects to create legible and usable typefaces, so we can easily choose? This may lead to better typeface design decisions, choices, and better typefaces in the future.
 Changing The Change: From What To What?
 Another reason why we need to measure typefaces and know how well they work is highlighted by David Sless, information design pioneer and director of the Communication Research Institute in Australia, in his article “Changing the change: from what to what?”:
 “Change is good. Design is all about change; bringing something into the world that didn’t exist before; changing from an undesirable to a desirable state of affairs; improvement; progress! And now we are even changing the change! […]Benchmarking is that part of the design process where you ask how an existing system is performing against agreed performance requirements set at the scoping stage of the design process. Putting the matter simply, if you change something and then claim that the change is an improvement, you need to have some before and after measurements. […]Much design work… is redesign rather than design from scratch. An important part of redesign is to ask: where are we right now? What is the current performance of this design? What is happening in the world now which we don’t want to happen, or we’d like to change? Where do we want to go? What do we want to achieve here? […]So I’m all in favour of change, even changing the change. But we need to know what we are changing from. […]Unless we look carefully at what we are doing now before making a change, we might throw out some good bits.”— David Sless
 
 This is one of the reasons we need to measure typefaces and know how well they perform. That way, when we design new ones in the future, we can learn from past data and then use that knowledge in future typefaces, rather than relying on a bit of research and personal self-expression. 
 Redesigning typefaces makes us end up in the same place (whether good or bad), and we are not necessarily making and designing better typefaces. Although typeface design provides us with both the aesthetic appeal to meet the functional needs, it is the functional need and its functional aspect that is frequently missing. 
 With the thoughts mentioned above, I would like to raise another debate, because I know that typographic discussions and debates are usually beneficial and productive for all involved.
 Are Typefaces Tools, Software, Objects, Products Or What?
 This is a question that is not easily answered. It depends on what position you decide to take. Kris Sowersby, director of Klim Type Foundry argues that typefaces are not tools in his article “A typeface is not a tool”:
 “In theory, designers could perform all of their typesetting jobs with the same one or two typefaces. But they don’t. I can almost guarantee this comes down to aesthetics. They choose a typeface for its emotive, visceral and visual qualities — how it looks and feels. Designers don’t use typefaces like a builder uses a hammer.The function of a typeface is to communicate visually and culturally.”— Kris Sowersby
 
 Though Sowersby points out a functional aspect, he makes no mention that typefaces are used to achieve certain and precise responses and effects from users’ behaviors and emotional responses. For example, I might choose typefaces specifically because of their legibility — when a typeface is considered legible and easy-to-read by people with less-than-good eyesight. And so a well-designed typeface (or tool) is crucial.
 Another reason I may choose a typeface is to bring a certain “more ideal” and to bring a “more specific” response and behavior from people. So, I may also choose a typeface as a tool for better communication with specific audiences. This is similar to why we choose a hammer over another, even though they all do the same job. There are 100s of different types of hammers, but builders do seem to have an “emotional favorite.”
 In addition, typefaces can be more or less legible on different screens and monitor resolutions because they can be rendered with varying degrees of quality and sharpness.
 Let us move on to a more precise and probably more important aspect, and that is testing data value.
 The Two Types Of Testing Data: “Subjective” And “Objective”
 When testing, there are two types of testing data: subjective (meaning mainly coming from a personal view and opinion) and objective (coming from a result from reality or the ability to do or not do something). They are valuable in their own ways. However, an objective measurement may be more desirable. It is important to know the difference between the two. Below is a brief description of both as it applies to our topic: 
 
 A subjective measure:A user says: “I can read this typeface better.” This may be the case and what the person feels. However, if the measurement, in this case, is “better,” then the questions are: how much better, what kind of a measure and how accurate a measure is “better,” and how much better (than what) is it? However, what one person likes may not be what another one likes. Is it better because I said so? They may not be able to describe or know why they like it, but they just say: “I like it.” Because this measure is based on what the person feels, it is not accurately measurable.  
 An objective measure:A user identified a letter correctly and within a certain timeframe. The data is either correct or incorrect, they could or could not do it, and they did or did not do it in a measurable recorded time span.
 
 Kevin Larson, principal researcher on Microsoft’s Advanced Reading Technologies team, explains: 
 “While I generally agree with you on the importance of objective data, and most of my work has collected reading speed measures of one kind or another, I think there can be interesting subjective data.”— Kevin Larson
 
 David Sless also states in his article “Choosing the right method for testing:”
 “The first is that inexperienced, untrained, or misguided information designers ask the wrong questions: what do people think of my designs? Which of my designs do they prefer? What must my artifact look like? What information must my artifact contain? The second reason is that asking the wrong questions about the design, leads inevitably to certain ways of asking questions — methods of testing which give inadequate answers.”— David Sless
 
 David Sless continues the discussion by adding [slightly reworded and edited by me]:
 “Attitude and opinion surveys, preference tests, expert opinion, and content-based design, are based on the wrong questions and are highly subjective because they come from people’s views, knowledge build-ups and preferences… The right, or much better, more easily measurable and more accurate question, is based on user performance, setting them tasks to do, then using diagnostic testing to see if they can or cannot do the tasks, and making any notes, possibly recording how long it took them to do it. A far more useful question to ask before you design a new information artifact or redesign an existing one is: what do I want people to be able to do with this artifact?”
 
 In summary, the most important question is: what do we want the users to do? Based on the previous examples and discussions in this article, we can see that not all data or information gained is necessarily useful or accurate.
 What Do We Want People To Do With Highly Legible Typefaces?
 Consulting Sless’ article “Changing the change: from what to what?” again:
 “A far more useful question to ask before you design a new information artifact or redesign an existing one is: what do I want people to be able to do with this artifact?”— David Sless
 
 Let’s try to outline what we want people to do with highly legible typefaces for extended reading, like reading large amounts of information, effectively and precisely:
 
 We want them to be able to recognize what each letter, word, and symbol is;
 We want the typeface to reflect and fit the content and message for the typeface to enhance and support it;
 We want them to understand, absorb and comprehend as much of the information as possible;
 We want to encourage, sustain and enable high motivation levels when looking at and reading the text;
 We want them to maybe form a bond with the text and typography, feeling that the information is high quality, respectful and worthy;
 We want to tire them as little as possible;
 We want to provide typography in the most efficient way, such as leading, tracking, kerning, typeface size, color, line length, hyphenation, capitalization, and word spacing;
 We want different categories of people — like people with vision impairments, people with low vision or very bad eyesight, people with dyslexia or aphasia, or who have specific letter requirements, like children learning to read — to at least have letters and symbols designed to support or fit their needs as best as possible. We want to allow accessibility via OpenType or stylistic options with the typeface, so they are available to use if needed. These points could be extended to language support as well.
 
 More Coming Up In Part 2
 Let us dive into more of these amazing complex issues (as I said they would be) in the second part of this article — Measuring The Performance Of Typefaces For Users (Part 2). We will look deeply at how we can test typefaces, how to get the best out of every aspect of the process, and more!
 Further Reading on Smashing Magazine
 
 “Micro-Typography: How To Space And Kern Punctuation Marks And Other Symbols,” Thomas Bohm
 “A Reference Guide For Typography In Mobile Web Design,” Suzanne Scacca
 “7 Gorgeous Free And Open-Source Typefaces And When To Use Them,” Noemi Stauffer
 “Exo 2.0, A Contemporary Geometric Sans Serif Font (Freebie),” Vitaly Friedman
 </content>
     </entry>
     <entry>
       <title>Collective #714</title>
         <link href="https://tympanus.net/codrops/collective/collective-714/"/>
       <updated>2022-06-02T13:36:36.000Z</updated>
       <content type="text">
 
 
  
 Inspirational Website of the Week: Custo
 Perfectly smooth animations with typography that just works. Our pick this week.
 Get inspired
 
 
 
 
         
 
 
 
 
 
         
 This content is sponsored via BuySellAds
 Build websites faster with Divi Cloud
 Divi Cloud is like Dropbox for your Divi websites: save something to Divi Cloud and it becomes available on all of your and your clients’ websites while you build them.
         Check it out
 
 
 
 
 
  
 A three.js Competition
 Tiiny Host’s three.js competition has started! Upload and host your web project and show off your WebGL skills!
 Check it out
 
 
 
 
 
  
 AutoAnimate
 A zero-config, drop-in animation utility that automatically adds smooth transitions to your web app.
 Check it out
 
 
 
 
 
  
 Sliderland
 A minimalist creative coding playground, using only HTML sliders. Make animations using math!
 Check it out
 
 
 
 
 
  
 IndigoStack
 The native macOS app which promises to revolutionise the way you configure and run local web servers on your Mac.
 Check it out
 
 
 
 
 
  
 Customizing Color Fonts on the Web
 WebKit now supports CSS @font-palette-values which can be used to access predefined color palettes of color fonts.
 Read it
 
 
 
 
 
  
 PushIn.js
 In case you didn’t know about it: PushIn.js is a lightweight parallax effect, built with JavaScript, that simulates an interactive dolly-in or push-in animation on a webpage.
 Check it out
 
 
 
 
 
  
 A calendar and weather forecast in the terminal
 A couple of cool terminal tricks to get calendars and the weather, plus how to assign aliases and pass arguments to bash functions. By Jamie Smith.
 Read it
 
 
 
 
 
  
 Tetra
 Tetra is a full stack reactive component framework for Django using Alpine.js.
 Check it out
 
 
 
 
 
  
 Can I Devtools?
 Can I DevTools is like Can I Use but for the browser devtools. Created and curated by Pankaj Parashar.
 Check it out
 
 
 
 
 
  
 Top games + source code from Gamedev.js Jam 2022
 The best 13 entries from the Gamedev.js Jam 2022.
 Check it out
 
 
 
 
 
  
 Processing Arrays non-destructively: &#x60;for-of&#x60; vs. &#x60;.reduce()&#x60; vs. &#x60;.flatMap()&#x60;
 Axel Rauschmayer looks at look at three ways of processing Arrays and shows how to use the features.
 Read it
 
 
 
 
 
  
 Drawing with CSS: Anime Character
 Alvaro Montoro explains how to draw an anime character with HTML and CSS from scratch.
 Read it
 
 
 
 
 
  
 Pure CSS image zoom
 Some amazing CSS magic by Jhey: a thread on how to pull off a pure CSS image zoom using CSS primitives.
 Check it out
 
 
 
 
 
  
 Plantarium
 Plantarium is an amazing tool for the procedural generation of 3D plants. Made by Max Richter.
 Check it out
 
 
 
 
 
  
 How I Built an “OS” with React for My Portfolio Website
 Dustin Brett discusses how he made his website look and feel like an Operating System with JavaScript.
 Watch it
 
 
 
 
 
  
 Scroll Btween
 Tween any CSS values on any DOM element in relation with its position on the viewport. No dependencies.
 Check it out
 
 
 
 
 
  
 The Modern JavaScript Tutorial
 Freshly updated: Simple, but detailed explanations with examples and tasks, including: closures, document and events, object oriented programming and more.
 Check it out
 
 
 
 
 
  
 Building a static website using Iles.js
 In this tutorial by Clara Ekekenta you will learn how to code a static website in Vuejs using Iles.js.
 Read it
 
 
 
 
 
  
 Cool CSS Hover Effects That Use Background Clipping, Masks, and 3D
 Temani Afif finalizes his series on hover effects and shows how to code complex CSS hover animations.
 Read it
 
 
 
 
 
  
 Arctype SQL Client
 Arctype is a fast and easy-to-use database client for writing queries, building dashboards, and sharing data with your team.
 Check it out
 
 
 
 
 
  
 State is hard: why SPAs will persist
 Nolan Lawson shares some thoughts why Single-Page Apps still persist.
 Read it
 
 
 
 
 
  
 Star Wars Scene Transition Effects in CSS
 Learn how to create those wipe transitions from Star Wars movies in CSS.
 Read it
 
 
 
 
 
  
 From Our Blog
 Scroll Animation Ideas for Image Grids
 Some ideas for scroll animations on image grids using the ScrollTrigger plugin of GSAP and the new smooth scroll library Lenis by Studio Freight.
 Check it out
 
 
 
 
 
  
 From Our Blog
 Coding an Infinite Slider using Texture Recursion with Three.js
 A coding session where you’ll learn how to recreate the infinite image slider from tismes.com using Three.js.
 Check it out
 
 
 
 
 
  
 From Our Blog
 Inspirational Websites Roundup #38
 A new hand-picked selection of fresh websites for your inspiration.
 Check it out
 
 
 The post Collective #714 appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>On online collaboration and our obligations as makers of software</title>
         <link href="https://www.baldurbjarnason.com/2022/on-online-collaboration/"/>
       <updated>2022-06-02T12:56:06.000Z</updated>
       <content type="text">
 My reading corner.
 
 
 This post is the final entry of three in a series where I go over what I learned from the user research I’ve been doing for Colophon Cards. The first was about markdown. The second was about the various different kinds of notetaking.
 It all began with blogging
 As I’ve mentioned earlier in this series, I’ve been blogging for almost two decades. What I didn’t mention was that I was a true believer in ‘Blogging’.
 —So, you believed in having a personal website, what of it?
 No, you see, at the time blogging was supposed to be so much more than that. I truly believed that it heralded the future of collaboration, communications, publishing, and intellectual discourse. I believed that the future of social media was an interconnected web of personal blogs. I believed that the future of intellectual discourse was in cross-blog debates on lofty topics.
 I believed the hype. I was quite wrong.
 Blogs didn’t quite die. You could even say that they won as they have become a cornerstone of the web’s infrastructure:
 
 40% of the web runs on WordPress, which is blog software.
 Substack and similar newsletter platforms offer a reverse-chronological list of posts and an RSS feed and are basically rebranded blogs.
 Podcasts are blogging with audio files.
 The affordances invented by blogs are everywhere. Many news sites maintain ongoing daily liveblogs on specific events, such as COVID-19 or the UK’s political scandal of the day. They may not call it a liveblog, but that’s what they are.
 Much of the web’s automation is built on feed data that’s shuffled from service to service. Feeds are the quintessential blog tech.
 Blogs transformed how we approach content management systems (CMSes), even when those CMSes don’t use any blog-related design affordances.
 Youtuber video essays are an evolution of early attempts at “video blogging” or video-based podcasting.
 
 Blog tech underpins almost everything you see online.
 So, while blogs didn’t quite have the profound effect on society, publishing, and media that I expected them to have, their influence is respectable and far-reaching.
 I am biased in favour of blogs, though. They had a major influence on my intellectual development, to the point where I still to this day revisit old blog post favourites for fresh inspiration.
 One such major influence, one that probably changed my entire outlook on both writing and software development, was Kathy Sierra and her blog Creating Passionate Users. Her writing sent me down a path of studying the learning process, skills development, expertise, mastery, and why they matter to software development and UI design.
 And Kathy Sierra’s social media story—that began with a group of self-styled ‘mean kids’ revelling in mean jokes and ended with literal nazis calling for mass murder—is the reason why I fell out of love with weblogs and began to distrust the tech industry’s approach to social media and online collaboration.
 
 Getting back to notetaking, or the one where I piss off the punditry (again)
 I reject the ‘external brain’ hypothesis for notetaking.
 Or, more specifically, I think it’s a counterproductive metaphor for notetaking.
 The hypothesis is straightforward: a complex notetaking system, built in the form of a structured web, acts as an extension of your brain and makes you smarter.
 The theory is sometimes stated; sometimes merely implied. But it’s the trend among those making notetaking apps or selling courses on notetaking to talk as if structured, interlinked notetaking will raise your IQ, help you think more clearly, and prompt a wellspring of new ideas. Your collection of notes, dutifully linked and tagged, woven into an impenetrable fabric of collections and context, are therefore serious work and the promise is that all of that effort will be rewarded with intelligence, memory, and genius. The notetaking system is complex and tough to use because it’s supposed to expand the cognitive space that we use to structure our thoughts and generate ideas.
 The problem is that even if our observations about these notetaking systems are correct, we don’t know if we have the arrow of causality pointing in the right direction.
 Maybe that German academic was a genius and he definitely used a complex notetaking system. But was he a genius because he had a complex and involved notetaking system? Or did a complex and involved system work for him because he was a genius?
 Is it the notetaking system that’s helping you think more clearly? Or is it the act of writing that forces you to clarify your thoughts?
 Is it the complex interlinked web of notes that helps you get new ideas? Or is it all the reading you’re doing to fill that notetaking app bucket?
 Is all of this notetaking work making you smarter? Or is it just indirectly forcing you into deliberate, goalless practice?
 If the external brain hypothesis is correct, then complexity is essential. You can’t have a neurological extension of the brain without neurological complexity. The involved structures and regimented processes exist to tap into specific structures in our brain. There might be variations but all should have a baseline complexity that can’t be abstracted away because, hey, the brain is complex.
 That’s the theory, at least.
 However, if my suspicions are correct, then the primary benefit from notetaking comes from regular, deliberate practice. It doesn’t matter if you’re sketching, journaling, collaging, jotting down bullet points, recording a daily video, or just writing. It doesn’t matter if you’re filing it all away with a detailed ontology into a structured database or if you’re dumping it all into a box. It’s the habitual practice—in a way that fits your skill, personality, and practices—that matters.
 If I’m right, then you can get the results of a complex notetaking system with a lot less work. Or, to be more specific, with a lot less wasted work—it should all go into the writing (or sketching, recording, etc.).
 The key is that the object of notetaking is never to take notes. It’s to do a better job:
 
 Build better software.
 Write better blog posts or finally finish that novel.
 Renovate that spare bedroom into something nice.
 Become more thoughtful about what you buy and eat.
 Write a better thesis.
 Make better art.
 Take better photographs.
 
 Notetaking should always be less work than the actual work you’re doing. When the notetaking process takes over, something has gone seriously wrong.
 
 How blogs influenced my thinking on the subject
 Kathy Sierra’s work has a single recurring idea from the very beginning of her first blog, Creating Passionate Users to her amazing 2015 book Badass: Making Users Awesome: the job of the tools we make is to set the user on a path of mastery. The tool shouldn’t get in the way of them improving their skills. It shouldn’t overload the UI with distractions. It should enable experimentation and practice. It should be designed in such a way as to enable and foster mastery.
 That requires us to understand what mastery and skills development is, how it comes from deliberate goalless practice, exposure to examples of work that exhibit the skills the user aspires to, from having a tight feedback loop that lets the end user be aware of how they are working when they need to, from increasing the resolution of the work when applicable. And the generally accepted key to mastery (or skill-specific expertise, if you want to be nitpicky), is twofold: deliberate practice and perceptual knowledge.
 In Kathy Sierra’s words, from her book Badass, first on practice:
 
 Practicing harder and longer can potentially make us even worse than if we did less practicing.
 Building deep expertise takes work, but of a very specific type that’s often the opposite of what people do when practicing.
 Practice does not make perfect
 
 On perceptual knowledge:
 
 The second attribute of those who became experts is this: they were exposed to high quantity, high quality examples of expertise. (page 128)
 
 
 After enough exposure with feedback, your brain began detecting patterns and underlying structures, without your conscious awareness. With more exposure, your brain fine-tuned its perception and eventually figured out what really mattered. Your brain was making finer distinctions and sorting signal from noise even if you couldn’t explain how. (page 133)
 
 These two ideas, deliberate practice and exposure to high-quality examples, should be the forces that drive the design and structure of most apps. That includes Colophon Cards.
 
 The research so far
 As I wrote in my last entry, I began my research into writing and notetaking well before I began the Colophon Cards project itself. I came to the project with a set of ideas. I’d based them on my research of existing studies and writing about expertise, mastery, creative work, and knowledge work. (And I reviewed a ton of forum threads on notetaking.)
 The purpose of the survey and the interviews was to see if I could validate or invalidate some of my guesses before I advance to the implementation stage proper. Better now than after I’ve implemented something.
 
 The survey helped me get a sense of the overall notetaking “enthusiast” scene and lent support to my theory that there is a core group of active users who are open to trying new apps.
 It helped me decide how to approach text formatting; whether to implement rich text or markdown.
 It gave me the tools and context to outline a categorisation of the various approaches to notetaking and gave me the framework to more clearly define and describe the approach I’m going to try out.
 It allowed me to test out a variety of static UI mockups to figure out the broad strokes of what worked and what didn’t.
 
 Finally, writing these entries where I analyse and outline the observations from the interviews while explaining where they stand relative to my other research, helped me consolidate my ideas into a cohesive theory of how a subset of the notetaking field works.
 The research I’ve done isn’t enough to lend my theories scientific legitimacy (for that I’d need way more money) but it’s enough to give me confidence in going forward.
 My hunches:
 
 When it comes to notetaking, knowledge work (e.g. most kinds of office work that require expertise of some sort) has more in common with creative work than not and benefits from similar approaches.
 That most kinds of notetaking don’t need complex organisational structures.
 That two-dimensional spaces (like a pinboard) are underused as organisational metaphors in creative software.
 That most of the usefulness of notetaking doesn’t come from a system, organisation, or references but the act of writing.
 That, ultimately, the goal of the notetaking is to make something for somebody else.
 
 There are many kinds of notes, but these are the kinds of notes I’m hoping to support. The app should help those in creative or knowledge work become better at their jobs.
 Most of these kinds of jobs involve other people.
 Couple that with the need for the high-quality examples required for mastery and the key to a great personal notetaking app is then, paradoxically, other people.
 Other people (the various types of collaboration)
 It can be jarring to listen to people describe the many ways they work with other people. Especially if you’re only used to the limited takes on collaboration that are the norm in the software industry.
 Tech today favours only two approaches to collaborative work:
 
 Multiplayer spaces that are shared in real time (or close to real time).
 A library where everybody sees and contributes to the same shared space.
 
 Both are essentially the same idea: a space with realtime synchronisation and presence signalling; and a space without.
 Examples of the former include Google Docs, Slack, Figma, and Zoom. Their design makes you aware of everybody else in the space, and their actions affect your perception of that space in real time (or close to real time).
 Examples of the latter include Wikipedia, various kinds of knowledge bases, Dropbox and Google Drive, and version control systems. They are all about making it easy for a group of people to work simultaneously in a space but without realtime awareness of each others actions.
 The makers of both kinds of apps often invest a lot of engineering hours into making sure that multiple people can work on the same document or file at the same time without causing conflicts or losing data.
 Other approaches that used to have favour in tech but are now out of fashion, despite their evident popularity among end users:
 
 Forums and discussion groups. Arguably the world’s favourite form of online collaboration, used for everything from coordinating the development of multi-million dollar software projects to a family picnic.
 Email: a federated messaging system where each user gets a personal data repository to manage shared data and messages.
 Folksonomy or collaborative tagging. Helps groups of people organise an ad hoc library of documents, web pages, or files.
 
 Most discussions of how to make collaborative software focus on one or more of these approaches.
 This is like describing a chair in terms of the hammers, saws, and screwdrivers used to make it. Great when talking to other carpenters who want to make another exactly like it. Not so great when you want to find out whether the chair works for sitting.
 We need to have a clear idea of what people are trying to accomplish with collaboration. Knowing the tools we can provide them with as UX designers is not enough.
 A non-exhaustive list of the primary goals of most collaborative office work:
 
 Consensus-building.
 Feedback and review.
 Knowledge sharing.
 Parallel work.
 
 Consensus-building
 The point of most meetings, brainstorming sessions, and workplace discussions isn’t to generate ideas, make decisions, or understand the situation. The point is to build consensus. The ideas or decisions are the most straightforward part of the process. Without consensus, they are all meaningless.
 Constructive examples from the interviews include:
 
 Meeting minutes in Google Docs or Hack.md to make sure that they reflect the group’s consensus on what was said and decided.
 Plan of action proposals that needed to be approved by several people to happen.
 
 For consensus-building, you need everybody to have an overview of what everybody has said, who has participated, and what the overall argument has been.
 Any time you have a group of people together where everybody is aware of everybody else’s presence and status, you tend to automatically end up with a group consensus, even if that wasn’t what you wanted. We are a social species, and we are really good at adjusting our own opinions to match that of the crowd.
 The Bandwagon Effect (and the equal and opposite anti-Bandwagon Effect) is what drives most online social interactions. It’s what tore blogs apart; what makes Twitter a hellsite; and what turned Facebook into the propaganda machine it is today.
 It also tends to lead to bad outcomes in collaborative work.
 Consensus-building is an essential part of work collaboration but it can’t be the only part. The dominance of consensus-building online collaboration tools directly leads to worse decisions.
 Here’s Kathy Sierra again talking about how this phenomenon is described in the book The Wisdom of Crowds (which, paradoxically, talks a lot about how crowds are dumb):
 
 And that’s all great and intuitive… until you get to humans. Humans, he said, demonstrate the opposite principle: more interactions equals dumber behavior. When we come together and interact as a group seeking consensus, we lose sophistication and intelligence. Ants get smarter while we get dumber.
 
 And.
 
 At its simplest form, it means that if you take a bunch of people and ask them (as individuals) to answer a question, the average of each of those individual answers will likely be better than if the group works together to come up with a single answer.
 
 Feedback and review
 Another common scenario is to get feedback or a review of an idea, proposal, or piece of work. You have a thing. You would like to improve said thing. So, you ask a bunch of people what they think, giving more weight to those with relevant expertise. It’s a time-tested strategy.
 The pitfall here is that if the participants are aware of each other’s contributions, they will almost always automatically switch to consensus-building instead of providing their honest feedback. Worst case scenario: the bandwagon effect gathers steam and drives you toward a crap decision.
 These kinds of disasters are a routine occurrence today. You see them in corporations, small and large, in movie studios, at publishing houses, and in governments. A product or project is released and all of a sudden you have a bunch of people in your inbox pointing out glaring, obvious, and fatal flaws that everybody involved had missed.
 To get the best feedback possible from the participants, you need to avoid the mechanisms of consensus-building. You need to ensure that everybody’s responses are kept separate and only visible to those responsible for integrating all of the feedback, something that’s surprisingly difficult to accomplish in modern collaborative software.
 The interviewees all had examples where they needed to get feedback from a specific person on something they did. But only those who were specifically writing or teaching as a part of their job did feedback in a structured way. Most of the examples of feedback or reviews that came up in the interviews were more accurately examples of consensus building as described above as they all involved:
 
 Groups of people.
 Who could see each other’s work.
 And who debated until they all agreed.
 
 That’s just the consensus trap all over again. Which is fine, if that’s what you actually wanted, but counterproductive if needed that feedback.
 Knowledge sharing
 This is the one everybody loves to try to solve because it’s a problem that everybody seems to have. The reasoning is straightforward and compelling:
 
 Most organisations have a lot of documents and data floating around that hardly ever gets revisited or used.
 They all have research, reading, and relevant information collecting dust.
 
 Stuff that should be informing the decisions and strategies of the company. Some of it sits unread in a knowledge base or a wiki. Some of it lies in the drives of individual employees who don’t have a way to share it productively.
 So much knowledge not being applied!
 Except that’s not how we work as human beings. If you haven’t read it, experienced it, and contextualised it, then it isn’t knowledge to you. Knowledge is a quality that people possess, not documents, and the only way to transfer it from one place to another is for people at both ends to apply themselves and make it their own.
 In terms of preserving and transmitting knowledge, most of the systems being used are working about as well as they ever can, and even if you could improve upon them, it’s highly unlikely that anybody would pay you for that improvement as it’s a pizza crust problem, to use a phrase coined by Amy Hoy.
 Using the term “knowledge” for knowledge sharing and knowledge bases is misleading. It’s the term of art, but it’s also wrong enough to lead you to make poor decisions. We should be talking about documentation and references. In other words: a library. At scale, when you’re dealing with a large collection of documents and papers, you’re probably big enough to hire somebody (or a team of somebodies) with an information science degree (or degrees).
 If you’re big enough to have a library where everything gets lost all the time but still too small to hire people who have the relevant expertise to solve the problem, you’re kind of screwed. Search only works up to a point.
 Thankfully, if keep your scope limited (small organisation or single department) there’s this neat thing that digital files can do that saves the day: digital files can be in multiple places at the same time.
 The issue with large shared document libraries at most companies is:
 
 They generally don’t have an organising principle beyond the personal tastes of the employee who volunteered to organise the Google Drive.
 If they do have an organising principle, it isn’t understood by most of those who use it, which leads them to move things around and leave items in the wrong place.
 Almost every time somebody puts a document in a place that makes sense to them, it’s lost to everybody else.
 
 Information science people know how to solve these problems and others that come up, but what if you don’t have one of those hanging around?
 The simplest solution, one that works surprisingly often, is for everybody to maintain their organisation for the document library.
 To use the shelf analogy: if you have a large enough shared bookshelf and you move a book to a place that makes sense to you, then that book is immediately lost to those who knew where it was in the old place.
 If you, on the other hand, have a bookshelf for each employee and unlimited copies of all of the books, then each person can organise their version of the library in a way that they understand and can navigate easily.
 If you ever wondered why every office has at least one person who emails themselves everything—files, PDFs, pictures, whatever—now you know.
 This approach has obvious limitations. It’s crap at helping you find stuff that came up before you started work in the organisation and it doesn’t scale to a large organisation.
 A bookmarking service that supports collaborative tagging (folksonomies) can often fill in that gap.
 Doesn’t always work. Even when it doesn’t, it’s probably still going to be better than the black hole of data that passes as your shared Google Drive or Dropbox.
 Almost everybody I interviewed described knowledge sharing as an ideal they aspired to but none of them practised it.
 Parallel work
 Finally, we get to the other kind of collaboration that modern software is good at (the first being consensus-building, which apps and the web excel at, to a fault).
 Namely, working people down to the bone—I mean, helping people work in parallel.
 With Google Docs, you can get a group of writers to work in parallel on whichever part of the document you want. With Git, a group of developers can be working on separate problems in the same codebase without issue. With Figma, you can have all of your designers working on your design library at the same time and the app will handle it without a hiccup.
 This can be great and this can be awful. The great part is obvious: sometimes you can sic a bunch of people at a problem and they’ll tear it apart. But also sometimes a problem can only be solved by a single person with expertise and others would only get in their way.
 The risk with these tools is that they are also frequently inadvertent consensus engines. Because tools like Google Docs and Figma show the work in a single shared space and make you aware of the actions and presence of others working in said space they end up having the same drive towards consensus as you get in UIs specifically designed to help a group come to a consensus.
 Amazing division of labour; mediocre outcomes.
 In Kathy Sierra’s words:
 
 I’m not dissing teams–our books are all collaborative efforts, and far better because of it. And we consider ourselves to be on a team that includes our publisher O’Reilly. It’s not teams that are the problem, it’s the rabid insistence on teamwork. Group think. Committee decisions.
 Most truly remarkable ideas did not come from teamwork. Most truly brave decisions were not made through teamwork. The team’s role should be to act as a supportive environment for a collection of individuals. People with their own unique voice, ideas, thoughts, perspectives. A team should be there to encourage one another to pursue the wild ass ideas, not get in lock step to keep everything cheery and pleasant.
 
 Everybody I interviewed had experience with parallel work, thanks to Google Docs. They all mentioned having worked on a document or a spreadsheet in parallel with others as a part of their job. The developers all used GitHub as well.
 Putting it all together
 —This sounds impossible! Might as well give up.
 It would be impossible if you tried to solve all of the problems I’ve described with one app. Instead what the research helped me do is narrow down the focus of the app and validate or invalidate some of my assumptions.
 As I described in the first entry in this series, heavy users in the notetaking scene are extraordinarily sensitive to lock-in and interoperability. While I’m hesitant to base the app on markdown—I think the format is a major roadblock for many on the path to master writing and notetaking—I need to make sure the app offers similar interop and compatibility guarantees as you get from a plain text format.
 In the second entry I described the different kinds of notetaking people use. The research helped me decide that I wanted to focus on making a notetaking app for structured creative work, operating on the assumption that knowledge work has more in common with creative work than not. I also reviewed some of the different ways experienced writers commonly structure their notes and research, focusing on two that I called The Box and The Map.
 And in this entry, I’ve been describing how practice and feedback are a vital part of helping people develop skills and expertise.
 Through this work I’ve been finessing the concept of the app and cutting it down to four main spaces or views, each with a specific purpose, all connected through feedback loops.
 
 The Journal
 The Dot Grid
 The Reading Corner
 The Sharing Space
 
 The Journal is your entry point. It ties everything together and prompts you to describe or contextualise the notes and bookmarks that you’ve added, edited, or deleted that day. As I described in my last entry, almost every book on writing or creative work strongly recommends a daily, goalless journal. It needs to be daily (or close to daily) because it’s practice. It needs to be goalless because it’s deliberate practice. You aren’t writing it to meet a deadline or finish a project. It’s there for you to empty your head, so to speak.
 The Dot Grid is The Box. It’s the semi-structured space you use to collect your things.
 Roy Peter Clark uses literal boxes, one for each project, and describes the process in his book Writing Tools in terms of saving string in a box:
 
 To save string, I need a simple file box. I prefer the plastic ones that look like milk crates. I display the box in my office and put a label on it, say “The Plight of Boys.” As soon as I declare my interest in an important topic, a number of things happen. I notice more things about my topic. Then I have conversations about it with my friends and colleagues. They feed my interest. One by one, my box fills with items: an analysis of graduation rates of boys versus girls; a feature on whether video games help or hinder the development of boys; a story about decreasing participation by boys in high school sports. This is a big topic, so I take my time. Weeks and weeks pass, sometimes months and months, and one day I’ll look over at my box and hear it whisper, “It’s time.” I’m amazed at its fullness, and even more astonished at how much I’ve learned just by saving string. (p. 215)
 
 The Reading Corner is where you read and review both the articles you’ve bookmarked and your own notes. It’s important when reviewing other people’s writing and your own that both happen in an identical environment. In theory, it should strengthen your perceptual knowledge to edit your writing in the same space as you read the (presumably) high-quality research that you’ve collected.
 Finally, The Sharing Space is the same as The Reading Corner except it’s specifically for getting feedback from other people. You should be able to take a note or a draft and get independent feedback from each of your reviewers.
 That’s the theory and the concept. This is what I hope Colophon Cards will be.
 
 Our obligation as makers of software, as explained by the collapse of the blogosphere
 In early 2007, a small group of popular bloggers started a site called Mean Kids, a tongue in cheek reference to some of the participants having previously been likened to “the mean kids in high school”. It claimed to be satire that poked fun at some of the people in the blogging world that annoyed them. That included Kathy Sierra (among others, it was a really mean-spirited effort).
 The problem with the Bandwagon Effect and the anti-Bandwagon Effect online is that it leads to escalation. Everything keeps escalating unless you specifically take measures to prevent it, right at the beginning, which this group didn’t (out of ideological opposition to content moderation) until it was too late. The tech industry is filled with well-meaning enablers of abuse and 2007 wasn’t any different.
 These events were well documented at the time. This take, by Joseph Reagle, is as good as any.
 The man who escalated the furthest was Andrew “Weev” Auernheimer who has since confessed several times to not only spreading lies about Kathy Sierra but also publishing her home address and social security number, encouraging people to send her ‘presents’ that showed what they thought of her.
 And they did. Some of them were death threats.
 So, Kathy stopped speaking publicly and stopped blogging. She retreated from the public web while Weev rose to stardom as a ‘hacktivist’. Many people in tech adored him for fighting against unfair laws that ‘stifled’ progress in the industry.
 This is when I lost faith in blogs and online social media. This dynamic, repeated endlessly since then, is why distributed social media like the ‘blogosphere’ collapses; why Twitter is a hellsite; why Facebook is the place that turns Granddad into a nazi; why federated social media like Mastodon breaks apart into smaller isolated clusters.
 The tech industry either doesn’t understand the harm being done by their idea of how social interactions work or it doesn’t care.
 More than any other, this is the event that made me a sceptic of the promises of progress that the software industry is fond of making. It hammered home the obligation the rest of us have, those who do care, in making sure that the software and technology that we work on don’t enable the kinds of abuse and harm that keeps happening with mainstream social and collaborative media.
 No app I make or work on should prioritise collaboration and work over the well-being of others. The obscurity or lack of popularity of my software or yours should not be the only thing that prevents it from being used for abuse.
 
 Kathy Sierra returned to social media and the public web in 2013, joining Twitter as seriouspony and launching a website to promote her upcoming book.
 For a while it seemed to be going well. The book, after all, is amazing. I highly recommend it. But then the crap kept coming. People kept repeating the lies Weev had told about her. He kept being heralded as a hacktivist and an important man in the fight against government overreach. Every time she tried to push back against the lies, she ended up being the one accused of being a troll.
 The tech industry loves its abusive dudes.
 So, she decided to stop. Only a few months after her return, she wrote a post explaining why, republished by Wired as Why the Trolls Will Always Win:
 
 I don’t have the luxury of assuming “it’s just online. Not REAL. It’s not like these people would ever do anything in the real world.” And what you don’t hear much about is what most targeted women find the most frightening of all: the stalkerish energy, time, effort, focus on… YOU. The drive-by hate/threat comment, no matter how vile, is just that, a comment that took someone 2.5 seconds to think and execute. It might be annoying, offensive, maybe intimidating the first few times. But you get used to those, after all, it’s not like somebody put time and effort into it.
 But Photoshopped images? Stories drawn from your own work? There’s a creepy and invasive horror knowing someone is pouring over your words, doing Google and Flickr image searches to find the perfect photo to manipulate. That someone is using their time and talent to write code even, about you. That’s not trolling, that’s obsession. That’s the point where you know it’s not really even about the Koolaid now…they’re obsessed with you.
 This is a very long way from the favorite troll talking point “Oh boohoo someone was mean on the internet.”
 
 Being online wasn’t worth the risk, so she left.
 Since then Weev came out as a Nazi, got himself a swastika tattoo, and more than once has excused or advocated for mass murder while maintaining one of the more prominent online neo-nazi communities.
 Despite this, I haven’t seen a single person who took his side over Kathy Sierra’s recant and apologise. Many instead doubled down, saying that in principle they were correct to defend a nazi’s freedom of speech.
 At most, when you press them, you get a vague apology about how in nerd circles you have to have a tolerance for strange politics. Which is why it was apparently impossible for them to register what sort of person Weev was until it was too late.
 It should worry you that these people, and others with similar priorities and similarly poor judgement, are still so influential in the tech industry.
 And the rest of us need to try to be better.
 The next steps for Colophon Cards
 Next, it’s time to start implementing the prototype proper and start testing actual designs.
 
 The Colophon Cards project is made possible by a grant from The Icelandic Centre for Research’s Technology Development Fund</content>
     </entry>
     <entry>
       <title>On online collaboration and our obligations as makers of software</title>
         <link href="https://www.baldurbjarnason.com/archive/"/>
       <updated>2022-06-02T12:56:06.000Z</updated>
       <content type="text"></content>
     </entry>
     <entry>
       <title>How To Build A Group Chat App With Vanilla JS, Twilio And Node.js</title>
         <link href="https://smashingmagazine.com/2022/06/build-group-chat-app-vanillajs-twilio-nodejs/"/>
       <updated>2022-06-02T09:30:00.000Z</updated>
       <content type="text">Chat is becoming an increasingly popular communication medium in both business and social contexts. Businesses use chat for customer and employee intra-company communication like with Slack, Microsoft Teams, Chanty, HubSpot Live Chat, Help Scout, etc. Most social networks and communication apps also offer chat as an option by default, like on Instagram, Facebook, Reddit, and Twitter. Other apps like Discord, Whatsapp, and Telegram are mostly chat-based, with group chats being one of their main functionalities. 
 While there exist numerous products to facilitate chat, you may need a custom-tailored solution for your site that fits your particular communication needs. For example, many of these products are stand-alone apps and may not be able to integrate within your own site. Having your users leave your website to chat may not be the greatest option as it can affect user experience and conversion. On the flip side, building a chat app from scratch can be a daunting and sometimes overwhelming task. However, by using APIs like Twilio Conversations you can simplify the process of creating them. These communication APIs handle group creation, adding participants, sending messages, notifications, among other important chat functions. Backend apps that use these APIs only have to handle authentication and make calls to these APIs. Front-end apps then display conversations, groups, and messages from the backend. 
 In this tutorial, you will learn how to create a group chat app using the Twilio Conversations API. The front end for this app will be built using HTML, CSS, and Vanilla JavaScript. It will allow users to create group chats, send invites, login, as well as send and receive messages. The backend will be a Node.js app. It will provide authentication tokens for chat invitees and manage chat creation. 
 Prerequisites
 Before you can start this tutorial, you need to have the following:
 
 Node.js installed. You’ll use it primarily for the backend app and to install dependencies in the front-end app.You can get it using a pre-built installer available on the Node.js downloads page.
 A Twilio account.You can create one on the Twilio website at this link.
 http-server to serve the front-end app.You can install it by running npm i -g http-server. You can also run it with npx http-server for one-off runs.
 MongoDB for session storage in the backend app.Its installation page has a detailed guide on how to get it running.
 
 The Backend App
 To send chat messages using Twilio API, you need a conversation. Chat messages are sent and received within a conversation. The people sending the messages are called participants. A participant can only send a message within a conversation if they are added to it. Both conversations and participants are created using the Twilio API. The backend app will perform this function.
 A participant needs an access token to send a message and get their subscribed conversations. The front-end portion of this project will use this access token. The backend app creates the token and sends it to the frontend. There it will be used to load conversations and messages. 
 Project Starter
 You’ll call the backend app twilio-chat-server. A scaffolded project starter for it is available on Github. To clone the project and get the starter, run:
 git clone https://github.com/zaracooper/twilio-chat-server.git
 cd twilio-chat-server
 git checkout starter
 
 The backend app takes this structure:
 .
 ├── app.js
 ├── config/
 ├── controllers/
 ├── package.json
 ├── routes/
 └── utils/
 
 To run the app, you’ll use the node index.js command. 
 Dependencies
 The backend app needs 8 dependencies. You can install them by running:
 npm i 
 
 Here’s a list of each of the dependencies:
 
 connect-mongo connects to MongoDB, which you’ll use as a session store;
 cors handles CORS;
 dotenv loads environment variables from the .env file that you will create in a later step;
 express is the web framework you’ll use for the backend;
 express-session provides middleware to handle session data;
 http-errors helps create server errors;
 morgan handles logging;
 twilio creates the Twilio client, generates tokens, creates conversations, and adds participants.
 
 Configuration
 The config folder is responsible for loading configuration from environment variables. The configuration is grouped into three categories: configuration for CORS, Twilio, and the MongoDB session DB. When the environment is development, you will load config from the .env file using dotenv.
 Start by creating the .env file on the terminal. This file is already added to the .gitignore file to prevent the sensitive values it contains from being checked into the repository.
 touch .env
 
 Here’s what your .env should look like:
 # Session DB Config
 SESSION_DB_HOST&#x3D;XXXX
 SESSION_DB_USER&#x3D;XXXX
 SESSION_DB_PASS&#x3D;XXXX
 SESSION_DB_PORT&#x3D;XXXX
 SESSION_DB_NAME&#x3D;XXXX
 SESSION_DB_SECRET&#x3D;XXXX
 
 # Twilio Config
 TWILIO_ACCOUNT_SID&#x3D;XXXX
 TWILIO_AUTH_TOKEN&#x3D;XXXX
 TWILIO_API_KEY&#x3D;XXXX
 TWILIO_API_SECRET&#x3D;XXXX
 
 # CORS Client Config
 CORS_CLIENT_DOMAIN&#x3D;XXXX
 
 You can learn how to create a user for your session DB from this MongoDB manual entry. Once you create a session database and a user who can write to it, you can fill the SESSION_DB_USER, SESSION_DB_PASS, and SESSION_DB_NAME values. If you’re running a local instance of MongoDB, the SESSION_DB_HOST would be localhost, and the SESSION_DB_PORT usually is 27017. The SESSION_DB_SECRET is used by express-session to sign the session ID cookie, and it can be any secret string you set. 
 In the next step, you will get credentials from the Twilio Console. The credentials should be assigned to the variables with the TWILIO_ prefix. During local development, the front-end client will run on http://localhost:3000. So, you can use this value for the CORS_CLIENT_DOMAIN environment variable.   
 Add the following code to config/index.js to load environment variables. 
 import dotenv from &#x27;dotenv&#x27;;
 
 if (process.env.NODE_ENV &#x3D;&#x3D; &#x27;development&#x27;) {
     dotenv.config();
 }
 
 const corsClient &#x3D; {
     domain: process.env.CORS_CLIENT_DOMAIN
 };
 
 const sessionDB &#x3D; {
     host: process.env.SESSION_DB_HOST,
     user: process.env.SESSION_DB_USER,
     pass: process.env.SESSION_DB_PASS,
     port: process.env.SESSION_DB_PORT,
     name: process.env.SESSION_DB_NAME,
     secret: process.env.SESSION_DB_SECRET
 };
 
 const twilioConfig &#x3D; {
     accountSid: process.env.TWILIO_ACCOUNT_SID,
     authToken: process.env.TWILIO_AUTH_TOKEN,
     apiKey: process.env.TWILIO_API_KEY,
     apiSecret: process.env.TWILIO_API_SECRET
 };
 
 const port &#x3D; process.env.PORT || &#x27;8000&#x27;;
 
 export { corsClient, port, sessionDB, twilioConfig };
 
 The environment variables are grouped into categories based on what they do. Each of the configuration categories has its own object variable, and they are all exported for use in other parts of the app.
 Getting Twilio Credentials From the Console
 To build this project, you’ll need four different Twilio credentials: an Account SID, an Auth Token, an API key, and an API secret. In the console, on the General Settings page, scroll down to the API Credentials section. This is where you will find your Account SID and Auth Token. 
 
 To get an API Key and Secret, go to the API Keys page. You can see it in the screenshot below. Click the + button to go to the New API Key page. 
 
 On this page, add a key name and leave the KEY TYPE as Standard, then click Create API Key. Copy the API key and secret. You will add all these credentials in a .env file as you shall see in subsequent steps.  
 
 Utils
 The backend app needs two utility functions. One will create a token, and the other will wrap async controllers and handle errors for them.
 In utils/token.js, add the following code to create a function called createToken that will generate Twilio access tokens:
 import { twilioConfig } from &#x27;../config/index.js&#x27;;
 import twilio from &#x27;twilio&#x27;;
 
 function createToken(username, serviceSid) {
     const AccessToken &#x3D; twilio.jwt.AccessToken;
     const ChatGrant &#x3D; AccessToken.ChatGrant;
 
     const token &#x3D; new AccessToken(
         twilioConfig.accountSid,
         twilioConfig.apiKey,
         twilioConfig.apiSecret,
         { identity: username }
     );
 
     const chatGrant &#x3D; new ChatGrant({
         serviceSid: serviceSid,
     });
 
     token.addGrant(chatGrant);
 
     return token.toJwt();
 }
 
 In this function, you generate access tokens using your Account SID, API key, and API secret. You can optionally supply a unique identity which could be a username, email, etc. After creating a token, you have to add a chat grant to it. The chat grant can take a conversation service ID among other optional values. Lastly, you’ll convert the token to a JWT and return it. 
 The utils/controller.js file contains an asyncWrapper function that wraps async controller functions and catches any errors they throw. Paste the following code into this file:
 
 function asyncWrapper(controller) {
     return (req, res, next) &#x3D;&gt; Promise.resolve(controller(req, res, next)).catch(next);
 }
 
 export { asyncWrapper, createToken };
 
 
 Controllers
 The backend app has four controllers: two for authentication and two for handling conversations. The first auth controller creates a token, and the second deletes it. One of the conversations controllers creates new conversations, while the other adds participants to existing conversations. 
 Conversation Controllers
 In the controllers/conversations.js file, add these imports and code for the StartConversation controller:
 
 import { twilioConfig } from &#x27;../config/index.js&#x27;;
 import { createToken } from &#x27;../utils/token.js&#x27;;
 import twilio from &#x27;twilio&#x27;;
 
 async function StartConversation(req, res, next) {
     const client &#x3D; twilio(twilioConfig.accountSid, twilioConfig.authToken);
 
     const { conversationTitle, username } &#x3D; req.body;
 
     try {
         if (conversationTitle &amp;&amp; username) {
             const conversation &#x3D; await client.conversations.conversations
                 .create({ friendlyName: conversationTitle });
 
             req.session.token &#x3D; createToken(username, conversation.chatServiceSid);
             req.session.username &#x3D; username;
 
             const participant &#x3D; await client.conversations.conversations(conversation.sid)
                 .participants.create({ identity: username })
 
             res.send({ conversation, participant });
         } else {
             next({ message: &#x27;Missing conversation title or username&#x27; });
         }
     }
     catch (error) {
         next({ error, message: &#x27;There was a problem creating your conversation&#x27; });
     }
 }
 
 
 The StartConversation controller first creates a Twilio client using your twilioConfig.accountSid and twilioConfig.authToken which you get from config/index.js. 
 Next, it creates a conversation. It needs a conversation title for this, which it gets from the request body. A user has to be added to a conversation before they can participate in it. A participant cannot send a message without an access token. So, it generates an access token using the username provided in the request body and the conversation.chatServiceSid. Then the user identified by the username is added to the conversation. The controller completes by responding with the newly created conversation and participant. 
 Next, you need to create the AddParticipant controller. To do this, add the following code below what you just added in the controllers/conversations.js file above:
 
 async function AddParticipant(req, res, next) {
     const client &#x3D; twilio(twilioConfig.accountSid, twilioConfig.authToken);
 
     const { username } &#x3D; req.body;
     const conversationSid &#x3D; req.params.id;
 
     try {
         const conversation &#x3D; await client.conversations.conversations
             .get(conversationSid).fetch();
 
         if (username &amp;&amp; conversationSid) {
             req.session.token &#x3D; createToken(username, conversation.chatServiceSid);
             req.session.username &#x3D; username;
 
             const participant &#x3D; await client.conversations.conversations(conversationSid)
                 .participants.create({ identity: username })
 
             res.send({ conversation, participant });
         } else {
             next({ message: &#x27;Missing username or conversation Sid&#x27; });
         }
     } catch (error) {
         next({ error, message: &#x27;There was a problem adding a participant&#x27; });
     }
 }
 
 export { AddParticipant, StartConversation };
 
 
 The AddParticipant controller adds new participants to already existing conversations. Using the conversationSid provided as a route parameter, it fetches the conversation. It then creates a token for the user and adds them to the conversation using their username from the request body. Lastly, it sends the conversation and participant as a response. 
 Auth Controllers
 The two controllers in controllers/auth.js are called GetToken and DeleteToken. Add them to the file by copying and pasting this code:
 
 function GetToken(req, res, next) {
     if (req.session.token) {
         res.send({ token: req.session.token, username: req.session.username });
     } else {
         next({ status: 404, message: &#x27;Token not set&#x27; });
     }
 }
 
 function DeleteToken(req, res, _next) {
     delete req.session.token;
     delete req.session.username;
 
     res.send({ message: &#x27;Session destroyed&#x27; });
 }
 
 export { DeleteToken, GetToken };
 
 
 The GetToken controller retrieves the token and username from the session if they exist and returns them as a response. DeleteToken deletes the session.
 Routes
 The routes folder has three files: index.js, conversations.js, and auth.js.  
 Add these auth routes to the routes/auth.js file by adding this code:
 
 import { Router } from &#x27;express&#x27;;
 
 import { DeleteToken, GetToken } from &#x27;../controllers/auth.js&#x27;;
 
 var router &#x3D; Router();
 
 router.get(&#x27;/&#x27;, GetToken);
 router.delete(&#x27;/&#x27;, DeleteToken);
 
 export default router;
 
 
 The GET route at the / path returns a token while the DELETE route deletes a token.
 Next, copy and paste the following code to the routes/conversations.js file:
 
 import { Router } from &#x27;express&#x27;;
 import { AddParticipant, StartConversation } from &#x27;../controllers/conversations.js&#x27;;
 import { asyncWrapper } from &#x27;../utils/controller.js&#x27;;
 
 var router &#x3D; Router();
 
 router.post(&#x27;/&#x27;, asyncWrapper(StartConversation));
 router.post(&#x27;/:id/participants&#x27;, asyncWrapper(AddParticipant));
 
 export default router;
 
 
 In this file, the conversations router is created. A POST route for creating conversations with the path / and another POST route for adding participants with the path /:id/participants are added to the router. 
 Lastly, add the following code to your new routes/index.js file. 
 import { Router } from &#x27;express&#x27;;
 
 import authRouter from &#x27;./auth.js&#x27;;
 import conversationRouter from &#x27;./conversations.js&#x27;;
 
 var router &#x3D; Router();
 
 router.use(&#x27;/auth/token&#x27;, authRouter);
 router.use(&#x27;/api/conversations&#x27;, conversationRouter);
 
 export default router;
 
 By adding the conversation and auth routers here, you are making them available at /api/conversations and /auth/token to the main router respectively. The router is then exported. 
 The Backend App
 Now it’s time to put the backend pieces together. Open the index.js file in your text editor and paste in the following code:
 
 import cors from &#x27;cors&#x27;;
 import createError from &#x27;http-errors&#x27;;
 import express, { json, urlencoded } from &#x27;express&#x27;;
 import logger from &#x27;morgan&#x27;;
 import session from &#x27;express-session&#x27;;
 import store from &#x27;connect-mongo&#x27;;
 
 import { corsClient, port, sessionDB } from &#x27;./config/index.js&#x27;;
 
 import router from &#x27;./routes/index.js&#x27;;
 
 var app &#x3D; express();
 
 app.use(logger(&#x27;dev&#x27;));
 app.use(json());
 app.use(urlencoded({ extended: false }));
 
 app.use(cors({
     origin: corsClient.domain,
     credentials: true,
     methods: [&#x27;GET&#x27;, &#x27;POST&#x27;, &#x27;DELETE&#x27;],
     maxAge: 3600 * 1000,
     allowedHeaders: [&#x27;Content-Type&#x27;, &#x27;Range&#x27;],
     exposedHeaders: [&#x27;Accept-Ranges&#x27;, &#x27;Content-Encoding&#x27;, &#x27;Content-Length&#x27;, &#x27;Content-Range&#x27;]
 }));
 app.options(&#x27;*&#x27;, cors());
 
 app.use(session({
     store: store.create({
         mongoUrl: mongodb://${sessionDB.user}:${sessionDB.pass}@${sessionDB.host}:${sessionDB.port}/${sessionDB.name},
         mongoOptions: { useUnifiedTopology: true },
         collectionName: &#x27;sessions&#x27;
     }),
     secret: sessionDB.secret,
     cookie: {
         maxAge: 3600 * 1000,
         sameSite: &#x27;strict&#x27;
     },
     name: &#x27;twilio.sid&#x27;,
     resave: false,
     saveUninitialized: true
 }));
 
 app.use(&#x27;/&#x27;, router);
 
 app.use(function (_req, _res, next) {
     next(createError(404, &#x27;Route does not exist.&#x27;));
 });
 
 app.use(function (err, _req, res, _next) {
     res.status(err.status || 500).send(err);
 });
 
 app.listen(port);
 
 
 This file starts off by creating the express app. It then sets up JSON and URL-encoded payload parsing and adds the logging middleware. Next, it sets up CORS and the session handling. As mentioned earlier, MongoDB is used as the session store.
 After all that is set up, it then adds the router created in the earlier step before configuring error handling. Lastly, it makes the app listen to and accept connections at the port specified in the .env file. If you haven’t set the port, the app will listen on port 8000.
 Once you’re finished creating the backend app, make sure MongoDB is running and start it by running this command on the terminal:
 NODE_ENV&#x3D;development npm start
 
 You pass the NODE_ENV&#x3D;development variable, so that configuration is loaded from the local .env file. 
 The Front-end
 The front-end portion of this project serves a couple of functions. It allows users to create conversations, see the list of conversations they are a part of, invite others to conversations they created, and send messages within conversations. These roles are achieved by four pages: 
 
 a conversations page,
 a chat page,
 an error page,
 a login page.
 
 You’ll call the front-end app twilio-chat-app. A scaffolded starter exists for it on Github. To clone the project and get the starter, run:
 
 git clone https://github.com/zaracooper/twilio-vanilla-js-chat-app.git
 cd twilio-vanilla-js-chat-app
 git checkout starter
 
 
 The app takes this structure:
 .
 ├── index.html
 ├── pages
 │   ├── chat.html
 │   ├── conversation.html
 │   ├── error.html
 │   └── login.html
 ├── scripts
 │   ├── chat.js
 │   ├── conversation.js
 │   └── login.js
 └── styles
     ├── chat.css
     ├── main.css
     └── simple-page.css
 
 
 The styling and HTML markup have already been added for each of the pages in the starter. This section will only cover the scripts you have to add. 
 Dependencies
 The app has two dependencies: axios and @twilio/conversations. You’ll use axios to make requests to the backend app and @twilio/conversations to send and fetch messages and conversations in scripts. You can install them on the terminal by running: 
 npm i
 
 The Index Page
 This page serves as a landing page for the app. You can find the markup for this page (index.html) here. It uses two CSS stylesheets: styles/main.css which all pages use and styles/simple-page.css which smaller, less complicated pages use.
 You can find the contents of these stylesheets linked in the earlier paragraph. Here is a screenshot of what this page will look like:
 
 The Error Page
 This page is shown when an error occurs. The contents of pages/error.html can be found here. If an error occurs, a user can click the button to go to the home page. There, they can try what they were attempting again. 
 
 The Conversations Page
 On this page, a user provides the title of a conversation to be created and their username to a form. 
 The contents of pages/conversation.html can be found here. Add the following code to the scripts/conversation.js file:
 window.twilioChat &#x3D; window.twilioChat || {};
 
 function createConversation() {
     let convoForm &#x3D; document.getElementById(&#x27;convoForm&#x27;);
     let formData &#x3D; new FormData(convoForm);
 
     let body &#x3D; Object.fromEntries(formData.entries()) || {};
 
     let submitBtn &#x3D; document.getElementById(&#x27;submitConvo&#x27;);
     submitBtn.innerText &#x3D; &quot;Creating...&quot;
     submitBtn.disabled &#x3D; true;
     submitBtn.style.cursor &#x3D; &#x27;wait&#x27;;
 
     axios.request({
         url: &#x27;/api/conversations&#x27;,
         baseURL: &#x27;http://localhost:8000&#x27;,
         method: &#x27;post&#x27;,
         withCredentials: true,
         data: body
     })
         .then(() &#x3D;&gt; {
             window.twilioChat.username &#x3D; body.username;
             location.href &#x3D; &#x27;/pages/chat.html&#x27;;
         })
         .catch(() &#x3D;&gt; {
             location.href &#x3D; &#x27;/pages/error.html&#x27;;
         });
 }
 
 When a user clicks the Submit button, the createConversation function is called. In it, the contents of the form are collected and used in the body of a POST request made to http://localhost:8000/api/conversations/ in the backend. 
 You will use axios to make the request. If the request is successful, a conversation is created and the user is added to it. The user will then be redirected to the chat page where they can send messages in the conversation. 
 Below is a screenshot of the conversations page:
 
 The Chat Page
 On this page, a user will view a list of conversations they are part of and send messages to them. You can find the markup for pages/chat.html here and the styling for styles/chat.css here. 
 The scripts/chat.js file starts out by defining a namespace twilioDemo. 
 window.twilioChat &#x3D; window.twilioChat || {};
 
 Add the initClient function below. It is responsible for initializing the Twilio client and loading conversations.
 
 async function initClient() {
     try {
         const response &#x3D; await axios.request({
             url: &#x27;/auth/token&#x27;,
             baseURL: &#x27;http://localhost:8000&#x27;,
             method: &#x27;GETget&#x27;,
             withCredentials: true
         });
 
         window.twilioChat.username &#x3D; response.data.username;
         window.twilioChat.client &#x3D; await Twilio.Conversations.Client.create(response.data.token);
 
         let conversations &#x3D; await window.twilioChat.client.getSubscribedConversations();
 
         let conversationCont, conversationName;
 
         const sideNav &#x3D; document.getElementById(&#x27;side-nav&#x27;);
         sideNav.removeChild(document.getElementById(&#x27;loading-msg&#x27;));
 
         for (let conv of conversations.items) {
             conversationCont &#x3D; document.createElement(&#x27;button&#x27;);
             conversationCont.classList.add(&#x27;conversation&#x27;);
             conversationCont.id &#x3D; conv.sid;
             conversationCont.value &#x3D; conv.sid;
             conversationCont.onclick &#x3D; async () &#x3D;&gt; {
                 await setConversation(conv.sid, conv.channelState.friendlyName);
             };
 
             conversationName &#x3D; document.createElement(&#x27;h3&#x27;);
             conversationName.innerText &#x3D; 💬 ${conv.channelState.friendlyName};
 
             conversationCont.appendChild(conversationName);
             sideNav.appendChild(conversationCont);
         }
     }
     catch {
         location.href &#x3D; &#x27;/pages/error.html&#x27;;
     }
 };
 
 
 When the page loads, initClient fetches the user’s access token from the backend, then uses it to initialise the client. Once the client is initialised, it’s used to fetch all the conversations the user is subscribed to. After that, the conversations are loaded onto the side-nav. In case any error occurs, the user is sent to the error page.
 The setConversion function loads a single conversation. Copy and paste the code below in the file to add it:
 
 async function setConversation(sid, name) {
     try {
         window.twilioChat.selectedConvSid &#x3D; sid;
 
         document.getElementById(&#x27;chat-title&#x27;).innerText &#x3D; &#x27;+ &#x27; + name;
 
         document.getElementById(&#x27;loading-chat&#x27;).style.display &#x3D; &#x27;flex&#x27;;
         document.getElementById(&#x27;messages&#x27;).style.display &#x3D; &#x27;none&#x27;;
 
         let submitButton &#x3D; document.getElementById(&#x27;submitMessage&#x27;)
         submitButton.disabled &#x3D; true;
 
         let inviteButton &#x3D; document.getElementById(&#x27;invite-button&#x27;)
         inviteButton.disabled &#x3D; true;
 
         window.twilioChat.selectedConversation &#x3D; await window.twilioChat.client.getConversationBySid(window.twilioChat.selectedConvSid);
 
         const messages &#x3D; await window.twilioChat.selectedConversation.getMessages();
 
         addMessagesToChatArea(messages.items, true);
 
         window.twilioChat.selectedConversation.on(&#x27;messageAdded&#x27;, msg &#x3D;&gt; addMessagesToChatArea([msg], false));
 
         submitButton.disabled &#x3D; false;
         inviteButton.disabled &#x3D; false;
     } catch {
         showError(&#x27;loading the conversation you selected&#x27;);
     }
 };
 
 
 When a user clicks on a particular conversation, setConversation is called. This function receives the conversation SID and name and uses the SID to fetch the conversation and its messages. The messages are then added to the chat area. Lastly, a listener is added to watch for new messages added to the conversation. These new messages are appended to the chat area when they are received. In case any errors occur, an error message is displayed.
 This is a screenshot of the chat page:
 
 Next, you’ll add the addMessagedToChatArea function which loads conversation messages.
 function addMessagesToChatArea(messages, clearMessages) {
     let cont, msgCont, msgAuthor, timestamp;
 
     const chatArea &#x3D; document.getElementById(&#x27;messages&#x27;);
 
     if (clearMessages) {
         document.getElementById(&#x27;loading-chat&#x27;).style.display &#x3D; &#x27;none&#x27;;
         chatArea.style.display &#x3D; &#x27;flex&#x27;;
         chatArea.replaceChildren();
     }
 
     for (const msg of messages) {
         cont &#x3D; document.createElement(&#x27;div&#x27;);
         if (msg.state.author &#x3D;&#x3D; window.twilioChat.username) {
             cont.classList.add(&#x27;right-message&#x27;);
         } else {
             cont.classList.add(&#x27;left-message&#x27;);
         }
 
         msgCont &#x3D; document.createElement(&#x27;div&#x27;);
         msgCont.classList.add(&#x27;message&#x27;);
 
         msgAuthor &#x3D; document.createElement(&#x27;p&#x27;);
         msgAuthor.classList.add(&#x27;username&#x27;);
         msgAuthor.innerText &#x3D; msg.state.author;
 
         timestamp &#x3D; document.createElement(&#x27;p&#x27;);
         timestamp.classList.add(&#x27;timestamp&#x27;);
         timestamp.innerText &#x3D; msg.state.timestamp;
 
         msgCont.appendChild(msgAuthor);
         msgCont.innerText +&#x3D; msg.state.body;
 
         cont.appendChild(msgCont);
         cont.appendChild(timestamp);
 
         chatArea.appendChild(cont);
     }
 
     chatArea.scrollTop &#x3D; chatArea.scrollHeight;
 }
 
 The function addMessagesToChatArea adds messages of the current conversation to the chat area when it is selected from the side nav. It is also called when new messages are added to the current conversation. A loading message is usually displayed as the messages are being fetched. Before the conversation messages are added, this loading message is removed. Messages from the current user are aligned to the right, while all other messages from group participants are aligned to the left.
 This is what the loading message looks like:
 
 Add the sendMessage function to send messages:
 
 function sendMessage() {
     let submitBtn &#x3D; document.getElementById(&#x27;submitMessage&#x27;);
     submitBtn.disabled &#x3D; true;
 
     let messageForm &#x3D; document.getElementById(&#x27;message-input&#x27;);
     let messageData &#x3D; new FormData(messageForm);
 
     const msg &#x3D; messageData.get(&#x27;chat-message&#x27;);
 
     window.twilioChat.selectedConversation.sendMessage(msg)
         .then(() &#x3D;&gt; {
             document.getElementById(&#x27;chat-message&#x27;).value &#x3D; &#x27;&#x27;;
             submitBtn.disabled &#x3D; false;
         })
         .catch(() &#x3D;&gt; {
             showError(&#x27;sending your message&#x27;);
             submitBtn.disabled &#x3D; false;
         });
 };
 
 
 When the user sends a message, the sendMessage function is called. It gets the message text from the text area and disables the submit button. Then using the currently selected conversation, the message is sent using its sendMessage method. If successful, the text area is cleared and the submit button is re-enabled. If unsuccessful, an error message is displayed instead. 
 The showError method displays an error message when it is called; hideError hides it.
 
 function showError(msg) {
     document.getElementById(&#x27;error-message&#x27;).style.display &#x3D; &#x27;flex&#x27;;
     document.getElementById(&#x27;error-text&#x27;).innerText &#x3D; There was a problem ${msg ? msg : &#x27;fulfilling your request&#x27;}.;
 }
 
 function hideError() {
     document.getElementById(&#x27;error-message&#x27;).style.display &#x3D; &#x27;none&#x27;;
 }
 
 
 This is what this error message will look like:
 
 The logout function logouts out the current user. It does this by making a request to the backend which clears their session. The user is then redirected to the conversation page, so they can create a new conversation if they’d like.
 function logout(logoutButton) {
     logoutButton.disabled &#x3D; true;
     logoutButton.style.cursor &#x3D; &#x27;wait&#x27;;
 
     axios.request({
         url: &#x27;/auth/token&#x27;,
         baseURL: &#x27;http://localhost:8000&#x27;,
         method: &#x27;DELETEdelete&#x27;,
         withCredentials: true
     })
         .then(() &#x3D;&gt; {
             location.href &#x3D; &#x27;/pages/conversation.html&#x27;;
         })
         .catch(() &#x3D;&gt; {
             location.href &#x3D; &#x27;/pages/error.html&#x27;;
         });
 }
 
 Add the inviteFriend function to send conversation invites:
 
 async function inviteFriend() {
     try {
         const link &#x3D; http://localhost:3000/pages/login.html?sid&#x3D;${window.twilioChat.selectedConvSid};
 
         await navigator.clipboard.writeText(link);
 
         alert(The link below has been copied to your clipboard.\n\n${link}\n\nYou can invite a friend to chat by sending it to them.);
     } catch {
         showError(&#x27;preparing your chat invite&#x27;);
     }
 }
 
 
 To invite other people to participate in the conversation, the current user can send another person a link. This link is to the login page and contains the current conversation SID as a query parameter. When they click the invite button, the link is added to their clipboard. An alert is then displayed giving invite instructions. 
 Here is a screenshot of the invite alert:
 
 The Login Page
 On this page, a user logs in when they are invited to a conversation. You can find the markup for pages/login.html at this link. 
 In scripts/login.js, the login function is responsible for logging in conversation invitees. Copy its code below and add it to the aforementioned file:
 
 function login() {
     const convParams &#x3D; new URLSearchParams(window.location.search);
     const conv &#x3D; Object.fromEntries(convParams.entries());
 
     if (conv.sid) {
         let submitBtn &#x3D; document.getElementById(&#x27;login-button&#x27;);
         submitBtn.innerText &#x3D; &#x27;Logging in...&#x27;;
         submitBtn.disabled &#x3D; true;
         submitBtn.style.cursor &#x3D; &#x27;wait&#x27;;
 
         let loginForm &#x3D; document.getElementById(&#x27;loginForm&#x27;);
         let formData &#x3D; new FormData(loginForm);
         let body &#x3D; Object.fromEntries(formData.entries());
 
         axios.request({
             url: /api/conversations/${conv.sid}/participants,
             baseURL: &#x27;http://localhost:8000&#x27;,
             method: &#x27;POSTpost&#x27;,
             withCredentials: true,
             data: body
         })
             .then(() &#x3D;&gt; {
                 location.href &#x3D; &#x27;/pages/chat.html&#x27;;
             })
             .catch(() &#x3D;&gt; {
                 location.href &#x3D; &#x27;/pages/error.html&#x27;;
             });
     } else {
         location.href &#x3D; &#x27;/pages/conversation.html&#x27;;
     }
 }
 
 
 The login function takes the conversation sid query parameter from the URL and the username from the form. It then makes a POST request to api/conversations/{sid}/participants/ on the backend app. The backend app adds the user to the conversation and generates an access token for messaging. If successful, a session is started in the backend for the user. 
 The user is then redirected to the chat page, but if the request returns an error, they are redirected to the error page. If there is no conversation sid query parameter in the URL, the user is redirected to the conversation page. 
 Below is a screenshot of the login page:
 
 Running the App
 Before you can start the front-end app, make sure that the backend app is running. As mentioned earlier, you can start the backend app using this command on the terminal:
 NODE_ENV&#x3D;development npm start
 
 To serve the front-end app, run this command in a different terminal window:
 http-server -p 3000
 
 This serves the app at http://localhost:3000. Once it’s running, head on over to http://localhost:3000/pages/conversation.html; set a name for your conversation and add your username, then create it. When you get to the chat page, click on the conversation, then click the Invite button. 
 In a separate incognito window, paste the invite link and put a different username. Once you’re on the chat page in the incognito window, you can begin chatting with yourself. You can send messages back and forth between the user in the first window and the second user in the incognito window in the same conversation. 
 Conclusion
 In this tutorial, you learned how to create a chat app using Twilio Conversations and Vanilla JS. You created a Node.js app that generates user access tokens, maintains a session for them, creates conversations, and adds users to them as participants. You also created a front-end app using HTML, CSS, and Vanilla JS. This app should allow users to create conversations, send messages, and invite other people to chat. It should get access tokens from the backend app and use them to perform these functions. I hope this tutorial gave you a better understanding of how Twilio Conversations works and how to use it for chat messaging.
 To find out more about Twilio Conversations and what else you could do with it, check out its documentation linked here. You can also find the source code for the backend app on Github here, and the code for the front-end app here.</content>
     </entry>
     <entry>
       <title>How to Create Block Theme Patterns in WordPress 6.0</title>
         <link href="https://css-tricks.com/how-to-create-block-theme-patterns-in-wordpress-6-0/"/>
       <updated>2022-06-01T15:30:15.000Z</updated>
       <content type="text">Block patterns, also frequently referred to as sections, were introduced in WordPress 5.5 to allow users to build and share predefined block layouts in the pattern directory. The directory is the home of a wide range of curated patterns designed by the WordPress community. These patterns are available in simple copy and paste format, require no coding knowledge and thus are a big time saver for users.
 
 
 
 Despite many articles on patterns, there is a lack of comprehensive and up-to-date articles on pattern creation covering the latest enhanced features. This article aims to fill the gap with a focus on the recent enhanced features like creating patterns without registration and offer an up-to-date step-by-step guide to create and use them in block themes for novices and experienced authors.
 
 
 
 
 
 
 
 Since the launch of WordPress 5.9 and the Twenty Twenty-Two (TT2) default theme, the use of block patterns in block themes has proliferated. I have been a big fan of block patterns and have created and used them in my block themes.
 
 
 
 The new WordPress 6.0 offers three major patterns feature enhancements to authors:
 
 
 
 Allowing pattern registration through /patterns folder (similar to /parts, /templates, and /styles registration).Registering patterns from the public patterns directory using the theme.json.Adding patterns that can be offered to the user when creating a new page.
 
 
 
 In an introductory Exploring WordPress 6.0 video, Automattic product liaison Ann McCathy highlights some newly enhanced patterns features (starting at 15:00) and discusses future patterns enhancement plans — which include patterns as sectioning elements, providing options to pick pattern on page creation, integrating pattern directory search, and more.
 
 
 
 
 
 
 
 
 
 Prerequisites
 
 
 
 The article assumes that readers have basic knowledge of WordPress full site editing (FSE) interface and block themes. The Block Editor Handbook and Full Site Editing website provide the most up-to-date tutorial guides to learn all FSE features, including block themes and patterns discussed in this article.
 
 
 
 Section 1: Evolving approaches to creating block patterns
 
 
 
 The initial approach to creating block patterns required the use of block pattern API either as a custom plugin or directly registered in the functions.php file to bundle with a block theme. The newly launched WordPress 6.0 introduced several new and enhanced features working with patterns and themes, including pattern registration via a /patterns folder and a page creation pattern modal.
 
 
 
 For background, let’s first briefly overview how the pattern registration workflow evolved from using the register pattern API to directly loading without registration.
 
 
 
 Use case example 1: Twenty Twenty-One
 
 
 
 The default Twenty Twenty-One theme (TT1) and TT1 Blocks theme (a sibling of TT1) showcase how block patterns can be registered in the theme’s functions.php. In the TT1 Blocks experimental-theme, this single block-pattern.php file containing eight block patterns is added to the functions.php as an include as shown here.
 
 
 
 A custom block pattern needs to be registered using the register_block_pattern function, which receives two arguments — title (name of the patterns) and properties (an array describing properties of the pattern).
 
 
 
 Here is an example of registering a simple “Hello World” paragraph pattern from this Theme Shaper article:
 
 
 
 register_block_pattern(
     &#x27;my-plugin/hello-world&#x27;,
     array(
         &#x27;title&#x27;   &#x3D;&gt; __( &#x27;Hello World&#x27;, &#x27;my-plugin&#x27; ),
         &#x27;content&#x27; &#x3D;&gt; &quot;&lt;!-- wp:paragraph --&gt;\n&lt;p&gt;Hello World&lt;/p&gt;\n&lt;!-- /wp:paragraph --&gt;&quot;,
     )
 );
 
 
 
 After registration, the register_block_pattern() function should be called from a handler attached to the init hook as described here.
 
 
 
  function my_plugin_register_my_patterns() {
     register_block_pattern( ... );
   }
 
   add_action( &#x27;init&#x27;, &#x27;my_plugin_register_my_patterns&#x27; );
 
 
 
 Once block patterns are registered they are visible in the block editor. More detailed documentation is found in this Block Pattern Registration.
 
 
 
 Block pattern properties
 
 
 
 In addition to required title and content properties, the block editor handbook lists the following optional pattern properties:
 
 
 
 title (required): A human-readable title for the pattern.content (required): Block HTML Markup for the pattern.description (optional): A visually hidden text used to describe the pattern in the inserter. A description is optional but it is strongly encouraged when the title does not fully describe what the pattern does. The description will help users discover the pattern while searching.categories (optional): An array of registered pattern categories used to group block patterns. Block patterns can be shown on multiple categories. A category must be registered separately in order to be used here.keywords (optional): An array of aliases or keywords that help users discover the pattern while searching.viewportWidth (optional): An integer specifying the intended width of the pattern to allow for a scaled preview of the pattern in the inserter.blockTypes (optional): An array of block types that the pattern is intended to be used with. Each value needs to be the declared block’s name.inserter (optional): By default, all patterns will appear in the inserter. To hide a pattern so that it can only be inserted programmatically, set the inserter to false.
 
 
 
 The following is an example of a quote pattern plugin code snippets taken from the WordPress blog.
 
 
 
 /*
 Plugin Name: Quote Pattern Example Plugin
 */
 
 register_block_pattern(
     &#x27;my-plugin/my-quote-pattern&#x27;,
      array(
       &#x27;title&#x27;       &#x3D;&gt; __( &#x27;Quote with Avatar&#x27;, &#x27;my-plugin&#x27; ),
       &#x27;categories&#x27;  &#x3D;&gt; array( &#x27;text&#x27; ),
       &#x27;description&#x27; &#x3D;&gt; _x( &#x27;A big quote with an avatar&quot;.&#x27;, &#x27;Block pattern description&#x27;, &#x27;my-plugin&#x27; ),
       &#x27;content&#x27;     &#x3D;&gt; &#x27;&lt;!-- wp:group --&gt;&lt;div class&#x3D;&quot;wp-block-group&quot;&gt;&lt;div class&#x3D;&quot;wp-block-group__inner-container&quot;&gt;&lt;!-- wp:separator {&quot;className&quot;:&quot;is-style-default&quot;} --&gt;&lt;hr class&#x3D;&quot;wp-block-separator is-style-default&quot;/&gt;&lt;!-- /wp:separator --&gt;&lt;!-- wp:image {&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:553,&quot;width&quot;:150,&quot;height&quot;:150,&quot;sizeSlug&quot;:&quot;large&quot;,&quot;linkDestination&quot;:&quot;none&quot;,&quot;className&quot;:&quot;is-style-rounded&quot;} --&gt;&lt;div class&#x3D;&quot;wp-block-image is-style-rounded&quot;&gt;&lt;figure class&#x3D;&quot;aligncenter size-large is-resized&quot;&gt;&lt;img src&#x3D;&quot;https://blockpatterndesigns.mystagingwebsite.com/wp-content/uploads/2021/02/StockSnap_HQR8BJFZID-1.jpg&quot; alt&#x3D;&quot;&quot; class&#x3D;&quot;wp-image-553&quot; width&#x3D;&quot;150&quot; height&#x3D;&quot;150&quot;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;!-- /wp:image --&gt;&lt;!-- wp:quote {&quot;align&quot;:&quot;center&quot;,&quot;className&quot;:&quot;is-style-large&quot;} --&gt;&lt;blockquote class&#x3D;&quot;wp-block-quote has-text-align-center is-style-large&quot;&gt;&lt;p&gt;&quot;Contributing makes me feel like I\&#x27;m being useful to the planet.&quot;&lt;/p&gt;&lt;cite&gt;— Anna Wong, &lt;em&gt;Volunteer&lt;/em&gt;&lt;/cite&gt;&lt;/blockquote&gt;&lt;!-- /wp:quote --&gt;&lt;!-- wp:separator {&quot;className&quot;:&quot;is-style-default&quot;} --&gt;&lt;hr class&#x3D;&quot;wp-block-separator is-style-default&quot;/&gt;&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;/div&gt;&lt;!-- /wp:group --&gt;&#x27;,
       )
 );
 
 
 
 Using patterns in a template file
 
 
 
 Once patterns are created, they can be used in a theme template file with the following block markup:
 
 
 
 &lt;!-- wp:pattern {&quot;slug&quot;:&quot;prefix/pattern-slug&quot;} /--&gt;
 
 
 
 An example from this GitHub repository shows the use of “footer-with-background” pattern slug with “tt2gopher” prefix in TT2 Gopher blocks theme.
 
 
 
 Early adopters of block themes and Gutenberg plugin took advantage of patterns in classic themes as well. The default Twenty Twenty and my favorite Eksell themes (a demo site here) are good examples that showcase how pattern features can be added to classic themes.
 
 
 
 Use case example 2: Twenty Twenty-Two
 
 
 
 If a theme includes only a few patterns, the development and maintenance are fairly manageable. However, if a block theme includes many patterns, like in TT2 theme, then the pattern.php file becomes very large and hard to read. The default TT2 theme, which bundles more than 60 patterns, showcases a refactored pattern registration workflow structure that is easier to read and maintain.
 
 
 
 Taking examples from the TT2 theme, let’s briefly discuss how this simplified workflow works.
 
 
 
 2.1: Registering Patterns Categories
 
 
 
 For demonstration purposes, I created a TT2 child theme and set it up on my local test site with some dummy content. Following TT2, I registered footer-with-background and added to the following pattern categories array list in its block-patterns.php file.
 
 
 
 /**
 * Registers block patterns and categories.
 */
 function twentytwentytwo_register_block_patterns() {
 	$block_pattern_categories &#x3D; array(
 		&#x27;footer&#x27;   &#x3D;&gt; array( &#x27;label&#x27; &#x3D;&gt; __( &#x27;Footers&#x27;, &#x27;twentytwentytwo&#x27; ) ),
 		&#x27;header&#x27;   &#x3D;&gt; array( &#x27;label&#x27; &#x3D;&gt; __( &#x27;Headers&#x27;, &#x27;twentytwentytwo&#x27; ) ),
 		&#x27;pages&#x27;    &#x3D;&gt; array( &#x27;label&#x27; &#x3D;&gt; __( &#x27;Pages&#x27;, &#x27;twentytwentytwo&#x27; ) ),
                 // ...
 	);
 
 	/**
 	 * Filters the theme block pattern categories.
 	 */
 	$block_pattern_categories &#x3D; apply_filters( &#x27;twentytwentytwo_block_pattern_categories&#x27;, $block_pattern_categories );
 
 	foreach ( $block_pattern_categories as $name &#x3D;&gt; $properties ) {
 		if ( ! WP_Block_Pattern_Categories_Registry::get_instance()-&gt;is_registered( $name ) ) {
 			register_block_pattern_category( $name, $properties );
 		}
 	}
 
 	$block_patterns &#x3D; array(
 		&#x27;footer-default&#x27;,
 		&#x27;footer-dark&#x27;,
 		&#x27;footer-with-background&#x27;,
 		//...
 		&#x27;header-default&#x27;,
 		&#x27;header-large-dark&#x27;,
 		&#x27;header-small-dark&#x27;,
 		&#x27;hidden-404&#x27;,
 		&#x27;hidden-bird&#x27;,
 		//...
 	);
 
 	/**
 	 * Filters the theme block patterns.
 	 */
 	$block_patterns &#x3D; apply_filters( &#x27;twentytwentytwo_block_patterns&#x27;, $block_patterns );
 
 	foreach ( $block_patterns as $block_pattern ) {
 		$pattern_file &#x3D; get_theme_file_path( &#x27;/inc/patterns/&#x27; . $block_pattern . &#x27;.php&#x27; );
 
 		register_block_pattern(
 			&#x27;twentytwentytwo/&#x27; . $block_pattern,
 			require $pattern_file
 		);
 	}
 }
 add_action( &#x27;init&#x27;, &#x27;twentytwentytwo_register_block_patterns&#x27;, 9 );
 
 
 
 In this code example, each pattern listed in the $block_patterns &#x3D; array() is called by foreach() function which requires a patterns directory file with the listed pattern name in the array which we will add in the next step.
 
 
 
 2.2: Adding a pattern file to the /inc/patterns folder
 
 
 
 Next, we should have all the listed patterns files in the $block_patterns &#x3D; array(). Here is an example of one of the pattern files, footer-with-background.php:
 
 
 
 /**
  * Dark footer wtih title and citation
  */
 return array(
 	&#x27;title&#x27;      &#x3D;&gt; __( &#x27;Footer with background&#x27;, &#x27;twentytwentytwo&#x27; ),
 	&#x27;categories&#x27; &#x3D;&gt; array( &#x27;footer&#x27; ),
 	&#x27;blockTypes&#x27; &#x3D;&gt; array( &#x27;core/template-part/footer&#x27; ),
   &#x27;content&#x27;    &#x3D;&gt; &#x27;&lt;!-- wp:group {&quot;align&quot;:&quot;full&quot;,&quot;style&quot;:{&quot;elements&quot;:{&quot;link&quot;:{&quot;color&quot;:{&quot;text&quot;:&quot;var:preset|color|background&quot;}}},&quot;spacing&quot;:{&quot;padding&quot;:{&quot;top&quot;:&quot;var(--wp--custom--spacing--small, 1.25rem)&quot;,&quot;bottom&quot;:&quot;var(--wp--custom--spacing--small, 1.25rem)&quot;}}},&quot;backgroundColor&quot;:&quot;background-header&quot;,&quot;textColor&quot;:&quot;background&quot;,&quot;layout&quot;:{&quot;inherit&quot;:true}} --&gt;
       &lt;div class&#x3D;&quot;wp-block-group alignfull has-background-color has-background-header-background-color has-text-color has-background has-link-color&quot; style&#x3D;&quot;padding-top:var(--wp--custom--spacing--small, 1.25rem);padding-bottom:var(--wp--custom--spacing--small, 1.25rem)&quot;&gt;&lt;!-- wp:paragraph {&quot;align&quot;:&quot;center&quot;} --&gt;
       &lt;p class&#x3D;&quot;has-text-align-center&quot;&gt;&#x27; .
       sprintf(
         /* Translators: WordPress link. */
         esc_html__( &#x27;Proudly powered by %s&#x27;, &#x27;twentytwentytwo&#x27; ),
         &#x27;&lt;a href&#x3D;&quot;&#x27; . esc_url( __( &#x27;https://wordpress.org&#x27;, &#x27;twentytwentytwo&#x27; ) ) . &#x27;&quot; rel&#x3D;&quot;nofollow&quot;&gt;WordPress&lt;/a&gt; | a modified TT2 theme.&#x27;
       ) . &#x27;&lt;/p&gt;
       &lt;!-- /wp:paragraph --&gt;&lt;/div&gt;
           &lt;!-- /wp:group --&gt;&#x27;,
 );
 
 
 
 Let’s reference the pattern in the footer.html template part:
 
 
 
 &lt;!-- wp:template-part {&quot;slug&quot;:&quot;footer&quot;} /--&gt;
 
 
 
 This is similar to adding heading or footer parts in a template file.
 
 
 
 The patterns can similarly be added to the parts/footer.html template by modifying it to refer to slug of the theme’s pattern file (footer-with-background):
 
 
 
 &lt;!-- wp:pattern {&quot;slug&quot;:&quot;twentytwentytwo/footer-with-background&quot;} /--&gt;
 
 
 
 Now, if we visit the patterns inserter of the block editor, the Footer with background should be available for our use:
 
 
 
 
 
 
 
 The following screenshot shows the newly created footer with background pattern on the front-end.
 
 
 
 
 
 
 
 Now that patterns have become the integral part of block theme, many patterns are bundled in block themes — like Quadrat, Seedlet, Mayland, Zoologist, Geologist — following the workflow discussed above. Here is an example of the Quadrat theme /inc/patterns folder with a block-pattern registration file and list of files with content source and required pattern header within return array() function.
 
 
 
 Section 2: Creating and loading patterns without registration
 
 
 
 Please note that this feature requires the installation of WordPress 6.0 or Gutenberg plugin 13.0 or above in your site.
 
 
 
 This new WordPress 6.0 feature allows pattern registration via standard files and folders – /patterns, bringing similar conventions like /parts, /templates, and /styles.
 
 
 
 The process, as also described in this WP Tavern article, involves the following three steps:
 
 
 
 creating a patterns folder at the theme’s rootadding plugin style pattern headerpattern source content
 
 
 
 A typical pattern header markup, taken from the article is shown below:
 
 
 
 &lt;?php
 /**
 * Title: A Pattern Title
 * Slug: namespace/slug
 * Description: A human-friendly description.
 * Viewport Width: 1024
 * Categories: comma, separated, values
 * Keywords: comma, separated, values
 * Block Types: comma, separated, values
 * Inserter: yes|no
 */
 ?&gt;
 
 
 
 As described in the previous section, only Title and Slug fields are required and other fields are optional.
 
 
 
 Referencing examples from recently released themes, I refactored pattern registration in this TT2 Gopher Blocks demo theme, prepared for a previous article on the CSS-Tricks.
 
 
 
 In the following steps, let’s explore how a footer-with-background.php pattern registered with PHP and used in a footer.html template is refactored.
 
 
 
 2.1: Create a /patterns folder at the root of the theme
 
 
 
 The first step is to create a /patterns folder at TT2 Gopher theme’s root and move the footer-with-background.php pattern file to /patterns folder and refactor.
 
 
 
 2.2: Add pattern data to the file header
 
 
 
 Next, create the following pattern header registration fields.
 
 
 
 &lt;?php
 /**
 * Title: Footer with background
 * Slug: tt2gopher/footer-with-background
 * Categories: tt2gopher-footer
 * Viewport Width: 1280
 * Block Types: core/parts/footer
 * Inserter: yes
 */
 ?&gt;
 &lt;!-- some-block-content /--&gt;
 
 
 
 A pattern file has a top title field written as PHP comments. Followed by the block-content written in HTML format.
 
 
 
 2.3: Add Pattern Content to the file
 
 
 
 For the content section, let’s copy the code snippets within single quotes (e.g., &#x27;...&#x27;) from the content section of the footer-with-background and replace the &lt;!-- some-block-content /--&gt;:
 
 
 
 &lt;!-- wp:group {&quot;align&quot;:&quot;full&quot;,&quot;style&quot;:{&quot;elements&quot;:{&quot;link&quot;:{&quot;color&quot;:{&quot;text&quot;:&quot;var:preset|color|foreground&quot;}}},&quot;spacing&quot;:{&quot;padding&quot;:{&quot;top&quot;:&quot;35px&quot;,&quot;bottom&quot;:&quot;30px&quot;}}},&quot;backgroundColor&quot;:&quot;background-header&quot;,&quot;textColor&quot;:&quot;foreground&quot;,&quot;className&quot;:&quot;has-foreground&quot;,&quot;layout&quot;:{&quot;inherit&quot;:true}} --&gt;
     &lt;div class&#x3D;&quot;wp-block-group alignfull has-foreground has-foreground-color has-background-header-background-color has-text-color has-background has-link-color&quot; style&#x3D;&quot;padding-top:35px;padding-bottom:30px&quot;&gt;&lt;!-- wp:paragraph {&quot;align&quot;:&quot;center&quot;,&quot;fontSize&quot;:&quot;small&quot;} --&gt;
     &lt;p class&#x3D;&quot;has-text-align-center has-small-font-size&quot;&gt;Powered by WordPress | TT2 Gopher, a modified TT2 theme&lt;/p&gt;
     &lt;!-- /wp:paragraph --&gt;&lt;/div&gt;
 &lt;!-- /wp:group --&gt;
 
 
 
 The entire code snippet of the patterns/footer-with-background.php file can be viewed here on the GitHub.
 
 
 
 Please note that the /inc/patterns and block-patterns.php are extras, not required in WordPress 6.0, and included only for demo purposes.
 
 
 
 2.4: Referencing the patterns slug in the template
 
 
 
 Adding the above refactored footer-with-background.php pattern to footer.html template is exactly the same as described in the previous section (Section 1, 2.2).
 
 
 
 Now, if we view the site’s footer part in a block editor or front-end of our site in a browser, the footer section is displayed.
 
 
 
 Pattern categories and types Registration (optional)
 
 
 
 To categorize block patterns, the pattern categories and types should be registered in theme’s functions.php file.
 
 
 
 Let’s consider an example of registering block pattern categories from the TT2 Gopher theme.
 
 
 
 After the registration, the patterns are displayed in the pattern inserter together with the core default patterns. To add theme specific category labels in the patterns inserter, we should modify the previous snippets by adding theme namespace:
 
 
 
 /**
 * Registers block categories, and type.
 */
 
 function tt2gopher_register_block_pattern_categories() {
 
 $block_pattern_categories &#x3D; array(
   &#x27;tt2gopher-header&#x27; &#x3D;&gt; array( &#x27;label&#x27; &#x3D;&gt; __( &#x27;TT2 Gopher - Headers&#x27;, &#x27;tt2gopher&#x27; ) ),
   &#x27;tt2gopher-footer&#x27; &#x3D;&gt; array( &#x27;label&#x27; &#x3D;&gt; __( &#x27;TT2 Gopher - Footers&#x27;, &#x27;tt2gopher&#x27; ) ),
   &#x27;tt2gopher-page&#x27; &#x3D;&gt; array( &#x27;label&#x27; &#x3D;&gt; __( &#x27;TT2 Gopher - Page&#x27;, &#x27;tt2gopher&#x27; ) ),
   // ...
 );
 
 /**
 * Filters the theme block pattern categories.
 */
 $block_pattern_categories &#x3D; apply_filters( &#x27;tt2gopher_block_pattern_categories&#x27;, $block_pattern_categories );
 
 foreach ( $block_pattern_categories as $name &#x3D;&gt; $properties ) {
   if ( ! WP_Block_Pattern_Categories_Registry::get_instance()-&gt;is_registered( $name ) ) {
     register_block_pattern_category( $name, $properties );
   }
 }
 add_action( &#x27;init&#x27;, &#x27;tt2gopher_register_block_pattern_categories&#x27;, 9 );
 
 
 
 The footer-with-background pattern is visible in the patterns inserted with its preview (if selected):
 
 
 
 
 
 
 
 This process greatly simplifies creating and displaying block patterns in block themes. It is available in WordPress 6.0 without using the Gutenberg plugin.
 
 
 
 Examples of themes without patterns registration
 
 
 
 Early adopters have already started using this feature in their block themes. A few examples of the themes, that are available from the theme directory, that load patterns without registration are listed below:
 
 
 
 Archeo – 12 patternsPendant – 13 patternsRemote – 11 patternsSkatepark – 10 patternsStewart – 17 patternsLivro – 16 patternsAvant-Garde – 14 patterns
 
 
 
 Section 3: Creating and using patterns with low-code
 
 
 
 The official patterns directory contains community-contributed creative designs, which can be copied and customized as desired to create content. Using patterns with a block editor has never been so easier!
 
 
 
 Any patterns from the ever-growing directory can also be added to block themes just by simple “copy and paste” or include in the theme.json file by referring to their directory pattern slug. Next, I will go through briefly how easily this can be accomplished with very limited coding.
 
 
 
 Adding and customizing patterns from patterns directory
 
 
 
 3.1: Copy pattern from directory into a page
 
 
 
 Here, I am using this footer section pattern by FirstWebGeek from the patterns directory. Copied the pattern by selecting the “Copy Pattern” button and directly pasted it in a new page.
 
 
 
 3.2: Make desired customizations
 
 
 
 I made only a few changes to the color of the fonts and button background. Then copied the entire code from the code editor over to a clipboard.
 
 
 
 
 
 
 
 If you are not familiar with using the code editor, go to options (with three dots, top right), click the Code editor button, and copy the entire code from here.
 
 
 
 3.3: Create a new file in /patterns folder
 
 
 
 First, let’s create a new /patterns/footer-pattern-test.php file and add the required pattern header section. Then paste the entire code (step 3, above). The pattern is categorized in the footer area (lines: 5), we can view the newly added in the pattern inserter.
 
 
 
 &lt;?php
  /**
  * Title: Footer pattern from patterns library
  * Slug: tt2gopher/footer-pattern-test
  * Categories: tt2gopher-footer
  * Viewport Width: 1280
  * Block Types: core/template-part/footer
  * Inserter: yes
  */
 ?&gt;
 
 &lt;!-- wp:group {&quot;align&quot;:&quot;full&quot;,&quot;style&quot;:{&quot;spacing&quot;:{&quot;padding&quot;:{&quot;top&quot;:&quot;100px&quot;,&quot;bottom&quot;:&quot;70px&quot;,&quot;right&quot;:&quot;30px&quot;,&quot;left&quot;:&quot;30px&quot;}}},&quot;backgroundColor&quot;:&quot;black&quot;,&quot;layout&quot;:{&quot;contentSize&quot;:&quot;1280px&quot;}} --&gt;
 &lt;div class&#x3D;&quot;wp-block-group alignfull has-black-background-color has-background&quot; style&#x3D;&quot;padding-top:100px;padding-right:30px;padding-bottom:70px;padding-left:30px&quot;&gt;&lt;!-- wp:columns --&gt;
 &lt;div class&#x3D;&quot;wp-block-columns&quot;&gt;&lt;!-- wp:column --&gt;
 &lt;div class&#x3D;&quot;wp-block-column&quot;&gt;&lt;!-- wp:heading {&quot;style&quot;:{&quot;typography&quot;:{&quot;fontStyle&quot;:&quot;normal&quot;,&quot;fontWeight&quot;:&quot;700&quot;,&quot;textTransform&quot;:&quot;uppercase&quot;}},&quot;textColor&quot;:&quot;cyan-bluish-gray&quot;} --&gt;
 &lt;h2 class&#x3D;&quot;has-cyan-bluish-gray-color has-text-color&quot; style&#x3D;&quot;font-style:normal;font-weight:700;text-transform:uppercase&quot;&gt;lorem&lt;/h2&gt;
 &lt;!-- /wp:heading --&gt;
 
 &lt;!-- wp:paragraph {&quot;style&quot;:{&quot;typography&quot;:{&quot;fontSize&quot;:&quot;16px&quot;}},&quot;textColor&quot;:&quot;cyan-bluish-gray&quot;} --&gt;
 &lt;p class&#x3D;&quot;has-cyan-bluish-gray-color has-text-color&quot; style&#x3D;&quot;font-size:16px&quot;&gt;One of the main benefits of using Lorem Ipsum is that it can be easily generated, and it takes the pressure off designers to create meaningful text. Instead, they can focus on crafting the best website data.&lt;/p&gt;
 &lt;!-- /wp:paragraph --&gt;
 
 &lt;!-- wp:social-links {&quot;iconColor&quot;:&quot;vivid-cyan-blue&quot;,&quot;iconColorValue&quot;:&quot;#0693e3&quot;,&quot;openInNewTab&quot;:true,&quot;className&quot;:&quot;is-style-logos-only&quot;,&quot;style&quot;:{&quot;spacing&quot;:{&quot;blockGap&quot;:{&quot;top&quot;:&quot;15px&quot;,&quot;left&quot;:&quot;15px&quot;}}}} --&gt;
 &lt;ul class&#x3D;&quot;wp-block-social-links has-icon-color is-style-logos-only&quot;&gt;&lt;!-- wp:social-link {&quot;url&quot;:&quot;#&quot;,&quot;service&quot;:&quot;facebook&quot;} /--&gt;
 
 &lt;!-- wp:social-link {&quot;url&quot;:&quot;#&quot;,&quot;service&quot;:&quot;twitter&quot;} /--&gt;
 
 &lt;!-- wp:social-link {&quot;url&quot;:&quot;#&quot;,&quot;service&quot;:&quot;instagram&quot;} /--&gt;
 
 &lt;!-- wp:social-link {&quot;url&quot;:&quot;#&quot;,&quot;service&quot;:&quot;linkedin&quot;} /--&gt;&lt;/ul&gt;
 &lt;!-- /wp:social-links --&gt;&lt;/div&gt;
 &lt;!-- /wp:column --&gt;
 
 &lt;!-- wp:column --&gt;
 &lt;div class&#x3D;&quot;wp-block-column&quot;&gt;&lt;!-- wp:heading {&quot;level&quot;:4,&quot;style&quot;:{&quot;typography&quot;:{&quot;textTransform&quot;:&quot;capitalize&quot;,&quot;fontStyle&quot;:&quot;normal&quot;,&quot;fontWeight&quot;:&quot;700&quot;,&quot;fontSize&quot;:&quot;30px&quot;}},&quot;textColor&quot;:&quot;cyan-bluish-gray&quot;} --&gt;
 &lt;h4 class&#x3D;&quot;has-cyan-bluish-gray-color has-text-color&quot; style&#x3D;&quot;font-size:30px;font-style:normal;font-weight:700;text-transform:capitalize&quot;&gt;Contact Us&lt;/h4&gt;
 &lt;!-- /wp:heading --&gt;
 
 &lt;!-- wp:paragraph {&quot;style&quot;:{&quot;typography&quot;:{&quot;fontSize&quot;:&quot;16px&quot;,&quot;lineHeight&quot;:&quot;1.2&quot;}},&quot;textColor&quot;:&quot;cyan-bluish-gray&quot;} --&gt;
 &lt;p class&#x3D;&quot;has-cyan-bluish-gray-color has-text-color&quot; style&#x3D;&quot;font-size:16px;line-height:1.2&quot;&gt;123 BD Lorem, Ipsum&lt;br&gt;&lt;br&gt;+123-456-7890&lt;/p&gt;
 &lt;!-- /wp:paragraph --&gt;
 
 &lt;!-- wp:paragraph {&quot;style&quot;:{&quot;typography&quot;:{&quot;fontSize&quot;:&quot;16px&quot;,&quot;lineHeight&quot;:&quot;1&quot;}},&quot;textColor&quot;:&quot;cyan-bluish-gray&quot;} --&gt;
 &lt;p class&#x3D;&quot;has-cyan-bluish-gray-color has-text-color&quot; style&#x3D;&quot;font-size:16px;line-height:1&quot;&gt;sample@gmail.com&lt;/p&gt;
 &lt;!-- /wp:paragraph --&gt;
 
 &lt;!-- wp:paragraph {&quot;style&quot;:{&quot;typography&quot;:{&quot;fontSize&quot;:&quot;16px&quot;,&quot;lineHeight&quot;:&quot;1&quot;}},&quot;textColor&quot;:&quot;cyan-bluish-gray&quot;} --&gt;
 &lt;p class&#x3D;&quot;has-cyan-bluish-gray-color has-text-color&quot; style&#x3D;&quot;font-size:16px;line-height:1&quot;&gt;Opening Hours: 10:00 - 18:00&lt;/p&gt;
 &lt;!-- /wp:paragraph --&gt;&lt;/div&gt;
 &lt;!-- /wp:column --&gt;
 
 &lt;!-- wp:column --&gt;
 &lt;div class&#x3D;&quot;wp-block-column&quot;&gt;&lt;!-- wp:heading {&quot;level&quot;:4,&quot;style&quot;:{&quot;typography&quot;:{&quot;fontSize&quot;:&quot;30px&quot;,&quot;fontStyle&quot;:&quot;normal&quot;,&quot;fontWeight&quot;:&quot;700&quot;,&quot;textTransform&quot;:&quot;capitalize&quot;}},&quot;textColor&quot;:&quot;cyan-bluish-gray&quot;} --&gt;
 &lt;h4 class&#x3D;&quot;has-cyan-bluish-gray-color has-text-color&quot; style&#x3D;&quot;font-size:30px;font-style:normal;font-weight:700;text-transform:capitalize&quot;&gt;Newsletter&lt;/h4&gt;
 &lt;!-- /wp:heading --&gt;
 
 &lt;!-- wp:paragraph {&quot;style&quot;:{&quot;typography&quot;:{&quot;fontSize&quot;:&quot;16px&quot;}},&quot;textColor&quot;:&quot;cyan-bluish-gray&quot;} --&gt;
 &lt;p class&#x3D;&quot;has-cyan-bluish-gray-color has-text-color&quot; style&#x3D;&quot;font-size:16px&quot;&gt;Lorem ipsum dolor sit amet, consectetur ut labore et dolore magna aliqua ipsum dolor sit&lt;/p&gt;
 &lt;!-- /wp:paragraph --&gt;
 
 &lt;!-- wp:search {&quot;label&quot;:&quot;&quot;,&quot;placeholder&quot;:&quot;Enter Your Email...&quot;,&quot;buttonText&quot;:&quot;Subscribe&quot;,&quot;buttonPosition&quot;:&quot;button-inside&quot;,&quot;style&quot;:{&quot;border&quot;:{&quot;width&quot;:&quot;1px&quot;}},&quot;borderColor&quot;:&quot;tertiary&quot;,&quot;backgroundColor&quot;:&quot;background-header&quot;,&quot;textColor&quot;:&quot;background&quot;} /--&gt;&lt;/div&gt;
 &lt;!-- /wp:column --&gt;&lt;/div&gt;
 &lt;!-- /wp:columns --&gt;&lt;/div&gt;
 &lt;!-- /wp:group --&gt;
 
 
 
 3.4: View the new pattern in the inserter
 
 
 
 To view the newly added Footer pattern from patterns library pattern, go to any post or page and select the inserter icon (blue plus symbol, top left), and then select “TT2 Gopher – Footer” categories. The newly added pattern is shown on the left panel, together with other footer patterns and its preview on the right (if selected):
 
 
 
 
 
 
 
 Registering patterns directly in theme.json file
 
 
 
 In WordPress 6.0, it is possible to register any desired patterns from the pattern directory with theme.json file with the following syntax. The 6.0 dev note states, “the patterns field is an array of [pattern slugs] from the Pattern Directory. Pattern slugs can be extracted by the [URL] in single pattern view at the Pattern Directory.”
 
 
 
 {
     &quot;version&quot;: 2,
     &quot;patterns&quot;: [&quot;short-text&quot;, &quot;patterns-slug&quot;]
 }
 
 
 
 This short WordPress 6.0 features video demonstrates how patterns are registered in the /patterns folder (at 3:53) and registering the desired patterns from the pattern director in a theme.json file (at 3:13).
 
 
 
 
 
 
 
 
 
 Then, the registered pattern is available in the patterns inserter search box, which is then available for use just like theme-bundled patterns library.
 
 
 
 {
   &quot;version&quot;: 2,
   &quot;patterns&quot;: [ &quot;footer-from-directory&quot;, &quot;footer-section-design-with-3-column-description-social-media-contact-and-newsletter&quot; ]
 }
 
 
 
 In this example, the pattern slug footer-section-design-with-3-column-description-social-media-contact-and-newsletter from the earlier example is registered via theme.json.
 
 
 
 Page creation pattern model
 
 
 
 As part of “building with patterns” initiatives, WordPress 6.0 offers a pattern modal option to theme authors to add page layout patterns into block theme, allowing site users to select page layout patterns (e.g., an about page, a contact page, a team page, etc.) while creating a page. The following is an example taken from the dev note:
 
 
 
 register_block_pattern(
     &#x27;my-plugin/about-page&#x27;,
     array(
         &#x27;title&#x27;      &#x3D;&gt; __( &#x27;About page&#x27;, &#x27;my-plugin&#x27; ),
         &#x27;blockTypes&#x27; &#x3D;&gt; array( &#x27;core/post-content&#x27; ),
         &#x27;content&#x27;    &#x3D;&gt; &#x27;&lt;!-- wp:paragraph {&quot;backgroundColor&quot;:&quot;black&quot;,&quot;textColor&quot;:&quot;white&quot;} --&gt;
         &lt;p class&#x3D;&quot;has-white-color has-black-background-color has-text-color has-background&quot;&gt;Write you about page here, feel free to use any block&lt;/p&gt;
         &lt;!-- /wp:paragraph --&gt;&#x27;,
     )
 );
 
 
 
 This feature is currently limited to Page Post Type only and not for “Posts Post Type”, yet.
 
 
 
 The page creation pattern modal can also be disabled completely by removing the post-content block type of all the patterns. An example sample code is available here.
 
 
 
 You can follow and participate in GitHub’s discussion from the links listed under the resource section below.
 
 
 
 Using patterns directory to build page
 
 
 
 Patterns from the directory can also be used to create the desired post or page layout, similar to page builders. The GutenbergHub team has created an experimental online page builder app using patterns directly from the directory (introductory video). Then the codes from the app can be copied and pasted in a site, which greatly simplifies the building complex page layout process without coding.
 
 
 
 In this short video, Jamie Marsland demonstrates (at 1:30) how the app can be used to create an entire page layout similar to page builder using desired page sections of the directory.
 
 
 
 Wrapping up
 
 
 
 Patterns allow users to recreate their commonly used content layout (e.g., hero page, call out, etc.) in any page and lower the barriers to presenting content in styles, which were previously not possible without coding skills. Just like the plugins and themes directories, the new patterns directory offers users options to use a wide range of patterns of their choices from the pattern directory, and write and display content in style.
 
 
 
 Indeed, block patterns will change everything and surely this is a game changer feature in the WordPress theme landscape. When the full potential of building with patterns effort becomes available, this is going to change the way we design block themes and create beautiful content even with low-code knowledge. For many creative designers, the patterns directory may also provide an appropriate avenue to showcase their creativity.
 
 
 
 
 
 
 
 Resources
 
 
 
 WordPress 6.0
 
 
 
 WordPress 6.0 Field Guide (WordPress Core)Exploring WordPress 6.0: Style Variations, Block Locking UI, Writing Improvements – 22 min video (Anne McCarthy)WordPress 6.0 features in 4mins (Dave Smith)What’s New in WordPress 6.0: New Blocks, Style Switching, Template Editing, Webfonts API, and Much More (Kinsta)
 
 
 
 Creating patterns
 
 
 
 Introduction to block patterns (Full Site Editing)Introduction to Block Patterns video, 14 mins (Learn WordPress)Block Patterns (Block Editor Handbook)So you want to make block patterns? (WordPress Blog)How to create and share low-code Block Patterns in WordPress (GoDaddy)
 
 
 
 Patterns enhancement (GitHub)
 
 
 
 Building with Patterns #38529Patterns as Sectioning Elements #39281Add: Option to pick a pattern on page creation. #40034Block Patterns for page creation. #38787Add: Page start options (templates and patterns) #39147
 
 
 
 Blog articles
 
 
 
 Gutenberg Patterns: The Future of Page Building in WordPress (Rich Tabor)Using Block Patterns to speed up WordPress site builds (GoDaddy)Block Patterns Will Change Everything (WP Tavern)
 
 How to Create Block Theme Patterns in WordPress 6.0 originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>SPAs, Shared Element Transitions, and Re-Evaluating Technology</title>
         <link href="https://css-tricks.com/spas-shared-element-transitions-and-re-evaluating-technology/"/>
       <updated>2022-06-01T14:12:25.000Z</updated>
       <content type="text">Nolan Lawson sparked some discussion when he described a noticeable shift away from single-page applications (SPAs):
 
 
 
 Hip new frameworks like Astro, Qwik, and Elder.js are touting their MPA [multi-page application] with “0kB JavaScript by default.” Blog posts are making the rounds listing all the challenges with SPAs: history, focus management, scroll restoration, Cmd/Ctrl-click, memory leaks, etc. Gleeful potshots are being taken against SPAs.I think what’s less discussed, though, is how the context has changed in recent years to give MPAs more of an upper hand against SPAs.
 
 
 
 It seems a number of folks really clung to that first part because Nolan published a follow-up to clarify that SPAs are far from doomed:
 
 
 
 [T]he point of my post wasn’t to bury SPAs and dance on their grave. I think SPAs are great, I’ve worked on many of them, and I think they have a bright future ahead of them. My main point was: if the only reason you’re using an SPA is because “it makes navigations faster,” then maybe it’s time to re-evaluate that.
 
 
 
 And there’s good reason he says that. In fact, the first article specifically points to work being done on Shared Element Transitions. If they move forward, we’ll have an API for animating/transitioning/sizing/positioning elements on page entrance and exist. Jake Archibald demonstrated how it works at Google I/O 2022 and the video is a gem.
 
 
 
 
 
 
 
 If you’re wondering how one page can transition into another, the browser takes screenshots of the outgoing page and the incoming page, then transitions between those. So, it’s not so much one page becoming another as much as it is the browser holding onto two images so it can animate one in while the other animates out. Jake says what’s happening behind the scene is a DOM structure is created out of pseudo-elements containing the page images:
 
 
 
 &lt;transition-container&gt;
   &lt;image-wrapper&gt;
     &lt;outgoing-image /&gt;
     &lt;incoming-image /&gt;
   &lt;/&gt;
 &lt;/&gt;
 
 
 
 We can “screenshot” a specific element if we want to isolate it and apply a different animation from the rest of the page:
 
 
 
 .site-header {
   page-transition-tag: site-header;
   contain: paint;
 }
 
 
 
 And we get pseudo-elements we can hook into and assign custom @keyframe animations:
 
 
 
 &lt;!-- ::page-transition&#x3D;container(root)  --&gt;
 &lt;transition-container&gt;
   &lt;!-- ::page-transition-image-wrapper(root)  --&gt;
   &lt;image-wrapper&gt;
     &lt;!-- ::page-transition-outgoing-image(root) --&gt;
     &lt;outgoing-image /&gt;
     &lt;!-- ::page-transition-incoming-image(root) --&gt;
     &lt;incoming-image /&gt;
   &lt;/&gt;
 &lt;/&gt;
 
 
 
 Dang, that’s clever as heck!
 
 
 
 It’s also proof in the pudding of just how much HTML, CSS, and JavaScript continue to evolve and improve. So much so that Jeremy Keith suggests it’s high time we re-evaluate our past judgment of some technologies:
 
 
 
 If you weren’t aware of changes over the past few years, it would be easy to still think that single page apps offer some unique advantages that in fact no longer hold true. […] But developers remain suspicious, still prefering to trust third-party libraries over native browser features. They made a decision about those libraries in the past. They evaluated the state of browser support in the past. I wish they would re-evaluate those decisions.
 
 
 
 The ingredients for SPAs specifically:
 
 
 
 In recent years in particular it feels like the web has come on in leaps and bounds: service workers, native JavaScript APIs, and an astonishing boost in what you can do with CSS. Most important of all, the interoperability between browsers is getting better and better. Universal support for new web standards arrives at a faster rate than ever before.
 
 
 
 HTML, CSS, and JavaScript: it’s still the best cocktail in town. Even if it takes a minute for it to catch up.
 
 SPAs, Shared Element Transitions, and Re-Evaluating Technology originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>Inspirational Websites Roundup #38</title>
         <link href="https://tympanus.net/codrops/2022/06/01/inspirational-websites-roundup-38/"/>
       <updated>2022-06-01T13:36:53.000Z</updated>
       <content type="text">Today we have a fresh collection of new websites for your inspiration! Lots of interesting color schemes, mostly toned down colors with some contrasty complements. Lovely fonts in use, sans serif in uppercase is always a hit and some very creative serifs make some websites shine!
 
 
 
 Hope you find some good inspiration here!
 
 
 
 Le 7ème Continent
 
 
 
 
 
 
 
 Custo
 
 
 
 
 
 
 
 HARKEN
 
 
 
 
 
 
 
 Daniel Spatzek
 
 
 
 
 
 
 
 basement.studio
 
 
 
 
 
 
 
 Sūnya
 
 
 
 
 
 
 
 SOWVITAL
 
 
 
 
 
 
 
 Isaac Powell
 
 
 
 
 
 
 
 Martin Briceno
 
 
 
 
 
 
 
 Espace Go
 
 
 
 
 
 
 
 Houseplant
 
 
 
 
 
 
 
 Geist
 
 
 
 
 
 
 
 Kim Kneipp
 
 
 
 
 
 
 
 601
 
 
 
 
 
 
 
 Sergey Parajanov
 
 
 
 
 
 
 
 Marble
 
 
 
 
 
 
 
 Dash Digital Studio
 
 
 
 
 
 
 
 Studio Beaucoup
 
 
 
 
 
 
 
 Matthew Fisher (Collection No.02)
 
 
 
 
 
 
 
 Display
 
 
 
 
 
 
 
 Here Beautility
 
 
 
 
 
 
 
 SellX
 
 
 
 
 
 
 
 Vendredi Society
 
 
 
 
 
 
 
 Wyre
 
 
 
 
 
 
 
 Longines Spirit Zulu Time
 
 
 
 
 
 
 
 Haven Studios
 
 
 
 
 
 
 
 ZiKD
 
 
 
 
 
 
 
 Model Works
 
 
 
 
 
 
 
 New Format
 
 
 
 
 
 
 
 Enter the Otherside
 
 
 
 
 
 
 
 Studio Oker
 
 
 
 
 
 
 
 Andy Davies
 
 
 
 
 The post Inspirational Websites Roundup #38 appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>Coding an Infinite Slider using Texture Recursion with Three.js</title>
         <link href="https://tympanus.net/codrops/2022/06/01/coding-an-infinite-slider-using-texture-recursion-with-three-js/"/>
       <updated>2022-06-01T10:39:45.000Z</updated>
       <content type="text">
 
 
 
 
 
 In this new ALL YOUR HTML coding session we’ll be recreating the infinite image slider seen on https://tismes.com/ made by Lecamus Jocelyn using texture recursion in Three.js.
 
 
 
 This coding session was streamed live on May 29, 2022.
 
 
 
 
 
 
 
 Image credit goes to https://www.instagram.com/tre.zen/
 
 
 
 Support: https://www.patreon.com/allyourhtml 
 
 
 
 Setup: https://gist.github.com/akella/a19954… 
 The post Coding an Infinite Slider using Texture Recursion with Three.js appeared first on Codrops.</content>
     </entry>
     <entry>
       <title>Don’t Sink Your Website With Third Parties</title>
         <link href="https://smashingmagazine.com/2022/06/dont-sink-website-third-parties/"/>
       <updated>2022-06-01T10:00:00.000Z</updated>
       <content type="text">You’ve spent months putting together a great website design, crowd-pleasing content, and a business plan to bring it all together. You’ve focused on making the web design responsive to ensure that the widest audience of visitors can access your content. You’ve agonized over design patterns and usability. You’ve tested and retested the site for errors. Your operations team is ready to go. In short, you’ve done your due diligence in the areas of site design and delivery that you directly control. You’ve thought of everything… or have you?
 Your website may be using more third-party services than you realize. These services use requests to external hosts (not servers you control) to deliver JavaScript framework libraries, custom fonts, advertising content, marketing analytics trackers, and more. 
 You may have a lean, agile, responsive site design only to find it gradually loaded down with more and more “extras” that are often put onto the site by marketing departments or business leaders who are not always thinking about website performance. You cannot always anticipate what you cannot control.
 There are two big questions:
 
 How do you quantify the impact that these third-party requests have on website performance?
 How do you manage or even mitigate that impact? 
 
 Even if you cannot prevent all third-party requests, web designers can make choices that will have an impact. In this article, we will review what third-party resource requests are, consider how impactful they can be to the user experience, and discuss common optimization strategies to reduce the impact on the user experience. By carefully considering how third-party requests will fit into your website during the design stage, you can avoid the most significant negative impacts.
 What Are Third-Party Services?
 In order to understand third-party services, it may be easier to start with your own website content. Any resource (HTML, CSS, JavaScript, image, font, etc.) that you host and serve from your own domain(s) is called a “first-party” resource. You have control over what these resources are. All other requests that happen when visitors load your pages can be attributed to other parties.
 Every major website on the Internet today relies — to some degree — on third-party services. The third-party in this case is someone (usually another commercial enterprise) other than you and your site visitors. In this case, we are not going to be talking about infrastructure services, such as a cloud computing platform like Microsoft Azure or a content distribution network like Akamai. Many websites use these services to deploy and run their businesses and understanding how they impact the user experience is important. 
 In this article, however, we are going to focus on the third-party services that work their way into the design of your web pages. These third-party resource requests load in your visitor’s browser while your web page is loading, even if your visitors don’t realize it. They may be critical to site functionality, or they have been added as an afterthought, but all of them can potentially affect how fast users perceive your page load times.
 The HTTP Archive tracks third-party usage across a large swath of all active websites on the Internet today. According to the Third Parties chapter of their 2021 Web Almanac report, “a staggering 94.4% of mobile sites and 94.1% of desktop sites use at least one third-party resource.” They also found out that “45.9% of requests on mobile and 45.1% of requests on desktop are third-party requests.”
 As it was noted in the report, third-party services share a few characteristics, such as:
 
 hosted on a shared and public origin,
 widely used by a variety of sites,
 uninfluenced by an individual site owner.
 
 In other words, third-party services on your site are outsourced and operated by another party other than you. You have no direct control over where and how the requests are being hosted online. Many other websites may be using the same service, and the company that provides it must balance how to run their services to benefit all of their customers, not just you.
 The upside to using third-party services on your site is that you do not need to develop everything you want to do yourself. In many cases, they can be super convenient to add or remove without having to push code changes to the site. The downside is that third-party requests can impact website visitors. Pages loaded up with dozens or hundreds of third-party calls can take longer to render or longer to become interactive.  
 What About Fourth-party Or Second-Party Services?
 While the earliest, simplest third-party services were simple 1x1 pixel images used for tracking visitors, almost all third-party requests today load JavaScript into the browser. And JavaScript can certainly make requests for additional network resources. If these follow-on requests are to a different host or service, you might think of them as “fourth-party services”. If the fourth-party service, in turn, makes a request to yet another domain, then you get “fifth-party service” requests, and so forth. Technically, all of them might be “third parties” in the sense that they are neither you (the “first party”) nor your site visitor (the “second party”), but I think it helps to understand that these services are even more removed from your direct control than the ones you work directly with.
 The most common scenario I see where fourth-party requests come into play is in advertising services. If you serve ads on your website through an ad broker, you may not even know what service will finally deliver the ad image that gets displayed in the browser.  
 Feeling like this is a little bit out of control? There’s at least one other way that resource requests you have no direct control over can impact your visitors’ experience. Sometimes, the visitor’s browser itself can be the origin of network activity. 
 For example, users can install browser plugins to suggest coupon codes when they are shopping, to scan web pages for malware, to play games or message friends, or do any number of other things. These plugins can fire off “second-party” requests in the middle of your page load, and there is nothing you can do about it. Unlike third-party services, the best you can do is be aware of what these second-party services are, so you know what to ignore when troubleshooting problems.
 Create Your Own Request Map
 As a discovery tool, request maps are great for identifying the source of third-party, fourth-party, fifth-party, etc., requests. They can also highlight very long redirection chains in your third-party traffic. Simon Hearne, an independent web performance consultant and one of the co-organizers of the London Web Performance Group, maintains an online Request Map tool that uses WebPageTest to get the data and Ghostery to visualize it.
 
 Ad Blockers Makes Sites Faster
 In addition to the browser plugins mentioned above, users love to install ad blockers. While many are motivated simply by a desire to see fewer ads, ad blockers also often make web pages load faster. Maciej Kocemba published research findings from Opera that showed that a typical website with ads could be rendered 51% faster if the ads were blocked.
 This is obviously a concern for any website owner that monetizes page impressions with ads. They may not realize it, but users may be motivated to block ads partly to deal with slow third-party and fourth-party resource requests that lead to frustrating experiences. Faster page loads may reduce the motivation to use ad blockers.
 The Revenue Trade-off You Need To Think About
 Poor performance of third-party services can have other business impacts even if your website does not use advertising. Researchers and major companies have been publishing case studies for years, proving that slower page load experiences impact business metrics, including conversion rate, revenue, bounce rate, and more. 
 No matter how valuable you think a particular third-party service is to your business, that benefit needs to be compared to the cost of lost visitor engagement. Can a fancy third-party custom font give your site a new look and feel? Yes. Will the conversion rate or session length go down slightly as users see slower page loads? Or will visitors find the new look and feel worth the wait?  
 How To Identify Problematic Third-Party Services On Your Website
 If you are like most websites, about half of the resource requests that load in your customers’ browsers — when they load a page from your website — are third-party requests. Identifying them should be straightforward.  
 Measuring Performance Impact
 To quantify the performance impact of third-party resource requests on the user experience, we need to start by measuring page load performance. Many web performance measurement tools can measure the network load times of individual resource requests, and others can measure the client-side impacts of JavaScript resource requests. You may find that no single tool will answer every performance question you have about third parties. These tools fall into several categories.
 Some tools that can be helpful in evaluating the impact of third-party resource requests are what you might describe as auditing tools. The most popular, by far, is the Google Lighthouse report (available in Chrome Developer Tools) and Google’s Page Speed Insights. These tools generally work with data from a single page load but go into some greater depth on impact than the tools designed for ongoing monitoring.
 Synthetic web performance measurements use scripts to visit one or more pages on your website from one or more probe locations. Much like a laboratory environment (and depending to some degree on the features offered by the particular tool), you have control over the variables of the measurement. You can adjust what browser is used, the kind of network connection to employ, the locations to test from, whether or not the browser’s cache is empty or full, how frequently to take the measurements, and more. 
 Because most of the variables remain fixed from one test run to the next, synthetic measurements are great for measuring the impact of change but less capable of accurately or comprehensively identifying real visitor experience. They are more of a benchmark than a true measurement of real user experience. Some of the popular synthetic measurement tools are WebPageTest, SiteSpeed.io, Splunk Synthetic Monitoring, and Dynatrace.
 For a more comprehensive measurement of visitor experience, you need Real User Measurements (RUM). RUM systems embed a small JavaScript payload onto every page of your site. The code interacts with industry-standard APIs implemented by modern browsers to collect performance data, augments it with additional custom data collection, and transmits this very high-resolution data about the page as a whole and every resource request. 
 The data may have some limitations, though — the only data that can be collected is what the APIs support, and Cross Origin Resource Sharing (CORS) restrictions in the browsers limit some details, especially around third-party resource requests. Some of the more popular RUM services are offered by Akamai, New Relic, Dynatrace, and AppDynamics.
 
 What To Measure
 Third-party resource requests can impact the user experience in several different ways, depending on whether they load early in the page load process or after the page is mostly complete. The risk you should be looking for with the measurement data you are collecting include:
 
 Delaying initial renderResources that load prior to the initial page render (or First Contentful Paint) can be the most impactful overall. Many studies have shown that site visitors are more sensitive to delays at this point in the page load experience than any point after some visual progress has been achieved. Look for third-party requests that force new DNS lookups, require establishing connections to new origins, introduce redirection chains, include substantial client-side processing delay, or take a long time to download.
 Other blocking effectsAny JavaScript resource that blocks other resources from being requested until its processing is completed is a concern. A third-party font request could cause render-blocking. Look for third-party JavaScript resources that block other JavaScript resources that are not being loaded asynchronously from being requested in a timely manner. Avoid third-party requests that introduce contention for scarce resources like bandwidth or CPU utilization. If a third-party resource request is blocking, consider alternatives or approaches to mitigate the risk if the third party is slower than normal or fails.
 Single Points of Failure (SPOFs)A resource request can be considered a SPOF if the web page fails to load or the load time is disastrously longer should the resource itself fail to load. For example, if a third-party host is down and your request takes 60 seconds to time out, if the initial render of the page is delayed 60 seconds as a result, then this is a SPOF.
 
 Testing The Impact Of Specific Requests
 Once you have identified potentially impactful third-party resource requests, measuring the specific performance impact of those requests can be challenging. Trying to separate the impact of a single request from all the others can be akin to trying to break down an alloy into its constituent metals because third-party requests are often made in parallel with first-party requests or third-party requests to other hosts, and they are competing with each other for the limited network, CPU, and memory resources of the client. Even with highly-detailed RUM or synthetic measurement data, it may not be practical.
 The best way to approach the problem is through applied testing. Specifically, deliver pages with the third-party request or service as normal, and compare the performance to pages delivered without that particular third-party service but which are otherwise identical.
 This is easiest to do with synthetic measurement tools. You can blackhole a particular domain so that the synthetic browser will never make the requests in the first place, simulating a page loading without that service on it. This can inform you about the performance (load times) impact of that third-party service. WebPageTest — a free synthetic measurement service (see the following three figures below for an example) — makes this easy.
 
 
 
 A more sophisticated approach is to perform multivariate testing on your production site. In a multivariate test, you serve a version of the page with the third-party tag on it to one segment of your visitor population, and the other segment gets a version of the page without the third-party tag.
 By using RUM tools, you can directly measure the real-world performance differences between the two test segments as well as the effects on business metrics (such as bounce rate, conversion or session length). Managing multivariate testing is a significant undertaking, but it can pay off in the long run.
 Design Optimizations
 Once you have a baseline of your site performance and some tools to test the basic performance impact of key third-party resource requests, it is time to implement some strategies to mitigate the impact that third-party services can have on performance.
 Consider Removing Unneeded Services
 By far, the most impactful change you can make is to remove any obsolete, unused, or unnecessary third-party tags from your site. After all, no resource loads faster than not making a resource request at all. Ironically, this may also be the most challenging optimization to put into practice. In many organizations, third-party tags or services “belong” to a variety of stakeholders, and finding a way to manage them is as much a cultural challenge as a technical one. Some basic steps to take include:
 
 Audit all third-party requests appearing on your pages on a periodic basis (for example, quarterly).To make sure you capture all third-party requests, use a RUM service that collects data about every page view. If a third-party domain is showing up in more than a small fraction of page views and you do not already know what it is, find out immediately. New third-party tags may have been added by some stakeholders within your organization, or you may be finding a fourth-party tag because a third-party service changed its behavior. Either way, you need to understand what the third-party tags are and who in your organization is using them.
 Keep records on third-party services.Specifically, you want to know who the internal stakeholder is that “owns” that service and how it gets on the site. Is it hard-coded into the page HTML source? Is there JavaScript injected on the page by a CDN configuration? Are you using one (or more than one) tag manager? When does the contract with that service expire? The important thing is to have all the information on hand to know how to suspend or remove every third-party service if it becomes a performance issue or suddenly stops working, and who in your organization that is going to need to know.
 Consider a periodic stakeholders meeting that includes a discussion of all third-party services to review the cost/benefit they introduce to the business. Even if it is still under contract, consider removing third-party services that stakeholders no longer use.
 
 Geographically Align Your Third-Party Services With Your Visitors
 If most of your visitors are in Europe, but a third-party service you are using is serving its resource content from the United States, those requests will likely have very slow load times as the traffic must cross an ocean each way. Some third-party services use a CDN of their own to ensure that they are serving requests from locations close to your visitors, but not all will do so. You may need to ensure that you are using appropriate hostnames or parameters in your requests. CDN Finder is a convenient tool to investigate which CDNs (if any) a third-party tag is using. 
 Loading Scripts Asynchronously
 Blocking other resource requests from being made by the browser (often called “parser blocking”) is one of the most impactful (in a negative way) things a third-party resource can do. Historically, browsers have blocked while loading scripts to ensure that the page load experience is predictable. If scripts always load and evaluate in the same order, there are no surprises. The downside to this is that it takes longer to load the page.
 Fortunately, identifying and blocking third-party script resources is relatively easy. Both WebPageTest and PageSpeed Insights (free-to-use tools) highlight resource requests that block other resource requests from being made. These tools work on one page (URL) at a time, so you will need to use them on a representative set of URLs to pick up all the blocking tags on your site.  
 Depending on how the third-party tag gets onto the page, you may be able to change a blocking script into a non-blocking script. Modern browsers support attributes to the script tag that gives the browser flexibility to load resources in a non-blocking manner. These are your basic options:
 
 &lt;script&gt;Without an additional attribute, many browsers will block the loading of subsequent scripts until after the script in question is loaded and evaluated. With third-party scripts, this is not only a performance concern but also a potential for a single point of failure (SPOF).
 &lt;script async&gt;With async, the browser can download the script resource in parallel with other HTML parsing and downloading activity, but it will evaluate the JavaScript immediately once it is done downloading and pause HTML parsing while the script evaluation happens. If the script evaluation needs to happen early in the page load, this is the best choice.
 &lt;script defer&gt;With defer, the script load will happen in parallel with HTML parsing and the fetching of other resources, and the script will only be evaluated after the HTML is fully parsed. This is the best choice for any third-party tag whose evaluation is less important than a fast render experience for your visitor.
 
 Cascading StyleSheets
 Another kind of blocking that can be impactful to the user experience is render-blocking. Cascading StyleSheets almost always block page render while they are being downloaded and evaluated because the browsers do not want to render content on the screen only to have to change how it looks partway through the page load. For this reason, best practice advice is to load CSS resources as early as you can in the page load, so the browser has all the information to render the page as soon as possible. 
 Third-party CSS requests are uncommon (mostly limited to custom font support), but if for some reason they are part of your site design, consider loading them directly through script tags in the base page HTML or through your CDN. Using a tag manager will just introduce additional delay in getting a critical resource into the browser as quickly as possible.   
 Some Further Thoughts On Fonts
 Like CSS, custom fonts are also render-blocking. Fonts can radically change the visual appearance of text, so browsers do not want to render text on the screen only to have a visually disruptive change mid-page load. Unlike CSS, I see far more sites using third-party resources for their custom fonts, with Google Fonts and Adobe Typekit being the most popular.
 Some implementations of custom fonts also involve loading third-party CSS, which introduces additional render-blocking. The resource requests for these fonts (.woff, woff2, or .ttf files, usually) are also not always done early in the page load. This is a problem for performance and a potential single point of failure.
 Here are some ideas for managing third-party custom fonts on your site:
 
 Give serious consideration to whether you need custom fonts at all.Page load times will be faster without them, and if the custom font is almost visually identical to some of the fantastic pre-installed system fonts now available in modern browsers, the brand impression benefit may be outweighed by the cost of slightly slower page loads frustrating your visitors.
 If custom fonts are a requirement, consider how to deliver them as first-party resources.You may be limited by font licensing restrictions in this respect, and serving fonts from your own domains will result in delivering more bytes to visitors from your CDN or ISP, which can increase costs. On the other hand, you no longer have a SPOF vulnerability, you gain control over caching headers, and your visitors can avoid making connections to yet another third-party host and all the delays that it introduces.
 If you cannot avoid having third-party fonts on your site, consider using font-display properties in your CSS.Setting the font-display property to swap (instead of block), for example, allows the browser to use system fonts until the custom fonts can be swapped in. If the visual change of the custom font is not too disruptive, this could be the best choice to give your visitors the content as early as possible while giving them the brand experience when the fonts do load. The fallback value is another choice that can incorporate a shorter blocking period and otherwise before behaving as a swap. The CSS-Tricks website has good documentation on font-display.
 
 Two Script Management Solutions
 One interesting approach to managing the performance impact of third parties is to move as many of them as possible to load via Web Workers. The core idea is to reserve the main thread in the browser for your first-party core scripts and let the browser manage and optimize your resource-intensive third-party scripts using Web Workers. Doing this is not trivial since Web Workers are limited to asynchronous communications with the main thread and many third-party scripts except synchronous access to browser resources, such as documents and windows. 
 A new open-source project called Partytown provides a library that implements a communications layer to make this work. It is still in the early stages of development, and you would want to test extensively for potential weird side effects. It might also not work well with a tag manager system if that’s a part of your architecture.
 Akamai Script Management is a solution that uses Service Workers. This service essentially acts as a proxy inside the browser that has knowledge about the third-party services on the site and a policy about how to handle specific third-party requests. The policy can block requests for specific third parties, defer their request to later in the page load, or change the waiting time before throwing a timeout error for a request. If a third-party request is render blocking but that third-party service is down, for example, Script Management can mitigate the impact by reducing the length of time that the browser waits before deciding that the response is never going to arrive.
 Conclusion
 Third-party resource requests have become an integral part of the web. These services can provide value to your business, but they do come at a potential cost to the user experience.
  You need the right tools for detection and measurement and knowledge of the best practices that help reduce the negative impacts of third-party requests.
 A great way to start managing the impacts of third-party requests on your site’s user experience is to audit your site to see which and how many third-party domains and requests are being used. Next, use performance measurement tools to identify those that have the potential to degrade the user experience through render-blocking, resource contention, or single points of failure. 
 As you apply changes to mitigate the impact of third parties, develop a plan to use ongoing testing (such as Real User Measurement Services) to keep on top of site changes and unexpected changes to your third-party services.
 By carefully considering how third-party requests will fit into your site during the design stage, you can avoid the most significant negative impacts. With ongoing performance monitoring, you can ensure that new problems with third-party requests are identified early. Don’t sink your website with third parties!</content>
     </entry>
     <entry>
       <title>Happy birthday, WPE WebKit!</title>
         <link href="https://webkit.org/blog/12733/happy-birthday-wpe-webkit/"/>
       <updated>2022-05-31T16:00:40.000Z</updated>
       <content type="text">
 WebKit is the purring engine of Safari, it’s true, but it has numerous ports and many contributors. These ports are used for all sorts of things, from powering Sony PlayStations to driving millions of embedded devices all over the world. Embedded use cases like smart home appliances, digital signage, and automotive displays are largely possible thanks to WPE WebKit, the official port of WebKit specifically optimized for embedded devices. That port is maintained by Igalia, an open-source consultancy headquartered in Spain. Their work on WPE WebKit goes a long way toward explaining why Igalia is the most prolific external contributor to the WebKit codebase, accounting for almost 17% of all commits in 2021.
 Igalia recently celebrated the fifth birthday of WPE WebKit with a blog post on their WPE web site, covering its evolution from a fork of WebKitGTK to a Wayland-based renderer to a framework compatible with almost any rendering backend before its public launch on 21 April 2017. They also promise a series of articles to come profiling the people who work on WPE WebKit and talking about some of the technical aspects of advancing such a project. You can read more about it in their post. Happy 5th birthday, WPE!</content>
     </entry>
     <entry>
       <title>Thin Platforms</title>
         <link href="https://stratechery.com/2022/thin-platforms/"/>
       <updated>2022-05-31T15:31:43.000Z</updated>
       <content type="text">The Department of Justice’s 1998 complaint against Microsoft accused the company of, amongst other things, tying the Internet Explorer browser to the Windows operating system:
 
   Internet browsers are separate products competing in a separate product market from PC operating systems, and it is efficient to supply the two products separately. Indeed, Microsoft itself has consistently offered, promoted, and distributed its Internet browser as a stand-alone product separate from, and not as a component of, Windows, and intends to continue to do so after the release of Windows 98…
   Microsoft’s tying of its Internet browser to its monopoly operating system reduces the ability of customers to choose among competing browser products because it forces OEMs and other purchasers to license or acquire the tied combination whether they want Microsoft’s Internet browser or not. Microsoft’s tying — which it can accomplish because of its monopoly power in Windows — impairs the ability of its browser rivals to compete to have their browsers preinstalled by OEMs on new PCs and thus substantially forecloses those rivals from an important channel of browser distribution.
 
 In retrospect, the complaint feels quaint for three reasons:
 First, Microsoft won the browser wars, and it didn’t matter; after peaking at 95% market share in 2004, Internet Explorer was first challenged by Firefox, which peaked at 32% market share in 2010, and then surpassed by Chrome in 2012:
 
 The reasons ended up being both a condemnation and an endorsement of the libertarian defense of Microsoft’s actions, depending on your timeframe: sure, the company leveraged its operating system dominance to gain browser market share, but the company also made a great browser (I personally switched with the release of version 4). And then, with Version 6 and its position seemingly secured, the company just stopped development; that is what opened the door to first Firefox and then Chrome, both of which were downloaded and installed by end users looking for something better. The market worked, eventually.
 Of course, the reason the market could work is that Windows was an open platform: sure, Microsoft controlled (and allegedly abused) what could be preinstalled on a new computer, but once said computer was in a user’s hands they could install whatever they wanted to, including alternative browsers. That gets to the second reason why the complaint feels quaint: today having a browser pre-installed is de rigueur for operating systems, and Apple’s iOS goes much further than simply pre-installing Safari: all alternative browsers must use Apple’s built-in rendering engine, which means they can only compete on user interface features, not fundamental functionality.1
 The third reason has to do with Microsoft itself.
 Thick and Thin
 As I noted last week in an Update, one of the overarching themes of CEO Satya Nadella’s Build developer conference keynote was the seemingly eternal tech debate about thin versus thick clients (to dramatically simplify — and run the risk of starting a flame war — thin clients are terminals for a centralized computer, while thick clients are computers in their own right, that sync):
 
   The biggest takeaway from this keynote is that for developers, at least the ones that Microsoft is courting, the thin client model has won — although the truth, as is so often the case with tech holy wars, has ended up somewhere in the middle. Here is the key distinction: there is and will continue to be a lot of work that happens locally; all of the assumptions around that work, though, will be as if the work is being done on the server. For example:
 
 GitHub Codespaces is an explicitly online environment that you can temporarily use locally.
 Azure Arc provides the the Azure control plane for an on-premises development environment.
 The Azure Container Apps service and Azure Kubernetes Service enable developers to write locally in the same environment they deploy to the cloud.
 
   Moreover, several other of the announcements were about patching up limitations in cloud development relative to local: Microsoft Dev Box, for example, enables the deployment of cloud-based VMs that mimic a local development environment for things like app development; Microsoft Cloud PC (which was previously announced) does the same thing for client applications.
 
 What makes this shift so striking is that it is being articulated by Microsoft; after all, Windows (along with Intel) was the dominant winner of the thick client era. Yes, Windows Server was an integral part of Microsoft’s enterprise dominance, but the foundation of the company’s strategy — as evidenced by the tactics used in the fight against Netscape — was the fact that Windows was the operating system on the devices people used. That, by extension, was precisely why mobile was so disruptive to the company: suddenly Windows was only on some of the devices people used; iOS and Android were on a whole bunch of them as well.
 I’ve spent many articles writing about how Satya Nadella weaned Microsoft off of its Windows-centric strategy; the pertinent point in terms of this Article comes from Teams OS and the Slack Social Network:
 
   The end of Windows as the center of Microsoft’s approach, and the shift to the cloud, though, did not mean the end of Microsoft’s focus on integration, or its attempt to be an operating system; the company simply changed its definition of what an operating system was; Satya Nadella said at a press briefing in 2019:
 
     The other effort for us is what we describe as Microsoft 365. What we are trying to do is bring home that notion that it’s about the user, the user is going to have relationships with other users and other people, they’re going to have a bunch of artifacts, their schedules, their projects, their documents, many other things, their to-do’s, and they are going to use a variety of different devices. That’s what Microsoft 365 is all about.
     Sometimes I think the new OS is not going to start from the hardware, because the classic OS definition, that Tanenbaum, one of the guys who wrote the book on Operating Systems that I read when I went to school was: “It does two things, it abstracts hardware, and it creates an app model”. Right now the abstraction of hardware has to start by abstracting all of the hardware in your life, so the notion that this is one device is interesting and important, it doesn’t mean the kernel that boots your device just goes away, it still exists, but the point of real relevance I think in our lives is “hey, what’s that abstraction of all the hardware in my life that I use?” – some of it is shared, some of it is personal. And then, what’s the app model for it? How do I write an experience that transcends all of that hardware? And that’s really what our pursuit of Microsoft 365 is all about.
   
   This is where Teams thrives: if you fully commit to the Microsoft ecosystem, one app combines your contacts, conversations, phone calls, access to files, 3rd-party applications, in a way that “just works”…This is what Slack — and Silicon Valley, generally — failed to understand about Microsoft’s competitive advantage: the company doesn’t win just because it bundles, or because it has a superior ground game. By virtue of doing everything, even if mediocrely, the company is providing a whole that is greater than the sum of its parts, particularly for the non-tech workers that are in fact most of the market. Slack may have infused its chat client with love, but chatting is a means to an end, and Microsoft often seems like the only enterprise company that understands that.
 
 Note that line about “3rd-party applications”: if Teams is the Windows of Microsoft’s new services strategy, then it follows that the platform opportunity for developers in Microsoft’s ecosystem is itself centered on Teams; that’s exactly what Nadella described in the Build keynote:
 
 
   Let’s talk about the future of work and how we’re making apps more contextual and people-centric, so you can build a new class of collaborative applications. It starts with Microsoft Graph, which underlies Microsoft 365 and makes available to you information about people, their relationships, and all of their artifacts. Today we are seeing developers around the world enriching their apps with Microsoft Graph. In fact, more than half of the Microsoft 365 tenant are using custom-built and 3rd-party apps powered by the Graph. With Graph connectors ISVs can extend their applications and have them be discovered as part of the user’s everyday tasks, whether they are writing an email, meeting on Teams, or doing a search. For example, data from an app can appear directly in an organization’s search results, as you can see in the experience Figma is building here. You can compose a mail and @-mention files from these apps in-line, and you can access them in Teams chat too. Another way that you can create interactive experiences is by building live actionable loop component using adaptive cards like partner Zoho does. Your users can make decisions and take action like updating the status of a ticket right in the flow of work, and updates are always live, like this one across Outlook, Teams, and Zoho.
   When you combine the Microsoft Graph with Microsoft Teams, you combine the data that describes how people work together with the place they work together. It’s incredibly powerful, and developers are extending their apps into Teams and embedding Teams in their apps. In fact, monthly usage of 3rd-party apps and custom-built solutions on Teams has grown 10x over the last two years, and more and more ISVs are generating millions of [dollars in] revenue from customers using apps built on Teams.
 
 “Graph connectors” are the new APIs.
 Windows versus Teams
 If the Windows platform looked like this…
 
 …then the new Teams platform looks like this:
 
 There are a few important observations to make about these differences.
 First, in the PC era the monopoly that mattered was being the only operating system on a single device. This, to be clear, is a technical necessity — while a PC could dual-boot into different operating systems, only one could run at a time2 — but it was the foundation of Windows’ market monopoly. After all, whichever operating system was running on the most devices most of the time was the operating system that developers would target; the more developers on a particular operating system, the more popular that operating system would be amongst end users, resulting in a virtuous cycle, aka a two-sided network, aka lock-in, aka a monopoly.
 Once mobile came along, though, not only did the number of devices proliferate, but so did the need for new user interfaces, power requirements, hardware re-imagining, etc.; this made it inevitable that Microsoft would miss mobile, because the company was approaching the problem from the completely wrong perspective.3 At the same time, this proliferation of devices meant that the point of integration — which enterprises still craved — moved up the stack. I wrote in 2015’s Redmond and Reality:4
 
   That is because there is in fact a need for an integrated solution on mobile. Look at Box, for example: the company obviously has a cloud component, but they also have multiple apps for every relevant — and non-relevant! — platform resulting in much better functionality than what Microsoft previously had to offer. Multiply that advantage across a whole host of services and it starts to make sense for the CIO to modularize her backend services in order to achieve integration when it comes to how those services are accessed:
   
 
 This is exactly what Microsoft would go on to build with Teams: the beautiful thing about chat is that like any social product it is only as useful as the number of people who are using it, which is to say it only works if it is a monopoly — everyone in the company needs to be on board, and they need to not be using anything else. That, by extension, sets up Teams to play the Windows role, but instead of monopolizing an individual PC, it monopolizes an entire company.
 Second, developers had much more power and flexibility in the old model, because they had direct access to the underlying PC. This had both advantages — anyone could make an app that could do anything, and users could install it directly — and disadvantages — anyone could make an app that could do anything, and users could install it directly. In other words, the same openness of the PC that presented an opportunity for Firefox and Chrome to dethrone Internet Explorer — and for Netscape to exist in the first place — also presented an opportunity for viruses, malware, and ransomeware.
 This latter point is the justification that Apple returns to repeatedly for its App Store model, even though a significant portion of the increased security of mobile devices is due to fundamentally different architectural choices made in designing the underlying operating system. Then again, these arguments go hand-in-hand: it’s those architectural choices in iOS (and Android) design that make App Store control possible; the broader point is that mobile set the expectation that developer freedom — and by extension, opportunity — would be limited by the operating system owner.
 A thin platform like Teams takes this even further, because now developers don’t even have access to the devices, at least in a way that matters to an enterprise (i.e. how useful is an app on your phone that doesn’t connect to the company’s directory, file storage, network, etc.). That means the question isn’t about what system APIs are ruled to be off-limits, but what “connectors” (to use Microsoft’s term) the platform owner deigns to build. In other words, not only did Microsoft build their new operating system as a thin platform, they ended up with far more control than they ever could have achieved with their old thick platform.
 Stripe OS
 Build wasn’t the only developer conference last week: Stripe also held Stripe Sessions, and one of the tentpole sections of the keynote was called “Finance OS”. Here’s Stripe co-founder and President John Collison:
 
 
   We’ve talked about payments, and how they’re highly strategic, and rapidly fragmenting, and we’ve talked about the business model innovations of adaptive enterprises and fintech everywhere. These trends are great news for the Internet economy, but a challenge for finance and business operations teams. The rate limiter for so many new opportunities isn’t the idea for a great product; it’s the mundane foundations. “Can we build for this? Can we get international operations off of the ground? Can we expand when we’re still not closing our books on time?” It’s never just about having the idea for a great product, it’s about being able to operate it, and that’s why we’re building a modern operating system for finance, and like any good OS, we’re focused on nailing the basics.
 
 Those basics included features like invoicing, billing, taxes, revenue recognition, and data pipelines, all of which sit on top of the various ways to gather, store, and distribute money that Stripe has abstracted away:
 
 This image, given its similarity to the one above, makes clear what was coming next:
 
 
   So we just heard about core revenue management capabilities, like invoicing, subscription billing, and handling tax. Even if you’re not using Stripe, these are the things you should want running like clockwork.
   But what about everything else? It’s like any operating system: core functionality needs to work perfectly out of the box, but the breadth of functionality of the platform is also really important, having an app to solve every use case. For things like customer messaging, you might want to use something like Intercom; for contracts, DocuSign; or, you might just to build your own tool. But often these workflows are highly integrated, so for years our users have been asking us for the tools of their choice to interoperate with Stripe…
   We’re thrilled to launch today Stripe Apps and the Stripe App Marketplace, where you can find or build best-of-breed tools that work naturally with Stripe.
 
 There are the missing pieces!
 
 “Working naturally with Stripe” doesn’t simply mean access to Stripe’s APIs; it means fitting in to the Stripe dashboard — Stripe is even including pre-made UI components so that 3rd-party apps look like they were designed by the fintech company:
 
 This is another thin platform: developers don’t have access to the core financial data of a company, nor does IT want them to; instead the opportunity is to sit on top of an abstraction layer that covers all of a company’s money-moving pieces, and to fit in as best as you can.
 
 Of course I am covering the Build and Stripe Sessions keynotes together because both happened the same week; at the same time, it was a fortuitous coincidence, because Stripe’s announcement brings important context to Microsoft’s approach. After all, I used the magic word “monopoly”; the truth, though, is that not only was an operating system monopoly inevitable, it also made perfect sense from a user perspective that important functionality — like browsing — became integrated with the core OS.
 Collison made the case as to why similar considerations should be front-and-center for thin platforms — there are things “you should want running like clockwork.” Microsoft would make a similar argument about Teams and its incorporation of things like file storage and communications, and, I would argue, Teams’ success in the market relative to Slack is evidence that the argument is a compelling one to customers. That Microsoft has so often seemed like the only enterprise company actually building for an enterprise’s ends, instead of solipsistically obsessing over being best-of-breed for one specific means, seems worth celebrating and emulating, not condemning and complaining.
 At the same time, it is also worth mourning the slow eclipse of the thick client model. Yes, things like malware were a pain and a drain on productivity, and the SaaS model has led to a plethora of new products that are accessible to companies without needing an IT department, but the big downside of the thick model in terms of what could go wrong and the necessity of IT created the conditions for massive upside, in this case the opportunity to make new apps — and by extension, new companies — without needing any permission, “connectors” or pre-made UI components. Alas, the tech industry is past the end of the beginning; welcome to middle age, where the only thickness is your waistline.
 I wrote a follow-up to this Article in this Daily Update.
 Apple argues, not without merit, that this is for security reasons; critics argue, with considerable | merit, that this restricts innovation on iOS and meaningful competition with the App Store. ↩Absent virtualization, although that wasn’t really feasible on user-level PCs at the time Windows was establishing its dominance ↩This interview with Tony Fadell includes an excellent discussion on this point in the context of Intel, which applies just as much to Microsoft. ↩With, I am ashamed to admit, probably the worst drawing in the history of Stratechery; as I recall I was late to a Chinese New Years’ Eve dinner! ↩</content>
     </entry>
     <entry>
       <title>Expand Your Horizons (June 2022 Desktop Wallpapers Edition)</title>
         <link href="https://smashingmagazine.com/2022/05/desktop-wallpaper-calendars-june-2022/"/>
       <updated>2022-05-31T12:30:00.000Z</updated>
       <content type="text">There’s an artist in everyone. Some bring their creative ideas to life with digital tools, others capture the perfect moment with a camera or love to grab pen and paper to create little doodles or pieces of lettering. And even if you think you’re far from being an artist, well, it might just be hidden somewhere deep inside of you. So why not explore it?
 
 For more than eleven years, our monthly wallpapers series has been the perfect opportunity to do just that: to break out of your daily routine and put your creative skills to the test. And, well, creative folks from across the globe once again took on the challenge this month and created unique and inspiring wallpapers for June 2022.
 
 The wallpapers in this collection come in versions with and without a calendar and can be downloaded for free. As a little bonus goodie, we also compiled some designs from our wallpapers archives at the end of this post. Maybe you’ll spot one of your almost-forgotten June favorites, too? A big thank-you to everyone who shared their designs with us — this post wouldn’t exist without you!
 
 
 You can click on every image to see a larger preview,
 We respect and carefully consider the ideas and motivation behind each and every artist’s work. This is why we give all artists the full freedom to explore their creativity and express emotions and experience through their works. This is also why the themes of the wallpapers weren’t anyhow influenced by us but rather designed from scratch by the artists themselves.
 Submit a wallpaper!Did you know that you could get featured in our next wallpapers post, too? We are always looking for creative talent.
 
 
 
 Editor’s Note
 In this month’s post, you will notice that some of the wallpapers are dedicated to Ukraine and its traditional embroidery patterns found in different regions of the country. As a design community, we can’t be silent in these times. It’s our obligation to help as much as we can, and so we are donating all proceeds of the “Interface Design Checklists PDF” to support Ukraine.We have already donated 16,944.73 EUR (US$ 18,663.86) to Aktion Deutschland Hilft e.V. and will continue to donate to other organizations as well. A heartfelt THANK YOU to our wonderful community for all of their help and support! 💞
 
 
 Create Your Own Path
 “Nice weather has arrived! Clean the dust off your bike and explore your hometown from a different angle! Invite a friend or loved one and share the joy of cycling. Whether you decide to go for a city ride or a ride in nature, the time spent on a bicycle will make you feel free and happy. So don’t wait, take your bike and call your loved one because happiness is greater only when it is shared. Happy World Bike Day!” — Designed by PopArt Studio  from Serbia.
 
 
 preview
 with calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 Old Kyiv
 “This picture is dedicated to Kiev (Kyiv), the capital of Ukraine. It is loosely based on a 13th century map — this is what the center of Kyiv looked like ca. 900 years ago! The original map also included the city wall — however, I decided not to wrap the buildings into the wall, since in my dream world, a city would not need walls.” — Designed by Vlad Gerasimov from Georgia.
 
 
 preview
 with calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1440x900, 1440x960, 1400x1050, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 without calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1440x900, 1440x960, 1400x1050, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 
 
 World Environment Day
 “On June 5th we celebrate World Environment Day — a moment to pause and reflect on how we impact Earth’s health. A few activities represented in this visual include conserving energy and water, shopping and growing local, planting flowers and trees, and building a sustainable infrastructure.” — Designed by Mad Fish Digital from Portland, OR.
 
 
 preview
 with calendar: 320x480, 1024x1024, 1280x720, 1680x1200, 1920x1080, 2560x1440
 without calendar: 320x480, 1024x1024, 1280x720, 1680x1200, 1920x1080, 2560x1440
 
 
 Summer Chamomile
 “Our designers were inspired by the lightness and innocence of June, as well as the desire to use bright colors. More calendars are here.” — Designed by MasterBundles from Ukraine.
 
 
 preview
 with calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 Ukrainian Embroidery
 
 Vlad Gerasimov from Georgia designed a series of wallpapers in which he explores traditional embroidery patterns found in different regions in Ukraine and the stories behind them.
 
 Kherson
 “This picture is dedicated to the Kherson region. Forms of fruits, flowers, leaves close to nature are typical of Kherson region. Viburnum motifs are usually found on women’s shirts, it is a symbol of beauty. Oak motifs are most often seen on boys’ shirts, they are a symbol of strength, development, and life. So, the boys wore a miraculous amulet of life-giving power of some kind.” — Designed by Vlad Gerasimov from Georgia.
 
 
 preview
 with calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 without calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 
 
 Mykolaiv
 “This picture is dedicated to the Mykolaiv region. A large number of embroidered products of Mykolaiv region is characterized by floral ornaments. The symbol of the triangle, which is found on embroidered shirts, has long been associated with the element of fire, where fire represents the striving for freedom. The main colors were mostly red, blue, black, and yellow.” — Designed by Vlad Gerasimov from Georgia.
 
 
 preview
 with calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 without calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 
 
 Kirovohrad
 “This picture is dedicated to the Kirovohrad region. From ancient times in Ukraine poppies were consecrated. People and cattle were sown with them. People believed that the poppy has a magical power that protects against all evil. Poppy motifs can be seen on the shirts of the Kirovohrad region.” — Designed by Vlad Gerasimov from Georgia.
 
 
 preview
 with calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 without calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 
 
 Ternopil
 “This picture is dedicated to the Ternopil region. Embroidery of Ternopil region is characterized by rich, dark, up to black, colors. Made of wool, thick, almost without gaps, the ornaments completely cover the sleeves of women’s shirts. Floral motifs are often found on the shirts of Ternopil region, in particular marigold flowers, sunflowers, mustard.” — Designed by Vlad Gerasimov from Georgia.
 
 
 preview
 with calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 without calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 
 
 Sumy
 “This picture is dedicated to the Sumy region. Geometric and plant-geometrized patterns were embroidered on the shirt from Sumy region. Most often there is an embroidered pattern called broken branch. This pattern has its ancient and deep meaning and depicts the creation of our Galaxy.” — Designed by Vlad Gerasimov from Georgia.
 
 
 preview
 with calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 without calendar: 800x480, 800x600, 1024x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1440x960, 1600x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 2560x1600, 2880x1800, 3072x1920, 3840x2160, 5120x2880
 
 
 Looking At The Stars
 “This month we travel to the stars. We find a planet and we sit down to observe the universe.” — Designed by Veronica Valenzuela from Spain.
 
 
 preview
 with calendar: 640x480, 800x480, 1024x768, 1280x720, 1280x800, 1440x900, 1600x1200, 1920x1080, 1920x1440, 2560x1440
 without calendar: 640x480, 800x480, 1024x768, 1280x720, 1280x800, 1440x900, 1600x1200, 1920x1080, 1920x1440, 2560x1440
 
 
 I’m So Good
 Designed by Ricardo Gimenes from Sweden.
 
 
 preview
 with calendar: 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 3840x2160
 without calendar: 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 3840x2160
 
 
 Awesome Summer Sunflowers
 “Sunflowers are one of the symbols of Ukraine. There is especially a lot of them in the south, where the fighting is now taking place. Our designers were inspired by our nature and belief in our victory and therefore created this calendar. More calendars are available at https://masterbundles.com/. Welcome!” — Designed by MasterBundles from Ukraine.
 
 
 preview
 with calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 Mr Broccoli Doesn’t Like You Either
 Designed by Ricardo Gimenes from Sweden.
 
 
 preview
 with calendar: 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 3840x2160
 without calendar: 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 3840x2160
 
 
 Summertime
 Designed by ActiveCollab from the United States.
 
 
 preview
 with calendar: 1080x1920, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 without calendar: 1080x1920, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 
 
 Oldies But Goodies
 
 Ready for more? Below you’ll find a little best-of from past June editions. Please note that these wallpapers don’t come with a calendar. Enjoy!
 
 Travel Time
 “June is our favorite time of the year because the keenly anticipated sunny weather inspires us to travel. Stuck at the airport, waiting for our flight but still excited about wayfaring, we often start dreaming about the new places we are going to visit. Where will you travel to this summer? Wherever you go, we wish you a pleasant journey!” — Designed by PopArt Studio from Serbia.
 
 
 preview
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 Summer Party
 Designed by Ricardo Gimenes from Sweden.
 
 
 preview
 without calendar: 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440, 3840x2160
 
 
 Dancing In The Summer Moonlight
 “If you’re happy and you know it, show some dance moves, because summer is finally here!” — Designed by ActiveCollab from the United States.
 
 
 preview
 without calendar: 1080x1920, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 Summer Coziness
 “I’ve waited for this summer more than I waited for any other summer since I was a kid. I dream of watermelon, strawberries, and lots of colors.” — Designed by Kate Jameson from the United States.
 
 
 preview
 without calendar: 320x480, 1024x1024, 1280x720, 1680x1200, 1920x1080, 2560x1440
 
 
 Solstice Sunset
 “June 21 marks the longest day of the year for the Northern Hemisphere — and sunsets like these will be getting earlier and earlier after that!” — Designed by James Mitchell from the United Kingdom.
 
 
 preview
 without calendar: 1280x720, 1280x800, 1366x768, 1440x900, 1680x1050, 1920x1080, 1920x1200, 2560x1440, 2880x1800
 
 
 Oh, The Places You Will Go!
 “In celebration of high school and college graduates ready to make their way in the world!” — Designed by Bri Loesch from the United States.
 
 
 preview
 without calendar: 320x480, 1024x768, 1280x1024, 1440x900, 1680x1050, 1680x1200, 1920x1440, 2560x1440
 
 Deep Dive
 “Summer rains, sunny days, and a whole month to enjoy. Dive deep inside your passions and let them guide you.” — Designed by Ana Masnikosa from Belgrade, Serbia.
 
 
 preview
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 
 
 Ice Creams Away!
 “Summer is taking off with some magical ice cream hot air balloons.” — Designed by Sasha Endoh from Canada.
 
 
 preview
 without calendar: 320x480, 1024x768, 1152x864, 1280x800, 1280x960, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1920x1080, 1920x1200, 2560x1440
 
 
 Strawberry Fields
 Designed by Nathalie Ouederni from France.
 
 
 preview
 without calendar: 320x480, 1024x768, 1280x1024, 1440x900, 1680x1200, 1920x1200, 2560x1440
 
 
 Bauhaus
 “I created a screenprint of one of the most famous buildings from the Bauhaus architect Mies van der Rohe for you. So, enjoy the Barcelona Pavillon for your June wallpaper.” — Designed by Anne Korfmacher from Germany.
 
 
 preview
 without calendar: 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 Merry-Go-Round
 Designed by Xenia Latii from Germany.
 
 
 preview
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 Summer Surf
 “Summer vibes…” — Designed by Antun Hirsman from Croatia.
 
 
 preview
 without calendar: 640x480, 1152x864, 1280x1024, 1440x900, 1680x1050, 1920x1080, 1920x1440, 2650x1440
 
 
 Melting Away
 Designed by Ricardo Gimenes from Sweden.
 
 
 preview
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1400x1050, 1440x900, 1366x768, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 Pineapple Summer Pop
 “I love creating fun and feminine illustrations and designs. I was inspired by juicy tropical pineapples to celebrate the start of summer.” — Designed by Brooke Glaser from Honolulu, Hawaii.
 &lt;img src&#x3D;&quot;https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/16db22ee-c7f8-47a3-856c-992c82cd61f9/june-16-pineapple-summer-pop-preview-opt.png&quot; alt&#x3D;&quot;Pineapple Summer Pop&quot;
 
 
 
 
 preview
 without calendar: 640x480, 800x600, 1024x768, 1152x720, 1280x720, 1280x800, 1280x960, 1366x768, 1440x900, 1680x1050, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 
 Fishing Is My Passion!
 “The month of June is a wonderful time to go fishing, the most soothing and peaceful activity.” — Designed by Igor Izhik from Canada.
 
 
 preview
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1400x1050, 1440x900, 1600x1200, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 Getting Better Everyday
 “The eternal forward motion to get better and excel.” — Designed by Zachary Johnson-Medland from the United States.
 
 
 preview
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 Expand Your Horizons
 “It’s summer! Go out, explore, expand your horizons!” — Designed by Dorvan Davoudi from Canada.
 
 
 preview
 without calendar: 800x480, 800x600, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 Happy Squatch
 “I just wanted to capture the atmosphere of late spring/early summer in a fun, quirky way that may be reflective of an adventurous person during this time of year.” — Designed by Nick Arcarese from the United States.
 
 
 preview
 without calendar: 320x480, 640x480, 800x480, 800x600, 1024x768, 1024x1024, 1152x864, 1280x720, 1280x800, 1280x960, 1280x1024, 1366x768, 1400x1050, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440
 
 
 Nine Lives
 “I grew up with cats around (and drawing them all the time). They are so funny… one moment they are being funny, the next they are reserved. If you have place in your life for a pet, adopt one today!” — Designed by Karen Frolo from the United States.
 
 
 preview
 without calendar: 1024x768, 1024x1024, 1280x800, 1280x960, 1280x1024, 1366x768, 1440x900, 1600x1200, 1680x1050, 1680x1200, 1920x1080, 1920x1200, 1920x1440, 2560x1440</content>
     </entry>
     <entry>
       <title>Do we have a lack of developers or a false assumption what the job is?</title>
         <link href="https://christianheilmann.com/2022/05/31/do-we-have-a-lack-of-developers-or-a-false-assumption-what-the-job-is/"/>
       <updated>2022-05-31T10:03:04.000Z</updated>
       <content type="text">
 
 	Last week I was at build Europe and talked to a lot of people about the developer market. The general consensus was that there is a huge lack of developers to hire. That there is not enough talent. When I poked further and asked about how people assess talent, it boiled down to people having the right degrees. We are desperate to have more computer scientists.
 
 	But do we really need computer scientists? I’m not saying a solid foundation in computing isn’t great and there are for sure a lot of tasks that need experts. But I also see that the down-to-the-metal work is a tiny percentage of the market. People who build web sites or apps are hardly ever starting from scratch.
 
 	Instead we build using packages of other people, frameworks to fail fast and re-iterate, reusable components and build systems. None of these are taught in schools in universities for the main reason that they are short-lived and constantly evolving. What stack a company uses changes over time and depending on the project. How we apply the stack is decided by the lead engineers and architects in the company.
 
 	Considering that we – for better or worse – build on the work of others aren’t the people we really look for another skillset than computer scientists? We need flexible implementors, people who can learn a new environment quickly and assess the quality of components they use.  We hardly ever get the chance or find the need to write code from scratch.  Digital librarians, so to say. A good librarian doesn’t know the content of all the books in the library but where to look to find the right information.
 
 	The quality of people like these is harder to assess, for sure. But a diploma also doesn’t mean people are effective developers either. Being someome without a degree myself I always loved that our market is accessible to many people. Maybe that should be a thing we should strive to celebrate and embrace more in our search for new people.
 
 	Retention of people in our market is terrible. By the time you are used to how a colleague ticks, they are likely to leave. Often the reason is that we hire programmers to build products using off-the-shelf components. No wonder they get bored.
 
 	Maybe we can turn that around by training people on the job better and lower the barrier to entry instead.</content>
     </entry>
     <entry>
       <title>Smashing Podcast Episode 47 With Sara Soueidan: Why Does Accessibility Matter?</title>
         <link href="https://smashingmagazine.com/2022/05/smashing-podcast-episode-47/"/>
       <updated>2022-05-31T05:00:00.000Z</updated>
       <content type="text">This article is a sponsored by Storyblok
 In this episode of the Smashing Podcast, we ask why accessibility really matters and why it is so important to get it right. Smashing’s Vitaly Friedman talks in-depth to Sara Soueidan to find out.
 
 
 Show Notes
 
 Sara’s personal website
 Sara on Twitter
 Practical Accessibility Online Course (coming soon)
 
 Weekly Update
 
 “Kubernetes For Frontend Developers” written by Benjamin Ajibade
 “The Ultimate Free Solo Blog Setup With Ghost And Gatsby” written by Greg Dickens
 “Lesser-Known And Underused CSS Features In 2022” written by Adrian Bece
 “Understanding Weak Reference In JavaScript” written by Frank Joseph
 “Manage Accessible Design System Themes With CSS Color-Contrast()” written by Daniel Yuschik
 
 Transcript
  Vitaly: She’s an independent web user interface and design systems engineer, author, speaker, and trainer based in Lebanon. She worked with companies and agencies all around the world from Netflix, and Telus, and the Royal Schiphol Group at Amsterdam airport, just to name a few, where she built digital products with focus on accessibility performance and of course cutting edge tech.
 Vitaly: Now, she also writes beautiful and very comprehensive, very, very comprehensive articles all around front-end SVG accessibility on her wonderful blog. And she’s also working, which might be a rumor; we’ll find out on her very own video course on web accessibility. So, in all, she’s an expert in creating accessible and beautiful interfaces.
 Vitaly: But did you know that she loves tea, drawings, and birds and has raised more than a dozen of them throughout her life? So, I shouldn’t be surprised to hear the birds chirping when we have a call just now. My smashing friends, please welcome Sara Soueidan. Hello, Sara, how are you?
 Sara: I am smashing. How are you?
 Vitaly: That’s wonderful. I don’t know. I feel like coffee today. I had already three, and I feel like I should get forth, but you’re not big on coffee, are you?
 Sara: No. I have my matcha sitting right next to me right now.
 Vitaly: Okay. That would be a very surprising start to the conversation. But what is your favorite tea, if I may ask?
 Sara: My favorite tea, if we’re not counting matcha as tea, even though it is actually tea, but if... okay. So, I’d say it’s either a matcha or ginger tea. I love ginger.
 Vitaly: Okay. Now, dear friends, if you ever want to ship any gifts to Sara, you know what to ship. Now, staying on the topic of food and drinks, and beverages, it’s interesting, Sara, because I know we’ve known each other for, I don’t know how many years now. And we even shared pizzas on very different occasions in various parts of the world, that surely counts for something.
 Vitaly: And one thing that really astonishes me when I think about our conversations and I think about you as a personality and just the incredible work that you put on the web, it’s just incredibly difficult for me to find many people who are more passionate about accessibility as you are.
 Vitaly: So, maybe you could give us a bit of a background of where this genuine empathy and excitement about accessibility comes from. Where did it all start, Sara? Where?
 Sara: Do you want the long version answer or the short version answer?
 Vitaly: The very long answer, if I can.
 Sara: Are you sure?
 Vitaly: No.
 Sara: Yes.
 Vitaly: Okay. But make a choice, whatever works for you works for me.
 Sara: Okay. So, if you want some background, I’ve always been the kind of person who loves helping people, even if they don’t directly ask for it. So, I’m just going to give you a quick example. Back when I was in college, I think it was my second year in college or in university, and it was the start of the year, and everyone was in the university, and we were registering and doing all the necessary paperwork that we needed to do to start going to class.
 Sara: And there were a lot of people, basically. And there was this one old man, he was standing in the middle of the crowd, and he was carrying a few papers in his hand and a pen. And he looked absolutely clueless. I could tell from his facial expressions that he was lost. He had no idea what he was supposed to do.
 Sara: So, I approached him and I asked him... I always do this, by the way. I’m not sure if this is a good thing or a bad thing, but I tend to do this with strangers a lot. So, I said, “Is there anything I can help you with?” He looked at me, and he said, “I have no idea basically how to fill the paperwork in.”
 Sara: He was there to register his son, who couldn’t be there. So, he was registering him instead, and he didn’t know how to do it. And I know how difficult it can be the first time. Because my first time, when I went to college for the first time, in the first year, it took me four days to register because there was no one there to help you.
 Sara: You can either find your way on your own, or you would just have to ask people. And if you don’t ask anyone, you’re just kind of stand there like that old man did. So, I asked him if I could help him, and he welcomed my help. I took the paperwork. I took the pen. I started asking him questions and filling in the paperwork for him.
 Sara: And then, when I finished, I told him exactly where to go next, exactly what to do. And I basically just helped him as much as I could. I do this with a lot of people. I don’t know; I think it’s just part of who I am. Helping people makes me feel amazing.
 Sara: Even the smallest acts of anything that you do in your daily life makes them meaningful and gives them purpose. So, to know that I have contributed a positive thing, no matter how small, into someone else’s life is wonderful. And I want my work to have a purpose, as well.
 Sara: So, when I started my career as a developer, I think it was in 2013, and this is something that I’m sharing for the first time, I went through a few years where I felt like what I was doing wasn’t very meaningful, to be honest. So, I was just doing what I was doing just to make some money, make a living, and that was it.
 Sara: But I would get designs someone made and turn them into something that worked, which is nice because I love doing that. But as always, I kept asking myself for years like, “What good is this contributing to the world?” I wanted to feel like the code that I’m writing can make a difference. And it took some time for me to finally get there.
 Sara: So, I started changing my path, so to speak, by choosing the clients that I wanted to work with. I started choosing clients that did meaningful things in the world. That way, if I help them create a website to expand the reach, it meant that I was contributing to something good in this world, as well.
 Sara: And I even expressed that on my Hire Me page, where I’m explicitly clear that I’m looking to do something meaningful in the world. So, there is that. And on the other hand, I’ve always been fascinated with design, in general. I’ve never taken a design class in my life, not in university, not outside of university, but I’ve always been interested in design because of how it directly impacted people’s lives.
 Sara: You can maybe start to see connection here. So, I’m going to give an example from the adjacent design field. Many people who have been following me for years know that I’m fascinated and very passionate towards interior design. And the best thing I’ve ever read about interior design is that interiors should be designed around how we live.
 Sara: So, how do you decide what you put in the kitchen, for example? How do you decide how much space you give a certain feature or piece of furniture in the house? The answer is based on how often it is needed by the people who will live in this house.
 Sara: A great interior designer will sit down with their client and ask them questions like, “How do you start your day? What does your typical day look like? What do you do when you wake up in the morning? Do you work from home? Do you like having people over? Do you like entertaining? What are your hobbies?”
 Sara: And the answers to all these questions they asked creates the framework and guides the interior design decisions of the house that the client will live in. The house is designed and built around the client’s lives, not the other way around.
 Sara: And I love that because if design isn’t about people, then it is selfish, right? Then, you’re not really designing for people anymore. And accessibility and inclusive design in general, but accessibility is not the same as inclusive design. Accessibility design is all about people.
 Sara: There is no room for selfishness, in my opinion. So, what you design and build either works for the user or it doesn’t. And if it doesn’t, then it needs to change because what good is anything that you built for people if they can’t use it?
 Sara: So, connecting these two things, I love design, and I love designing for people, and I love helping people. So, when I first learned about accessibility, and I found out that some of the work that I was building prior to that was possibly creating barriers to access for people.
 Sara: I felt horrible. And I started feeling more responsibility, and I started digging more into accessibility and learning more about it. And what I love the most about it is it makes me feel more like a designer. Why? Because, well, I do believe that all of us, we are designers, one way or another.
 Sara: And the work that we do and the decisions that I make as a design engineer when I write code, these decisions have a direct impact on the user experience, and that makes me a designer. So, every decision that I make has a direct impact on the experience and the inclusivity and accessibility of the interfaces I built.
 Sara: A decision such as, for example, what HDML element I choose to use, how do I apply the styles because CSS affects semantics, which affect accessibility, which development strategy I follow, which is a progressive enhancement or something else. Whether I use platform features or a third-party library, which library do I choose?
 Sara: How does it perform? All of these things, they impact your work, and eventually, the user experience. And design should be about people. Accessibility is about people, which means that it gives code purpose. It gives the work that I do purpose, which brings me back... the first thing that I mentioned, I want to feel like what I’m doing has a purpose and is actually beneficial for people.
 Sara: And this is why I love accessibility, in general, because it is a concrete, practical type of design, something either works or it doesn’t. And there is no room for bad decisions because it’s about the user, not about you. So yeah, I’m passionate about accessibility because-
 Vitaly: I could tell. Yeah, that’s incredible to hear. And actually, a few things that have really connected the dots for me, as well, because when I think about the work that I’m doing, as well, I feel like... we don’t even notice that sometimes, but all these minor decisions about the labeling of navigation and the way we design navigation, and the way we make buttons to look like buttons.
 Vitaly: All those things can have tremendous impact. We might feel like, “Okay, we’re just moving pixels around.” And we’re just making things a little bit more nice, right? But I think that the right significant decisions that we end up, that we use to really help somebody in different situations, complete the task, to find information that they need, and so on.
 Vitaly: And one thing that really comes up in my mind every time we have this conversation is that you often identify yourself with a design engineering role. And I see that some teams are moving towards that role now. Where in the past, it was just a developer, and then it was just a designer, and then it was a front-end developer and backend developer.
 Vitaly: And if you have an interface designer, you have to use the experience designer. So, maybe you could share a little bit of light on how you see design engineering as a role. And maybe many of our listeners now actually are design engineers without even knowing that. So, how would you define it? And do you think that companies actually understand what it means?
 Sara: I have actually an essay, not just an article, an essay that is all about just defining the role of a design engineer. And from my point of view, I even have a domain name that I purchased just to put that on it. If I try to describe it with just a few words because I already gave a super long answer to the previous question-
 Vitaly: That was a wonderful answer. It wasn’t long at all.
 Sara: Thank you. Okay. So, as design engineers, we specialize in implementing designs. That is the general description, right? We specialize in implementing designs, but then, of course, there is how you implement it. So, I like to label myself as an inclusive design engineer, which is something that I’m doing on my new website, which is not public yet.
 Sara: So, an inclusive design engineer is someone who, in my opinion, uses accessibility and progressive enhancement as a framework to build inclusive interfaces. But, as a design engineer, in general, is someone who works directly with designers, hopefully, because this is what I think the role should be.
 Sara: We should be working directly with designers, helping them make decisions and informing design decisions with our accessibility and code knowledge, basically, because they complement each other. We write HDML. We write CSS. We write presentational JavaScript, mostly.
 Sara: And with a strong focus on accessibility, hopefully, because that should be part of every design engineer’s job. And then, of course, the strategy that you choose and the frameworks that you use, and I’m not talking about CSS frameworks or JavaScript frameworks here, but like I said, this... let me just call it the strategy.
 Sara: Strategies that you use and what exactly you focus on, and how you do your job as a design engineer probably differs between people because like I said, I’m a progressive enhancement advocate, others are maybe not. So, we would differ in these small details.
 Sara: But a design engineer is someone who works with designers, helps inform design decisions and makes sure that designs are implemented in a way that hopefully works for as many users as possible.
 Vitaly: Right. So then, as a design engineer, at this point, how would you then actually work? So, what would your process be like? So, when you think about implementing a particular design, do you break it down into components? Do you think about navigation landmarks first?
 Vitaly: How do you actually start building things? What is your maybe mindset in that framework of getting accessible results? Maybe you could describe it a little bit.
 Sara: Okay. So, when you say that you divide the... you not divide, basically looked at the design and then start thinking in components, that very much depends on the process that you work with the designer. So, if I’m working with someone who has handed over a design, like an entire page, the process is different from when I’m working with the designer, like I did with Yan Persy.
 Sara: I’ve mentioned him multiple times on Smashing Hour, by the way, because working with him was one of my favorite ways of working with the designers because he didn’t have like a full finished design for me to implement. We worked in tandem. We worked together. And he changed some of the things in the design.
 Sara: So, for example, he wasn’t using a responsive type scale like we do in front-end development now using CSS variables and viewport units, et cetera. So, we had this discussion, and then he shifted the way the design process went. It was different.
 Sara: So, we both started building the site in terms of components, and then assembling those components into what we called slices back then, and then assembling the slices into the entire website. So, how I start from whether it’s components or not, depends on the process that I work with the designer.
 Sara: But then, if I get a little bit more technical, like if I have a component that I want to build, how do I go about doing that in layers, I would say? Again, progressive enhancement, the first thing that I think about is how does this work? How does this look like?
 Sara: How does someone perceive this if JavaScript is disabled? And what happens if there is no style? So, I just start with the bare minimum, HTML, because HTML defines the semantics. The semantics give meaning to the content that I’m creating.
 Sara: So, which HTML elements do I need to tell the user what this thing is to give semantics? If HDML already contains an element that represents this component or this element that I’m building, I use that. If not, then I start to thinking about ARIA attributes; which ARIA attributes do I need?
 Sara: How much ARIA do I need? ARIA is... it’s not an enhancement; it’s necessary for a lot of dynamic and interactive components. But I always try to think of it as a last resort, not a first one, so always semantic HTML first. How much can I get done with just semantic HTML?
 Sara: How accessible is it? Do I need something? Do I need to polyfill some semantics using ARIA? If I do, then I start thinking about that. And then applying CSS, and how does CSS affect these semantics? Does it? Does it not?
 Sara: Do I have to do something extra to make sure that something remains accessible after I style it? Like, for example, if you stripped away the default list styles on list elements, which is something you probably... and many people probably already know by now, if you set list style to none, for example, on an unordered list, then Safari or WebKit, WebKit, in general, is going to remove the semantics of the list.
 Sara: And VoiceOver is not going to announce that anymore. So, what do I do in that case? Do I need those semantics? Do I go into the HTML and add them again using a role attribute or not? So, I think about this stuff in layers. Start with HTML semantics. Do I need ARIA? How do I style this? And then, interactivity is always the last layer that I think about and that I build into components.
 Vitaly: Right. Makes sense. It’s interesting what you’re saying that it’s a process. It doesn’t seem like a simple process, especially when you think about like literally implementing quite a complex interface, which may also have all kinds of different views and maybe single page application in the back and so on.
 Vitaly: And one thing that I’m struggling with when I’m doing work with clients and trying to make things more inclusive and interfaces maybe a bit more usable is that very often web accessibility is still seen as this little thing. Like, “Okay, that’s just semantics.” “Okay, so we’re going to use buttons for buttons.”
 Vitaly: But it’s actually much, much, much more than that. And I’m wondering, what do you think... like, where do we actually stand in terms of accessibility today? It’s very hard for me, personally, for example, to imagine a new project being released without even considering accessibility.
 Vitaly: I think that might have been possible maybe a decade ago. I think today, it would be very difficult to imagine a brand new project that’s going to be advertised everywhere on posters that is not accessible at all, some parts accessible, but maybe not everything.
 Vitaly: So, what do you think, has accessibility not become just the natural part of every design implementation process, or are we way, way, way, far away from this yet?
 Sara: I think we are not too far, but we’re still far. So, there is definitely a lot more awareness on accessibility. I hope so, at least, because I only follow like less than 250 people on Twitter. And most of the people in my circle are people who either work with accessibility or care about accessibility.
 Sara: So, if I were to judge the current situation based on my little circle, I would say that accessibility is doing great, and people care about it a lot. And they work to make their content more accessible. But I can’t speak for everyone because I know that this isn’t the case for everyone.
 Sara: I know that there are still many developers who just simply don’t care. Because with accessibility, you either care or you don’t care; this is it. If you don’t care, then you are basically not doing any accessibility work at all.
 Sara: And then, on the other hand, those that do care about accessibility and try to implement it in their work, some of them are finding difficulty because they get lost in all of the resources out there, and where should they go? Where do they start? This is why I’m creating the accessibility course now, to hopefully help with that a little bit.
 Sara: So, we are definitely doing much better than we did like five years ago, let me say, five. But I don’t think we’re just exactly there yet. No. I think it’s going to take more time.
 Vitaly: Yeah. But then, I also hear developers telling me all the time, “Well, hold on. But the platform is evolving so beautifully at this point. We have not only the wonderful CSS feature coming along, but also we have these incredible things that common UI components, like input type date for a date picker, the dialogue for models, details, and summary for accordions.”
 Vitaly: And very often, what I find is that they just use those things, and they think that, “Okay, well, since these are native components available on the platform, they surely are accessible.” And then, come eye along, and then there is trouble. I’m wondering, at this point, what would you say in this position?
 Vitaly: Like, those things, would you recommend to use them ever? Or where are there? Ideally, it would be a very nice idea and situation where we ended up with all those native components just available out of the box, beautiful, accessible, inclusive, and all of that. Are we there yet?
 Sara: No.
 Vitaly: Are we again, far away from it?
 Sara: No. No. We’re definitely not there yet. I know that the dialogue element, for example, has been pretty... I don’t want to say completely inaccessible, but it has had a lot of accessibility issues for years now. And I think it only started getting better this year. And then, input type equal date, I rarely ever used it because, to be honest, I don’t think that it offers the best usability anyway, even if it is accessible, which I think it’s... I don’t know.
 Sara: I haven’t used it in a very long time, so I can’t even tell if it’s fully accessible or not. But I think the last I heard was that it wasn’t and that it was a usability nightmare. So, even if something is technically accessible, that doesn’t mean that it’s going to be usable. Definitely a lot of tests.
 Sara: And I like this quote by my friend, Scott O’Hara, that he said in one of his talks. He said, “Technology and user expectations change rapidly. And we should always test to ensure not only emerging patterns work correctly but try untrue patterns continue to work as we expect.”
 Sara: This is me, now, continuing. Sometimes even something that you know works may stop working as you expect. Browsers may create new heuristics, for example. And the way they... not interpret, the way they present something because the user may change on any day.
 Sara: Also, a note about details and summary, which is something that I had a discussion about today, details and summary are not the best choice for an accordion. They can be used for an accordion, but even they... like, when you choose any component, the first thing you have to think about is the semantics.
 Sara: What are the semantics that are going to be conveyed? Because the semantics determine the non-visual interface for a non-sighted user, for a screen reader, for a user, for example, assuming they’re a non-sighted screen reader, the user.
 Sara: Details in summary, they have their own quirks when it comes to semantics. So, the summary has a button roll, which means that it is conveyed as a button to assistive technologies. And buttons eat up the semantics of the elements inside of them.
 Sara: So, if you have a heading, which is what you would normally have in an accordion, and if you put that inside of a summary, then the heading is not going to be conveyed as a heading anymore. Of course, there are exceptions because sometimes browsers try to quote, fix our misuse of ARIA or our misuse of semantics.
 Sara: And they try to help stream users by conveying things that we may have broken as developers, but it doesn’t mean that all browsers do that. So, definitely, always, you need to test. And if details in summary, for example, if you use that, and if the headings are not exposed as headings, and then the user cannot use those headings to navigate, for example, anymore.
 Sara: So, even if something is technically accessible, yes, they can access the contents of summary. Yes, they can access the contents of the details. But you have to think about what semantics you are conveying and how they affect the usability of the interface and sometimes maybe navigation, so there are a lot of things to keep in mind.
 Vitaly: Yeah. So, it’s interesting that you brought up the testing for accessibility at this point. Because when we run our workshops, and every now and then, we... in my workshop, I tend to just explain to people how security works. And I always ask the same question. And for the last, I don’t know, two, three, four years, maybe now, I’ve been asking the same question.
 Vitaly: So, who is hearing this VoiceOver for the very first time? And these are usually designers or developers coming to those workshops. And very often, you would see a vast majority of people hearing things for the very first time. So, maybe you could also share a bit of light in how do you actually test accessibility?
 Vitaly: Do you always have screen reader or VoiceOver on or maybe any other tools? Could you also, maybe, run us through the process of testing your components for accessibility?
 Sara: Okay. So, there are quite a few things that I like to use, and I’m going to mention them in no particular order; definitely browser DevTools to inspect the accessibility tree because you can get a lot of insight on the accessibility of the elements and components that you’re building from the accessibility tree.
 Sara: Because basically, the accessibility tree is the accessibility... contains the accessibility information that the browser has created for assistive technologies to announce. So, when you look at the accessibility tree, you can get an idea of how an element is going to be announced by a screen reader that accesses and gets that information from the browser via the accessibility API, of course.
 Sara: So, the dev tools for accessibility tree. There are a lot of extensions that I like to use, for example, to see the document outline on a page or to see the landmarks on a page. If I’m doing an accessibility audit, I would definitely use an automated testing tool such as asking DevTools, for example.
 Sara: As far as screen reader go, definitely like... you cannot just test on one screen reader. And I have been guilty of this. I mean, I’m not like preaching something that I don’t practice now, but I know that I didn’t practice this before. I don’t have access to a windows machine.
 Sara: So, I recently... not recently, like a few months ago, I started using Nvidia A on my windows virtual machine. And I also recently got a license for JAWS because JAWS is not free, but Nvidia is free. So, I used VoiceOver with Safari on iOS. Sometimes I test on other browsers, as well, just because sometimes, maybe a VoiceOver using may be using another browser.
 Sara: But generally speaking, VoiceOver and Safari are the best combination, and users typically know that. And on windows, I test Nvidia A with Firefox, Nvidia A with Chrome. And the narrator is also built into Windows, so I use that for testing as well.
 Sara: And JAWS is the most popular screen meter according to the WebAIM screen reader, user survey. So yes, you have to test using multiple screen readers and browser combinations because just like you cannot test your website only on one browser.
 Sara: Like say, you’ve built a website, and you want to test, if everything is working as expected, all your CSS and stuff, you don’t just test it on one browser, right? You test it on most modern browsers and possibly even on IE if you still have to support that. Just like you test on multiple browsers, you also have to test on multiple screen readers, if you can. So, this is what I do, in general.
 Vitaly: Yeah. So, you also mentioned in one of the Smashing Hours that tool.
 Sara: Assistive labs.
 Vitaly: Assistive labs, which is like browsers tech for screen readers, which is really need to see, as well. And I think, for me, it’s really this really interesting world of other browsers, I would say because we tend to focus a lot on what are some of the fancy new features we get in Firefox and in Chrome, and in Safari.
 Vitaly: Just in general, would you say that the development of screen readers is... the frequency of updates, is it similar or is it something like maybe there is a new version coming up every six months or only just once a year, because we have this comparability, right, stuff happening across browsers.
 Vitaly: So, as much as it used to where you’re using Firefox or Chrome, or Safari, or Edge, at this point, do you see that it’s also moving in the world of screen readers towards this comparability mode... not mode, compatibility across different screens readers? Or is it... maybe you could share just a bit of light about that world and that universe of screen readers?
 Sara: To be honest, I’ve never dug that deep into it. So, I haven’t been monitoring, for example, screen reader updates, like how often Nvidia is updated and how often JAWS is updated. But I do know that even if JAWS or Nvidia is updated, not all screen reader users are going to update their software because they’re aware of... a lot of things may break for them.
 Sara: And if they already have an environment that works, nobody wants to break something that works for them. So, I know that many screen reader users do not update their software as often as we may think that they do.
 Vitaly: Right. Well, of course, talking about browsers, at this point, I do have to bring out the wonderful notion of wonderful CSS. And obviously, I do have some questions about CSS, as well. And one thing that I definitely have to ask, and I know what your answer is going to be, but I still like hearing it every single time.
 Vitaly: So, I am going to bring this up. I have a feeling... well, I know that you have or maybe don’t have strong feelings about CSS methodologies or frameworks, or JavaScript frameworks for that matter. Do you have any favorites, or do you not? Do you always just work with what the project requires?
 Vitaly: How do you pick your battles? Would you ever use any framework or CSS framework library Tailwind, CSS and JS; I don’t know. I mean, a short, no would suffice.
 Sara: I can’t just say no, because it depends on the project. If I’m working with a team and everyone on the team is using Tailwind, then I will definitely be using Tailwind with them. But I’ve never had to do that yet. And I’ve been super lucky with... actually, I would even say privileged with the projects that I’ve worked on so far.
 Sara: So, no, I don’t use any CSS frameworks. I prefer not to use them because they come with a lot of... and I’m not, not talking specifically about any particular framework here. Most of them come with a lot of overhead. And for me personally, I feel that trying to remove all the unnecessary CSS or learn something or learn it from... it’s just so much faster for me to build something from scratch, literary.
 Sara: Like, I have some CSS that I’ve created over the years that I moved from one project to the other, and of course, I constantly update that, and I used that. It’s like a mini, tiny framework that I used. Like, there are some utility classes that I used in there.
 Sara: Some settings, I called them these settings files for setting up the type scale and the tokens for theming and all that stuff. But I would definitely rather not use a CSS framework. I don’t have super strong feelings about them. I personally used a combination of BEM ITCSS and utility classes in my work.
 Sara: And I only add as much CSS as I need. So, if I need a utility class, I add it to the utility class list that I have. If not, I don’t just add it just in case I’m going to need it. I’m super minimal when it comes to writing CSS.
 Vitaly: Right. Sorry, can you hear the voices of the wonderful people on the remote corners of the internet asking for that little framework that you have created to be open source, maybe?
 Sara: I will. I do plan on doing that. Yes. The course has taken up most of my time. My website has been neglected. My blog has literally been abandoned for months, and I’m going to do... like, even the website that I’m using, I built the course website from scratch using 11T.
 Sara: I’m even considering sharing that as a framework, if anyone wants to use it someday. So, a lot of stuff that I have on my to-do list, but I’m postponing all of it until after the course is released because I need to get this done.
 Vitaly: Right. So, maybe let’s just jump into the course. I think that we’ve been speaking about it a couple of times already, but I could not be more excited to actually get this course finally released. Well, do you think you could actually share a bit of insight about what’s going to be about, when it’s going to be released, and where wonderful people listening to this show can subscribe to updates to actually get it when it does get released?
 Sara: Okay. Updates, subscription, e-mail, newsletter on practical-accessibility.today, that is the website for where the course is going to be hosted currently. It just includes an overview of what the course is about and a link to subscribe to the newsletter.
 Sara: But hopefully next month, when the backend is finally ready... because we’re doing everything from scratch, and I hired a friend of mine to build the backend and all the payment stuff into it. Once that is finished, the website is going to be updated with more details about the course. So, I’m going to introduce the course in a short video.
 Sara: There’s going to be a more detailed table of contents. I haven’t shared a table of contents yet because it keeps changing a lot. Like, even yesterday I added a new section or a new chapter in between two other chapters. So, if I had shared the table of contents before, it wouldn’t have been super accurate.
 Sara: So hopefully, in a month... I think during the next Smashing Hour, I’m going to be making an important update on the course.
 Vitaly: Oh, that’s cool. That’s nice. That’s nice. Can I ask you just on that point? I find it so difficult to record videos. I always see like, “Oh, no, no, no. I shouldn’t have said that.” I should brief rewind back, and then I should re-edit and then I should change.
 Vitaly: And then, I keep going all the time, and it takes me, I don’t know, hours to just record 10 or 15 minutes of stuff. Is it the same for you? Or do you just go?
 Sara: I’m already worried about this because I haven’t recorded anything final yet. I’ve only done a few, so like some testing and editing stuff. I’m starting with a course in reverse, actually. I’m not recording first. I’m going to give more details about the process and everything later, once it’s finished.
 Sara: But I’ve decided to do things in a different way so that when it’s time to do the recordings and the editings, I hope will hopefully have eased things for myself, so that they don’t take as much time. And something that I need to keep reminding myself of is because I’m a perfectionist, and that sometimes is a bad thing.
 Sara: I’m just going to assume that I’m on stage in a conference and just like, I can’t edit every single word I say on stage. I’m going to try to just ignore some things in the videos. That’s going to be super difficult, especially because I know that I can edit them, but it’s definitely going to take some self-discipline to do that.
 Vitaly: Yeah. So, it’s impossible for me. I always say like, “Oh, no, no, no, of course, I can go back,” and surely I can come back. So, in the end, it just takes hours. But I think that we all cannot wait for the video course to be finally released and get our hands on the videos.
 Vitaly: This is very, very exciting. Maybe talking about excitement, I know that there are so many wonderful new features coming to the web. I don’t know when it’s coming. Is it like Chrome 103 something, where we should be expecting the :has() pseudo-class coming in, container queries coming in? It’s like Christmas is coming early.
 Sara: The year of CSS.
 Vitaly: Yeah, the year of CSS. So, maybe you could just share a bit of light about what are you excited about at this point? What is the thing that keeps you awake at night where you think, “Oh, if it only was available today, I would use it all over in my projects.” What would that be? Or what are you most excited about these days in CSS?
 Sara: Well, CSS doesn’t keep me up at night, but I do look forward to things like definitely subgrid. I know that I was one of the people who started requesting container queries years ago, and then we finally got them. But then, at this point, we were already doing a lot of intrinsic responsive design already and using flexbox and CSS grid to create responsive components that don’t require container queries anymore.
 Sara: Although, I mean, they are still important, and I will definitely still be using them. But probably what I’m personally more excited about is cascade layers and subgrid because almost every single project that I’ve used, especially since I worked on the Prismic Slices Project in 2019, that project changed the way I started building websites, at least for me.
 Sara: It influenced the work that we did on the ?? website with Yan. And it also influences now, my own work on my website, for example. Instead of thinking of either pages or small components, there is this middle ground, which is slices.
 Sara: And layout within slices always... like I’ve always wanted the ability to inherit the grit on the parent container of the parent container into the child. And so, subgrid is going to be one of the things that I will probably need even more than container queries in my work.
 Sara: Cascade layers, I wouldn’t say that I need it, but the way it allows me to organize my CSS, the same way the CSS is organized inside of my head, so to speak, that is one of the reasons why I’m excited about it as well.
 Vitaly: Okay. And then, maybe just a few final questions to finally wrap up, just because I’m very, very curious, so I’m sure you have a couple of books lying around at this point; how do you organize your books? Are they organized by topic? Are they organized by color? I met some people doing that. How do you organize them?
 Sara: By color, but not like the rainbow style color that other people do.
 Vitaly: Okay. How many pencils or pens do you have on your table most of the time?
 Sara: Probably two, like one or two.
 Vitaly: And how many screens?
 Sara: You mean for work?
 Vitaly: Yes.
 Sara: Right now, just two, the laptop and an external display, 32-inch.
 Vitaly: Okay. Because for me, moving to a secondary display was really a deal-breaker, so it’s just incredible. And then finally, one thing that I do want to ask, and it’s totally unrelated, is, do you happen to have a printer?
 Sara: No, I haven’t had one in more than two decades, I think.
 Vitaly: Yeah. Now, I feel just lonely because every time I bring this up, because I just got a printer, like what, two months ago. And I’m very proud of this because this is like me having a printer like the first time in two decades, seems like I’m the only person who’s buying printers at this point. That makes me very, very sad.
 Sara: I mean, you’ve lived in Germany, right, and you still do a lot of printed paperwork there, so you need it.
 Vitaly: Yes, indeed. You’re absolutely right. Well, okay, now we know that. All right. So, we’ve been learning a little bit about what it means to design and create more accessible interfaces today; what have you been learning about lately, Sara? Maybe one interesting insight or one unusual thing that you’ve learned recently, which really changed your views, maybe it’s just something that somebody said to you, which has influenced your work or just the way you’re thinking about design or about development, anything in that department?
 Sara: Nothing that big, but a lot of small detail.
 Vitaly: Like what?
 Sara: There is super technical thing.
 Vitaly: Okay. So, when it comes to implementation of accessible components, and things like that.
 Sara: Yeah. There are a lot of things that I learned from digging really deep into specifications. And I love that because my go-to resource to learn about almost anything starting with CSS and other things, is to go to the specifications first. And there’s so much I’ve learned from that recently.
 Vitaly: All right. Excellent. Well, so, if you, dear listener, would like to hear more from Sara, you can follow her on Twitter, which is @SaraSoueidan. And also, find all her work on her website at sarasoueidan.com/.
 Vitaly: And also, don’t forget to subscribe to practical-accessibility.today, which as we’ve heard today will be released soon. So, this is something I’m very, very much looking forward to.
 Sara: In the summer, hopefully. That’s what I’m aiming for.
 Vitaly: Well, that’s fantastic news one way or the other. Well, thanks so much for joining us today, Sara. Do we have any parts in words?
 Sara: Thank you for having me. Today is Global Accessibility Awareness Day. So, if there is something or one thing that you can do today, I would say go either learn something new about accessibility. Or, if you already have the knowledge, fix something on your own website or on somebody else’s website, like open a PR, or fix an issue that exists somewhere out there. Spread the word on accessibility, and subscribe to my newsletter.</content>
     </entry>
     <entry>
       <title>What’s That (Dev) Tool?</title>
         <link href="https://smashingmagazine.com/2022/05/whats-that-dev-tool/"/>
       <updated>2022-05-30T13:30:00.000Z</updated>
       <content type="text">Have you ever looked to see what other tools were available to you within the DevTools toolbox? You’re probably using the same few panels over and over again — I know I am!
 It turns out there are more than thirty (30!) individual panels in Chrome DevTools (as well as other Chromium-based browsers, such as Edge). Safari and Firefox have fewer panels but still probably more than you use on any given day.
 
 When I realized this, it gave me an idea for a silly game where you’d try to name as many panels as you could in under one minute. Play it here and try your luck (no cheating, OK?): What’s That Tool?
 
 This game is silly, of course. As a web developer, remembering the exact names of each and every tool in DevTools isn’t important. It’s more important to know how to use these tools when needed.
 But the game does prove a point: there are many more tools than what people use or even know about! The whole goal of this game is — by the time the minute is gone and the full list is displayed — to realize, “oh wow, that’s a lot of tools I had no idea even existed.”
 So, why are there so many? Let’s face it, DevTools is crammed with buttons, tabs, and features. How did we get here, and is there a way out?
 The Story Of An Explosion
 In the early 2000s, web development was very different than it is now. Back then, most of the complexity lay in generating the right HTML code from your server. The browsers’ ability to “view source” was enough to debug the odd table colspan problems. JavaScript was only getting started on the web, and CSS was nowhere near the feature-full language it is now.
 So, on top of the old alert() debugging trick, the very few tools we used for front-end code debugging were very specialized; they only did one thing. Venkman only did JavaScript debugging, Aardvark was focused on inspecting elements, and Console2 displayed nice JavaScript log messages.
 Joe Hewitt) brought it all together under one tool called Firebug which was a Firefox extension. It was an absolute revolution for web developers throughout the world! Around 2010, Firebug was probably the most used front-end debugging tool, and Firefox was a dominant browser. Crazy! Right?
 
 Even if you have never used Firebug and started your web development journey in more recent times, I’m willing to bet this user interface feels familiar.
 Though the DevTools we use now aren’t very different from what Firebug used to look like, it felt like we had fewer things to worry about back then and, therefore, fewer tools to help them with.
 As the screenshot above shows, there weren’t many tools at all:
 
 Console to view logs and execute JavaScript,
 HTML tab to view and edit the page’s DOM and the applied CSS styles,
 JavaScript debugger,
 Network tab to check the downloaded resources and HTTP requests.
 
 Fast forward to 15 years in the future, to now. The user interface of the browser tools we use hasn’t changed much, but the sheer number of panels skyrocketed! Here’s a quick and incomplete (and very approximative) history of when new panels got introduced in Chrome DevTools:
 
   
         
             Year
             Panels
         
     
     
         
             2008
             Console, Elements, Sources, Network, JavaScript profiler
         
         
             2010
             Performance (called Timeline at the time)
         
     
             2013
             Rendering, Layers
         
     
             2014
             CSS Overview, Network Conditions
         
     
             2015
             Security, Memory
         
     
             2016
             Animations
         
     
             2017
             Coverage, Lighthouse (called Audits at the time), Performance Monitor, Network Request Blocking
         
     
             2018
             Changes, Accessibility
         
     
             2020
             Media, WebAuthn, WebAudio, Issues
         
     
             2021
             Memory Inspector, Recorder
         
         
             2022
             Performance Insights
         
     
 
 
 There are many reasons why the number of new panels keeps growing. Some of them are good, while others are more questionable:
 
 The number of features and APIs available to the Web platform is constantly and quickly increasing. Now, there are many more things a web developer can do on the web than 15 years ago. Some of those things can’t easily be debugged with the 4 or 5 tools we had back then, thus it required new panels in DevTools.
 Our discipline is way more developed than it used to be. Front-end web development was maybe regarded as a little less interesting or important 15 years ago. It has now proven itself as a much deeper field of computer engineering that requires knowledge of not only programming but also performance optimization, accessibility, user experience, progressive enhancements, and more.These things tend to sometimes require specialized tools too.
 People who write the code for the browsers and DevTools also need tools themselves, and sometimes they end up as new panels. The Protocol Monitor panel in Chromium is a great example of this.
 Deleting things is really hard! You’re bound to break people’s workflows if you do it. So, things tend to accumulate over time. Chrome, for example, has three tools to do performance optimizations: Performance, Performance Insights, and JavaScript profiler.
 Finally, there seems to be a general tendency to add new things rather than improve what’s already in place. I get it; it’s more exciting to most people to build new things rather than fix bugs. But it tends to make the software more complicated over a long period of time. And this has most probably been at play in DevTools, too.
 
 In any case, we’re here now with probably what’s one of the most advanced tooling suites any application platform could dream of. But it’s also one of the most complex that no one uses to its full potential.
 Is This A Problem?
 Yes! Put simply, DevTools is a very complicated product, and its user interface can be scary.
 Where other products have five main user scenarios, DevTools has dozens, if not hundreds. Do you need to simulate a mobile screen? Detect color contrast? Convert between font units? See JSON responses? DevTools can do it all! And these are just a few random examples.
 So, with that many options and features, it isn’t a surprise that the user interface is complex so much that new users can feel very overwhelmed in their first-run experience. But even experienced users don’t necessarily know what’s available outside of the same few panels they’re used to.
 This is starting to become a serious usability problem, in my opinion, one that may sometimes discourage new people in their learning journey. People coming to DevTools today are likely to be used to newer development products that are much easier to use. The digital tool space is undergoing a big change in this direction, and the browser DevTools haven’t started moving yet.
 This isn’t a simple problem to solve either. As I said before, there are some really good reasons why we need a lot of specialized tooling. So, the complexity is needed, but I believe it should be opt-in rather than opt-out!
 Indeed, the DevTools learning curve is getting very steep because so much information is presented to users right from the start, and people have to learn to ignore the parts they don’t know about or think they don’t need.
 Why Don’t We Just Clean It All Up?
 Well, it’s complicated. There are many user scenarios built in DevTools, and there are probably as many workflows as there are people using DevTools out there. Even the most rarely used tools are here for a reason, and the few people who use them may depend on them.
 In my experience working on DevTools, removing old/un-maintained/rarely used panels for the sake of making the codebase easier to work with always proved to be a bad idea, especially when done without enough customer research.
 In fact, while I worked on Firefox, we tried removing the Fonts panel at some point in Firefox DevTools, and the response was pretty instant and strong — so much that we put it back in! We lacked the necessary customer understanding of how this tool was being used and what unique scenarios it supported.
 In 2016, the 3D view panel had to be removed because it wasn’t supported anymore by the new (back then) Firefox architecture. To this day, more than six years later, people still complain that it’s gone (note that you can still use it in Edge)!
 As a final example, the Chrome team removed the Properties sidebar pane in 2020 but later added it again after seeing how much people needed it.
 Usage numbers alone aren’t a good measure of a tool’s worth. Some tools may be used by a few people only, but they may depend on them to do their jobs. Proper user research and understanding of the various user roles and scenarios (and DevTools has a lot of them!) are needed to be able to simplify the product.
 A Way Out?
 I want to propose two directions that I think have a lot of potential when it comes to improving the situation with DevTools: 
 
 simplifying the DevTools core and opening it up to more powerful extensions, so that users make their own tools;
 being bold and taking risks with a radically user-friendly user interface.
 
 A Powerful But Simple Core, Boosted With Extensions
 Visual Studio Code is such an amazing product! Many people use it and not only web developers. It’s built on a very strong core that can be extended tremendously.
 This way, the people who work on VS Code don’t have to worry about all of the features that people might need. VS Code is a bit like DevTools in the sense that no two people have the same needs and workflows, and there are hundreds of ways to use the product.
 So, the team’s job is to build a solid foundation that allows basic editing and top that up with an extension API that lets other people dig deep into the platform and add extra features.
 There are many advantages to this, but one that is of particular interest to me here is that complexity is opt-in. When you install VS Code the first time, it’s not overwhelming. It’s a text editor, and you can use it to edit text right off the bat. But if your project has special needs, like checking code quality or doing some custom syntax highlighting, then you can install all the fancy extensions you want and get the extra functionality you need.
 The VS Code extension API goes really deep into how much you can customize the editor, and I believe this is largely responsible for its success.
 DevTools, however, is built differently. You can also install extensions in the browser to add new panels to DevTools, but there aren’t very many useful extensions available outside of the major framework ones (React, for example). The teams who work on DevTools are the ones who pretty much do all the tools that a web developer might need.
 By using the browser extensions API, creating a new panel in DevTools isn’t too hard, but the API isn’t as advanced as in VS Code. In particular, there is no way to extend the existing tools to augment their functionality. This is a serious limitation that I think is responsible for the low number of useful extensions we have at our disposal.
 As an example, the amazing Motions DevTools extension allows you to view and edit animations on the web. But it’s limited to its own panel container, and it can’t integrate with the Elements panel right next to it, which would have been useful to simplify user workflows and re-use existing components, such as the color picker.
 
 Although they have now gone back to a more traditional tabbed navigation which seems to work better with developers, I appreciate this early attempt to make a more user-friendly interface that’s also more consistent with what people knew at the time.
 This also goes to show that very special care needs to be taken to bring developers along a journey of user interface change in DevTools.
 This brings me to the team working on the Edge DevTools now (which, full disclosure, I am part of). I believe this is currently the only team doing something in this area.
 Our current experiment is called Focus Mode, and it is effectively the first attempt at redesigning the entire DevTools product UI.
 
 Focus Mode is available to users of DevTools on the Canary and Dev pre-release channels of Microsoft Edge by enabling the “Focus Mode” experiment from the DevTools Settings. Most users of these channels should in fact already have it on, as our team is gradually rolling out the feature and listening to user feedback in order to ensure this is not disruptive to existing workflows and a welcome change.
 Based on this feedback, we will continue rolling out Focus Mode to users of the Beta channel and eventually to the normal release version of Edge.
 Now, it might not seem like a big change at first, but this is only a first step in an iterative approach to creating a more approachable user interface. Again, changing things in this product is complicated, so our team is taking things slow.
 But if you look closely, there are a few major changes to the UI that try to make things less cluttered and more streamlined.
 The most visible changes are located in the top toolbar. Here is an animation showing a comparison of what the toolbar looks like with and without Focus Mode:
 (Large preview)
 
 
 The list of warnings, errors, and infos is now gone from the toolbar, and instead, it appears as colored badges on the Console and Issues panel tabs, removing some clutter.
 The Settings, Feedback, and main menu icons have been grouped under just one menu button in the top-right corner, further reducing clutter.
 Tabs now have icons, so they’re easier to see and tell apart.
 
 Here are a few more things coming with Focus Mode.
 The + button in the toolbar shows all available tools with their icons making it easier to re-open a tool you’ve closed before and maybe more inviting to try tools you haven’t tried yet.
 
 It’s also possible to switch the tabs to a vertical orientation. Being positioned to the left and hiding the labels further reduces the noise in the central part of the window, letting you focus on the code. Additionally, it matches UI patterns that people are growing used to in other tools (for example, the Activity bar in VS Code or vertical tabs in Edge).
 
 And finally, the drawer in DevTools was re-designed. The drawer is this area of the user interface that appears at the bottom when you press the Esc key on the keyboard, and that normally contains the Console.
 It was introduced as a way to have access to the Console at the same time as other tools, and all browser DevTools have this now. But over the years, the Chrome team added more and more things to the drawer, in particular secondary tools that were useful but not quite popular enough for a spot on the main tab bar (e.g., the Rendering panel was added there).
 I think it’s come to a point where it’s hard to know for sure which tool is available in which area. Edge — with Focus Mode — is taking a different approach. The drawer is now called Quick View, which is always visible at the bottom of the toolbox (so you don’t even have to know to press Escape) and can be used to display any tool you want.
 
 I’m very excited about where Focus Mode is going, and I can’t wait for our team to start exploring what’s next in this area.
 If you want to try Focus Mode out, make sure you have a copy of Edge (you can also get a pre-release version if you prefer to have the latest changes), open DevTools, and if you don’t already have it ON, press F1, then go to Experiments and check the Focus Mode box.
 Let the team know what you think about it — and if you have other ideas — by filing new issues on our DevTools GitHub repository.
 I believe that a user-friendly DevTools that is both more welcoming to learners and inclusive of everyone’s needs is possible, and together, we can make it happen. As a community, let’s demand more from our friendly DevTools teams!
 There are full-time dedicated DevTools product engineering teams working for each browser vendor. They keep adding new features and fixing bugs, but they can only do a good job with our collective help. 
 Tell them if the UI doesn’t work for you. Let them know about your most common workflows. How many clicks did you need? Do you often forget where things are? Do you wish things were named differently? Input like this can lead to changes that make a big difference for millions of users.
 As mentioned, we’re actively seeking feedback on this experiment and other DevTools features. You can leave comments on our GitHub repository. Other browsers also like to hear feedback on their DevTools which you can do at the Mozilla bugzilla tracker for Firefox, on the Chromium bug tracker for Chrome, and on the WebKit bugzilla tracker for Safari.
 Thanks for reading! And see you next time.</content>
     </entry>
     <entry>
       <title>Just How Long Should Alt Text Be?</title>
         <link href="https://css-tricks.com/just-how-long-should-alt-text-be/"/>
       <updated>2022-05-27T14:25:39.000Z</updated>
       <content type="text">I teach a class over at the local college here in Long Beach and a majority of the content is hosted on the Canvas LMS so students can access it online. And, naturally, I want the content to be as accessible as possible, so thank goodness Canvas has a11y tooling built right into it.
 
 
 
 But it ain’t all that rosy. It makes assumptions like all other a11y tooling and adheres to guidelines that were programmed into it. It’s not like the WCAG is baked right in and updated when it updates.
 
 
 
 
 
 
 
 The reason this is even on my mind is that Jeremy yesterday described his love for writing image descriptions:
 
 
 
 I enjoy writing alt text. I recently described how I updated my posting interface here on my own site to put a textarea for alt text front and centre for my notes with photos. Since then I’ve been enjoying the creative challenge of writing useful—but also evocative—alt text.
 
 
 
 I buy into that! Writing alt text is a challenge that requires a delicate dance between the technical and the creative. It’s both an opportunity to make content more accessible and enhance the user experience.
 
 
 
 One of those programmed guidelines in the Canvas tool is a cap of 120 characters on alt text. Why 120? I dunno, I couldn’t find any supporting guideline or rule for that exact number. One answer is that screen readers stop announcing text after 125 characters, but that’s apparently untrue, at least today. The general advice for how long alt text should be comes in varying degrees:
 
 
 
 Jake Archibald talks of length in terms of emotion. Detail is great, but too much detail might distort the focal point, which makes total sense.Dave sees them as short, succinct paragraphs.Carrie Fisher suggests a 150-character limit not because screen readers will truncate them but more as a mental note that maybe things are getting too descriptive.Daniel Göransson says in this 2017 guide that it comes down to context and knowing when certain details of an image are worth additional explanation. But he generally errs on the side of conciseness.
 
 
 
 So, how long should alt text be? The general consensus here is that there is no hard limit, but more of a contextual awareness of what purpose the image serves and adapting to it accordingly.
 
 
 
 Which gets me back to Jeremy’s article. He was writing alt text for a group of speaker headshots and realized the text was all starting to sound the same. He paused, thought about the experience, compared it to the experience of a sighted user, and created parity between them:
 
 
 
 The more speakers were added to the line-up, the more I felt like I was repeating myself with the alt text. […] The experience of a sighted person looking at a page full of speakers is that after a while the images kind of blend together. So if the alt text also starts to sound a bit repetitive after a while, maybe that’s not such a bad thing. A screen reader user would be getting an equivalent experience.
 
 
 
 I dig that. So if you’re looking for a hard and fast rule on character counts, sorry to disappoint. Like so many other things, context is king and that’s the sort of thing that can’t be codified, or even automated for that matter.
 
 
 
 And while we’re on the topic, just noticed that Twitter has UI to display alt text:
 
 
 
 Now if only there was more contrast between that text and the background… a11y is hard.
 
 Just How Long Should Alt Text Be? originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>The recycling backlog</title>
         <link href="https://daverupert.com/2022/05/the-recycling-backlog/"/>
       <updated>2022-05-27T14:13:00.000Z</updated>
       <content type="text">In Austin we have three types of trash: trash, recycling, and compost. And then if I’m honest there’s a fourth and fifth variety: e-waste and a bag of old clothes and toys to donate. The big three live outside in the side yard and those last two live in piles in my garage until I can’t handle looking at them anymore.
 The side yard has three enormous trash bins. The compost is always empty. The trash fills up, but we can atomic elbow that last bag in there. No problem. Our recycling… well, our recycling is always full and overflowing. You see, we’re bottlenecked by the city’s two week pick up cycle. When the bin fills up we create staging areas with paper bags to triage the excess recycling. The minute the city hauls off the previous load, we’ve filled up half of our capacity on our next two week cycle.
 There’s piles of recycling TODOs all over my house, not in a hoarder way, but in a shabby chic, mid-century modern meets the reality of a pandemic way. French country. Chip n’ Jo.
 The big issue is our recycle bin is DDoS’d by cardboard from Amazon. All those great ideas that came with boxes inside boxes. The swells of orders at birthdays and holidays. Those gifts become a hazard. Boxes on top of boxes because if you prematurely optimize and flatten them they get more slippery and you fall on them. It creates pressure and stress on the in-home recycling operation. Time and mental energy spent on sorting, squashing, grouping, cutting, and planning what can go out in the next week’s load.
 I hear I can add capacity by calling the city and requesting a second recycling bin. That seems like a lot to always have a fourth enormous trashcan sitting around. I don’t need that much capacity. Or do I?
 The city allows me to bundle my boxes as a strategy. If I cut up all the boxes into 2ft×2ft flat pieces and tie that up, I can set that next to the recycling and the city will pick it up. It’s a lot of work to prepare that and any threat of rain will leave you with a soggy mess.
 I can add temporary capacity from gracious neighbors. I’d use their garbage cans and pay them back with confections, hoping I don’t burn through the goodwill. This is probably the simplest solution, but also a lot of work for trash.
 To top it all off, there’s a rumor going around that none of the recycling ever gets recycled. They say it goes to the landfill because China quit buying our trash. I hate it, but kind of believe it.
 Anyways, backlogs.</content>
     </entry>
     <entry>
       <title>What people think that web developers do vs. what we really do</title>
         <link href="https://christianheilmann.com/2022/05/27/what-people-think-that-web-developers-do-vs-what-we-really-do/"/>
       <updated>2022-05-27T09:09:56.000Z</updated>
       <content type="text">What people think web developers do:
 
 	
 
 	
 		Code, deploy and use, with coding being the main task
 	
 
 	What web developers really do:
 
 	
 
 	
 		Coding
 		Require random 3rd party framework code
 		Updating Node/NPM
 		Try the last command again with &#x60;sudo&#x60;
 		Checking on mobile
 		Shaking fist towards Cupertino
 		Looking things up on the web
 		Pasting Stack Overflow solutions
 		Resize the browser/use emulation
 		Discover that a browser extension was the problem all along
 		Explain again that things can’t look the same everywhere
 		Add ARIA till everything is fine
 		Trace performance issues back to analytics/ad code
 		Convert content to usable formats
 		Adding/Removing console.log()
 		Invalidate caches
 		Using
 		Summon Imhotep using blood magic to help with a rendering bug
 	
 
 	TL;DR: Learning to code isn’t the main skill you need as a developer</content>
     </entry>
     <entry>
       <title>Open-Source 3dicons Library: Case Study And Free Downloads</title>
         <link href="https://smashingmagazine.com/2022/05/3dicons-open-source-library-case-study-download/"/>
       <updated>2022-05-27T09:00:00.000Z</updated>
       <content type="text">In February 2021, I began studying 3D illustration using Blender. Like most beginners in the field of 3D design, I also created my first donut by following BlenderGuru’s (Andrew Price) tutorials. I firmly believe that learning by doing is the best way to learn. Picking an icon and following the steps on how to construct it in the 3D space was a great challenge for me, as each icon required a different approach to modeling.
 While learning, I noticed that the number of icons in my project had grown to almost 30, and at this moment, I decided to release them under an open-source license. I started preparing more icons for the most commonly used cases, and in the end, I created exactly 120 icons.
 
 Then the main objective evolved into creating a set of icons that could be used on product screens, presentations, and social media posts. These icons should be rendered as product-ready assets, but I also wanted to share the editable source files with the optimized and exported icons.
 
 In this article, I will share my experience of learning and managing to release this large set, where the images alone take up 3.8 GB in size! I have divided it into five parts:
 
 Why Open Source?
 Tools Used
 Process and Learning
 Release
 Conclusion and Download
 
 Note: If you just want to download the 3dicons library, you can jump straight to the “Conclusion and Download” section at the end of the article.
 Why Open Source?
 Firstly, I believe in open-source design. Often, a project starts with the intent of learning and practice, when we redesign existing websites and apps like Facebook and Netflix, or various design screens, objects, or icons of interest, and then we post the results on Dribbble, Behance, and many other social platforms. We do this to improve our own skills, but when someone works on a concept, why not also share the source files with the community so that people can learn and benefit from how the design was crafted?
 By sharing our source files and best practices, we uplift the community and make the Design world better. After realizing this (and also drawing inspiration from Pablo Stanley’s work), I decided to do something similar.
 In 2019, I shared my first source file, which had 100+ illustrations. I had these illustrations sitting on my hard drive for years, but they were only taking up storage space. I wanted to make this resource available to the community and share it under an open-source license. Then I thought the same about the 100+ 3dicons, which I next created with the sole purpose of learning 3D design.
 Important: Releasing my icons for free does not mean that I want to diminish the value of other artists’ work.
 Secondly, there are a lot of 3D illustrations and icons available online for free or for a small fee. Unfortunately, most of the free resources out there do not allow you to modify them as you see fit — these freebies are often nothing more than a link to purchasing the final product. At the same time, many designers in the community are just starting out and need low-cost resources to learn from and use in their projects. These 3dicons should help them to get started, and when they are ready, they can either create their own 3D icons and art or buy and then build upon some existing ones. 
 Update: 3dicons has just been nominated as one of the finalist projects in the Figma Community Awards! (Editor’s Note in the last minute)
 Tools Used
 As a designer, I’m a huge fan of Apple products! But learning 3D on a 16&quot; MacBook was not good enough for me, so I decided to build my own PC specifically for learning 3D design. The following are some of the hardware and software tools that I used to craft the project’s files: 
 
 Windows machine (AMD Ryzen 7 3800X, nvidia GeForce RTX 2080);
 Blender: for all modeling, rendering, and compositing;
 PureRef: helping me collect references for icons, colors, and other things;
 Figma: using it for arranging the icons and for designing the web page;
 iPad with the Procreate app: for sketching, doodling, and ideation;
 Notion: it’s a tool for managing everything, and after Figma, it’s my next favorite tool on the list! :-)
 
 Process And Learning
 I created all these icons in Blender. Some of them were first doodled on an iPad using Procreate and then modeled in 3D. After modeling, all icons were rendered in different angles and styles for multipurpose use. I was considering releasing to the public both the rough master file and all the production-ready assets, and for this, I needed some sort of a plan.
 Planning Stage
 To manage all my projects, I use Notion. First, I created a board and listed all the initial requirements there. 
 
 Modeling
 Some icons are easy to make and can be directly created from a reference. Some of them were conceived as doodles on my iPad using Procreate and then imported via Google Photos to my Windows machine. I created a basic guide window, and then all the icons were put together as a single file. This initial stage was quick, but then properly rendering all those models one by one was quite a challenging bit of work.
 
 Challenges
 Creating stuff for yourself is easy — nobody will see what’s inside your files. However, when you prepare to share a source file with someone else, you have to spend a lot of time managing and arranging all the layers and elements inside it.
 Modeling
 There is no doubt that modeling in Blender is very smooth. Learning was a trial and error process for me when I started. I figured it might be necessary to have a guide to make sure that all icons were the same size and positioned well. So, I made a basic guide with two cubes, one for the icon’s maximum boundaries and one for just the end edge of the area. Next, I modeled all the icons and stored them all in one file. I was looking for quality icons instead of low-poly ones, so I added surface modifiers to smooth them out.
 
 If you’re new to modeling, check “Modeling for Absolute Beginners” — a Blender tutorial by Surfaced Studio.
 
 Key point:When you apply the subdivision surface modifier, it increases the file size. I want to share .obj or .fbx files as well, so I need to apply all modifiers. Otherwise, it will mostly be low-poly icons, since modifiers will be removed.
 
 Lighting
 A perfectly lighted model looks good. If you don’t light up your model properly, no matter how much effort you put into modeling, the result won’t look right. Every object has a different shape and needs different levels of light intensity. Since I decided to render icons from three different angles, each angle required a different light position and light intensity. It is time-consuming and complicated to change the same lights for different models.
 
 If you’re new to lighting, check “Lighting for Beginners (Intro)” — a Blender tutorial series by Blender Guru.
 
 
 Key point:As a result, I created three sets of the same lights with similar three-point light settings: ISO, Dynamic, and Front. The lights’ positions were adjusted accordingly.
 
 
 I did not have a problem when choosing colors, but in the 3D world, color intensity is affected by the light source, so this is an important thing to keep in mind. Additionally, I used to add gradients with light shades or complementary colors of the primary color rather than a direct “flat” color.
 Key point:Managing the same color for all angles is a challenge. I tried and failed a few times before finally fixing the colors and gradients for all icons. Also, I found that high contrast is a better option in Blender when you work with vibrant colors.
 
 
 
 Rendering
 It is quite challenging to render so many icons in four styles and at three different angles. Blender supports scripting, but it was difficult for me to learn too many things at the same time. Also, automatic rendering with the help of a script might not work for all icons due to their shape and angle — different icons will require different light amounts and intensity before being rendered. Therefore, the project was released later than expected.
 
 Key points:In the end, I created two cameras: one Normal camera and the second TrueISO camera. I used the Normal camera to render the front and dynamic angles, while the TrueISO camera was used to shoot the icon in its isometric form.Rather than creating multiple cameras for the different angles, I used timelines and controlled the rotation and position of the objects.I used 800 samples to render all these icons. By increasing the icon sample size, the rendering times will also increase. Rendering a single image with a resolution of 2400 × 2400 pixels took 2-4 minutes.
 
 Note: Samples are the noise that appears as your scene is rendering. In the Render panel, you define the number of samples, and then Blender stops once it reaches it. The more samples, the clearer the render will be, but it will also take a longer time to complete.
 Compression and Post-processing
 I tried to calculate the render times for the 120 icons in 12 variations (4 styles × 3 angles). When doing the first tests, at first, I was satisfied with the rendering at 3000x3000 px with 16-bit color depth. However, I discovered that one image file was about 15 MB in size. When I calculated the time and space needed for 1,200+ images rendered from different angles, I was shocked — I would need more than 8 GB in the storage space (including the source file), and also Figma couldn’t handle all those images in a single file either.
 Key points:I could further optimize the images using Photoshop, but wouldn’t it be too time-consuming to add another step to the post-processing? Instead, I decided to change the resolution to 2400x2400 px, which is still quite large for an icon. Lastly, I changed the color depth to 8-bit and used 100% compression straight from Blender. And then, the single file size was reduced to 4 MB or less. The final icon library now was around 4 GB in size, which was manageable.At first, I planned to render each icon separately with different channels and arrange them all in Figma with the help of components; this would allow me to easily change colors, lighting, and shadows right in Figma. Unfortunately, Figma files became very heavy, and I had to abandon the idea.
 
 
 
 Release
 In total, more than two months of hard work (mostly done during the night) were needed to complete this project, from crafting the icons to rendering them and finally to hosting and building the project’s website.
 Figma Community
 I had trouble creating more flexible versions of the Figma files as this icon set grew to more than 4 GB in size. Figma stopped responding as soon as I uploaded 300+ images, which is a lot of data! How did I solve this issue? I used ImageOptim to compress the images and then added them to Figma again. In addition, I dropped the idea of creating variations as the file size became excessively large. After that, I organized all the compressed 1000 × 1000 px images and shared them with the Figma community. (Eventually, I will create a customizable variant for Figma specifically, or maybe a plugin that will allow the icons to be easily imported and edited within Figma.)
 ImageOptim: It is a powerful image lossless compression tool software that provides “lossless” compression services for PNG images, reducing file size by 60%–90%.
 Here are the settings that I used to compress icons:
 
 
 Hosting
 I host all of my projects directly on GitHub pages, although I occasionally use other hosting services. The main question was where to host over 3 GB of data, so people can easily download the icons. 
 
 GumroadAs the first example, Gumroad doesn’t offer a free option for downloading files over 200 MB in size — if I hosted the project on Gumroad, I would need to charge for it at least 1$. So, I tried to explore a few other solutions.
 Google DriveIt should work, but there are bandwidth limitations after some time.
 DropboxSame, after a certain point, the bandwidth issue will come up.
 AWSI have used Amazon AWS in my experiential projects. Although usually a good solution, it’s quite complicated to set up.
 WebflowI tried it once, as I’m not a big fan of no-code solutions (at the moment).
 Digital OceanAfter exploring a bit, I decided to store all my icons on the Digital Ocean hosting. It’s quite affordable at only 5$/month starting price. However, after running 3dicons for months, I discovered Digital Ocean is not an affordable solution after a certain bandwidth limit has been reached. I am charged 60-80 USD per month for the download bandwidth over 7 TB.
 
 Hopefully, I will be able to find a better solution than the Digital Ocean hosting so that 3dicons may continue to provide frictionless downloads with an affordable maintenance/download fee. If you have any suggestions or you think you could help, message me on Twitter!
 Website Domain
 Honestly, I bought the 3dicons.co domain without any reason in 2020. Luckily for me, it was perfect for my new project!
 
 Frontend
 
 WebflowNo doubt, a great no-code web solution. However, it limits my freedom to customize things, so I’m not a big fan, even though I have used it for clients.
 WixGood for beginners. You can easily create sites by dragging and dropping.
 Writing the code myselfI love coding all my projects. Most of my projects use the following stack:
 Gatsby FrameworkAn easy and lightweight way to create static pages that can be directly hosted on GitHub pages. Gatsby is an open-source framework that combines functionality from React, GraphQL, and Webpack into a single tool.
 GitHub pagesGitHub allows you to host static pages directly as well as add custom domains with SSL security without any additional cost.
 
 
 
 Assets Creation
 I use Figma Design for all my design needs, and the 3dicons project was no exception — I used Figma to help me create all the social media posts and Designs.
 Conclusion and Download
 Now, I do know quite a few things about 3D design, but it wasn’t so when I was only beginning to learn. At the start of the journey, I wasn’t familiar with how to handle 3D files and how to render them in such a way that there would be very few hiccups. Fast-forward a few months of learning and experiments, and I think that the most important thing is that the final outcome looks good and that I have hope that the community will like and support this project. As an example, recently, Morflax integrated 3dicons into their mesh project.
 Note: The Morflax mesh lets you customize the colors of the icons and add scenes on the web without requiring the use of any complex 3D software. You can try the project for yourself.
 Download
 You can get the latest version of the icons from https://3dicons.co/, and as I have more plans for 3dicons (including adding a few more icons to the existing set), make sure to check the project’s website for updates.
 
 You can also download the icons set here:
  Figma (Compressed)
  PNG Dynamic
  PNG ISO
  PNG Front
  Blender File
  Fbx File
  Download all versions
  
 
 Last updated in May 2022. For the latest versions, please check the 3dicons official webpage
 
 License
 There are no restrictions on how you can use the 3dicons since they were released under a CC0 license (“No Rights Reserved”).
 Need help?
 There is a little FAQ Notion board that should help you when you want to change the icons and colors using Blender. You can also reach out to me on Twitter (@realvjy) for feedback and suggestions, and you can leave a comment here — I’d be happy to reply. </content>
     </entry>
     <entry>
       <title>Beautify GitHub Profile</title>
         <link href="https://css-tricks.com/beautify-github-profile/"/>
       <updated>2022-05-26T18:13:28.000Z</updated>
       <content type="text">It wasn’t long ago that Nick Sypteras showed us how to make custom badges for a GitHub repo. Well, Reza Shakeri put Beautify GitHub Profile together and it’s a huuuuuuge repo of different badges that pulls lots of examples together with direct links to the repos you can use to create them.
 
 
 
 And it doesn’t stop there! If you’re looking for some sort of embeddable widget, there’s everything from GitHub repo stats and contribution visualizations, all the way to embedded PageSpeed Insights and Spotify playlists. Basically, a big ol’ spot to get some inspiration.
 
 
 
 
 
 
 
 Some things are simply wild!
 
 
 
 I bet Jhey would like to get his hands on those cuboids!
 
 
 
 Just scrolling through the repo gives me flashes of the GeoCities days, though. All it needs is a sparkly unicorn and a tiled background image to complete the outfit. 👔
 To Shared Link — Permalink on CSS-Tricks
 Beautify GitHub Profile originally published on CSS-Tricks. You should get the newsletter.</content>
     </entry>
     <entry>
       <title>Manage Accessible Design System Themes With CSS Color-Contrast()</title>
         <link href="https://smashingmagazine.com/2022/05/accessible-design-system-themes-css-color-contrast/"/>
       <updated>2022-05-26T09:30:00.000Z</updated>
       <content type="text">There’s certainly no shortage of design systems available to use when building your next project. Between IBM’s Carbon, Wanda and Nord, there are plenty of terrific design systems to choose from. Yet, while each one contains its own nuances and opinions, most share a similar goal — simplifying the development process of creating beautifully accessible user interfaces.
 It’s an admirable goal and, honestly, one that has led me to shift my own career into design systems. But a core feature at the foundation of many design systems is the extensibility for theming. And why wouldn’t it be? Without some flexibility for branding, every product using a particular system would look the same, à la Bootstrap around 2012.
 While providing support for custom themes is vital, it also leaves the most well-intentioned system’s accessibility at the mercy of the implementation. Some teams may spend weeks, if not months, defining their ideal color palette for a rebranding. They’ll labor over each shade and color combination to ensure everything is reliable, informative, and accessible.
 Others simply can’t and/or won’t do that.
 It’s one thing to require alt text on an img element or a label for an input element, but enforcing accessible color palettes is an entirely different beast. It’s a beast with jagged yellow teeth, fiery-red eyes, and green scales covering its body like sheets of crocodile armor.
 At least you think it is. For all you know, it could be a beast of nothing more than indistinct shades of black and slightly darker black.
 And therein lies the problem.
 The CSS Color-Contrast() Function
 Building inclusive products doesn’t mean supporting devices but supporting the people using them.
 
 The CSS color-contrast() function is an experimental feature which is currently a part of Color Module 5. Its purpose — and the reason for the excitement of this article — is to select the greatest contrasting color from a list when compared against a base color.
 
 For the sake of this article, we will refer to the first parameter as the “base color” and the second as the “color list.” These parameters can accept any combination of browser-supported CSS color formats, but be weary of opacities. There’s an optional third parameter, but let’s look at that later. First, let’s define what we mean by this being an experimental feature.
 At the time of writing, the color-contrast() feature is only available in the Safari Technology Preview browser. The feature can be toggled through the Develop and Experimental Features menus. The following demos will only work if the feature is enabled in that browser. So, if you’d like to switch, now wouldn’t be the worst time to do so.
 Now, with the base syntax, terminology, and support out of the way, let’s dive in. 🤿 
 Color Me Intrigued
 It was Rachel Andrew’s talk at AxeCon 2022, “New CSS With Accessibility in Mind”, where I was introduced to color-contrast(). I scribbled the function down into my notebook and circled it multiple times to make it pop. Because my mind has been entirely in the world of design systems as of late, I wondered how big of an impact this little CSS feature could have in that context.
 In her presentation, Rachel demoed the new feature by dynamically defining text colors based on a background. So, let’s start there as well, by setting background and text colors on an article.
 article {
   --article-bg: #222;
 
   background: var(--article-bg);
   color: color-contrast(var(--article-bg) vs #FFF, #000);
 }
 
 We start by defining the --article-bg custom property as a dark grey, #222. That property is then used as the base color in the color-contrast() function and compared against each item in the color list to find the highest contrasting value.
 
     
         
             Base Color
             Color List
       Contrast Ratio
         
     
     
         
       #222
       #FFF
       15.9
         
         
             #222
             #000
       1.31
         
     
 
 
 As a result, the article’s color will be set to white, #FFF.
 But this can be taken further.
 We can effectively chain color-contrast() functions by using the result of one as the base color of another. Let’s extend the article example by defining the ::selection color relative to its text.
 article {
   --article-bg: #222;
   --article-color: color-contrast(var(--article-bg) vs #FFF, #000);
 
   background: var(--article-bg);
   color: var(--article-color);
 
   ::selection {
     background: color-contrast(var(--article-color) vs #FFF, #000);
   }
 }
 
 Now, as the text color is defined, so will its selection background.
 
 The optional third parameter for color-contrast() defines a target contrast ratio. The parameter accepts either a keyword — AA, AA-large, AAA, and AAA-large — or a number. When a target contrast is defined, the first color from the color list that meets or exceeds it is selected.
 
 This is where color-contrast() could really empower design systems to enforce a specific level of accessibility.
 Let’s break this down.
 .dark-mode {
   --bg: #000;
   --color-list: #111, #222;
 }
 
 .dark-mode {
   background: var(--bg);
   color: color-contrast(var(--bg) vs var(--color-list));
 
   &amp;.with-target {
     color: color-contrast(var(--bg) vs var(--color-list) to AA);
   }
 }
 
 The magic here happens when the two color declarations are compared.
 The base .dark-mode class does not use a target contrast. This results in the color being defined as #222, the highest contrasting value from the color list relative to its base color of black. Needless to say, the contrast ratio of 1.35 may be the highest, but it’s far from accessible.
 Compare this to when the .dark-mode and .with-target classes are combined, and a target contrast is specified. Despite using the same base color and color list, the result is much different. When no value in the color list meets the AA (4.5) target contrast, the function selects a value that does. In this case, white.
 This is where the potential of color-contrast() is the brightest.
 In the context of design systems, this would allow a system to enforce a level of color accessibility with very granular control. That level could also be a :root-scoped custom property allowing the target contrast to be dynamic yet global. There’s a real feeling of control on the product side, but that comes at a cost during the implementation.
 There’s a logical disconnect between the code and the result. The code doesn’t communicate that the color white will be the result. And, of course, that control on the product side translates to uncertainty with the implementation. If a person is using a design system and passes specific colors into their theme, why are black and white being used instead?
 The first concern could be remedied by understanding the color-contrast() feature more deeply, and the second could be alleviated by clear, communicative documentation. However, in both cases, this shifts the burden of expectation onto the implementation side, which is not ideal.
 In some cases, the explicit control will justify the costs. However, there are other drawbacks to color-contrast() that will need to be considered in all cases.
 Not All That Glitters Is Gold
 There are inevitable drawbacks to consider, as with any experimental or new feature, and color-contrast() is no different.
 Color And Visual Contrasts Are Different Things
 When using color-contrast() to determine text color based on its background, the function is comparing exactly that — the colors. What color-contrast() does not take into consideration are other styles that may affect visual contrast, such as font size, weight, and opacity.
 This means it’s possible to have a color pairing that technically meets a specific contrast threshold but still results in an inaccessible text because its size is too small, weight is too light, or its opacity is too transparent.
 To learn more about accessible typography, I highly recommend Carie Fisher’s talk, “Accessible Typography Essentials.”
 Custom Properties And Fallbacks
 Since CSS custom properties support fallback values for when the property is not defined, it seemed like a good approach to use color-contrast() as a progressive enhancement.
 --article-color: color-contrast(#000 vs #333, #FFF);
 color: var(--article-color, var(--fallback-color));
 
 If color-contrast() is not supported, the --article-color property would not be defined, and therefore the --fallback-color would be used. Unfortunately, that’s not how this works.
 An interesting thing happens in unsupported browsers — the custom property would be defined with the function itself. Here’s an example of this from Chrome DevTools:
 
 Because the --article-color property is technically defined, the fallback won’t trigger.
 However, that’s not to say color-contrast() can’t be used progressively, though. It can be paired with the @supports() function, but be mindful if you decide to do so. As exciting as it may be, with such limited support and potential for syntax and/or functionality changes, it may be best to hold off on sprinkling this little gem throughout an entire codebase.
 @supports (color: color-contrast(#000 vs #fff, #eee)) {
   --article-color: color-contrast(var(--article-color) vs #fff, #000);
 }
 
 The Highest Contrast Doesn’t Mean Accessible Contrast
 Despite the control color-contrast() can offer with colors and themes, there are still limitations. When the function compares the base color against the list and no target contrast is specified, it will select the highest contrasting value. Just because the two colors offer the greatest contrast ratio, it doesn’t mean it’s an accessible one.
 h1 {
   background: #000;
   color: color-contrast(#000 vs #111, #222);
 }
 
 In this example, the background color of black. #000 is compared against two shades of dark grey. While #222 would be selected for having the “greatest” contrast ratio, pairing it with black would be anything but great.
 No Gradient Support
 In hindsight, it was maybe a touch ambitious trying gradients with color-contrast(). Nevertheless, through some testing, it seems gradients are not supported. Which, once I thought about it, makes sense.
 If a gradient transitioned from black to white, what would the base color be? And wouldn’t it need to be relative to the position of the content? It’s not like the function can interpret the UI. However, Michelle Barker has experimented with using CSS color-mix() and color-contrast() together to support this exact use case.
 It’s not you, color-contrast(), it’s me. Well, it’s actually the gradients, but you know what I mean.
 Wrapping Up
 That was a lot of code and demos, so let’s take a step back and review color-contrast().
 The function compares a base color against a color list, then selects the highest contrasting value. Additionally, it can compare those values against a target contrast ratio and either select the first color to meet that threshold or use a dynamic color that does. Pair this with progressive enhancement, and we’ve got a feature that can drastically improve web accessibility.
 I believe there are still plenty of unexplored areas and use cases for color-contrast(), so I want to end this article with some additional thoughts and/or questions.
 How do you see this feature being leveraged when working with different color modes, like light, dark, and high contrast? Could a React-based design system expose an optional targetContrast prop on its ThemeProvider in order to enforce accessibility if the theme falls short? Would there be a use case for the function to return the lowest contrasting value instead? If there were two base colors, could the function be used to find the best contrasting value between them?
 What do you think? 
 Resources
 
 “New CSS with Accessibility in Mind”, Rachel Andrew
 “Exploring color-contrast() for the First Time”, Chris Coyier
 Color-Contrast() on MDN 
 Support stats on caniuse.com
 Color-Contrast() on W3 Color Module Level 5
 
 Further Reading on Smashing Magazine
 
 “When CSS Isn’t Enough: JavaScript Requirements For Accessible Components”, Stephanie Eckles
 “A Complete Guide To Accessible Front-End Components”, Vitaly Friedman  
 “Making A Strong Case For Accessibility”, Todd Libby
 “Translating Design Wireframes Into Accessible HTML/CSS”, Harris Schneiderman
 </content>
     </entry>
     <entry>
       <title>A teleprompter script for video recordings</title>
         <link href="https://christianheilmann.com/2022/05/25/a-teleprompter-script-for-video-recordings/"/>
       <updated>2022-05-25T21:03:21.000Z</updated>
       <content type="text">Currently I am recording a lot of videos and I found the fastest way to do them is to write the script and read it out to my camera and then record screencasts and edit them to fit the narration. In order to make sure I read from the top of the screen and reading into the camera I had to scroll my scripts a lot, which was annoying so I wrote a small teleprompter script to work around that problem.
 
 	You get a text box to write or paste your script and pressing the “show as prompt” button will display one paragraph at a time at the top of the screen in a large font. You can click the document or press the right arrow key to move forward and Escape to go back to editing.
 
 	
 
 	If you go to https://codepo8.github.io/prompter/ you can see it in action and the source is also on GitHub. It is nothing fancy and a lot was written by GitHub CoPilot for me anyways.</content>
     </entry>
     <entry>
       <title>Release Notes for Safari Technology Preview 146</title>
         <link href="https://webkit.org/blog/12745/release-notes-for-safari-technology-preview-146/"/>
       <updated>2022-05-25T20:43:31.000Z</updated>
       <content type="text">Safari Technology Preview Release 146 is now available for download for macOS Big Sur and of macOS Monterey 12.3 or later. If you already have Safari Technology Preview installed, you can update in the Software Update pane of System Preferences on macOS.
 This release covers WebKit revisions 293023-293745.
 Note: Tab Groups do not sync in this release.
 Web Inspector
 
 Elements Tab
 
 Fixed hovering over a node in the Layout panel to now highlight it on the page (r293189)
 Fixed &lt;button&gt; and &lt;select&gt; elements appearing in the list of Flex containers (r293565)
 
 
 Sources Tab
 
 Added ability for local overrides to entirely block a request (r293409)
 
 
 Timelines Tab
 
 Fixed importing a timeline leaves the overview non-scrollable/non-zoomable until windows is resized (r293727)
 
 
 Graphics Tab
 
 Improved display of GLenums and GLbitfield in WebGL canvas recordings (r293541, r293706)
 
 
 
 CSS
 
 Fixed ::first-letter when used in shadow content (r293497)
 Fixed revert-layer in shadow tree contexts (r293725)
 Fixed cascade rollback for deferred properties (r293485)
 Related properties sharing a computed value (r293602)
 Made word-wrap CSS property an alias of overflow-wrap (r293521)
 Made -webkit-transform-style an alias of transform-style (r293524)
 Unprefixed the -webkit-user-select CSS property (r293089)
 Removed some unimplemented -webkit-appearance keywords (r293511)
 Updated the user-agent stylesheet to include table { text-indent: initial } to conform with the HTML standard (r293322)
 
 JavaScript
 
 Added ISO8601 based Temporal.PlainDate getters behind a flag (r293708)
 Enabled change-array-by-copy (r293348)
 Fixed WASM to throw consistent exceptions for memory.init and memory.copy (r293252)
 Fixed JS stack traces to report the correct column number in CR-LF line ending style HTML files (r293672)
 
 Forms
 
 Fixed setting the correct selection range for textarea when updating the default value (r293673)
 Fixed constructed FormData object to not contain an entry for the submit button that was used to submit the form (r293444)
 Fixed user-select: none to have no effect on editability (r293028)
 
 Media
 
 Fixed the media controls overflow button flickering sometimes (r293658)
 Fixed HTMLMediaElement getting multiple interruptions for invisible autoplay (r293609)
 Fixed MediaSession.setPositionState() (r293488)
 
 Rendering
 
 Fixed the quirk to only stretch the percent height body when it is the document element’s child (r293647)
 Made contain: layout on the html element change  position: fixed behavior (r293209)
 
 Scrolling
 
 Fixed smooth scrolling behavior when focusing a scroll container before beginning to scroll (r293260)
 
 HTML
 
 Fixed &lt;link rel&#x3D;preconnect&gt; always sending credentials to a different origin, ignoring crossorigin&#x3D;anonymous (r293503)
 
 Shared Worker
 
 Fixed resuming a suspended remote shared worker when a new SharedWorker object is created (r293173)
 
 Service Worker
 
 Fixed Service Worker loads to not expose some ResourceTiming information (r293418)
 Fixed Service Worker update to refresh imported scripts in addition to the main script (r293506)
 Fixed Service Worker to not intercept embed- or object-related loads (r293417)
 Fixed ServiceWorkerRegistration updates to fail if called from an installing Service Worker context (r293719)
 Fixed URL.createObjectURL to not be exposed in Service Worker contexts (r293717)
 
 Web API
 
 Fixed Web Locks held in a Worker not getting released on page refresh or exit (r293329)
 
 Accessibility
 
 Changed to not expose ARIA roleDescription value on “generic” elements (e.g. div and span) unless an explicit role value is also defined (r293345)
 
 Security
 
 Fixed mixing strict-dynamic and unsafe-inline Content Security Policies (r293603)
 Set top origin of CORS preflight requests (r293591)
 </content>
     </entry>
     <entry>
       <title>Customizing Color Fonts on the Web</title>
         <link href="https://webkit.org/blog/12662/customizing-color-fonts-on-the-web/"/>
       <updated>2022-05-25T14:00:27.000Z</updated>
       <content type="text">
 Color fonts provide a way to add richness to your designs without sacrificing any of the many benefits of using plain text. Regardless of how decorative a color font is, the underlying text is always searchable, copy/paste-able, scalable, translatable, and compatible with screen readers.
 WebKit now supports CSS @font-palette-values. With this at-rule, you can access predefined color palettes provided by the font designer, and you can customize them in order to make the color font a perfect match for the colors in your designs.
 
 
 ONCE upon a time in the middle of winter, when the flakes of snow were falling like feathers from the clouds, a Queen sat at her palace window, which had an ebony black frame, stitching her husband’s shirts. While she was thus engaged and looking out at the snow she pricked her finger, and three drops of blood fell upon the snow. Now the red looked so well upon the white that she thought to herself, “Oh, that I had a child as white as this snow, as red as this blood, and as black as the wood of this frame!” Soon afterwards a little daughter came to her, who was as white as snow, and with cheeks as red as blood, and with hair as black as ebony, and from this she was named “Snow-White.” And at the same time her mother died.
 
 
 THIS answer so angered the Queen that she became quite yellow with envy. From that hour, whenever she saw Snow-White, her heart was hardened against her, and she hated the little girl. Her envy and jealousy increased so that she had no rest day or night, and she said to a Huntsman, “Take the child away into the forest. I will never look upon her again. You must kill her, and bring me her heart and tongue for a token.” The Huntsman listened and took the maiden away, but when he drew out his knife to kill her, she began to cry, saying, “Ah, dear Huntsman, give me my life! I will run into the wild forest, and never come home again.”
 
 You can try out @font-palette-values today in Safari 15.4 or later.
 
 Color palettes work great in WebKit with the COLRv0 font file format, and we are investigating other formats like SVG.
 Background on the font above
 The font in this demo is a revival of Bradley, a “fairytale blackletter” originally released in 1895. The typeface came with a special set of ornate Initial caps meant for drop caps (see also ::initial-letter) and other titling uses, which David digitized this past December for his Font of the Month Club, just in time for the holidays.
 Each glyph is made up of a handful of distinct layers (letterform, backdrop, ornate linework, letter outline, and border). Making the layers different-yet-coordinated colors adds depth to the design, taking it beyond what a simple foreground/background can provide. It felt like the perfect use case for a color font with multiple color palettes, and a unique opportunity for a 127-year-old font to play a small part in an emerging font technology.
 Palettes in CSS
 Fonts can define one or more of their own color palettes inside the CPAL table inside the font file. The palettes are ordered, and so they are identified by index. For example, a font might define color palette #3 that uses blues and greens, but another palette #5 might use reds and oranges. Colors within a palette inside the font are also identified by index – all palettes contain the same number of colors within themselves.
 These color palettes can be tweaked or overridden in CSS using font-palette-values. An example looks like this:
 @font-palette-values --lilac-blossom {
     font-family: &quot;Bradley Initials DJR Web&quot;;
     base-palette: 7;
     override-colors: 0 #fff, 1 #F3B0EB;
 }
 This example means “Make a color palette named Lilac Blossom, that, when applied to Bradley Initials DJR Web, is just like the 7th palette in the font, but overrides color #0 in that palette to be white, and color #1 in the palette to be #F3B0EB.” If you don’t want to override any colors, that’s no problem – just delete the entire override-colors descriptor.
 You can then apply this color palette by simply supplying it to the font-palette property like this:
 font-palette: --lilac-blossom;
 
 The Round Table
 
  
 Progressive Enhancement
 If you’re using an older version of Safari, or a different browser which doesn’t understand the font-palette property, it will render the default (0th) color palette in the font. Here’s what the above example would look like in such a browser:
 
 The Round Table
 
  
 If the fallback behavior is undesirable for your particular font, you can detect browsers that understand CSS color palettes in your stylesheet by using the @supports media query, like so:
 @supports (font-palette: --lilac-blossom) {
     .lilacblossom {
         font-palette: --lilac-blossom;
     }
 }
 
 @font-palette-values --lilac-blossom {
     font-family: &quot;Bradley Initials DJR Web&quot;;
     base-palette: 7;
     override-colors: 0 #fff, 1 #F3B0EB;
 }
 Dark Mode
 Not all color palettes are clearly visible on all backgrounds. Without color fonts, the color used to render text was entirely determined by the CSS author, but now that fonts can have color palettes defined inside them, it’s up to CSS authors to pick or create a color palette that is legible on the background it’s being rendered on. This can be particularly tricky when font fallback occurs, or when the user has blocked some fonts from loading.
 Fonts such as Bradley Initials DJR Web have an extra tool for helping with this, though. Fonts can indicate that certain palettes inside them are usable with light backgrounds or usable with dark backgrounds, and these palettes are hooked up to the font-palette property. You don’t even have to use @font-palette-values!
 So, if you want to use a color palette on a dark background, you can simply say font-palette: dark, like this:
 
 
 ABCDEFG
 
 
 And the same thing for a light background: font-palette: light:
 
 
 ABCDEFG
 
 
 Because the font-palette property has no effect on non-color fonts, it’s safe to set it in conjunction with the prefers-color-scheme media query, like this:
 @media (prefers-color-scheme: dark) {
     :root {
         background: black;
         color: white;
         font-palette: dark;
     }
 }
 Fallback
 Because @font-palette-values blocks are scoped to a specific font, you can make multiple of them that share a name. This is really powerful – it means you can define a single color palette name, and have it applied differently to whatever font happens to be rendered with it. Here’s an example:
 @font-palette-values --lilac-blossom {
     font-family: &quot;Bradley Initials DJR Web&quot;;
     base-palette: 1;
 }
 
 @font-palette-values --lilac-blossom {
     font-family: &quot;Megabase&quot;;
     base-palette: 2;
 }
 
 &lt;div style&#x3D;&quot;font-palette: --lilac-blossom;&quot;&gt;
     &lt;div style&#x3D;&quot;font-family: &#x27;Bradley Initials DJR Web&#x27;;&quot;&gt;Pizza is amazing!&lt;/div&gt;
     &lt;div style&#x3D;&quot;font-family: &#x27;Megabase Web&#x27;;&quot;&gt;Is there any food better than pizza?&lt;/div&gt;
 &lt;/div&gt;
 This will have Bradley Initials DJR Web’s Lilac Blossom palette applied to Bradley Initials DJR Web, and Megabase’s Lilac Blossom palette applied to Megabase. And you only had to specify the font-palette property once!
 Contextual color
 In addition to pulling colors from palettes, some color fonts set aside special shapes that are connected to the foreground color of the current element. This makes them extra flexible, but it also means that these shapes operate independently from font-palette. In these cases, you can simply use the color property to change their color, like in this demo using Megabase.
 &lt;div style&#x3D;&quot;font-family: &#x27;Megabase Web&#x27;;&quot;&gt;
     &lt;div style&#x3D;&quot;color: black;&quot;&gt;They were just pushed into space.&lt;/div&gt;
     &lt;div style&#x3D;&quot;color: blue;&quot;&gt;As much as I care about you.&lt;/div&gt;
 &lt;/div&gt;
 
 
 They were just pushed into space.
 As much as I care about you.
 
 
 Conclusion
 Of course, with power comes responsibility; just because you can change colors, doesn’t mean you always should. Often the colors in a palette are coordinated to harmonize aesthetically, so it’s good to have a sense of how they are meant to relate to one another. You can look at the font’s predefined color palettes to see how the font designer assigned the roles for each color in the palette, and tweak accordingly.
 It is also important to choose colors that contrast strongly against the background in order to keep your text readable and your webpage accessible. Colors in the palette that are used to form the base of the letters should typically “pop” against the background, while supporting layers like shadows, outlines, and decorative elements might contrast less in order to keep them from overpowering the letterforms.
 Color fonts are a great improvement over graphic images, because they work by default with screen readers, copy/paste, and find-in-page. Also, they gracefully show fallback text if the font somehow fails to load, and they reflow if the browser window resizes. Not only that, color fonts are more flexible than graphic images, because they can incorporate the foreground color of the element using them into the design of the font.
 Color fonts are not meant to take the place of single-color fonts. But used in moderation, at a big enough size in the right circumstance, they can be the perfect icing on the cake!
 You can contact Myles C. Maxfield at mmaxfield@apple.com or @Litherum, and you can contact David Jonathan Ross at david@djr.com or @djrrb, and you can find David’s work at djr.com.</content>
     </entry>
     <entry>
       <title>Understanding Weak Reference In JavaScript</title>
         <link href="https://smashingmagazine.com/2022/05/understanding-weak-reference-javascript/"/>
       <updated>2022-05-25T09:30:00.000Z</updated>
       <content type="text">Memory and performance management are important aspects of software development and ones that every software developer should pay attention to. Though useful, weak references are not often used in JavaScript. WeakSet and WeakMap were introduced to JavaScript in the ES6 version.
 Weak Reference
 To clarify, unlike strong reference, weak reference doesn’t prevent the referenced object from being reclaimed or collected by the garbage collector, even if it is the only reference to the object in memory.
 Before getting into strong reference, WeakSet, Set, WeakMap, and Map, let’s illustrate weak reference with the following snippet:
 // Create an instance of the WeakMap object.
 let human &#x3D; new WeakMap():
 
 // Create an object, and assign it to a variable called man.
 let man &#x3D; { name: &quot;Joe Doe&quot; };
 
 // Call the set method on human, and pass two arguments (key and value) to it.
 human.set(man, &quot;done&quot;)
 
 console.log(human)
 
 The output of the code above would be the following:
 WeakMap {{…} &#x3D;&gt; &#x27;done&#x27;}
 
 man &#x3D; null;
 console.log(human)
 
 The man argument is now set to the WeakMap object. At the point when we reassigned the man variable to null, the only reference to the original object in memory was the weak reference, and it came from the WeakMap that we created earlier. When the JavaScript engine runs a garbage-collection process, the man object will be removed from memory and from the WeakMap that we assigned it to. This is because it is a weak reference, and it doesn’t prevent garbage collection.
 It looks like we are making progress. Let’s talk about strong reference, and then we’ll tie everything together.
 Strong Reference
 A strong reference in JavaScript is a reference that prevents an object from being garbage-collected. It keeps the object in memory.
 The following code snippets illustrate the concept of strong reference:
 let man &#x3D; {name: &quot;Joe Doe&quot;};
 
 let human &#x3D; [man];
 
 man &#x3D;  null;
 console.log(human);
 
 The result of the code above would be this:
 // An array of objects of length 1. 
 [{…}]
 
 The object cannot be accessed via the dog variable anymore due to the strong reference that exists between the human array and object. The object is retained in memory and can be accessed with the following code:
 console.log(human[0])
 
 The important point to note here is that a weak reference doesn’t prevent an object from being garbage-collected, whereas a strong reference does prevent an object from being garbage-collected.
 Garbage Collection in JavaScript
 As in every programming language, memory management is a key factor to consider when writing JavaScript. Unlike C, JavaScript is a high-level programming language that automatically allocates memory when objects are created and that clears memory automatically when the objects are no longer needed. The process of clearing memory when objects are no longer being used is referred to as garbage collection. It is almost impossible to talk about garbage collection in JavaScript without touching on the concept of reachability.
 Reachability
 All values that are within a specific scope or that are in use within a scope are said to be “reachable” within that scope and are referred to as “reachable values”. Reachable values are always stored in memory.
 Values are considered reachable if they are:
 
 values in the root of the program or referenced from the root, such as global variables or the currently executing function, its context, and callback;
 values accessible from the root by a reference or chain of references (for example, an object in the global variable referencing another object, which also references another object — these are all considered reachable values).
 
 The code snippets below illustrate the concept of reachability:
 let languages &#x3D; {name: “JavaScript”};
 
 Here we have an object with a key-value pair (with the name JavaScript) referencing the global variable languages. If we overwrite the value of languages by assigning null to it…
 languages &#x3D; null;
 
 … then the object will be garbage-collected, and the value JavaScript cannot be accessed again. Here is another example:
 let languages &#x3D; {name: “JavaScript”};
 
 let programmer &#x3D; languages;
 
 From the code snippets above, we can access the object property from both the languages variable and the programmer variable. However, if we set languages to null…
 languages &#x3D; null;
 
 … then the object will still be in memory because it can be accessed via the programmer variable. This is how garbage collection works in a nutshell.
 Note: By default, JavaScript uses strong reference for its references. To implement weak reference in JavaScript, you would use WeakMap, WeakSet, or WeakRef.
 Comparing Set and WeakSet
 A set object is a collection of unique values with a single occurrence. A set, like an array, does not have a key-value pair. We can iterate through a set of arrays with the array methods for… of and .forEach.
 Let’s illustrate this with the following snippets:
 let setArray &#x3D; new Set([&quot;Joseph&quot;, &quot;Frank&quot;, &quot;John&quot;, &quot;Davies&quot;]);
 for (let names of setArray){
   console.log(names)
 }// Joseph Frank John Davies
 
 
 We can use the .forEach iterator as well:
  setArray.forEach((name, nameAgain, setArray) &#x3D;&gt;{
    console.log(names);
  });
 
 A WeakSet is a collection of unique objects. As the name applies, WeakSets use weak reference. The following are properties of WeakSet():
 
 It may only contain objects.
 Objects within the set can be reachable somewhere else.
 It cannot be looped through.
 Like Set(), WeakSet() has the methods add, has, and delete.
 
 The code below illustrates how to use WeakSet() and some of the methods available:
 const human &#x3D; new WeakSet();
 
 let paul &#x3D; {name: &quot;Paul&quot;};
 let mary &#x3D; {gender: &quot;Mary&quot;};
 
 // Add the human with the name paul to the classroom. 
 const classroom &#x3D; human.add(paul);
 
 console.log(classroom.has(paul)); // true
 
 paul &#x3D; null;
 
 // The classroom will be cleaned automatically of the human paul.
 
 console.log(classroom.has(paul)); // false
 
 On line 1, we’ve created an instance of WeakSet(). On lines 3 and 4, we created objects and assigned them to their respective variables. On line 7, we added paul to the WeakSet() and assigned it to the classroom variable. On line 11, we made the paul reference null. The code on line 15 returns false because WeakSet() will be automatically cleaned; so, WeakSet() doesn’t prevent garbage collection.
 Comparing Map and WeakMap
 As we know from the section on garbage collection above, the JavaScript engine keeps a value in memory as long as it is reachable. Let’s illustrate this with some snippets:
 let smashing &#x3D; {name: &quot;magazine&quot;};
 // The object can be accessed from the reference.
 
 // Overwrite the reference smashing.
 smashing &#x3D; null;
 // The object can no longer be accessed.
 
 Properties of a data structure are considered reachable while the data structure is in memory, and they are usually kept in memory. If we store an object in an array, then as long as the array is in memory, the object can still be accessed even if it has no other references.
 let smashing &#x3D; {name: &quot;magazine&quot;};
 
 let arr &#x3D; [smashing];
 
 // Overwrite the reference.
 smashing &#x3D; null;
 console.log(array[0]) // {name: &#x27;magazine&#x27;}
 
 We’re still able to access this object even if the reference has been overwritten because the object was saved in the array; hence, it was saved in memory as long the array is still in memory. Therefore, it was not garbage-collected. As we’ve used an array in the example above, we can use map too. While the map still exists, the values stored in it won’t be garbage-collected.
 let map &#x3D; new Map();
 
 let smashing {name: &quot;magazine&quot;};
 
 map.set(smashing, &quot;blog&quot;);
 
 // Overwrite the reference.
 smashing &#x3D; null;
 
 // To access the object.
 console.log(map.keys());
 
 Like an object, maps can hold key-value pairs, and we can access the value through the key. But with maps, we must use the .get() method to access the values.
 According to Mozilla Developer Network, the Map object holds key-value pairs and remembers the original insertion order of the keys. Any value (both objects and primitive values) may be used as either key or value.
 Unlike a map, WeakMap holds a weak reference; hence, it doesn’t prevent garbage collection from removing values that it references if those values are not strongly referenced elsewhere. Apart from this, WeakMap is the same as map. WeakMaps are not enumerable due to weak references.
 With WeakMap, the keys must be objects, and the values may be a number or a string.
 The snippets below illustrate how WeakMap works and the methods in it:
 // Create a weakMap.
 let weakMap &#x3D; new WeakMap();
 
 let weakMap2 &#x3D; new WeakMap();
 
 // Create an object.
 let ob &#x3D; {};
 
 // Use the set method.
 weakMap.set(ob, &quot;Done&quot;);
 
 // You can set the value to be an object or even a function.
 weakMap.set(ob, ob)
 
 // You can set the value to undefined.
 weakMap.set(ob, undefined);
 
 // WeakMap can also be the value and the key.
 weakMap.set(weakMap2, weakMap)
 
 // To get values, use the get method.
 weakMap.get(ob) // Done
 
 // Use the has method.
 weakMap.has(ob) // true
 
 weakMap.delete(ob)
 
 weakMap.has(ob) // false
 
 One major side effect of using objects as keys in a WeakMap with no other references to it is that they will be automatically removed from memory during garbage collection.
 Areas of Application of WeakMap
 WeakMap can be used in two areas of web development: caching and additional data storage.
 Caching
 This a web technique that involves saving (i.e. storing) a copy of a given resource and serving it back when requested. The result from a function can be cached so that whenever the function is called, the cached result can be reused.
 Let’s see this in action. Create a file, name it cachedResult.js, and write the following in it:
 
  let cachedResult &#x3D; new WeakMap();
  // A function that stores a result.
 function keep(obj){
 if(!cachedResult.has(obj){
   let result &#x3D; obj;
   cachedResult.set(obj, result);
   }
 return cachedResult.get(obj);
 }
 
 
 let obj &#x3D; {name: &quot;Frank&quot;};
 
 let resultSaved &#x3D; keep(obj)
 
 obj &#x3D; null;
 
 // console.log(cachedResult.size); Possible with map, not with WeakMap
 
 
 If we had used Map() instead of WeakMap() in the code above, and there were multiple invocations on the function keep(), then it would only calculate the result the first time it was called, and it would retrieve it from cachedResult the other times. The side effect is that we’ll need to clean cachedResult whenever the object is not needed. With WeakMap(), the cached result will be automatically removed from memory as soon as the object is garbage-collected. Caching is a great means of improving software performance — it could save the costs of database usage, third-party API calls, and server-to-server requests. With caching, a copy of the result from a request is saved locally.
 Additional Data
 Another important use of WeakMap() is additional data storage. Imagine we are building an e-commerce platform, and we have a program that counts visitors, and we want to be able to reduce the count when visitors leave. This task would be very demanding with Map, but quite easy to implement with WeakMap():
 let visitorCount &#x3D; new WeakMap();
 function countCustomer(customer){
    let count &#x3D; visitorCount.get(customer) || 0;
     visitorCount.set(customer, count + 1);
 }
 
 Let’s create client code for this:
 let person &#x3D; {name: &quot;Frank&quot;};
 
 // Taking count of person visit.
 countCustomer(person)
 
 // Person leaves.
 person &#x3D; null;
 
 With Map(), we will have to clean visitorCount whenever a customer leaves; otherwise, it will grow in memory indefinitely, taking up space. But with WeakMap(), we do not need to clean visitorCount; as soon as a person (object) becomes unreachable, it will be garbage-collected automatically.
 Conclusion
 In this article, we learned about weak reference, strong reference, and the concept of reachability, and we tried to connect them to memory management as best we could. I hope you found this article valuable. Feel free to drop a comment.</content>
     </entry>
     <entry>
       <title>Karol Śliwka</title>
         <link href="https://www.logodesignlove.com/karol-sliwka"/>
       <updated>2022-05-24T11:51:45.000Z</updated>
       <content type="text">
 
 Born in 1932 in the Silesian Foothills in Harbutowice, southern Poland, Karol Śliwka began his art education in 1946 at the Evening School of Painting, Sculpture and Graphics in Bielsko-Biała.
 “I was 13 years old and I had been disarming mines in the field. One of them exploded. To this day I work with one eye. My dad decided that I would have to stay on the farm. But I insisted. I said I would learn, and it happened.”
 
 
 
 His education continued at the State Secondary School of Art Techniques, graduating with a diploma in 1953. He would later pass entry exams for the Academy of Fine Arts in Warsaw, graduating in 1959.
 “I went to Warsaw with only a few zlotys with me, which my mother gave me for a roll, but that was not enough for food or for the train. So I admit that I was riding on a fake ticket. In the spot, I ate in a cheap bar in Mariensztat, where one meal had to be enough for breakfast, lunch and dinner. I remember it was a rhubarb soup and a thin macaroon. It was the only thing I could do.”
 “The scholarship was absolutely not enough for me. I also used to smoke cigarettes. I had no choice but to earn some extra money somewhere. In the second year, thanks to a friend, we won orders in a scientific and technical publishing house for the design of book covers, and when we were in the third year, it was this publishing house that announced a nationwide competition for a graphic sign. I had no idea about this area of ​​design, so I would go to the library and see what these characters look like, what they are. I tried and sent a few proposals for the competition. It was attended by an older generation of pre-war graphic artists who were still working and doing beautiful things. When I saw their works next to mine at the post-competition exhibition I was extremely ashamed. How could I send such scribbles?! It had nothing to do with a professionally designed sign, it was some kind of smear on a napkin in a coffee shop.”
 You can read more of this time in Karol’s life, and how his energy turned from painting to design, in an insightful interview on weekend.gazeta.pl (in Polish, I used Chrome’s translate). Or if you prefer to listen and watch, there’s an interview on YouTube that’s very well pieced together, first aired (I think) during the 2018 exhibition of his work.
 
 
 
 
 
 
 After his fine art graduation (studying graphic design at night as it wasn’t an option in Warsaw at the time) he began working in the field of applied graphics, later joining the Association of Polish Artists and Designers and the Society of Authors ZAiKS.
 
 
 
 
 
 
 
 
 
 The book above (photos via czytamaja.pl) can be picked up here (also reviewed on YouTube by Christophe De Pelsemaker). Patrick Hardziej is the designer behind the book (among many other interesting design ventures), and he took time out to be interviewed by Counter Print’s Jon Dowling. Worth a read.
 
 
 
 Karol received the Award of the Minister of Culture and Art for his graphic art at the first Polish Exhibition of Graphic Marks in 1969.
 Other awards and distinctions included (to name just a few):
 First Degree Prize of the Prime Minister, the Golden Olympic Laurel, in the field of art. Gold medal at the Polish Poster Biennale in Katowice. Silver for a series of Olympic posters. A “Golden Chestnut” for packaging. An award for a postage stamp in the The United Nations. The Knight’s Cross of the OOP. The Golden Cross of Merit. The Golden Badge “For merits for Warsaw.” The badge “Meritorious Cultural Activist.” And the Gold Medal for Merit to Culture, GLORIA ARTIS.
 He died on September 10, 2018, in Cieszyn, southern Poland.
 
 
 
 Many of Karol’s marks are found in the Logobook archive, and more samples of his design work are at the official (though dated) karolsliwka.pl.
 
 </content>
     </entry>
     <entry>
       <title>Developing An Award-Winning Onboarding Process (Case Study)</title>
         <link href="https://smashingmagazine.com/2022/05/developing-award-winning-onboarding-process-case-study/"/>
       <updated>2022-05-24T11:30:00.000Z</updated>
       <content type="text">The notion of onboarding is all about helping users quickly and easily find value in your offering. Speed and ease of use are equally important because users might lose interest if going through an onboarding takes more time or is more complicated than what they expected. Speed and ease of use are also relative to a person’s point of view: a salesperson can have vastly different expectations for an onboarding than a developer. 
 A well-constructed onboarding process boosts engagement, improves product adoption, increases conversion rates, and educates users about a product. Optimizing the onboarding experience is a journey. You should have a plan but be agile, utilizing processes and tools to garner feedback from target users in a bid to constantly improve. 
 In this article, we will walk you through how we developed the onboarding processes for platformOS from the very beginning. You will be able to follow how we carried out user experience research, how our onboarding has changed over time, what assumptions we made, and how we adjusted them. We will talk about all the tools we used as examples, but the same processes can be implemented with a wide variety of other tools. You will get practical examples and a complete overview of how we built our onboarding, with insights into UX research and the specifics of working with different audience segments. 
 Our audience has always combined technical people with various levels of programming skills, and non-technical people who come to our docs to evaluate if platformOS would be a good fit for their projects like Project Owners, Business Analysts, and Project Managers. Because our main target audience is divided into different segments, you will also get a glimpse of the processes we developed for our documentation, developer education, and developer relations. 
 Challenge: Onboarding For Different Target Audiences
 platformOS is a model-based application development platform aimed at front-end developers and site builders automating infrastructure provisioning and DevOps. 
  DevOps is a combination of development methodologies, practices, and tools that enable teams to evolve and improve products at a faster pace to better serve their customers and compete more effectively in the market. Under a DevOps model, development and operations teams are merged into a single team where the engineers work across the entire application lifecycle, from development and test to deployment to operations.
 
 Our main target audience is developers, and the foundation for their onboarding, education, and support is our developer portal — but our onboarding has to cater to other target audience segments as well. 
 Defining Our Target Audience Segments
 We defined our target audience during the discovery phase of the Design Thinking process that we used to plan our developer portal. Since then, we have frequently revalidated the results to see if we are on the right track because we want to be sure that we understand the people who will be using our product, and what motivates them. We also know that in the lifecycle of a product this audience can change as a result of product positioning, and how well we can address their needs. 
 Our target audience currently has four segments:
 
 Experienced developers,
 Junior developers,
 Agency Owner, Sales/Marketing,
 PM, Business Analyst.
 
 
 User Base Shifts
 We created the first target audience map when we started planning our developer portal. In the discovery phase, we mapped out four proto-personas that covered the following segments: Experienced Developers, Junior Developers, Site Builders, and Marketplace Owners.
 We revalidated these results a year later, and we realized that our audience had shifted a bit. 
 
 The Experienced Developers and the Junior Developers stayed as the main target audiences. However, we collected new knowledge related to the needs of the junior devs. They needed more detail to be able to understand and start working with the product. This new information helped us specify their user journey.
 At this point, the Site Builders were the smallest group. We identified we needed to address the needs of the developers group first, creating a strong foundation to support site builders in the platform.
 The non-technical segment shifted on the way. The Marketplace Owners segment was divided into two separate audiences: the Agency Owners, who have a sales and marketing background, and the Business Analysts, who have an enterprise background in business management or transformation — a new audience who started to show interest in our product.
 
 Along the way, we were able to specify the needs of these audiences in more detail. These details helped with the prioritization of the onboarding tasks and kept our focus on the needs of the audience. 
 Defining Entry Points For Target Audience Segments
 Getting to know the needs of the target audience segments provided guidance for identifying the entry points to the product.
 
 The Agency Owners’ key goal is to work on multiple web projects that they host and manage on the platform. They won’t work on the platform themselves, but they would like to know the status and the progress of the platform without worrying about DevOps. They need to see the business perspective, the security, and that they are part of a reliable ecosystem with a helpful community around without diving deep into the technical part of the product.
 The Business Analysts’ goal is to identify solution providers for their specific business problems. They need to find a long-term solution that fits with their use case, is scalable, and gives them the possibility for easy evaluation that shows the key business values in action.
 The Junior Developers’ goal is to learn the basics without much hassle, under the guidance of experienced community members. They need clear technical communication on how to set up a dev environment and how to troubleshoot common errors.
 The Experienced Developers’ goal is to find a solution that is reliable and flexible enough for all their project needs and at the same time provides good performance. They need to be able to evaluate quickly if it’s a good fit, then see how their project could work on the platform. They also need to see that the platform has a future with a solid community behind it.
 
 All segments needed an actionable onboarding where they can interact with the product (and engage with the community) based on their level of technical knowledge. 
 
 In the non-technical journey, users can go from the 1-click route that takes them through registering on the Partner Portal to creating a demo site and installing the blog module by clicking through a setup wizard.
 In the semi-technical journey, users can create a sandbox in which they can experiment by cloning a demo site from our GitHub repository, and they also have the option to go through our “Hello, World!” guide.
 In the technical journey, users can follow a more complex tutorial that walks them through the steps of creating an app on platformOS from setting up their development environment to deploying and testing their finished app. It explains basic concepts, the main building blocks, and the logic behind platformOS, while also giving some recommendations on the workflow.
 
 
 How We Approached The Challenge: Methods And Tools
 We followed various methods to tackle different aspects of the main challenge. We selected a Design process to follow, used many different user research methods to collect insights and feedback from our users, chose a framework for our editorial workflow and technical implementation that could work well for our Agile, iterative process and our target audience, and went with an approach for content production that allowed community members to contribute early on. 
 Design Thinking
 Because of the strategic role our developer portal plays in the adoption and use of our product, we wanted to use a creative design process that solves traditional business problems with an open mindset.
 Our goal was to:
 
 help our community to be able to use our documentation site for their needs as early as possible;
 measure user needs and iterate the product based on the feedback;
 keep the long-term user and business goals in mind and take a step closer with each iteration.
 
 We found the Design Thinking framework a perfect fit because it is a user-centric approach that focuses on problem-solving while fostering innovation. 
 
 We followed the stages of the design thinking process:
 
 EmpathizeIn the beginning, we explored our audience, our documentation needs, and existing and missing content through in-depth interviews and workshops. 
 DefineThen, we defined personas and our Content Inventory.
 IdeateWe shared our ideas for content and features through a Card Sorting exercise.
 PrototypeBased on our findings, we created a sitemap and prioritized content needs, and created layouts and wireframes. Content production started based on the results of our discovery phase.
 TestWe followed an iterative, Docs as Code approach: at each stage, we work with quick feedback rounds, deploy often, and improve features and content based on feedback from real users.
 
 User Research
 In the life of a product, each development stage has a fitting UX research method that we can use, depending on the business plans, time constraints, stage of product/feature, and the current concerns.
 In the last three years we used the following methods:
 
 InterviewsWe met with users, sales, and support persons to discuss in-depth what the participant experienced about various topics.
 Remote Usability TestingWe asked potential or current users of the product to complete a set of tasks during this process, and we observed their behavior to define the usability of the product. We used two types of remote usability testing:
 Moderated: We conducted the research remotely via screen-sharing software, and the participants joined in from their usual work environment. This approach is advantageous when analyzing complex tasks — where real-time interaction and questioning with participants are essential.
 Unmoderated: We sent tasks for users to complete in their own time. As moderators are not present, we measured less complex tasks and focused on the overall level of satisfaction they experienced when interfacing with the product.
 
 
 Card SortingA quantitative or qualitative method, where we ask users to organize items into groups and assign categories to each group. This process makes it possible to reflect the users’ mental model on the architecture. 
 Tree testsWe used tree tests to validate the logic of the used information architecture. We gave users a task to find certain elements in the navigation structure and asked them to talk about where they would go next to accomplish the task.
 Surveys, QuestionnairesWe used questionnaires and surveys to gather a large amount of information about a topic. This quantitative data can help us have a better understanding of specific topics that we can further research to understand what motivates users.
 Analytics reviewWe used site analytics to gather quantitative data about usage patterns and identify possible flow breaks. Based on the data we either fixed the problem or if needed, we further tested with usability research.
 
 Docs As Code And CI/CD
 We engaged our users in an Agile and iterative process right from the beginning discovery phase. This ensured that we were able to test and validate all of our assumptions, and quickly make modifications if needed. As our internal team members and our community participants are distributed, we needed a workflow that made it possible to collaborate on changes, large or small, remotely. Consequently, we needed a robust approach to version control accommodating authors, reviewers, and editors all working on content concurrently. As we wanted to encourage developers to contribute, we needed a framework that they’re familiar with. We also wanted to make our documentation open-source, so that anyone could duplicate and reuse it for their own projects. Based on these requirements, we decided to follow the Docs as Code approach. 
 Documentation as Code or Docs as Code refers to a philosophy of writing documentation with the same tools as software coding. This means following the same workflows as development teams, including being integrated into the product team. It enables a culture where writers and developers both feel they have ownership of the documentation and work together to aim for the best possible outcome. In our case, we didn’t only have writers and developers working on our onboarding but also UX researchers, account and project managers, and of course, a range of users in varying roles.  
 Our documentation is in a separate repository on GitHub. We have a central branch, and we work locally in a dedicated branch, then we send pull requests for review to be merged into the main branch. To preview docs, we use our own staging site which is an exact copy of the live documentation site. 
 Once we accept changes, we take steps to push them live almost immediately. To maintain the integrity of the site during this process, we follow the practice of continuous integration and continuous deployment (CI/CD). We run test scripts automatically and deploy the codebase to staging. If a test fails, an error report is generated. Alternatively, if everything goes well, our CI/CD of choice — GitHub Actions — deploys the codebase to production and sends us a notification. We release updates continuously, at times merging multiple changes in a single day, at other times only once or twice a week. 
 
 Editorial Workflow
 Docs as Code provides the foundation for our processes, but for the various users to work efficiently together, we needed to define a clear editorial workflow that worked for all participants (internal and external writer, developer, contributor, and so on) and for all stages of the process (writing, reviewing, editing); but that was also simple enough to involve new contributors. Following Docs as Code, each stage of our workflow is in git, including project management (contributors can also add tickets to report issues or requests). 
 These are the steps of our editorial workflow:
 
 Write new content in Markdown using the templates. You can use any editor that can produce Github Flavored Markdown. 
 Submit the new topic as a pull request on GitHub. 
 Review. We have a peer-review system in place for code and docs alike. Topics are reviewed by both technical reviewers (developers) and writers. 
 Edit as needed. Repeat steps 3-4 until approved. 
 Merge approved pull request. 
 Deploy to staging, then to production.
 
 
 Our editorial workflow ensures that contribution works the same way for everyone, and we support our contributors with guidelines and ready-to-use templates. 
 Content Production And Contribution
 When we started developing our onboarding and documentation, we followed the Content First approach. We planned to develop some initial content that we could work with, but even before that, we decided what types of content we would need and outlined the structure of each content type. These outlines became templates that ensure consistency and encourage contribution. 
 We were inspired by topic-based authoring and DITA, in the sense that we decided to have three main content types for our documentation, tutorials that describe how to accomplish a task, concepts that provide background information and context, and references like our API Reference. Our onboarding consists of tutorials that link to concepts and references when needed. 
  DITA, short for Darwin Information Typing Architecture, is an XML standard, an architectural approach, and a topic-based writing methodology where content is authored in topics rather than in larger documents or publications. A DITA topic must make sense in its own right.
 
 Involving our users from the beginning ensured that we could test and validate all of our assumptions, and quickly modify anything if needed. This proved to be a time and cost-efficient approach: although we edit and rewrite our content, and change things on our documentation site all the time, we don’t run the risk of creating large chunks of work that have to be thrown away because they don’t correspond to the needs of our users.
 Constant collaboration also builds trust: as our process is completely transparent, our community continuously knows what we’re working on and how our docs evolve, and community members can be sure that their opinions are heard and acted upon. 
 Involving the community from an early stage means that our users saw lots of stuff that was partially done, missing, or ended up totally rewritten. So, for all of this to work, our users had to be mature enough to give feedback on half-done content, and we had to be level-headed enough to utilize sometimes passionate criticism. 
 Encouraging Contribution
 We wanted to make it very easy to get involved for all segments of our target audience, so we offer several ways to contribute, taking into consideration the time contributors have available, and their skill level. We describe ways for our community members to get involved in our Contributor Guide. For some quick editing, like fixing typos or adding links, contributors can edit the content easily on the GitHub UI. For heavy editing, adding new content, or for developers who prefer to use git, we provide a complete Docs as Code workflow. This approach proved to be extremely valuable for our onboarding. We got direct feedback on where users struggled with a step or had too little or too much information, and we could immediately make adjustments and verify that we have fixed the issue. 
 To help contributors write larger chunks of text or complete topics, we provide guidelines and templates to start from:
 
 Style GuideOur style guide contains guidelines for writing technical content (e.g. language, tone, etc.) and each content type in our documentation (e.g. tutorials, concept topics, etc.).
 
 
 
 TemplatesOur site uses Liquid pages, but to make editing easier for contributors, we write documentation content in Markdown and use a Markdown converter to turn it into Liquid. Our templates include all non-changeable content and placeholders with explanations for the parts that are editable. Placeholders provide information on the recommended format (e.g. title) and any requirements or limitations (e.g. maximum number of characters). 
 
 
 We thank all of our contributors by giving recognition to them on our Contributors page as well as on our GitHub repository’s README page.
 Communication
 Our team and community members are scattered across different time zones. Similarly to how we communicate among team members, we use mostly asynchronous and sometimes real-time communication tools to communicate with our community. We even leverage real-time communication tools, like a video conference, to become somewhat asynchronous. For example, video conferences and webinars are recorded, and community members can discuss them on various channels.
 
 pOS Community siteOne of our main communication channels is our community site, where you can ask, answer, upvote, and downvote questions, and get to know other members of the platformOS Community. More features coming soon!
 Slack supportOne of our main communication channels is dedicated Slack channels, where community members ask questions, share ideas, and get to know our team members and each other. Based on their feedback, community members have confirmed how helpful it is to be able to communicate directly with us and each other: they can share what they’ve learned, plan their module development in sync with our roadmap and each other’s projects, and allocate their resources according to what’s going on in the business and the wider community. This communication seeds the documentation site with the most sought-after topics.
 Video conferenceWe regularly have video conferences over Zoom called Town Halls, where community members and the platformOS team share news, demo features, and modules and have the opportunity to engage in real-time, face-to-face conversation. Our team and community members are distributed over different continents, so we try to accommodate participants in different time zones by rotating the time of this event so that everyone has the chance to participate. We also share the recording of each session. 
 User experience researchBesides getting constant feedback from the community through the channels described above, we plan regular checkpoints in our process to facilitate testing and course correction. During development, we tie these checkpoints to development phases. At the end of each larger release, we conduct user interviews and compile and share a short survey for community members to fill out. This helps us clarify the roadmap for the next development phase.
 
 We make sure to keep community members informed about what’s happening through different channels: 
 
 Status reportsWe regularly share status reports on our blog to keep our community updated on what we’ve achieved, what we are working on, and what we are planning for the near future. Our status reports also include calls for contribution and research participation and the results and analysis of UX research. Subscribers can also choose to receive the status reports via email newsletter.
 Release notesWe share updates regarding new features, improvements, and fixes in our release notes.
 BlogWe regularly share articles about best practices and general news on our blog.
 
 Accessibility And Inclusiveness
 We address accessibility right from the design phase, where we use Figma’s Able accessibility plugin. We regularly test for accessibility with various tools and ensure that the site complies with all accessibility requirements. 
 From a technical writing perspective, we support Accessibility and Usability by providing well-structured, clear, concise, and easy-to-understand copy. All of our documentation topics follow a predefined structure (predefined headings, steps, sections, link collections, and so on) applicable to that topic type (tasks, concepts, references), inspired by the principles of topic-based authoring. 
 Semantic HTML is important for Accessibility, and we make sure not to style text any other way than through Markdown which is then translated into HTML. This way, screen readers can properly navigate through the content, and it also helps overall consistency when, for example, we want to do a design update.
 We also review all content to ensure accessible and inclusive language as specified in our style guide. 
 How We Developed Our Onboarding: Rounds And Lessons Learned
 Developing Our Onboarding Using Continuous Iteration Rounds
 At the beginning of the project, we started with a focused effort around discovery to identify the main business goals and user needs. As a result of this research, we were able to articulate the big picture. After we had all the user journeys and a sitemap for the big picture plan, we were able to break it down to identify the first iteration that would become the first working MVP version of the site.  
 Moving forward, we continue to follow an iterative approach, moving fast with an agile mindset. Steps: gather user feedback, identify areas of improvement and possible new directions, define the solution based on resources, business goals, and user needs, and implement it. This circle repeats indefinitely. So, we have an overarching plan outlined for our documentation that we keep in mind, but we always focus on the next couple of action steps we’d like to take.
 We can highlight five distinctive rounds that had a great impact on the development of our developer portal.
 
 For our onboarding process, we started with exploring the requirements following the Design Thinking approach. Through a Card Sorting session, we explored the areas of interest for each target audience and that helped us define the topics that concern them the most. This worked as a persona-based content prioritization for the documentation site.
 We wanted to guide our users with actionable items that they can try out on our site as a next step. At this point, we were already aware that our target audience shifted. The interviews and the support feedback helped us understand their needs that pointed in two main directions. We needed an easy journey for non-technicals and another one for technicals who like to understand the logic of the platform. In this stage, we planned, tested, and developed the first version of the 1-click journey and the sandbox.
 We already had experienced platform users who we wanted to see in action. Using remote field studies, we discovered how they use the tools, the documentation site, and the partner portal we provide. At the same time, we started to conduct continuous onboarding interviews with partners who joined the platform. The two research directions helped us to realize how users with a varying degrees of experience interpret the platform.
 By this point, our content grew a lot on the developer portal, and we wanted to discover if we needed a structural and content reorganization based on the user research. 
 In this latest round, we wanted to dedicate some time to fine-tuning and adjustments, and to double down on the developer portal’s accessibility and inclusiveness.
 
 Round 1: Identifying The Target Audience Segments, Defining Proto-Personas, Base Discovery
 With the Design Thinking workshops, we first focused on understanding our users. Based on the user research results, we defined the proto-personas and created a detailed description of each to show their needs and expectations and help us identify who we were designing for. It provided a good foundation for guiding the ideation process and prioritizing features based on how well they address the needs of one or more personas.
 
 On our documentation site, we are working with a large amount of data that we need to present clearly to all users. To define a Content Inventory:
 
 we created a list of our proto-personas’ needs based on the problems they needed to solve with the platform;
 we created a detailed list of content from our previous documentation site and identified missing, reusable, and non-reusable content for our future site;
 we analyzed the competitor sites to create a list of inspirations.
 
 We ideated with the workshop participant using a Card Sorting exercise. The task was to map out the Content Inventory elements and define connections between them. The result showed us the connected areas and the proto-persona’s preference through color coding.
 
 Based on the Content Inventory and the results of the Card Sorting sessions, we outlined the Information Architecture by creating a sitemap and the navigation for our future site. This plan included all the needs that were discovered and offered a roadmap to keep track of site improvements, content needs, and project phases. 
 During the Card Sorting sessions, we explored areas of interest for each user persona and, on the sitemaps, we highlighted these as user journeys. We also validated the importance of these areas to assign higher priorities to the ones that need more attention. This process kept our focus on the most important needs of the personas. 
 The most important sections for the four segments:
 
 Experienced Developers: Quickstart guide, How to guide, API docs;
 Junior Developers: Quickstart guide, Tutorials, Conceptual documentation;
 Site Builders: Quickstart guide, Tutorials, FAQ, Forum;
 Marketplace Owners: About platformOS, Blog.
 
 This concluded our Information Architecture phase. We have discovered and organized all the information we needed to continue to the next phase, where we started creating templates for content types, building the wireframes for each page, producing content, and making Design decisions.
 Round 2: Onboarding Strategy And Testing Of The Onboarding Process
 Strategy
 Before we jumped into planning an onboarding strategy, we did a revalidation on proto-personas. At that point, we discovered that our audience shifted to Experienced developers, Junior developers, Agency Owner, Sales/Marketing, PM and Business Analyst, and we realized that we needed to cover a broader spectrum of needs than previously identified.
 We interviewed 20 platformOS users. We identified how long they have been using the system, how they use the platform, what the key ‘aha’ moments were, what struggles they faced, and how they solved them. Their needs pointed in two main directions: we needed an easy journey for non-technicals and another one for technicals, covering those with less experience as well as those more capable developers who wished to understand the deeper logic and nuances of platformOS.
 Our main goals with the new onboarding strategy were:
 
 to connect our systems (developer portal — partner portal — platform), so our users can go through their discovery experience in one flow during their first visit;
 to provide an actionable stepped process that the users can walk through;
 allow users/personas to quickly identify the most fitting journey.
 
 
 Usability Test
 We conducted remote Usability Test sessions in three rounds to validate the platformOS onboarding process. 
 The onboarding section connects the Documentation site and the Partner Portal where users can select one of three journeys based on their programming experience. The goal was to learn how users with different levels of technical knowledge reacted to the three journeys. Are they able to quickly identify what is included in each journey? If yes, how do they engage from that time forward? Did they follow the pathway most appropriate for them?
 During the Usability study, we asked users to do several short tasks using a prototype of the planned features built with Figma. We used both moderated and unmoderated remote usability testing techniques and conducted extra tests with platformOS team members to verify the represented business, technical, and content goals.
 We conducted six moderated remote Usability Tests in two rounds and set up three unmoderated remote Usability Tests. These tests were separated into three rounds, and after each round, we updated the prototype with the test results.
 Based on the test results, we decided that instead of showing three options to the users, we show the two quickest options: 1-click install and Build a basic ‘Hello world’ app. This helps them to quickly decide which is the best fit for them, and at the same time they can immediately try out the platformOS basics. Then, if they want to, they can check out our third journey — the Get Started guide that explains how to build a to-do app.
 We redesigned the Instance welcome screen to help users identify the next steps. Based on the results, we had to optimize the UI copy to make it comfortable for non-technical users as well.
 As the flow connects two sites and shows the product, the main goal was to show that the user is on the right track and still on the selected journey. We achieved it by showing the steps of the journey upfront, using consistent wording, and allowing the user to step back and forth.
 
 Round 3: Remote Field Study And Onboarding Interviews
 In this round, the goal was to examine the overall journey of the experienced and prospective pOS users, focusing on both successes and challenges they are facing. We conducted an interview with a remote field study to get a better understanding of how they work and what processes they are using.
 We focused on four main topics:
 
 Development with pOS (workflows, preferences on version control, tools),
 Community and collaboration (support, discussions),
 Developer Portal (overall experience, obstacles, suggestions for improvements),
 Partner Portal (usage, dashboard preferences).
 
 Key insights from the user research results:
 
 The development with platformOS has a flexible and limitless offering which is a great strength of the system, but it also means that learning the workings of the platform, especially in the very beginning, takes more effort and patience from developers.Solution: Templates might provide aid during the learning process.  
 
 As platformOS is new in the market, there’s not much information on Google or StackOverflow yet. On the positive side, the pOS team always provides great support via Slack and introduces new solutions in Town Hall meetings, status reports, and release notes.Solution: To further strengthen the community, a separate Community Site can be an efficient and quick platform for peer-to-peer support by having a search function, and users can follow useful topics.  
 
 Related to the Developer Portal, we saw that the user easily gets to the documentation and finds the solution for most of their use cases. However, the search results were not precise enough in some cases, and the naming of the tutorials caused uncertainty about where to find items.Solution: Run a content reorganization session for the tutorials and fix the search function.  
 
 We discovered that the Partner Portal was used mostly at the beginning of the projects by experienced devs. Junior developers preferred that they can find helping instructions on the instances page that supported their work on the new instances. Agency Owners/Business Analyst preferred to use the site to see the payments related information and the analytics of the instance use. We saw that they generally had problems handling the permissions related to the instances and identifying the hierarchy between their instances.Solution: Partner Portal design update with new information structure of the instances and permissions.
 
 
 Round 4: Structural And Content Reorganization, User Testing, Implementation
 Structural And Content Reorganization
 In this round, we renamed the Tutorials section to Developer Guide. This was in line with our plan to extend our tutorials in this section with more concept topics, as requested. We planned to have a comprehensive Get Started section for beginners with the “Hello, World!” tutorial and the Build a To-do List App series, and the Developer Guide for everyone working with platformOS — from users who have just finished the Get Started guides to experienced platformOS developers. This separated and highlighted the onboarding area of the site, and this is when the current structure of our Get Started section came to be: a separate tutorial for when you start your journey with platformOS, that you can use as a first step to go through the more advanced onboarding tutorials. 
 Card Sorting
 At this point, we had 136+ topics in our Tutorials section organized into 27 groups, and we knew that we wanted to add more. Based on user feedback, we could improve the usability of the Tutorials section by organizing the topics better. Our goal was to identify a structure that best fits users’ expectations. We used a Card Sorting exercise to reach our goal. 
 We have analyzed the inputs, and based on the results, we concluded that seven categories can cover our 27 topics: Data management, Schema, Templates, Modules and Module examples, Partner Portal, Third-Party Systems, and Best Practices. We used the similarity matrix and the category namings to identify which topics are connected and what names users suggested for them.
 With this research, we managed to restructure the Tutorials section to become in line with the mental models of the users.
 Round 5: Fine-Tuning, Content Production
 In the latest round, we added the possibility, on our onboarding, to start from a template. Based on our discovery, the marketplace template is a good option for site builders who would like to have a marketplace up and running fast and don’t want to explore the development in detail. 
 The pOS marketplace template is a fully functional marketplace built on platformOS with features like user onboarding, ad listings and ads, purchase and checkout process, and online payment. Following the tutorial we added, users can deploy this code within minutes to have a list of working features and start customizing the back- and front-end code.
 We also keep fine-tuning our content for clarity, brevity, readability, accessibility, and inclusive language. We have regular accessibility reviews where we pay attention to aspects, such as terminology, technical language, gender-neutral pronouns, and informative link text while avoiding ableist language, metaphors, and colloquialisms. We summarized our experience with fine-tuning accessibility in the article “Code and Content for Accessibility on the platformOS Developer Portal” which includes examples of what we changed and how. 
 Future Plans
 The platformOS Developer Portal was very positively received and even won a few peer-reviewed awards. We are honored and grateful that our efforts have yielded such great recognition. We will keep revalidating and improving our onboarding just like we have been doing since the beginning. We are also working on a developer education program for our soon-to-be-launched community site that includes various learning pathways that will try to accommodate users’ different learning styles and also offer ways for them to get more involved with our developer community. 
 
 Conclusions
 So, after years of working on our onboarding, what are our key takeaways?
 
 Don’t feel pressured to get everything right the first time around. Instead, become comfortable with change and consider each adjustment progress.
 Get to know your target audience and be ready to revalidate and shift target audience segments based on your findings.
 Get familiar with different user research methods to know when to use which approach. Carry out extensive user research and, in turn, listen to your users. To support feedback, allow users multiple different channels to give you feedback.
 Choose a flexible workflow, so that the editorial process does not become an obstacle to continuous change. We love Docs as Code.
 A product is never ready. Shaping and updating an already done flow is perfectly fine.
 Iteration and prioritization are your best friends when it comes to delivering large amounts of work.
 
 
 We hope that this case study helps and encourages you as you build an onboarding experience for your product.  </content>
     </entry>
     <entry>
       <title>Warner Bros. Discovery</title>
         <link href="https://stratechery.com/2022/warner-bros-discovery/"/>
       <updated>2022-05-23T11:44:25.000Z</updated>
       <content type="text">Last week the Wall Street Journal ran a profile of longtime Discovery CEO (now Warner Bros. Discovery CEO) David Zaslav entitled There’s a New Media Mogul Tearing Up Hollywood, adding “Zas Is Not Particularly Patient”. After opening with an anecdote about Zaslav complaining about Warner Bros. backing a Clint Eastwood movie, even though they thought it would fail, the profile states:
 
   Mr. Zaslav, who last month took over the company resulting from Discovery’s merger with AT&amp;T Inc.’s WarnerMedia, has given every indication he wants to be a talent-friendly mogul, schmoozing with industry personalities at the Beverly Hills Hotel. But the 62-year-old cable-industry veteran, a protégé of the late Jack Welch, longtime CEO of General Electric Co., has shown he isn’t afraid to ruffle the industry’s elite.
   He and his team have been scouring the company’s books, making it clear spending needs to be reined in. They have abandoned projects they consider costly and unnecessary. That included pulling the plug on CNN+, barely a month after previous management launched the streaming service, and canceling a DC Comics superhero movie in development. He has given an unwelcome jolt to executives in the WarnerMedia empire who were happy when AT&amp;T decided to part with it in the merger, hoping there would be less financial scrutiny—not more…
   Mr. Zaslav has few options other than drastic moves. The deal brought the new company—now home of Warner Bros. and cable channels including HBO, CNN, TNT, Food Network and HGTV—$55 billion in debt, and he has promised to cut at least $3 billion in costs. He has given executives a few weeks to provide restructuring and business plans. “We are not trying to win the direct-to-consumer spending war,” Mr. Zaslav said on an April earnings call. On the call, Chief Financial Officer Gunnar Wiedenfels called out the nearly $30 billion the company spends making and marketing content, saying: “We intend to drive the highest level of financial discipline here.”
 
 The profile continued in the same vein, and came across as fairly negative; that is why I appreciated it, because I am otherwise extremely bullish about the potential for Zaslav’s new company.
 HBO and Discovery Synergies
 I wrote a year ago when the deal between AT&amp;T and Discovery was announced that Warner Bros. and Discovery had excellent synergies:
 
   Start with the cable bundle: yes, cord-cutting continues, but there are still a lot of households with cable, and this new company will have significantly enhanced bargaining power with distributors. WarnerMedia’s combination of live sports, news, premium television, and scripted shows was already quite strong; Discovery brings a highly differentiated set of channels from HGTV to Discovery to Food Network that not only attract distinct demographics, but also are particularly effective at driving advertising.
   Another set of synergies come in the two companies burgeoning direct-to-consumer offerings. Once again the breadth of content is a good thing: HBO Max and Discovery+ have something for everyone in the household. The types of content are complementary as well; back in 2018 I explained in the context of Friends:
 
     While most of the Netflix attention is paid to original series, the truth is that there are two types of shows on the streaming network:
 
 Original series drive new subscribers
 “Filler”, that is, content that is there when subscribers simply want something to watch, keeps people subscribed
 
 
   Discovery content is excellent filler [while HBO excels at original series].
 
 Zaslav and CFO Gunnar Wiedenfels made all of these points on Warner Bros. Discovery’s inaugural earnings call in late April. Start with the second point above, about streaming synergy. Zaslav, in response to a question about advertising (more on this in a moment), brought up the fact that Discovery+ has very low churn:
 
   We’re in the market already with an ad-light product. We’re the ones that were out there very early saying ad-light looks really compelling, because it’s a great consumer proposition. Our users, the churn was very low; we were doing between two and four minutes of advertising and generating $5, $6 in incremental revenue, and as it scaled, we started to make more. And so, we said very early on, we’re going to switch to offer consumers what they want, a lower-priced opportunity with a small number of advertising.
 
 HBO Max isn’t doing so well in this regard, at least according to Zaslav a few minutes later:
 
   We have some work to do on the platform itself that will be significant. But we also think that one of the big opportunities here is going to be churn reduction. There is meaningful churn on HBO Max, much higher than the churn that we have seen. And so, the ability for us to come together is part of one of the thesis here that managing churn, and we’ve seen this because we’ve been added in Europe for eight years, as we begin to manage churn in a meaningful way, that provides a real meaningful growth.
 
 The benefit of coming together is exactly what I noted a couple of years ago: hits may capture customers, but filler content — especially a wide range of it — keeps them:
 
   What you need is a diversity of content for everybody in the home, and they may come in for Euphoria [from HBO], but our research shows that people watch Euphoria, their favorite second show to watch is 90 Day Fiance [from TLC]. Having a diversity of content is a reason why people are spending hours with Discovery+…when you put all of this diversity of content together, there is content for kids, there is content for teens, it’s basically everybody in the family, why would you go anywhere else. We have all the movies, we have all of the library content that you want…
   If you look at HBO right now, what it really needs is precisely what we have. When they are finished with watching Winning Time, they can go and watch Friends or watch Big Bang or watch their favorite movie or go over and watch Oprah or watch some TLC shows just for fun. So, we believe and we see this in Europe where we tried to offer, we thought that the answer was just to offer niche high quality that you get high-quality shock and all content together with a lot of nutrition, in our case in Europe, together with sport and you offer something that everybody in the family uses, and the churn goes way down, it’s much harder to churn out of a product when your kids use it or your significant other uses it or your mom and dad are watching, but also if you find yourself watching it more often. So, I think it’s precisely why we did this deal. And I think everything tells us that it’s going to make us stronger and more compelling because of the breadth of the quality menu of IP that we have.
 
 In short, HBO Max plus Discovery+ is a bundle, with all of the attendant advantages that entails (and which certainly did not apply to a standalone CNN+ service). Of course this also strengthens Warner Bros. Discovery’s hand in terms of the linear TV bundle as well; Zaslav said in his prepared remarks:
 
   One of the company’s unique assets is the linear network group, and in 2021 taken together, we enjoyed the number one share in total television total day in all key demos and people 2+. And we have the greatest brands: HG, Food, HBO, Discovery, CNN, NBA, March Madness, NHL, Magnolia, The Oprah Winfrey Network. Our balanced verticals and content genres across scripted, lifestyle, sports and news provide us with significant opportunities to not only cross-promote for the benefit of the portfolio, but also to offer compelling reach and targeting campaigns for our advertising partners.
 
 Don’t forget negotiating leverage; Wiedenfels noted:
 
   US distribution revenues were up 11% year-over-year, largely driven by the growth of Discovery+ subscribers throughout 2021, while linear affiliate revenues were also up year-over-year as rate increases continued to outpace subscriber declines. Our fully distributed subscribers were down 4% as were total portfolio subscribers when correcting for the impact of the sale of our Great American Country network in early June last year.
 
 Yes, cords are still being cut, especially last year, but the story of cable for the last several years has been the jettisoning of cost-driven subscribers in favor of charging full price for people who actually want linear TV, which, in the long run, means that the linear TV bundle is primarily the sports and news bundle. Warner Bros. is well-placed for this new world, thanks to its combination of sports rights on TNT and TBS, along with CNN. That, though, isn’t a guarantee of profitability; Zaslav noted in an answer explaining why news was more profitable than sports:
 
   When it comes to sports, we’re very careful about sports. And the TNT and Warner team was clever about getting long term rights which we’re going to get a lot of benefit from, but sports are rented and news is scalable.
 
 The unscripted content that Discovery specializes in is even more scalable, and far cheaper; now, instead of negotiating with cable providers like Comcast for a collection of channels that customers like, but don’t necessarily need (particularly since Discovery+ is an option), Warner Bros. Discovery will be negotiating for a bundle that includes sports and news and filler. Those sports rights may eat up a lot of TNT and TBS’s carriage fees; the real money will be made on the extra pennies added to the rest of the channels in the portfolio.
 And then, of course, that money can be spent on streaming: sure, Netflix has the advantage of having a larger customer base, but no one — other than Netflix executives until a month ago — said that you could only use subscription dollars and nothing else to acquire streaming content.
 Advertising’s Continued Strength
 Then there is advertising. First — and in contrast to Netflix’s agonizing on the matter — I enjoyed how matter-of-fact Zaslav was about having ad-supported streaming tiers:
 
   In streaming, we have a massive opportunity to reach the widest possible addressable market by offering a range of tiers, all with the most compelling and complete portfolio of content. A premium and attractively priced ad-free direct-to-consumer product, a lower-priced ad-light tier, something we have had tremendous success with and is our highest ARPU product, and in some very price-sensitive markets outside the United States, we can even offer an advertiser-only product.
 
 Secondly, Warner Bros. Discovery can provide a one-stop shop for advertisers across streaming and TV; Zaslav said in his opening remarks:
 
   The combined strengths of both organizations’ client relationships, advanced advertising, programmatic, sponsorships and direct-to-consumer, ad-light streaming services, all positioned the company with a unique hand. I’ve personally spent quite a bit of time with key advertisers and agencies, and I’m so impressed with the combined capability of our platforms and our ability to uniquely serve the needs of our clients, including integrating sports alongside our broad entertainment offerings.
 
 The bit about integrating sports refers to the fact that Warner Bros. actually had multiple advertising teams (one for sports and one for everything else — but none for streaming); I’m not entirely clear how the company ended up in this position, but given the fact the company’s businesses were built in an era where ad agencies sat in the middle between advertisers on one side and inventory on the other, it makes a certain sort of sense. Today, though, the goal is to be as large and as integrated as possible, the better to share data and provide effective targeting at scale; that offering is particularly compelling given that streaming is the best place to reach all of those people that cut the cord.
 Still, even with the cord-cutting, TV advertising continues to be a good business; Zaslav reflected:
 
   We recognize that 4% of subscribers are down and viewership on the platform is down…Long-term, there’s no question that the business is challenging, but CPMs are increasing, advertisers still are looking for inventory, because it’s the most effective inventory in long-form video. And look, remember, broadcast for a period of 20 years was declining and CPMs were increasing. I was at NBC in the mid-’90s when Welsh was saying this can’t continue. We can’t have smaller and smaller audiences and make more and more money. And I think he was right or maybe he will be right eventually, but it’s almost 30 years later and the advertisers are still paying more than the hurdle rate of decline.
   So, we will be leaning in with efficiencies and effectiveness to our traditional business, which generates an awful lot of free cash flow…We now have the same or in many cases, the largest reach on television in the US. And the ability to use our own inventory to promote to and from all of our products and the efficiency of doing that and the cost savings of doing it, I think is a big plus for us.
 
 There are a few things going on here, and both go back to an Article I wrote in 2016 called TV Advertising’s Surprising Strength — and Inevitable Fall. The key insight in that piece was that huge swathes of the economy, from large CPG companies to big box retailed to auto makers, were all built around TV advertising, which meant they would prop up the medium far longer than people thought:
 
   Brands uniquely suited to TV are probably by definition less suited to digital advertising, which at least to date has worked much better for direct response marketing. No one is going to click a link in their feed to buy a car or laundry detergent, and a brick-and-mortar retailer doesn’t want to encourage shopping to someone already online. So after a bit of experimentation, they’re back with TV.
   Still, I think Facebook and Snapchat in particular will figure brand advertising out: both have incredibly immersive advertising formats, and both are investing in ways to bring direct response-style tracking to brand advertising, including tracking visits to brick-and-mortar retailers.
 
 Six years on, and “Surprising Strength” remains correct, while “Inevitable Fall” is the prediction that is looking shakier: to the extent that brand advertising is going digital, a lot of the shift seems to be doing so primarily as streaming video ads, a shift that will only accelerate as Netflix, HBO Max, and Disney all launch ad products. The most privacy-invasive practices of Facebook et al, meanwhile, have been curtailed (and rightly so — products that cross over into real-world tracking are where I have always drawn a hard line).
 Just as important, though, are the impact of changes like ATT on the small and medium-sized businesses that were threatening the biggest advertisers like a hundred duck-sized horses: by destroying their ability to effectively coordinate their advertising spend, ATT and similar regulations have breathed new life into the old ecosystem, which ultimately plays to the benefit of the largest advertising sellers, including Warner Bros. Discovery.
 Reasons for Optimism
 Like I said, I’m pretty optimistic about Warner Bros. Discovery, which is why I appreciated the Wall Street Journal article I opened with: it’s fair to wonder if the exact sort of clarity of thinking and explicit commitment to financial results that Zaslav demonstrated on that earnings call will translate into managing talent and navigating Hollywood, particularly given the huge debt load the new company is carrying.
 That noted, the other reason to be bullish is that Warner Bros. Discovery’s strategy is, in contrast to Netflix, back to the future; Zaslav said at the beginning of the call:
 
   These last few months in our industry have been an important reminder that while technology will continue to empower consumers of video entertainment, the recipe for long-term success is still made up of a few key ingredients. Number one, world-class IP content that is loved all over the globe; two, distribution of that content on every platform and device where consumers want to engage, whether it’s theatrical or linear or streaming; three, a balanced monetization model that optimizes the value of what we create and drives diversified revenue streams; and four, finally, durable and sustainable free cash flow generation.
 
 This isn’t anything new: the Hollywood model has always been about creating compelling content once and then monetizing it through every possible means; in Zaslav’s view a self-owned streaming platform is just an addition to the old model, not a wholesale replacement.
 Moreover, things like movies in theaters still have their advantages: they build IP and build awareness for the other content models, above and beyond the money they make. Oh, and to take this update full circle, they also make talent like Eastwood happy. To that end, I suspect in the long run that flexibility and pragmatism in content distribution, combined with real discipline about cash flow, will prove to be more compelling than the same combination in reverse.</content>
     </entry>
     <entry>
       <title>Lesser-Known And Underused CSS Features In 2022</title>
         <link href="https://smashingmagazine.com/2022/05/lesser-known-underused-css-features-2022/"/>
       <updated>2022-05-23T09:30:00.000Z</updated>
       <content type="text">After reading Louis Lazaris’ insightful article “Those HTML Attributes You Never Use”, I’ve asked myself (and the community) which properties and selectors are lesser-known or should be used more often. Some answers from the community surprised me, as they’ve included some very useful and often-requested CSS features which were made available in the past year or two.
 The following list is created with community requests and my personal picks. So, let’s get started!
 all Property
 This is a shorthand property which is often used for resetting all properties to their respective initial value by effectively stopping inheritance, or to enforce inheritance for all properties.
 
 initialSets all properties to their respective initial values.
 inheritSets all properties to their inherited values.
 unsetChanges all values to their respective default value which is either inherit or initial.
 revertResulting values depend on the stylesheet origin where this property is located.
 revert-layerResulting values will match a previous cascade layer or the next matching rule.
 
 
 aspect-ratio for Sizing Control
 When aspect-ratio was initially released, I thought I won’t use it outside image and video elements and in very narrow use-cases. I was surprised to find myself using it in a similar way I would use currentColor — to avoid unnecessarily setting multiple properties with the same value.
 With aspect-ratio, we can easily control size of an element. For example, equal width and height buttons will have an aspect ratio of 1. That way, we can easily create buttons that adapt to their content and varying icon sizes, while maintaining the required shape.
 
 I assumed that this issue cannot be fixed, and I moved on. One of the tweets from the community poll suggested that I should look into font-variant-numeric: tabular-nums, and I was surprised to find a plethora of options that affect font rendering. 
 For example, tabular-nums fixed the aforementioned issue by setting the equal width for all numeric characters.
 
 Render Performance Optimization
 When it comes to rendering performance, it’s very rare to run into these issues when working on regular projects. However, in the case of large DOM trees with several thousands of elements or other similar edge cases, we can run into some performance issues related to CSS and rendering. Luckily, we have a direct way of dealing with these performance issues that cause lag, unresponsiveness to user inputs, low FPS, etc.
 This is where contain property comes in. It tells the browser what won’t change in the render cycle, so the browser can safely skip it. This can have consequences on the layout and style, so make sure to test if this property doesn’t introduce any visual bugs.
 
 .container {
   /* child elements won&#x27;t display outside of this container so only the contents of this container should be rendered*/
   contain: paint;
 {
 
 
 This property is quite complex, and Rachel Andrew has covered it in great detail in her article. This property is somewhat difficult to demonstrate, as it is most useful in those very specific edge cases. For example, Johan Isaksson covered one of those examples in his article, where he noticed a major scroll lag on Google Search Console. It was caused by having over 38 000 elements on a page and was fixed by containing property!
 As you can see, contain relies on the developer knowing exactly which properties won’t change and knowing how to avoid potential regressions. So, it’s a bit difficult to use this property safely. 
 However, there is an option where we can signal the browser to apply the required contain value automatically. We can use the content-visibility property. With this property, we can defer the rendering of off-screen and below-the-fold content. Some even refer to this as “lazy-rendering”.
 Una Kravets and Vladimir Levin covered this property in their travel blog example. They apply the following class name to the below-the-fold blog sections.
 
 .story {
   content-visibility: auto; /* Behaves like overflow: hidden; */
   contain-intrinsic-size: 100px 1000px;
 }
 
 
 With contain-intrinsic-size, we can estimate the size of the section that is going to be rendered. Without this property, the size of the content would be 0, and page dimensions would keep increasing, as content is loaded.
 Going back to Una Kravets and Vladimir Levin’s travel blog example. Notice how the scrollbar jumps around, as you scroll or drag it. This is because of the difference between the placeholder (estimated) size set with contain-intrinsic-size and the actual render size. If we omit this property, the scroll jumps would be even more jarring.
 See the Pen Content-visibility Demo: Base (With Content Visibility) by Vladimir Levin.
 Thijs Terluin covers several ways of calculating this value including PHP and JavaScript. Server-side calculation using PHP is especially impressive, as it can automate the value estimation on larger set of various pages and make it more accurate for a subset of screen sizes.
 Keep in mind that these properties should be used to fix issues once they happen, so it’s safe to omit them until you encounter render performance issues.
 Conclusion
 CSS evolves constantly, with more features being added each year. It’s important to keep up with the latest features and best practices, but also keep an eye out on browser support and use progressive enhancement.
 I’m sure there are more CSS properties and selectors that aren’t included here. Feel free to let us know in the comments which properties or selectors are less known or should be used more often, but may be a bit convoluted or there is not enough buzz around them.
 Further Reading on Smashing Magazine
 
 CSS Custom Properties In The Cascade
 Simplifying Form Styles With accent-color
 Understanding CSS Grid: Creating A Grid Container
 HTML5 SVG Fill Animation With CSS3 And Vanilla JavaScript
 </content>
     </entry>
     <entry>
       <title>The Ultimate Free Solo Blog Setup With Ghost And Gatsby</title>
         <link href="https://smashingmagazine.com/2022/05/ultimate-free-solo-blog-setup-ghost-gatsby/"/>
       <updated>2022-05-20T08:30:00.000Z</updated>
       <content type="text">These days it seems there are an endless number of tools and platforms for creating your own blog. However, lots of the options out there lean towards non-technical users and abstract away all of the options for customization and truly making something your own.
 If you are someone who knows their way around front-end development, it can be frustrating to find a solution that gives you the control you want, while removing the admin from managing your blog content.
 Enter the Headless Content Management System (CMS). With a Headless CMS, you can get all of the tools to create and organize your content, while maintaining 100% control of how it is delivered to your readers. In other words, you get all of the backend structure of a CMS while not being limited to its rigid front-end themes and templates.
 When it comes to Headless CMS systems, I’m a big fan of Ghost. Ghost is open-source and simple to use, with lots of great APIs that make it flexible to use with static site builders like Gatsby.
 In this article, I will show you how you can use Ghost and Gatsby together to get the ultimate personal blog setup that lets you keep full control of your front-end delivery, but leaves all the boring content management to Ghost.
 Oh, and it’s 100% free to set up and run. That’s because we will be running our Ghost instance locally and then deploying to Netlify, taking advantage of their generous free tier.
 Let’s dive in!
 Setting Up Ghost And Gatsby
 I’ve written a starter post on this before that covers the very basics, so I won’t go too in-depth into them here. Instead, I will focus on the more advanced issues and gotchas that come up when running a headless blog.
 But in short, here’s what we need to do to get a basic set-up up and running that we can work from:
 
 Install a local version of the Gatsby Starter Blog
 Install Ghost locally
 Change the source data from Markdown to Ghost (swap out gatsby-source-file system for gatsby-source-ghost)
 Modify the GraphQL queries in your gatsby-node, templates, and pages to match the gatsby-source-ghost schema
 
 For more details on any of these steps, you can check out my previous article.
 Or you can just start from the code in this Github repository.
 Dealing With Images
 With the basics out of the way, the first issue we run into with a headless blog that builds locally is what to do with images.
 Ghost by default serves images from its own server. So when you go headless with a static site, you will run into a situation where your content is built and served from an edge provider like Netlify, but your images are still being served by your Ghost server.
 This isn’t ideal from a performance perspective and it makes it impossible to build and deploy your site locally (which means you would have to pay monthly fees for a Digital Ocean droplet, AWS EC2 instance, or some other server to host your Ghost instance).
 But we can get around that if we can find another solution to host our images &amp;mdash, and thankfully, Ghost has storage converters that enable you to store images in the cloud.
 For our purposes, we are going to use an AWS S3 converter, which enables us to host our images on AWS S3 along with Cloudfront to give us a similar performance to the rest of our content.
 There are two open-source options available: ghost-storage-adapter-s3 and ghost-s3-compat. I use ghost-storage-adapter-s3 since I find the docs easier to follow and it was more recently updated.
 That being said, if I followed the docs exactly, I got some AWS errors, so here’s the process that I followed that worked for me:
 
 Create a new S3 Bucket in AWS and select Disable Static Hosting
 Next, create a new Cloudfront Distribution and select the S3 Bucket as the Origin
 When configuring the Cloudfront Distribution, under S3 Bucket Access:
 
 Select “Yes, use OAI (bucket can restrict access to only Cloudfront)”
 Create a New OAI
 And finally, select “Yes, update the bucket policy”
 
 This creates an AWS S3 Bucket that can only be accessed via the Cloudfront Distribution that you have created.
 
 
 
 
 Then, you just need to create an IAM User for Ghost that will enable it to write new images to your new S3 Bucket. To do this, create a new Programmatic IAM User and attach this policy to it:
 {
     &quot;Version&quot;: &quot;2012-10-17&quot;,
     &quot;Statement&quot;: [
         {
             &quot;Sid&quot;: &quot;VisualEditor0&quot;,
             &quot;Effect&quot;: &quot;Allow&quot;,
             &quot;Action&quot;: &quot;s3:ListBucket&quot;,
             &quot;Resource&quot;: &quot;arn:aws:s3:::YOUR-S3-BUCKET-NAME&quot;
         },
         {
             &quot;Sid&quot;: &quot;VisualEditor1&quot;,
             &quot;Effect&quot;: &quot;Allow&quot;,
             &quot;Action&quot;: [
                 &quot;s3:PutObject&quot;,
                 &quot;s3:GetObject&quot;,
                 &quot;s3:PutObjectVersionAcl&quot;,
                 &quot;s3:DeleteObject&quot;,
                 &quot;s3:PutObjectAcl&quot;
             ],
             &quot;Resource&quot;: &quot;arn:aws:s3:::YOUR-S3-BUCKET-NAME/*&quot;
         }
     ]
 } 
 
 
 With that, our AWS setup is complete, we just need to tell Ghost to read and write our images there instead of to its local server.
 To do that, we need to go to the folder where our Ghost instance is installed and open the file: ghost.development.json orghost.production.json.(depending on what environment you’re currently running)
 Then we just need to add the following:
 {
   &quot;storage&quot;: {
   &quot;active&quot;: &quot;s3&quot;,
   &quot;s3&quot;: {
     &quot;accessKeyId&quot;: &quot;[key]&quot;,
     &quot;secretAccessKey&quot;: &quot;[secret]&quot;,
     &quot;region&quot;: &quot;[region]&quot;,
     &quot;bucket&quot;: &quot;[bucket]&quot;,
     &quot;assetHost&quot;: &quot;https://[subdomain].example.com&quot;, // cloudfront
     &quot;forcePathStyle&quot;: true,
     &quot;acl&quot;: &quot;private&quot;
   }
 }
 
 
 The values for accessKeyId and secretAccessKey can be found from your IAM setup, while the region and bucket refer to the region and bucket name of your S3 bucket. Finally, the assetHost is the URL of your Cloudfront distribution.
 Now, if you restart your Ghost instance, you will see that any new images you save are in your S3 bucket and Ghost knows to link to them there. (Note: Ghost won’t make updates retroactively, so be sure to do this first thing after a fresh Ghost install so you don’t have to re-upload images later)
 Handling Internal Links
 With Images out of the way, the next tricky thing we need to think about is internal links. As you are writing content in Ghost and inserting links in Posts and Pages, Ghost will automatically add the site’s URL to all internal links.
 So for example, if you put a link in your blog post that goes to /my-post/, Ghost is going to create a link that goes to https://mysite.com/my-post/.
 Normally, this isn’t a big deal, but for Headless blogs this causes problems. This is because your Ghost instance will be hosted somewhere separate from your front-end and in our case it won’t even be reachable online since we will be building locally.
 This means that we will need to go through each blog post and page to correct any internal links. Thankfully, this isn’t as hard as it sounds.
 First, we will add this HTML parsing script in a new file called replaceLinks.js and put it in a new utils folder at src/utils:
 const url &#x3D; require(&#x60;url&#x60;);
 const cheerio &#x3D; require(&#x27;cheerio&#x27;);
 
 const replaceLinks &#x3D; async (htmlInput, siteUrlString) &#x3D;&gt; {
   const siteUrl &#x3D; url.parse(siteUrlString);
   const $ &#x3D; cheerio.load(htmlInput);
   const links &#x3D; $(&#x27;a&#x27;);
   links.attr(&#x27;href&#x27;, function(i, href){
     if (href) {
       const hrefUrl &#x3D; url.parse(href);
       if (hrefUrl.protocol &#x3D;&#x3D;&#x3D; siteUrl.protocol &amp;&amp; hrefUrl.host &#x3D;&#x3D;&#x3D; siteUrl.host) {
         return hrefUrl.path
       }
 
       return href;
     }
 
   });
   return $.html();
 }
 
 module.exports &#x3D; replaceLinks;
 
 
 Then we will add the following to our gatsby-node.js file:
 exports.onCreateNode &#x3D; async ({ actions, node, getNodesByType }) &#x3D;&gt; {
   if (node.internal.owner !&#x3D;&#x3D; &#x60;gatsby-source-ghost&#x60;) {
     return
   }
   if (node.internal.type &#x3D;&#x3D;&#x3D; &#x27;GhostPage&#x27; || node.internal.type &#x3D;&#x3D;&#x3D; &#x27;GhostPost&#x27;) {
     const settings &#x3D; getNodesByType(&#x60;GhostSettings&#x60;);
     actions.createNodeField({
       name: &#x27;html&#x27;,
       value: replaceLinks(node.html, settings[0].url),
       node
     })
   }
 }
 
 
 You will see that we are adding two new packages in replaceLinks.js, so let’s start by installing those with NPM:
 npm install --save url cheerio
 
 
 In our gatsby-node.js file, we are hooking into Gatsby’s onCreateNode, and specifically into any nodes that are created from data that comes from gatsby-source-ghost (as opposed to metadata that comes from our config file that we don’t care about for now).
 Then we are checking the node type, to filter out any nodes that are not Ghost Pages or Posts (since these are the only ones that will have links inside their content).
 Next, we are getting the URL of the Ghost site from the Ghost settings and passing that to our removeLinks function along with the HTML content from the Page/Post.
 In replaceLinks, we are using cheerio to parse the HTML. Then we can then select all of the links in this HTML content and map through their href attributes. We can then check if the href attribute matches the URL of the Ghost Site — if it does, we will replace the href attribute with just the URL path, which is the internal link that we are looking for (e.g. something like /my-post/).
 Finally, we are making this new HTML content available through GraphQL using Gatsby’s createNodeField (Note: we must do it this way since Gatsby does not allow you to overwrite fields at this phase in the build).
 Now our new HTML content will be available in our blog-post.js template and we can access it by changing our GraphQL query to:
 ghostPost(slug: { eq: $slug }) {
   id
   title
   slug
   excerpt
   published_at_pretty: published_at(formatString: &quot;DD MMMM, YYYY&quot;)
   html
   meta_title
   fields {
   html
   } 
 }
 
 
 And with that, we just need to tweak this section in the template:
 &lt;section
   dangerouslySetInnerHTML&#x3D;{{ __html: post.html }}
   itemProp&#x3D;&quot;articleBody&quot;
 /&gt;
 
 
 To be:
 &lt;section
  dangerouslySetInnerHTML&#x3D;{{ __html: post.fields.html }}
   itemProp&#x3D;&quot;articleBody&quot;
 /&gt;
 
 
 This makes all of our internal links reachable, but we still have one more problem. All of these links are &lt;a&gt;anchor tags while with Gatsby we should be using Gatsby Link for internal links (to avoid page refreshes and to provide a more seamless experience).
 Thankfully, there is a Gatsby plugin that makes this really easy to solve. It’s called gatsby-plugin-catch-links and it looks for any internal links and automatically replaces the &lt;a&gt; anchor tags with Gatsby &lt;Link&gt;.
 All we need to do is install it using NPM:
 npm install --save gatsby-plugin-catch-links
 
 
 And add gatsby-plugin-catch-links into our plugins array in our gatsby-config file.
 Adding Templates And Styles
 Now the big stuff is technically working, but we are missing out on some of the content from our Ghost instance.
 The Gatsby Starter Blog only has an Index page and a template for Blog Posts, while Ghost by default has Posts, Pages, as well as pages for Tags and Authors. So we need to create templates for each of these.
 For this, we can leverage the Gatsby starter that was created by the Ghost team.
 As a starting point for this project, we can just copy and paste a lot of the files directly into our project. Here’s what we will take:
 
 The entire folder src/components/common/meta — we will copy this into our src/components folder (so we will now have a folder src/components/meta)
 The component files Pagination.js and PostCard.js — we will copy these into our src/components folder
 We will create a src/utils folder and add two files from their src/utils folder: fragments.js and siteConfig.js
 And the following templates from their src/templates folder: tag.js, page.js, author.js, and post.js
 
 The meta files are adding JSON structured data markup to our templates. This is a great benefit that Ghost offers by default on their platform and they’ve transposed it into Gatsby as part of their starter template.
 Then we took the Pagination and PostCard.js components that we can drop right into our project. And with those components, we can take the template files and drop them into our project and they will work.
 The fragments.js file makes our GraphQL queries a lot cleaner for each of our pages and templates — we now just have a central source for all of our GraphQL queries. And the siteConfig.js file has a few Ghost configuration options that are easiest to put in a separate file.
 Now we will just need to install a few npm packages and update our gatsby-node file to use our new templates.
 The packages that we will need to install are gatsby-awesome-pagination, @tryghost/helpers, and @tryghost/helpers-gatsby.
 So we will do:
 npm install --save gatsby-awesome-pagination @tryghost/helpers @tryghost/helpers-gatsby
 
 
 Then we need to make some updates to our gatsby-node file.
 First, we will add the following new imports to the top of our file:
 const { paginate } &#x3D; require(&#x60;gatsby-awesome-pagination&#x60;);
 const { postsPerPage } &#x3D; require(&#x60;./src/utils/siteConfig&#x60;);
 
 
 Next, in our exports.createPages, we will update our GraphQL query to:
 {
   allGhostPost(sort: { order: ASC, fields: published_at }) {
       edges {
           node {
               slug
           }
       }
   }
   allGhostTag(sort: { order: ASC, fields: name }) {
       edges {
           node {
               slug
               url
               postCount
           }
       }
   }
   allGhostAuthor(sort: { order: ASC, fields: name }) {
       edges {
           node {
               slug
               url
               postCount
           }
       }
   }
   allGhostPage(sort: { order: ASC, fields: published_at }) {
       edges {
           node {
               slug
               url
           }
       }
   }
 }
 
 
 This will pull all of the GraphQL data we need for Gatsby to build pages based on our new templates.
 To do that, we will extract all of those queries and assign them to variables:
 // Extract query results
   const tags &#x3D; result.data.allGhostTag.edges
   const authors &#x3D; result.data.allGhostAuthor.edges
   const pages &#x3D; result.data.allGhostPage.edges
   const posts &#x3D; result.data.allGhostPost.edges
 
 
 Then we will load all of our templates:
 // Load templates
   const tagsTemplate &#x3D; path.resolve(&#x60;./src/templates/tag.js&#x60;)
   const authorTemplate &#x3D; path.resolve(&#x60;./src/templates/author.js&#x60;)
   const pageTemplate &#x3D; path.resolve(&#x60;./src/templates/page.js&#x60;)
   const postTemplate &#x3D; path.resolve(&#x60;./src/templates/post.js&#x60;)
 
 
 Note here that we are replacing our old blog-post.js template with post.js, so we can go ahead and delete blog-post.js from our templates folder.
 Finally, we will add this code to build pages from our templates and GraphQL data:
 // Create tag pages
 tags.forEach(({ node }) &#x3D;&gt; {
     const totalPosts &#x3D; node.postCount !&#x3D;&#x3D; null ? node.postCount : 0
 
     // This part here defines, that our tag pages will use
     // a &#x60;/tag/:slug/&#x60; permalink.
     const url &#x3D; &#x60;/tag/${node.slug}&#x60;
 
     const items &#x3D; Array.from({length: totalPosts})
 
     // Create pagination
     paginate({
         createPage,
         items: items,
         itemsPerPage: postsPerPage,
         component: tagsTemplate,
         pathPrefix: ({ pageNumber }) &#x3D;&gt; (pageNumber &#x3D;&#x3D;&#x3D; 0) ? url : &#x60;${url}/page&#x60;,
         context: {
             slug: node.slug
         }
     })
 })
 
 // Create author pages
 authors.forEach(({ node }) &#x3D;&gt; {
     const totalPosts &#x3D; node.postCount !&#x3D;&#x3D; null ? node.postCount : 0
 
     // This part here defines, that our author pages will use
     // a &#x60;/author/:slug/&#x60; permalink.
     const url &#x3D; &#x60;/author/${node.slug}&#x60;
 
     const items &#x3D; Array.from({length: totalPosts})
 
     // Create pagination
     paginate({
         createPage,
         items: items,
         itemsPerPage: postsPerPage,
         component: authorTemplate,
         pathPrefix: ({ pageNumber }) &#x3D;&gt; (pageNumber &#x3D;&#x3D;&#x3D; 0) ? url : &#x60;${url}/page&#x60;,
         context: {
             slug: node.slug
         }
     })
 })
 
 // Create pages
 pages.forEach(({ node }) &#x3D;&gt; {
   // This part here defines, that our pages will use
   // a &#x60;/:slug/&#x60; permalink.
   node.url &#x3D; &#x60;/${node.slug}/&#x60;
 
   createPage({
       path: node.url,
       component: pageTemplate,
       context: {
           // Data passed to context is available
           // in page queries as GraphQL variables.
           slug: node.slug,
       },
   })
 })
 
 // Create post pages
 posts.forEach(({ node }) &#x3D;&gt; {
     // This part here defines, that our posts will use
     // a &#x60;/:slug/&#x60; permalink.
     node.url &#x3D; &#x60;/${node.slug}/&#x60;
     createPage({
         path: node.url,
         component: postTemplate,
         context: {
             // Data passed to context is available
             // in page queries as GraphQL variables.
             slug: node.slug,
         },
     })
 })
 
 
 Here, we are looping in turn through our tags, authors, pages, and posts. For our pages and posts, we are simply creating slugs and then creating a new page using that slug and telling Gatsby what template to use.
 For the tags and author pages, we are also adding pagination info using gatsby-awesome-pagination that will be passed into the page’s pageContext.
 With that, all of our content should now be successfully built and displayed. But we could use a bit of work on styling. Since we copied over our templates directly from the Ghost Starter, we can use their styles as well.
 Not all of these will be applicable, but to keep things simple and not get too bogged down in styling, I took all of the styles from Ghost’s src/styles/app.css starting from the section Layout until the end. Then you will just paste these into the end of your src/styles.css file.
 Observe all of the styles starting with kg — this refers to Koening which is the name of the Ghost editor. These styles are very important for the Post and Page templates, as they have specific styles that handle the content that is created in the Ghost editor. These styles ensure that all of the content you are writing in your editor is translated over and displayed on your blog correctly.
 Lastly, we need our page.js and post.js files to accommodate our internal link replacement from the previous step, starting with the queries:
 Page.js
 ghostPage(slug: { eq: $slug } ) {
   ...GhostPageFields
     fields {
       html
      }
 }
 
 
 
 Post.js
 ghostPost(slug: { eq: $slug } ) {
   ...GhostPostFields
     fields {
       html
     }
 }
 
 
 And then the sections of our templates that are using the HTML content. So in our post.js we will change:
 &lt;section
 className&#x3D;&quot;content-body load-external-scripts&quot;
 dangerouslySetInnerHTML&#x3D;{{ __html: post.html }} /&gt;
 
 
 To:
 &lt;section
 className&#x3D;&quot;content-body load-external-scripts&quot;
 dangerouslySetInnerHTML&#x3D;{{ __html: post.fields.html }} /&gt;
 
 
 And similarly, in our page.js file, we will change page.html to page.fields.html.
 Dynamic Page Content
 One of the disadvantages of Ghost when used as a traditional CMS, is that it is not possible to edit individual pieces of content on a page without going into your actual theme files and hard coding it.
 Say you have a section on your site that is a Call-to-Action or customer testimonials. If you want to change the text in these boxes, you will have to edit the actual HTML files.
 One of the great parts of going headless is that we can make dynamic content on our site that we can easily edit using Ghost. We are going to do this by using Pages that we will mark with ‘internal’ tags or tags that start with a # symbol.
 So as an example, let’s go into our Ghost backend, create a new Page called Message, type something as content, and most importantly, we will add the tag #message.
 Now let’s go back to our gatsby-node file. Currently, we are building pages for all of our tags and pages, but if we modify our GraphQL query in createPages, we can exclude everything internal:
 allGhostTag(sort: { order: ASC, fields: name }, **filter: {slug: {regex: &quot;/^((?!hash-).)*$/&quot;}}**) {
     edges {
         node {
             slug
             url
             postCount
         }
     }
 }
 //...
 allGhostPage(sort: { order: ASC, fields: published_at }, **filter: {tags: {elemMatch: {slug: {regex: &quot;/^((?!hash-).)*$/&quot;}}}}**) {
     edges {
         node {
             slug
             url
             html
         }
     }
 }
 
 
 We are adding a filter on tag slugs with the regex expression /^((?!hash-).)*$/. This expression is saying to exclude any tag slugs that include hash-.
 Now, we won’t be creating pages for our internal content, but we can still access it from our other GraphQL queries. So let’s add it to our index.js page by adding this to our query:
 query GhostIndexQuery($limit: Int!, $skip: Int!) {
     site {
       siteMetadata {
         title
       }
     }
     message: ghostPage
       (tags: {elemMatch: {slug: {eq: &quot;hash-message&quot;}}}) {
         fields {
           html
         }
     }
     allGhostPost(
         sort: { order: DESC, fields: [published_at] },
         limit: $limit,
         skip: $skip
     ) {
       edges {
         node {
           ...GhostPostFields
         }
       }
     }
   }
 
 
 Here we are creating a new query called “message” that is looking for our internal content page by filtering specifically on the tag #message. Then let’s use the content from our #message page by adding this to our page:
 //...
 const BlogIndex &#x3D; ({ data, location, pageContext }) &#x3D;&gt; {
   const siteTitle &#x3D; data.site.siteMetadata?.title || &#x60;Title&#x60;
   const posts &#x3D; data.allGhostPost.edges
   const message &#x3D; data.message;
 //...
 return (
   &lt;Layout location&#x3D;{location} title&#x3D;{siteTitle}&gt;
     &lt;Seo title&#x3D;&quot;All posts&quot; /&gt;
     &lt;section
       dangerouslySetInnerHTML&#x3D;{{
         __html: message.fields.html,
       }}
     /&gt;
   )
 }
 
 
 
 
 Finishing Touches
 Now we’ve got a really great blog setup, but we can add a few final touches: pagination on our index page, a sitemap, and RSS feed.
 First, to add pagination, we will need to convert our index.js page into a template. All we need to do is cut and paste our index.js file from our src/pages folder over to our src/templates folder and then add this to the section where we load our templates in gatsby-node.js:
 // Load templates
  const indexTemplate &#x3D; path.resolve(&#x60;./src/templates/index.js&#x60;)
 
 
 Then we need to tell Gatsby to create our index page with our index.js template and tell it to create the pagination context.
 Altogether we will add this code right after where we create our post pages:
 // Create Index page with pagination
   paginate({
       createPage,
       items: posts,
       itemsPerPage: postsPerPage,
       component: indexTemplate,
       pathPrefix: ({ pageNumber }) &#x3D;&gt; {
           if (pageNumber &#x3D;&#x3D;&#x3D; 0) {
             return &#x60;/&#x60;
           } else {
               return &#x60;/page&#x60;
             }
       },
   })
 
 
 Now let’s open up our index.js template and import our Pagination component and add it right underneath where we map through our posts:
 import Pagination from &#x27;../components/pagination&#x27;
 //...
       &lt;/ol&gt;
       &lt;Pagination pageContext&#x3D;{pageContext} /&gt;
     &lt;/Layout&gt;
 //...
 
 
 Then we just need to change the link to our blog posts from:
 &lt;Link to&#x3D;{post.node.slug} itemProp&#x3D;&quot;url&quot;&gt;
 
 
 to: 
 &lt;Link to&#x3D;{&#x60;/${post.node.slug}/&#x60;} itemProp&#x3D;&quot;url&quot;&gt;
 
 
 This prevents Gatsby Link from prefixing our links on pagination pages — in other words, if we didn’t do this, a link on page 2 would show as /page/2/my-post/ instead of just /my-post/ like we want.
 With that done, let’s set up our RSS feed. This is a pretty simple step, as we can use a ready-made script from the Ghost team’s Gatsby starter. Let’s copy their file generate-feed.js into our src/utils folder.
 Then let’s use it in our gatsby-config.js by replacing the existing gatsby-plugin-feed section with:
 {
   resolve: &#x60;gatsby-plugin-feed&#x60;,
   options: {
       query: &#x60;
       {
           allGhostSettings {
               edges {
                   node {
                       title
                       description
                   }
               }
           }
       }
     &#x60;,
       feeds: [
           generateRSSFeed(config),
       ],
   },
 }
 
 
 We will need to import our script along with our siteConfig.js file:
 const config &#x3D; require(&#x60;./src/utils/siteConfig&#x60;);
 const generateRSSFeed &#x3D; require(&#x60;./src/utils/generate-feed&#x60;);
 //...
 
 Finally, we need to make one important addition to our generate-feed.js file. Right after the GraphQL query and the output field, we need to add a title field:
 #...
 output: &#x60;/rss.xml&#x60;,
 title: &quot;Gatsby Starter Blog RSS Feed&quot;,
 #...
 
 
 Without this title field, gatsby-plugin-feed will throw an error on the build.
 Then for our last finishing touch, let’s add our sitemap by installing the package gatsby-plugin-advanced-sitemap:
 npm install --save gatsby-plugin-advanced-sitemap
 
 
 And adding it to our gatsby-config.js file:
 {
   resolve: &#x60;gatsby-plugin-advanced-sitemap&#x60;,
   options: {
       query: &#x60;
         {
             allGhostPost {
                 edges {
                     node {
                         id
                         slug
                         updated_at
                         created_at
                         feature_image
                     }
                 }
             }
             allGhostPage {
                 edges {
                     node {
                         id
                         slug
                         updated_at
                         created_at
                         feature_image
                     }
                 }
             }
             allGhostTag {
                 edges {
                     node {
                         id
                         slug
                         feature_image
                     }
                 }
             }
             allGhostAuthor {
                 edges {
                     node {
                         id
                         slug
                         profile_image
                     }
                 }
             }
         }&#x60;,
         mapping: {
             allGhostPost: {
                 sitemap: &#x60;posts&#x60;,
             },
             allGhostTag: {
                 sitemap: &#x60;tags&#x60;,
             },
             allGhostAuthor: {
                 sitemap: &#x60;authors&#x60;,
             },
             allGhostPage: {
                 sitemap: &#x60;pages&#x60;,
             },
         },
         exclude: [
             &#x60;/dev-404-page&#x60;,
             &#x60;/404&#x60;,
             &#x60;/404.html&#x60;,
             &#x60;/offline-plugin-app-shell-fallback&#x60;,
         ],
         createLinkInHead: true,
         addUncaughtPages: true,
     }
 }
 }
 
 
 The query, which also comes from the Ghost team’s Gatsby starter, creates individual sitemaps for our pages and posts as well as our author and tag pages.
 Now, we just have to make one small change to this query to exclude our internal content. Same as we did in the prior step, we need to update these queries to filter out tag slugs that contain ‘hash-’:
 allGhostPage(filter: {tags: {elemMatch: {slug: {regex: &quot;/^((?!hash-).)*$/&quot;}}}}) {
     edges {
         node {
             id
             slug
             updated_at
             created_at
             feature_image
         }
     }
 }
 allGhostTag(filter: {slug: {regex: &quot;/^((?!hash-).)*$/&quot;}}) {
     edges {
         node {
             id
             slug
             feature_image
         }
     }
 }
 
 
 Wrapping Up
 With that, you now have a fully functioning Ghost blog running on Gatsby that you can customize from here. You can create all of your content by running Ghost on your localhost and then when you are ready to deploy, you simply run:
 gatsby build
 
 
 And then you can deploy to Netlify using their command-line tool:
 netlify deploy -p
 
 
 Since your content only lives on your local machine, it is also a good idea to make occasional backups, which you can do using Ghost’s export feature.
 This exports all of your content to a json file. Note, it doesn’t include your images, but these will be saved on the cloud anyway so you don’t need to worry as much about backing these up.
 I hope you enjoyed this tutorial where we covered:
 
 Setting up Ghost and Gatsby;
 Handling Ghost Images using a storage converter;
 Converting Ghost internal links to Gatsby Link;
 Adding templates and styles for all Ghost content types;
 Using dynamic content created in Ghost;
 Setting up RSS feeds, sitemaps, and pagination.
 
 If you are interested in exploring further what’s possible with a headless CMS, check out my work at Epilocal, where I’m using a similar tech stack to build tools for local news and other independent, online publishers.
 Note: You can find the full code for this project on Github here, and you can also see a working demo here.
 Further Reading on Smashing Magazine
 
 “Building Gatsby Themes For WordPress-Powered Websites,” Paulina Hetman  
 “Building An API With Gatsby Functions,” Paul Scanlon
 “Advanced GraphQL Usage In Gatsby Websites,” Aleem Isiaka
 “Gatsby Serverless Functions And The International Space Station,” Paul Scanlon
 </content>
     </entry>
     <entry>
       <title>Kubernetes For Frontend Developers</title>
         <link href="https://smashingmagazine.com/2022/05/kubernetes-front-end-developers/"/>
       <updated>2022-05-19T11:30:00.000Z</updated>
       <content type="text">Kubernetes, also known as k8s, was coined by a Google engineer in mid-2014 and is now widely used throughout the developer ecosystem. According to the Kuberenetes Documentation: 
 “Kubernetes is an open-source platform for managing containerized workloads and services that allow declarative configuration and automation. It enables developers to build containerized applications that can react to critical application needs in the event of a traffic spike or a service failure.” 
 
 Dockers and Containers
 Docker is the most popular container technology tool. It is a tool used for building, running, and deploying containerized applications. An application’s code, libraries, tools, dependencies, and other files are all contained in a Docker image; when a user executes an image, it turns into a container. An image built will always only run one container (an image can have multiple layers and then be used to build multiple images, but an image will always only create one container).
 Docker images can be likened to a template of instructions to build a container. It helps to abstract application code from the underlying infrastructure, simplifying version management and enabling portability across various deployment environments.
 Basically, a containerized application is stateless, i.e. it does not retain session information. And since multiple instances of a container image can run simultaneously, developers use containers as instances that can be initiated to replace failed ones without disruption to the application’s operation.
 In terms of resource management, it is very important to know that the use of resources is highly constrained, unlike VMs, as its access to physical resources (memory, storage, and CPU) is limited by the host OS. As a result, containers are lighter and more scalable than virtual machines.
 “Containers are the foundation of modern application design and development, which means it will be almost impossible for any IT organization to avoid a container commitment.”— Tom Nolle, “Explore Container Deployment Benefits And Core Components”
 
 For a container deployment, there is the server for Container hosting with OS support together with a container management tool like Docker or ContainerD. When there is a need for application capabilities, such as the ability to scale under load and recover from hardware failures without disruption or human intervention, then an orchestration tool would be needed.
 Kubernetes(K8s) is a container orchestration tool that takes over tasks that would otherwise require manual intervention, preventing lots of time-consuming routine checks, changes in configuration, updates, and other software maintenance work. Using Kubernetes deployment would significantly help to automate such repetitive processes and makes loads of manual jobs a breeze when working on a production-ready application. 
 Why Would A Frontend Developer Use Kubernetes?
 For an improved digital experience, Microservices have been successfully implemented by tech giants, such as Netflix, Google, Amazon, and other industry leaders; businesses see this architecture as a cost-effective means of expanding their operations. On the other hand, microservice architecture has gained popularity in recent years due to its effectiveness in developing, deploying, and scaling multiple application backends.
 Adoption of the Microservice architecture in production would be generally considered good practice when an application is simply getting too large for any single developer to fully maintain, or there is an increase in orchestration and interaction between services after every release. It is important to know that not all levels of business organization would be required to use microservices to benefit from using Kubernetes.
 K8s Or Docker Compose: Which One Should I Use?
 Docker-compose is a tool that accepts a YAML file that specifies a cross container application and automates the creation and removal of all those containers without the need to write several docker commands for each one. It should be used for testing and development.
 Kubernetes, on the other hand, is a platform for managing production-ready containerized workloads and services that allows for declarative setup as well as automation.
 Getting Familiar With Terminology
 To utilize Kubernetes efficiently, one must have a reasonable understanding of its terminologies. Here are a few key terminologies to get you started:
 
 DockerA container resource referring to a Docker image and provides all of the necessary configurations that Kubernetes needs (deploy, execute, expose, monitor, and safeguard the Docker container).
 ContainerThe fundamental concept is the Container — since Kubernetes is a container orchestration tool. A container is a standard unit of software that packages up code and all that the application depends on to run reliably.
 PodsA pod is a collection of one or more containers with common storage and network resources, as well as a set of rules for how the containers should be run. It is the smallest deployable unit that Kubernetes allows you to create and manage. 
 NodesThe components (physical computers or virtual machines) that run these applications are known as worker nodes. Worker nodes carry out the tasks that the master node has assigned to them.
 ClusterA cluster is a collection of nodes that are used to run containerized applications. A Kubernetes cluster is made up of a set of master nodes and a number of worker nodes. Minikube is highly suggested for new users who want to start building a Kubernetes cluster.
 ObjectsKubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses objects to represent the state of our cluster. They describe the desired state for running applications, the available resources for the running applications, and the policies guiding these applications. It holds information about cluster workload.
 NamespacesNamespaces are a way to distribute cluster resources needed for use in situations with various teams or projects with a large number of users.
 Ingress ControllerKubernetes Ingress is an API object that manages external users’ access to services in a Kubernetes cluster by providing routing rules. This external request is frequently made using HTTPS/HTTP. You can easily set up rules for traffic routing with Ingress without having to create a bunch of Load Balancers or expose each service on the node.
 
 “Enterprises are drifting further away from monoliths and closer toward a microservice architecture for every app, website, and digital experience they develop.”— Kaya Ismail (2018)
 
 
 Monolithic architecture is a common way of building an infrastructure as a single unit that includes a user interface, a server-side framework, and a relational database. Since all of the app’s layers and components are interconnected, changing one component would require you to update the entire app.
 Another approach is the Microservice architecture. By using the microservices approach, a complex program is broken down into loosely linked parts. Loose coupling establishes a system in which each service or component has its own logically distinct lifecycle, protocols, and database. This standalone component can be designed, implemented, scaled, and managed separately from the rest of the app which will continue to function normally.
 
 We begin to wonder how this can interact, as we consider the two methods. This is where containerization comes in: each stand-alone unit is packed into a container which is then enclosed in a pod (which contains multiple containers that are shared in a cluster of nodes). When a Pod contains several containers, they are handled as a single entity, and all containers in the Pod share the Pod’s resources, including namespace, IP address, and network ports at the same time.
 
 (K8s) is a container-centric infrastructure manager. It manages container lifecycles, that is to say, it optimizes container orchestration deployment by provisioning Pods (creating and destroying them) based on the application’s requirements. Kubernetes exposes pods to requests using the Service object which maps IP addresses to a collection of pods. The service ensures routes to the Pods from any authorized source (within or outside the cluster) through a designated port.
 As for frontend developers, we do need to have basic knowledge of setting up the inter-communication between this infrastructure and services. A clear understanding of how things function is essential.
  Often referred to as an Ops problem and that by giving it up, we are missing out on opportunities to understand the possibilities of what we create and how we can improve its availability in production.
 
 While the Ops role should be in charge of cluster setup, configuration, and management, the developer should be aware of and responsible for putting up the bare minimum necessary to run their app.
 As a result, understanding the fundamentals of Kubernetes will allow you to optimize the configuration of your application and make it more scalable; and such to participate in the release process of what they created.
 Teams would benefit from having a general understanding of Kubernetes around the software stack while sharing terminologies and addressing their project. It also allows you complete control over the entire project lifecycle — from code to implementation — allowing you to test the application’s deployment, understand how your project should be deployed, and assist in maintaining the layer as well as specifying the environment. 
 A good example is when a friend Cari (software engineer) described her experience working with a team a few years ago, where the backend engineers were only interested in getting involved in frontend development and Kubernetes alone. They willingly chose to learn Kubernetes and wouldn’t want to directly work with the backend layers, but only consume it. The team enjoyed having the ability to define how their application is deployed on Kubernetes.
 Having this control over their project helps them be a part of the release journey of their projects, and more importantly will allow you to optimize the configuration of your application and make it more scalable. Also, understanding the production-deployment configuration allows developers to spot and fix crucial micro performance issues such as caching, amount of requests, and time-to-first-byte, as well as knowing how staging/testing differs from production before releasing the app.
 There are scenarios where the terminology often overlaps between front-end and back-end teams, for example:
 
 port and servicesWhen specifying a version of an app deployed, and then the ports exposed e.g. “we can interact with our application using the service name and port xx: xx”.
 namespacesKnowing that our project sits in a namespace on Kubernetes. Namespaces are a way for multiple users to share cluster resources.
 RBAC (Role-Based Access and Control)Knowing how the access control permissions on Kubernetes affect your projects.
 
 An understanding of how Docker images build and run will allow teams to clearly communicate the requirements for each application in the cluster.
 Deployment and service
 The resources that you identified in a configuration are created by a deployment. All of the resources that make up a deployment are specified in a configuration file. 
 To make a deployment, you’ll need a configuration file. YAML syntax is required for a configuration file. It contains fields, such as version, name, kind, replicas field (the desired number of Pod resources), selector, and the label field, etc, as shown below:
 ....
   name: 
 spec:
   selector:
     matchLabels:
       app: 
       tier: 
 
   replicas:
     metadata:
       labels:
         app: 
         tier: 
 
     spec:
       containers:
         - name: 
           image: 
           ...
 
 A Kubernetes Service, on the other hand, is an abstraction layer that describes a logical group of Pods and allows for external traffic exposure, load balancing, and service discovery for such Pods.
 Updating a Deployment
 A deployment can be updated to the desired object state from the current state, this is often done declaratively by updating the objects of interest in the configuration file and then deploying as an update. With a rolling update deployment strategy, old Pod resources will be gradually replaced with new ones. This means that two Pod resource versions can be deployed and accessed at the same time while ensuring that there is no downtime.
 
 Backend Service Object
 In this article, the Pods in the front-end Deployment will run an Nginx image that will be configured to proxy requests to a Service with the key of app: backend. We assume the backend team has already ensured that their pods are running on the cluster, and that we only want to create and connect our app via a deployment to the backend. 
 In order to allow access to the backend application, we need to create a service for it. A Service creates a persistent IP address and DNS name entry for the application in question which makes it accessible to other pods. It also uses selectors to find the Pods that it routes traffic to.
 Here is an example of a backend-service.yaml configuration file which would expose the backend app to other pods in the cluster. Below is the sample backend service configuration file:
 ---
 apiVersion: v1
 kind: Service
 metadata:
   name: backend-serv
 spec:
   selector:
     app: hello
     tier: backend
   ports:
   - protocol: TCP
     port: 80
     ...
 
 The above YAML file shows that the service is configured to route traffic to the Pods that have the label app: hello and tier: backend of the cluster through port 80 only.
 Creating the Frontend
 You typically create a container image of your application and push it to a registry (e.g. docker registry) before referring to it in a Pod. Since this is an introductory article, we will make use of a sample front-end image from the google container repository. The frontend sends requests to the backend worker Pods by using the DNS name given to the backend Service which is the value of the name field in the backend-service.yaml configuration file.
 The Pods in the front-end Deployment run an Nginx image that is configured to proxy requests to the backend Service. The configuration file specifies the server and the listening port. When an ingress is created in Kubernetes, nginx upstreams point to services that match specified selectors.
 nginx.conf configuration file
 upstream Backend {   
     server backend-serv;
 }
 server {
     listen 80;
     location / {
                 proxy_pass http://Backend;
     }
 }
 
 
 Note that the internal DNS name used by the backend Service inside Kubernetes is used to specify.
 The frontend, like the backend, has a Deployment and a Service. The setup for the front-end Service has type: LoadBalancer which means that the Service would use a load balancer provisioned by the cloud service provider and would be reachable from outside the cluster.
 ---
 apiVersion: v1
 kind: Service
 metadata:
   name: frontend-serv
 spec:
   selector:
     app: hello
     tier: frontend
   ports:
   - protocol: &quot;TCP&quot;
     port: 80
     targetPort: 80
   type: LoadBalancer
 ...
 
 ---
 apiVersion: apps/v1
 kind: Deployment
 metadata:
   name: frontend-depl
 spec:
   selector:
     matchLabels:
       app: hello
       tier: frontend
       track: stable
   replicas: 1
   template:
     metadata:
       labels:
         app: hello
         tier: frontend
         track: stable
     spec:
       containers:
         - name: nginx
           image: &quot;gcr.io/google-samples/hello-frontend:1.0&quot;
           ...
 
 Create The Front-End Deployment And Service
 Now, that the configuration files are ready, we can run a kubectl apply command to create the resources as specified:
 kubectl apply -f [insert URL to saved frontend-deployment YAML file] 
 
 kubectl apply -f [insert URL to saved frontend-service YAML file]
 
 The output verifies that both resources were created:
 deployment.apps/frontend-depl created
 service/frontend-serv created
 
 
 
 Interacting with the Front-end Deployment and Service
 You can use this command to obtain the external IP address, once you’ve created a LoadBalancer Service:
 kubectl get service frontend-serv --watch
 This shows the front-end Service’s configurations and monitors for changes. The internal cluster IP would be immediately provisioned, while the external IP address is initially marked as pending:
 
   NAME          TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)  AGE
 frontend-serv   LoadBalancer   10.xx.xxx.xxx   &lt;pending&gt;     80/TCP   10s
 
 
 As soon as an external IP is provisioned, however, the configuration updates to include the new IP under theEXTERNAL-IPheading:
 
 NAME       TYPE           CLUSTER-IP      EXTERNAL-IP        PORT(S)  AGE
 frontend-serv   LoadBalancer   10.xx.xxx.xx   XXX.XXX.XXX.XXX    80/TCP   1m
 
 
 The provisioned IP can be used to communicate with the front-end service from outside the cluster. We can now send traffic through the frontend, because the frontend and backend are now linked. Using the curl command on the external IP of your front-end Service, we can reach the endpoint.
 Conclusion
 A company-wide vision for providing reliable software may be driven by a broad understanding of Kubernetes and how your application works on Kubernetes. The microservices architecture is more beneficial for complex and evolving applications. It offers effective solutions for handling a complicated system of different functions and services within one application.
  Microservices are ideal when it comes to the platforms, covering many user journeys and workflows. But without proper microservices expertise, applying this model would be impossible.
 However, it is critical to understand that the microservice design is not appropriate for every level of business organization. You should start with a monolith if your business idea is new and you want to validate it. For a small technical team attempting to design a basic and lightweight application, microservices can be considered superfluous, since monolith can be deployed via Kube without any problems, and you can still benefit from replication options and other features.
 Further Reading On Smashing Magazine
 
 “From Chaos To System In Design Teams,” Javier Cuello
 “A Recipe For A Good Design System,” Atila Fassina
 “Building A Large-Scale Design System For The U.S. Government (Case Study),” Maya Benari
 “How To Create An Information Architecture That Is Easy To Use,” Paul Boag
 </content>
     </entry>
     <entry>
       <title>Smashing Podcast Episode 46 With Vitaly Friedman: Who Is Elliot Jay Stocks?</title>
         <link href="https://smashingmagazine.com/2022/05/smashing-podcast-episode-46/"/>
       <updated>2022-05-17T11:00:00.000Z</updated>
       <content type="text">This article is a sponsored by Wix
 In this episode, we ask how one man can go from designing websites for local bands to heading up Google Fonts Knowledge. Smashing’s Vitaly Friedman talks to Elliot Jay Stocks to find out.
 
 
 Show Notes
 
 Google Fonts Knowledge
 Smashing Conf San Francisco 2022
 Elliot’s personal website
 Elliot on Twitter
 
 Weekly Update
 
 Performance Game Changer: Browser Back/Forward Cache written by Barry Pollard
 Magical SVG Techniques written by Cosima Mielke
 How Even Small UX Changes Can Result In An Increase In Conversion written by Denis Studennikov
 How To Use Google CrUX To Analyze And Compare The Performance Of JS Frameworks written by Dan Shappir
 Top Tasks: To Focus On What Matters You Must De-Focus On What Doesn’t written by Gerry McGovern
 
 Transcript
  Vitaly Friedman: He loves typography from the bottom of his heart. And in recent years, he’s led creative direction for several products and services, including the print magazines 8 Faces and Lagom. He worked as a creative director of Adobe Typekit now called Adobe Fonts, and these days he’s running things at Google Fonts Knowledge.
 Vitaly: Outside of the realm of design, he also does electronic music as other form, and has also released music on several independent labels. Well, we know that he’s an expert in typography and electronic music, but did you know that he’s an avid fan of underground Icelandic techno music from the late ’90s and usually dreams about pixels, font sizes, and REM units.
 Vitaly: My Smashing friends, please welcome Elliot Jay Stocks. Elliot, hello and how are you doing today?
 Elliot Jay Stocks: I am smashing, thank you.
 Vitaly: Oh, well you look smashing as well if I may so. You haven’t changed a bit.
 Elliot: That’s very kind of you to say. I was going to already say that I feel smashing even before I was directed to do so.
 Vitaly: Oh, I don’t know who directed you to do that at all. Don’t you reveal the secrets that we have here.
 Vitaly: Elliot, when we actually look back now, I don’t know, we saw each other when like the last time, was it like 15 years ago?
 Elliot: I was going to say, &quot;That’s crazy. No way,&quot; but actually, I mean maybe, yeah.
 Vitaly: 15? No, not 15.
 Elliot: When we saw each other, oh, sorry. I thought you meant when we first met. When we first met, maybe it was 15 years ago.
 Vitaly: Yes. And when did we see each other last time?
 Elliot: Oh, wow. It’s been a long time since I’ve seen anybody.
 Vitaly: Yeah. Well you do see a lot of electronic music though.
 Elliot: Yes. That’s true.
 Vitaly: So that counts for something.
 Elliot: Oh, boy. I think it was in, I don’t know, Jonathan Snook was there. Where were we? It was you, me and Jonathan Snook. I don’t know. Was it during a talk, maybe?
 Vitaly: I have a feeling it was maybe, probably a room with people and the stage and you probably were speaking. Right?
 Elliot: Maybe.
 Vitaly: That’s probably... But before we actually dive into the specifics, I know that many of our listeners will have heard about your work and your blog post and also your music and all the wonderful things that you’re doing. But I’m always interested in people, like really coming back to the roots, there must be something that started all of this, right?
 Vitaly: And so I really want to hear a little bit about your backstory. So how does a boy with curly hair born in the suburbs of London gets his life through kind of thoroughly defined by web topography and design and electronic music. How does that happen? Tell us.
 Elliot: Well, thank you for the question. I think a lot of it is all just accidents, happy accidents, and just trying stuff out and seeing where that goes and not being too worried about the future plans, I suppose. I suppose I think that’s kind of probably the thing that’s defined my career path as it were.
 Elliot: So when I was a kid, I did a lot of drawing. I remember like my dad teaching me things about perspective and stuff when I was young and I used to do things like a lot of illustrations for the school, they’d have like a play on and I’d do the little pamphlet illustration for the program of the play and all sorts of things like that.
 Elliot: So I was always kind of doing drawing for fun and then asked by people to do with the school, to do like other stuff around that. It was always kind of art based. And I didn’t really dabble with design properly until I suppose I graduated from high school. And before I went to uni, I took a year out and I worked in a music shop in Virgin Megastores, when they still existed.
 Elliot: And I was doing a little bit of music at the time with some other people who worked there and we released a CD, which sounds very quaint now doesn’t it?
 Vitaly: Yeah. Oh a little bit. How old were you then?
 Elliot: Well, so I would’ve been 18, I guess.
 Vitaly: Yeah. Okay. So, that’s what cool people do then in nowadays.
 Elliot: Yeah.
 Vitaly: They just sing on CDs.
 Elliot: That’s right. Well maybe, and the managers of the store allowed us to put the CD on sale in the store. And so it fell to me to design the cover and I’d started dabbling with... I don’t know what it was.
 Elliot: It was like some sort of version of Photoshop, like a free version of Photoshop or something like that. And I was doing kind of stuff with terrible Photoshop filters and things like that. And-
 Vitaly: So that’s where it’s all started. I can see that, now.
 Elliot: Yeah. It was all this kind of thing. And I did the POS and the website for it. And it was the first website I’d kind of done. Well, yeah, it was my first experience. And the web was relatively new at this time. And I used an editor called Homestead, which was like a sort of WYSIWYG editor. And that whole thing was just kind of my first experience of design, I suppose.
 Elliot: Web design and print design. And that kind of, I guess, ignited that interest in that. And then for a few years at university, I did a lot of stuff on the side for music, for bands’ websites and stuff like that.
 Elliot: And then when I graduated from university, I got a job working for the record label, EMI working on a bunch of music sites. So, that was kind of how that all came about, really. I was working a lot of web stuff. It was all very music related and it sort of just happened, I guess, a little bit by accident and gradually over the years, I kind of... I don’t think I’m a web designer anymore.
 Elliot: I kind of realized recently that’s in the past that I’ve sort of gone on to do other things. But everything kind of came from there and there’s been a really strong, I suppose, musical current throughout the whole thing and lots of side projects.
 Elliot: And this is also what I’m going to be talking about at Smashing Conf as well, in fact. This whole thing and how it’s all interrelated and how a lot of side projects have led to, as I said, these kind of happy accidents, doing stuff that’s really fun just by sort of trying it out and not having too much of a plan.
 Vitaly: No, it’s interesting because every time I think about my childhood and how things used to be when I was growing up, I always think about these important people who kind of defined my career, my view on things. I don’t know, but I always, I mean especially over the last couple of weeks now, I’ve been for some reason thinking about the blog post by Mark Bolton and the blog that you were writing.
 Vitaly: And this was a very exciting time for me. I mean, you have no idea just how I just really felt that this is it, this is really changing my life. I kind of had this feeling in my head when I was kind of working through this and in my heart as well. So maybe just kind of throwing this question at you, maybe you could talk about people who really defined your way of thinking about design?
 Vitaly: Like what really made the most significant impact. Was it maybe a talk somewhere? Maybe it’s an article that you read, a book? I don’t know, just a random coincidence where you just met somebody and they said something? What was that really kind of defined your work in many ways?
 Elliot: Yeah, oh, wow. I remember those years as well. That felt really fun and everything on the web was new and we were all sort of just figuring it out and it was kind of the wild west of the web design days. I loved it.
 Elliot: But yeah, I guess from my own perspective, I’ve definitely been, in the early days, I was really influenced by a lot of those people doing some really cutting edge Flash websites. So this is probably around 2001, 2002, 2003, I guess, to advanced PreyStation, tokyoplastic, My Pet Skeleton.
 Vitaly: Oh, those were the times.
 Elliot: Yeah. The real heady days of Flash-based web design. And they were very influential on me, not just for the web, but they just, like you said, something you felt in your heart, it’s just this exciting, &quot;Wow, this is this just amazing stuff going on. It’s not just the web, but about design in general and creativity in general.&quot; And that really, really was just a very exciting time, and that kind of influenced me a lot in those early years.
 Elliot: I think that whole kind of grungy style and that very kind of David Carson influence thing. I don’t think I realized until later that it was a lot of David Carson influence on that lot. You know, people like JUXT Interactive who I loved at the time and kind of looking back now, it’s a very David Carson kind of thing. But that was very influential on me.
 Elliot: And then I got into, when I was working in the music industry and I was working at EMI, we started to move away from Flash and web standards was becoming a thing. And obviously like Zeldman’s writing and Eric Meyer and Dave Shay was doing the CSS Zen Garden. And then that again was like this really exciting, like, &quot;Oh, wow, what is this whole new way of exploring the web and building on the web and designing on the web and working within these new constraints?&quot;
 Elliot: And I think aesthetically, I went away from that kind of outlandish grungy stuff. Well, I mean, eventually I did, to more kind of like clean and eventually my focus on typography and things like that. And I think it was probably, I mean, Erik Spiekermann obviously is a hugely influential person anyway. And his kind of... I think sort of knowing Erik and getting to know him through projects like 8 Faces and things like that, his influence on the real minutiae of typographic design, the real specific geeky details, that kind of led me down that path into less focused on, I guess, some of the big, &quot;How do you design something from scratch?&quot; But more into like, how do we really tweak and refine this very small part of a design to be the best it can possibly be?
 Elliot: And I think almost like Erik is kind of at the opposite end to the David Carson side of design. But I think in sort of recent years, I’ve gone a bit more in that direction. And as you get older, I think your interest change. And for a long time, I was very, I don’t know, I guess really distrustful of people who were kind of involved in design, but didn’t want to do the whole thing.
 Elliot: And I was really distrustful of people who were kind of like directors, but they weren’t necessarily at the coalface doing the design work. Whereas now I’ve sort of come round to being okay with just focusing on one part of it and letting other people kind of get concerned with design systems, and developing things.
 Elliot: And I don’t know, web design is such a different beast these days anyway, I think.
 Vitaly: Yeah, I don’t know. Is it just me, Elliot? But I feel like every time somebody brings up a notion of web designs, isn’t it like a term from 2000 somehow? Or 2010 or something, web design? Like we’re just UX designs. We are UX engineers. We are... I don’t know. There are so many old kind of different roles. What role do you see yourself now? Who are you today? Like if you had to define your role, like something that really, I don’t know, a term that really defines and captures your essence, what would that be?
 Vitaly: I don’t know, if it’s a good answer, question to answer.
 Elliot: It’s a great question to answer. And one, I’m constantly asking myself. Yeah, I don’t know. At the moment I am sort of describing, well, I’m not really actively describing myself, but I suppose the work that I’ve done recently and I’m doing at the moment is more of, I guess I’m sort of a ... Oh boy, I don’t know. I guess it’s like a typographic consultant, I suppose, in that I’m doing a lot of work that is very... I mean, all design is and should be typography focused in many ways, but it is very focused on typography with... I’m doing very little in terms of like hands-on design these days, but it’s more kind of helping steer something from a typographic perspective, and advising people on that.
 Elliot: And the work that I’ve been doing with Google is all about sort of education around typography in general.
 Elliot: But prior to this, I was doing a lot of creative director roles. So I sort of stopped being a designer in the more traditional sense and was more just a creative director. Sort of from my time when I was a Typekit. And then the roles that I then had after that, they were all kind of creative director roles.
 Elliot: But and again, fairly recently when the pandemic kind of hit and I lost the job that I was currently working in at the time as creative director for an agency in London called Maido, Everyone kind of, that sort of had to disband, and I found myself doing some... Sort of going back to, well 2020 was a really weird year and everyone had to kind of go and do these different, sort of take on different kind of work to make ends meet and everything.
 Elliot: And then after that, I think I realized that I didn’t really want to go back into that design leadership thing for a while. I think I’d got a little bit jaded by just sort of... It’s not that I wanted to be at the coalface, designing everything and building everything again, but I also wanted to do something that was a little bit more, I guess, kind of insular and kind of self-contained. And not involving big teams and stuff like that.
 Elliot: And yeah, that’s kind of led me even further away from web design in a way, which has been nice. For a while, I was having this real existential crisis of trying to answer that question of who am I, what am I doing? And I think-
 Vitaly: But I think we have pretty good understanding now, do we?
 Elliot: Yeah. I think now I’m comfortable with where I am at for now. Ask me tomorrow, I’ll change my mind.
 Vitaly: Okay. I will definitely ask tomorrow as well. But now actually and it interesting looking back with, because you had all the different roles and you worked with all the different people and we just briefly talked about some influential people and who changed your view on things. And in my life it was you. You don’t even remember, I’m sure. I think you don’t even remember.
 Elliot: That’s very kind of you.
 Vitaly: I remember us. Yeah, I will explain in a moment why. Because when we were working on some project, who knows what project that was over the last, I don’t know, 11, 12 years now, 15, maybe. I remember you saying one thing. I think it was a navigation design that we changed in Smashing of 2013, ’14 something.
 Vitaly: And you said, &quot;Well, if something is different, you need to make it look very different. It can’t be just close enough or a little bit different. It has to be bold and decisive and different enough, so people can notice that this is a decision and not a mistake.&quot;
 Vitaly: Right. And I remember this, we were there. I mean, this has stuck with me for quite some time and actually many things that you’re mentioning about paying attention to details and being very careful and all those things, they kind of define my kind of way of working as well.
 Vitaly: But I didn’t spend a lot of time working with different agents in all the different roles. Basically I have for last, what 12 years, I’ve been in the same, more or less the same position. But looking at you now, because you’ve been working with all the different teams and all the different people, is there something that you would recommend to yourself when you’re working with them?
 Vitaly: Maybe do some... A little bit more of that. Do a little bit less of this in your career, as you kind of keep the ball and keep rolling. Is there something that you wish you would have done differently?
 Elliot: Mm yeah. Well, first of all, thank you for saying that’s, that’s really awesome to know that that was influential and helpful. And yeah, I don’t quite remember that, but that is awesome. And that must have been when we were doing the Smashing redesign, which was-
 Vitaly: Yeah, I think so.
 Elliot: Yeah. A while ago now.
 Vitaly: Like six, seven, eight years ago now.
 Elliot: Yeah. Wow. Yeah. In terms of the sort of career advice, things I wish I’d known when I was younger, stuff like that, I think learning to trust your gut is super important. And there were definitely times when I look back on projects that I said yes to that maybe I’d already got that gut feeling that they might not be great and perhaps I shouldn’t have taken them on. And I did anyway.
 Elliot: And I think listening to your gut, if you’ve got a feeling that says, &quot;I shouldn’t be doing this,&quot; for whatever reason, then there’s probably a valid reason for why you’re feeling that way. I’ve got a print that Erik did that I bought from his print shop P98A and it says, &quot;Don’t work with assholes, don’t work for assholes.&quot; Or maybe the other way round. But the meaning is the same.
 Elliot: And that again is like I think sometimes you can tell quite early on how someone is going to be, and it’s useful to not persevere with projects that are perhaps run by or with assholes. So I wish that I had had that framed on my wall earlier on in my career. It’s on my wall now, but it perhaps should have been a mantra that was adopted sooner.
 Elliot: But I think one of the things that I’m very grateful for is, that I’ve been in the position where I’ve been able to pursue things that I really care about. And although passion is this thing that’s kind of, this term that everyone says, it’s bandied about the whole time.
 Elliot: I think it’s really important. And I’ve always thought it’s important, to really care about what you’re doing. And the reality is that we are at work for most of the day, most of the hours that we are awake in the day. And so we should be spending those hours doing something we love.
 Elliot: Now, that’s all well and good. You know, that’s not necessarily helpful advice to give to somebody who may be stuck in a job that they absolutely need to stay in to pay the bills.
 Elliot: But I think it’s not necessarily about just going, &quot;Right. I’m going to quit my job and going pursue my dreams.&quot; It’s about sort of finding meaning in what you’re doing. And if there isn’t a direct way to do that in your day job, then I think side projects have always been a great outlet for that.
 Elliot: And for me personally, being able to, I guess, seek creative fulfillment through side projects has led me to pursue those passions almost, as I said before, by accident. It’s the side projects and just sort of going for them and not worrying too much about the consequences that have, later on, led to the really good work.
 Elliot: So even if you can’t leave your current job now that you might hate because you want to pursue your passions, I think finding a way to work your passions into it somehow, or to express yourself through a side project will help you eventually get involved with projects that you do really care about. And I’ve certainly been fortunate enough that that’s been the case.
 Elliot: And some of the jobs I’ve had have been direct results of the work that I’ve done on side projects. And then those jobs in themselves have led to further things. And there’s always been kind of that snowball effect. And so I mean, it’s hard to, I think when you’re younger and you’re starting out, it’s hard to necessarily... There’s on the one hand, that you can be sort of like full of this youthful naivety and kind of go, &quot;Yeah, let’s go for it and do whatever,&quot; but that can also lead to some not great situations.
 Elliot: But definitely looking back, being slightly older and having done this for a while now, I can definitely say that the times where things really worked or where I was really happy were because I sort of just followed what I was interested in, rather than what was the kind of sensible perceived route that I should take.
 Vitaly: Yeah. I think it’s, for me personally, it’s really a matter of being strategic and there are so many things I wish I had known earlier and not solely related to design or UX or web or topography or anything. But it’s just, sometimes you might even think about just very routine, basic life stuff. Right?
 Vitaly: I mean, you know me, but I’ve been exploring the world of cutting cucumbers and watermelons for, I don’t know how many months and years now, and I still haven’t found the right way. And I’m always disappointed with my outcome. The same goes for coffee and for so many other things, which could be just small things that would be really, really enjoyable. Right?
 Vitaly: And so, for example, one thing that I really wish I would know a bit more about is just how to do basic simple accounting. How to estimate better, how to deliver on time, how to get a bit more disciplined and things. Because these are all the things that I had to learn over time.
 Vitaly: But, oh, my. I’ve been overestimating, underestimating, going wild and just literally guess working all the way. Do you have sort of a structure, system? How do you work? Are you one of those people who are very like, Paul Maduro and 45 minutes and then this. The alarm goes on and off I go making a break. Or how do you work?
 Elliot: Yeah, no, I’m definitely not one of those people. I really struggle, to be honest, I have.
 Vitaly: Oh, by the way, not to say that we have anything against these people. They’re very kind, they very productive. Don’t mean to be kind of disapproving in any way, just looking about different ways of how we all work today.
 Elliot: Yeah. I have, so somewhat, I guess, it’s a little bit of a contradiction, but I try and set up a relatively focused schedule because I’m very easily distracted. So someone might look at my calendar or my approach to productivity and perhaps think that I’m quite well organized. And I think it’s, I’m not. This is why I’m using these things to try and and focus myself.
 Elliot: So a few years ago, Jessica Hische posted a thing about her calendar, that she’d blocked out part parts of the day to kind of be productive. And it was really interesting. And I think I wrote a blog post, where I sort of had my own take on it. Although, I’ve since kind of changed that.
 Elliot: But still the idea is basically blocking out time on your calendar to say, &quot;This is productivity time. This is client work time. This is freelance project time, or this is family time, whatever.&quot; That sort of helps. That part of that structure’s been forced on me in a good way by having a family.
 Elliot: So I have two young kids now and I always stop at five o’clock to go and have dinner with them. And I usually pick up some work stuff later on in the evening, even if it’s just kind of messages and stuff like that. But I have a pretty rigid stop time, which is, which is really nice because it means that I get some time with my family.
 Elliot: And it also just forces a bit of a structure on my day. Plus I have things like school pickups and clubs that the kids do. And lunch and very specific things that aren’t that movable these days, which is good, which is really good.
 Elliot: But I’ve also recently, I think to try and combat the fact that I’m quite easily distracted and just go down different rabbit holes, I’ve started moving to a paper based to-do list. So I still use things which I love as an app for that. And I use Notion for all kinds of general to-dos. But for my every single day, I write down on a little card, all of my tasks.
 Elliot: So I was influenced by Jeff Sheldon and his project, I think it’s called Analog, where he did a nice kickstart with these beautifully designed a little cards. And he had a little system going for the priorities in his card and a nice little case for it, and all this kind of thing. I wanted to sort of take that, but make a much more low-fi version.
 Elliot: So I made this system called Today And Soon, although I’ve since kind of changed it to just be focused on Today’s, which is basically I made a little template, a Moo card template that you can just download for free and get it printed. And it’s a series of tick boxes and there’s something about writing it down.
 Elliot: You know, I’ve got my where you can’t see this. I can show you, but you won’t be able to hear this. And this is written down. And it’s some basic stuff that I want to do every day. You know, I want to Duolingo every day, and I’ve got to call this person today and I’ve got to finish this particular bit of work and all this kind of stuff.
 Elliot: But just having it written down and like literally sat there next to my iMac, like balancing against my monitor there. And I get to do a big check mark with a big Sharpie every time I finish something, big or small, has really helped recently. Really helped with just keeping me focused. And I have stuff on there that’s like a big work task or it’s buy new flea medication for the dog or something.
 Elliot: But the sense of being productive by checking those things off the list is really nice. And so that coupled with a fairly rigid calendar and time kind of blocked off, has really helped my productivity. And I think that’s kind of, I suppose that’s kind of how I work.
 Elliot: But it’s still, my work day-to-day is a lot of like flitting between different things. It’s like some time in Figma working on some illustrations for Google Fonts Knowledge. And it’s time in Google Docs writing or editing, and it’s time in Notion doing some planning and it’s time on social media stuff, doing bits for my music project.
 Elliot: And it’s a little bit, and time in chat talking to colleagues and planning stuff and meetings and whatnot. And it’s quite varied. And I think that variation can easily lead to distraction, but also I do quite like having things varied. I’ve realized over time that I’m not very good at just staying and doing one thing. I can’t sort of sit down there at nine o’clock and design all day and then finish at five.
 Elliot: Like that’s never really been me and I’ve certainly failed when I’ve tried to do that.
 Vitaly: Yeah. So I think it’s interesting because for me, sometimes I feel like we are maybe twins from different universes or something like that, I don’t even know. Because I mean, I have moved my calendar quite a bit and I actually, I think my partner in late December, just planning ahead for the next year. We were sitting down, we just really thinking about what was the year like, and what’s the next year it’s going to be like? And of course it’s a very common thing, and some people would say, &quot;Well, everybody’s doing that or whatever,&quot; but it was really critical because I really had to kind of question everything.
 Vitaly: That’s really been on my agenda for the last couple of months now. I just, it’s impossible for me to read a book. I’m questioning every single sentence in the book, now. It’s just really, really hard and it really changed because then I totally revamped my calendar.
 Vitaly: And so I block out Fridays altogether, and there are dedicated times for meetings. And that’s it. And this kind of structure thing again, is probably something that gives you sort of, I don’t know, comfortable framework to work within?
 Elliot: Yeah.
 Vitaly: Right. So you just know that, okay, you’re going to do this and you have limitations in terms of the amount of time you will spend on this, because this is going to be a cutoff at five o’clock or six o’clock.
 Vitaly: So I can totally see how kind of how it all comes together, how it’s all working for you as well.
 Vitaly: Are there any things that you just let go? This was actually quite important for me as well, because I’ve been working with a couple of projects and we had to think about not the design strategy, but the deleting strategy or archiving strategy-
 Elliot: Oh yeah. Interesting.
 Vitaly: ... for very old and outdated content. So what are some things that you just recently let go or just stopped doing and that helped you as well?
 Elliot: Yeah, that’s a really good question because I think it is so important to say no. And I remember doing talks a few years ago when I was kind of talking about freelance life and stuff like that, and talking a lot about being confident in saying no to clients and turning away work that you didn’t agree with and stuff like that. In terms recently, I guess it’s been sort of juggling stuff.
 Elliot: I’ve, for a long time, had the opportunity to do maybe like a little bit of freelance work on the side and stuff like that. And I’ve recently with Google Fonts Knowledge and stuff, just settled into doing kind of one fixed kind of solid thing all day. Just working on Google Fonts Knowledge pretty much. And that’s been really nice to do.
 Elliot: That said, I mean, I still have my music projects and a lot of admin around running the label and stuff like that. So that still happens in the evenings and things like that. And as I said, there’s bit of like social media posts and stuff like that. So my mind is still bouncing around these different things, but I’ve definitely turned down a lot of freelance projects that have come my way, just because it’s... I know that it’d be so easy to fill the hours doing that stuff.
 Elliot: And I’m personally not very good at sitting there and just relaxing. Like I have this often detrimental need to be creating or making something. And I don’t really, I’m not great at playing video games and stuff because I feel like, &quot;Oh, I should be making some music or taking a thing on or doing more work or whatever.&quot;
 Elliot: Like I said, having kids has definitely helped in that my time with them is my time with them. And that’s really nice because nothing really eats into that, apart from the very occasional meeting or something like that. But on the whole, it’s dedicated.
 Elliot: But yeah, I think there’s nothing recently apart from just saying no to some other freelance projects coming in. But I should do more sitting down and relaxing and just being okay with not doing much.
 Vitaly: I think everybody’s saying that. And then nobody really does. I think personally I find it so difficult to just sit and do nothing. It’s just so, I mean, maybe I’m just impatient and I always have these questions raising up. These question marks coming up in my head. It’s sometimes it’s just difficult to fall asleep because I think, &quot;Oh, I have this idea for that thing and I should be following this and I should be writing it down and I should not be writing this down, but then maybe I want to write it down.&quot; Kind of this ongoing story.
 Elliot: Yeah, yeah.
 Vitaly: But I mean, you’re adventurous, you’re just exploring and it’s just... I know that we’ll be kind of wrapping up shortly, but I do want to just find out how do you end up becoming or getting, kind of embarking on this journey from music on a new level? Because I know that for a while you have not been on that journey, you’ve been doing a lot of design and maybe I’m wrong. Please, correct me if I’m wrong.
 Vitaly: But it’s only recently that we had this conversation, a few potentially on DJing at Smashing Conferences as well. And that’s something I wouldn’t imagine, like I say, 10 years ago when I saw you speaking on stage.
 Vitaly: So you really fell deeply in love with electronic music again and now having your own label and all. And can you tell us just briefly that story? It’s like, why and how, and just how it happened and also where it goes?
 Elliot: Yeah, sure. I mean, so I have actually been doing music for many, many years, in a very non-serious way. So I sort of started releasing some of my own music when I was in that year off that I mentioned before university. So I was like 18, and I released self-released a couple of EPs after that, but I was never really serious about it.
 Elliot: And it wasn’t until, I think it was like 2015, something like that, 2014, 2015, where there were a couple of catalysts. One is that I’d kind of always liked some electronic music, but I was more into kind of rock and metal and stuff like that. And it wasn’t until then that I discovered some techno that for me, I thought, &quot;Wow, this is really interesting music. This is people doing something that I haven’t really heard.&quot;
 Elliot: And although it was kind of dance music, it’s not just about dancing. There’s just way more to it than that. And it was really interesting to me. There were a few different artists doing some cool stuff around this time, Monique and Shifted and KiloWatt and Manny D and some people that I just come across. And I found their music genuinely really interesting as a listener.
 Elliot: But it spurred me on to kind of say, &quot;Oh, maybe I could do some stuff like this.&quot; And then at exactly the same time I bought some hardware synths from... You can see them, they’re in the background there, these Volcas from, from Korg. And they’re really cheap. They’re analog synths, but they a really, really cheap, really dirty, and they’re just really fun to play with.
 Elliot: And as soon as I was playing with them and like tweaking, turning knobs and moving sliders and just playing with the hardware and all those kind of happy accidents, again, that come with playing around with stuff like that. And coupled with the influence of these new artists, I thought, &quot;Wow, this is really interesting. Maybe I could make this kind of thing.&quot;
 Elliot: And it just sort of, I suppose, made me start taking it a little bit more seriously. And there was one other catalyst. So in 2015, we had our first daughter and sort of from then, my time to be productive musically, but in general has definitely been very limited.
 Elliot: And ironically that’s the time when I spent the most kind of going, &quot;Right, I have to do this thing, I have to be serious about music now.&quot; But I really do think that having that limited time to do this in, whereas before where the world is your oyster, you can spend all the time in the world, having a tiny window in which to be productive has actually helped me focus.
 Elliot: Again, that was sort of happened to me rather than anything that... I didn’t kind of like plan for that level of productivity, but that really did help.
 Elliot: And so I released my first EP as other form in 2017 and since, and even then I did it and then I kind of went quiet for a bit, but around 2019, things started to pick up again. Started to be making a lot more music, put something else out on a different label. And then I played a gig in Berlin at the end of 2019 and was like, &quot;Hey, this is the start of playing live in like some clubs around the world and stuff.&quot;
 Elliot: And then of course, we all know how 2020 went, but during that time, during the pandemic and everything, I really kind of doubled down on getting music out and growing the label and releasing other people’s music not just my own stuff. And it’s been just a whole other adventure, as you say. Just kind of working out that side of things and I love it.
 Elliot: It’s very different to design. I don’t think there are many parallels, really. There are certain organizational things that have helped. I mean, I kind of run my design life and my music life on Notion, for instance. But in terms of like their creativity and the kind of label admin stuff, I think it’s very different to my kind of day jobby stuff, and also takes up quite a lot of time. But it’s all fun.
 Elliot: As soon as it stops being fun, I’m going to stop doing it.
 Vitaly: Yeah. Well, I think that it’s incredible to see this energy in your eyes when I can see it now.
 Elliot: Thank you.
 Vitaly: And it’s wonderful to see you really kind of shining through, and maybe who knows maybe in a couple of months or so, we’ll see you all over the world. And I know that you will be in some parts of the world, that will be San Francisco.
 Elliot: That’s right.
 Vitaly: For the smashing conference. So that might be the time when we should be expecting your live performance, as well. What is a snippet of it, right?
 Elliot: Maybe, maybe.
 Vitaly: Well, maybe we’ll see about that. Well, if you, dear listener would like to hear more from Elliott. You can find him on Twitter where he’s well, what a big surprise, Elliot @elliotjaystocks, and also on his website, which is also a big surprise, elliotjaystocks.com. So you can always follow along and see what Elliot has to say, and also what he’s working on.
 Vitaly: Well, thank you so much today for joining us, Elliot. Do you have any parting words of wisdom with the wonderful peoples listening to us today?
 Elliot: No, I don’t have any parting words of wisdom. I hope you didn’t come here expecting wisdom.
 Vitaly: Well, that counts for something, right? Well, thank you so much, Elliot and I’m very much looking forward to seeing you in San Francisco.
 Elliot: You too. Thank you for having me, Vitaly.</content>
     </entry>
     <entry>
       <title>The Modern Way To Create And Host A WordPress Site</title>
         <link href="https://smashingmagazine.com/2022/05/elementor-modern-way-create-host-wordpress-site/"/>
       <updated>2022-05-17T10:30:00.000Z</updated>
       <content type="text">This article is a sponsored by Elementor
 What does it take to design a new website in 2022? Well, it’s possible to break this process into a few steps. You likely start with choosing a domain name for your website and buying it. Next, you will compare various hosting services and choose the one that satisfies your needs, and lastly, you will select a content management system (CMS) that fits your needs. After installing CMS on your hosting provider, you can start designing your website.
 Sounds tedious, right? With so many tools available on the market right now, finding the right tools can take hours or even days, and there is no guarantee that you will have a decent solution in the end. 
 This proved to be the catalyst for Elementor’s latest solution: the Elementor Cloud Website. The Elementor team aimed to overcome this challenge. The Elementor Cloud Website is a new tool that will expand the WordPress website builder’s offerings with built-in hosting features. This hosting solution is based on the Google Cloud Platform, known for world-class security, scalability, and reliability. The platform offers 20 GB storage, 100 GB bandwidth, and 100K monthly visits which are enough for most types of websites. What it means is that with Elementor’s Cloud Website, web designers and developers can build and host their WordPress websites without the need to switch between different tools.
 Creating A New Website With Elementor Cloud Website
 The process of creating a new website with Cloud Website is simplified and faster. If you’re new to Elementor, you can visit the Elementor Cloud Website and click “Join Now” to create a new account. Once you’ve created a new account, you need to choose the option “Build &amp; Publish a complete Hosted Elementor Website”.
 If you are an Elementor user, you can log in to the Elementor admin panel where you will see a large button named “Create Cloud Website” at the top right corner of the page:
 
 Once you click this button, you will be invited to provide basic information about your website step-by-step. You will provide the website’s name, domain name, and type of website.
 
 All websites created using Elementor Cloud Website will be hosted on subdomain .elementor.cloud by default, but you can easily link your own with no extra costs.
 
 You will be asked to specify the type of website you want to build. Elementor will use this information to suggest a relevant pre-designed website kit for your project. 
 
 Lastly, you will be asked to choose the visual kit for your website from the list of available options. You can always choose to design a website from scratch. For our example, I decided to use the Basic kit from the list suggested by Elementor. 
 
 Once you do that, Elementor will start building your website. The process of creating a new instance of your website will take a few minutes. Once it is done, you will see a shiny new instance of your website ready for use. Our instance is called WakeUp.
 
 You may notice the badge “Site Lock is On.” When Site Lock is enabled, search engines won’t display your site, so you don’t need to worry that someone will have access to your unfinished website. 
 But what exactly comes with a newly created instance? You will have access to the WordPress dashboard, Elementor editor, and website settings. 
 
 Let’s click “Edit with Elementor” to start designing our website right away. 
 Modifying Page Layout With Elementor Editor
 All front-end designs of your website can be done with the Elementor editor. Elementor offers a WYSIWYG (What You See Is What You Get) editor for your web pages. When you introduce changes to your web page, you immediately see the results of your changes. For example, when we want to change the text in the hero section, all we need to do is to click on the content block and modify the text in it. 
 
 If you want to add a new content section on your page, add a new section, choose the most relevant widget from the collection of 100+ widgets, and drag &amp; drop it into this section. Elementor supports almost all types of content — from text to visuals. 
 
 But the process of editing is not limited to text or images. You can introduce fancy animated effects such as motion effects on scrolling. Such an effect can help you create a more dynamic scrolling experience and can convey a feeling that your website is alive.
 For example, here, I defined the “Fade In” effect with a duration of 300 ms for the section that comes after the Hero section and describes features that our product offers:
 
 The great thing about the Elementor editor is that you can check how your website looks on mobile or any other medium. Simply click “Responsive” Mode in the bottom left menu (next to the button “Update”) and click on the “Mobile” option. What you will see is how your website will look on mobile devices:
 
 Managing The Website Using WordPress
 Elementor Cloud Website provides access to full WordPress instances. It’s the same version of WordPress that you might download from the official WordPress website and install on your local or remote machine. Out of the box, this instance of WordPress comes with all essential features. You will have admin access to the CMS and be able to manage users who can access your website, import/export content, and install required plugins. But Elementor Cloud Website also offers a few neat benefits. WordPress comes with a pre-installed Elementor Pro plugin, Activity Log, which tells you what activities are taking place on your dashboard, and a collection of hundreds of visual themes that you can use for your website. Elementor Hello theme is already pre-installed as well.
 Another vital thing about Elementor Cloud Website is that you have all ownership of your content. If you decide to move your website to another platform (i.e., switch from WordPress to any other CMS), you can export your website at any time.
 Setting Backups And Connecting Domain
 Once you finish designing your website, you need to do two things: create a backup and connect the domain name. There are two kinds of people: those who back up and those who have never lost all their data. With Elementor Cloud Website, setting up backups is easy. All you have to do is click the “Manage This Website” option and find the “Backups” section. 
 
 As you can see, Elementor creates automatic backups for your website every 24 hours, but you can also create your own backup at any other moment using the “Create new backup” section.  
 Next, you need to connect your domain name to this website. It can also be done on this page, in the Manage Domain section. Some web creation platforms charge extra for connecting a custom domain name, but Elementor Cloud Website does it at no additional cost. 
 You can notice that an SSL certificate is also enabled for our website. Elementor Cloud Website provides its users with a built-in free Secure Socket Layer (SSL) from Cloudflare. It gives you a couple of benefits — better search engine ranking (Google ranks SSL higher than non-SSL counterparts) and better user experience (if your website requires data input, site visitors will never see a &quot;Not Secure connection&quot; message in their browser). You can also install your own SSL certificate if required.
 
 After a custom domain is connected, we can safely turn &quot;Site Lock&quot; off so search engines can discover our website. It also can be done in this section. 
 Measuring The Performance Of Our Newly Created Website
 Website performance plays a crucial role in how users think and feel about our website. Nobody likes interacting with slow websites, and when it comes to web performance, every second counts (literally). With every second you make your users wait for the site to load, you increase the chance that they abandon your site.
 Measuring performance should be a regular exercise for site owners, so let’s use a popular tool called PageSpeed Insights to see how our newly created website performs. As you can see, the website has a score of 94 out of 100, which is really good. 
 
 You may wonder, what might cause slow website performance? While many problems can be responsible for slow site loading, high latency and using heavy imagery are at the top of the list:
 
 High latency.Latency is the time it takes for data to pass from one point on a network to another. High latency can be caused by geographic location. For example, your website is located in the US, but your visitor accesses it from Europe.
 Using many heavy imagery assets.Rendering too many high-resolution images in raw formats such as PNG can be time-consuming.
 
 But if you use Elementor Cloud Website, you are safe because it uses content delivery networks (CDNs) distributed worldwide. A CDN allows for the quick transfer of assets needed for loading, including images and videos. Your website content is stored in over 200 locations globally, so it responds quickly wherever your visitors are.
 “Okay, this tool demonstrates solid performance when we test it for one visitor, but what happens when hundreds or thousands of visitors access it?”
 
 Rest assured, you will be fine. Elementor Cloud Website harnesses the power of Kubernetes, an open-source system for the management and scaling of containerized applications. Kubernetes enables Elementor Cloud Website to scale its hosting both fast and reliably regardless of a website’s usage, whether it draws ten or ten thousand visitors.
 How Much Does An Elementor Cloud Website Cost?
 It took us less than an hour to create and publish a website using Elementor Cloud Website. You may assume that this tool will cost a lot of money. But in reality, this solution only costs $99.00/year (excluding VAT). For this sum, you will receive 100GB bandwidth for your website, 100K monthly unique visits, and 20GB of storage for all your content and assets. It’s important to note that Elementor Pro is thrown into the mix (a plan that costs $49 per year). There are no hidden costs. All those features are included at one price. 
 Conclusion
 Web design should be fun and accessible for anyone. After all, when we build a new website, our primary goal is to share our idea, product, or service with the world, not to dive into solving technical problems. Elementor Cloud Website saves us from the tedious and monotonous work of comparing various web services and helps us to focus on what’s really important—sharing our vision with people we care about.  
 This article has been kindly supported by our dear friends at Elementor whose goal is to empower web creators to design, publish and manage powerful and beautiful WordPress websites using the most comprehensive all-in-one design solution. Thank you!</content>
     </entry>
     <entry>
       <title>New WebKit Features in Safari 15.5</title>
         <link href="https://webkit.org/blog/12669/new-webkit-features-in-safari-15-5/"/>
       <updated>2022-05-16T17:30:06.000Z</updated>
       <content type="text">After the feature-packed release of Safari 15.4 two months ago, WebKit’s work for this version of Safari focused predominately on polishing existing features and fixing bugs.
 Safari 15.5 does contain three new technologies for web developers — support for the inert property in HTML; support for the worker-src Content Security Policy directive; and the new minimumViewportInset and maximumViewportInset APIs for implementing new CSS Viewport Units in WKWebView-based apps.
 Safari 15.5 is available for macOS Monterey 12.4, macOS Big Sur, macOS Catalina, iPadOS 15.5, and iOS 15.5. You can update to Safari 15.5 on macOS Big Sur and macOS Catalina by going to System Preferences → Software Update → More info, and choosing to update Safari.
 Developer Features
 Let’s look first at the HTML inert attribute. When set on an element, the attribute makes that element non-interactive by preventing mouse and keyboard focus, clicks, edits or text selection. It also hides the element from assistive technologies. For more information about inert, including a demo showing how inert can make the partially-offscreen content in a carousel visible, but inactive, read Non-interactive Elements with the inert Attribute.
 Next, let’s look at support for worker-src from Content Security Policy Level 3. The worker-src directive provides web developers a way to restrict which URLs are allowed to be sources for worker scripts (Worker, SharedWorker, or ServiceWorker). This can be used to prevent already loaded scripts from loading more scripts in the form of workers, a situation that has potential to be susceptible to malicious attack through using excessive CPU for computation. We also updated Content Security Policy console logging in Web Inspector.
 And last, we’ve added the minimumViewportInset and maximumViewportInset APIs to WKWebView so app developers can add support for all of the new CSS Viewport Units to their browser or other application on iOS, iPadOS and macOS. The minimumViewportInset corresponds to the large measurement, and maximumViewportInset corresponds to the small measurement. The new CSS Viewport Units, which shipped in Safari 15.4, include small (svw, svh, svi, svb, svmin, svmax), large (lvw, lvh, lvi, lvb, lvmin, lvmax), dynamic (dvw, dvh, dvi, dvb, dvmin, dvmax), and logical (vi, vb) units.
 Fixes and Polish
 Now, let’s get to the list of bug fixes and feature polish.
 HTML
 
 Fixed SVG tags behind modal dialogs to not be clickable
 Fixed the Dialog element only animating once
 Fixed rendering a USDZ loaded as the main resource
 Fixed uploading “.pages” files to file inputs accepting “.pages” and “.jpeg” files
 
 Web API
 
 Prevented BroadcastChannel from communicating across distinct opaque origins
 Fixed respecting website policies during COOP-based process swap
 Fixed PointerEvent.movementX always 0
 Fixed resolving a fetch promise when a page enters page cache
 Fixed pointer events to perform a hit test only if there is not a pointer capture target override
 Fixed computing the site for cookies when the document is created by window.open
 Fixed Element.focus({preventScroll: true}) to correctly prevent scrolling on iOS
 
 CSS
 
 Fixed scrolling background-attachement: fixed
 Fixed background-clip: text to work with display: flex
 Fixed rendering for many position: sticky elements
 Fixed position: sticky elements moving incorrectly while scrolling
 Fixed text contents in &lt;span&gt; with opacity not updating when a sibling element has will-change: transform
 Fixed :focus-visible matching on the wrong element when focused via script
 Fixed text-shadow getting clipped
 Fixed behavior of a position: sticky element within contain: paint
 Fixed aspect-ratio with percentage widths
 Fixed returning the default computed style for elements without the transition or animation shorthands
 
 Authentication
 
 Aligned WebAuthn implementation to match specifications to use the default pubKeyCredParams list if the list in makeCredential is empty
 
 Content Security Policy
 
 Fixed blocking image content in object elements
 Fixed sending violation reports to the document for a detached element
 Improved nonce hiding from the DOM
 Updated Content Security Policy handling of JavaScript URLs
 
 Media
 
 Fixed key rotation for single key audio in modern EME paired with a native HLS player
 Fixed disabled Control Center spatial control when playing a video in Safari
 Fixed loading a model in QuickLook when passing extra parameters
 Fixed muted video that sometimes becomes paused when taken to fullscreen
 Fixed video playback on iPhone 7
 Fixed video playback for HEVC content encodings that generate many b-frames with a wide sliding window
 Fixed HLS stream currentTime sometimes jumping backwards
 Fixed clicking on the progress bar often pausing a YouTube video
 Fixed blob videos slowing to pause
 Fixed audio echo after the camera us paused or unpaused
 Fixed playback of HTML5 embedded audio with unbounded range requests
 Fixed the video poster disappearing prematurely on play, leaving a transparent video element
 
 WebRTC
 
 Fixed incorrect label returned by getUserMedia regardless of language selected
 Reduced perceived audio latency
 
 Rendering
 
 Fixed text wrapping for windows that exceed a certain width
 Fixed a Korean webfont rendering issue
 Fixed an issue where a transform change sometimes resulted in bad rendering
 Fixed a flash of missing text content with transform-related animations
 Changed to use colgroup for table sizing when it comes after thead, tbody, tfoot, or tr elements
 Fixed two bopomofo tone marks to move to the correct place in vertical text with a particular bopomofo font
 
 Apple Pay
 
 Fixed the Apple Pay Sheet to return billingContact on iOS
 
 WebGL
 
 Fixed WebGL rendering when using preserveDrawingBuffer on iOS
 Fixed a number of issues related to multisampling that were breaking a lot of WebGL content
 Fixed handling TypedArray with AllowShared to be accepted
 Fixed WEBGL_multi_draw validation
 
 Web Inspector
 
 Fixed large message handling from remote devices
 Fixed repeated opening and closing
 
 Compatibility
 
 Fixed launching Microsoft Teams from Safari
 
 SFSafariViewController
 
 Fixed a noticeable delay in playback when rotating a full screen YouTube video
 
 Safari Extensions
 
 Fixed a crash clicking on Safari App Extension toolbar items
 Fixed an issue where SFContentBlockerManager.getStateOfContentBlocker()  could return an incorrect value on iOS
 Added support for optional_host_permissions for Safari Web Extensions
 
 Feedback
 We love hearing from you. Send a tweet to @webkit, @jensimmons or @jonathandavis to share your thoughts on this release. If you run into any issues, we welcome your feedback on the Safari UI or your WebKit bug report about web technology. Filing issues really does make a difference.
 Download the latest Safari Technology Preview to stay at the forefront of the web platform and to use the latest Web Inspector features. You can also use the WebKit Feature Status page to watch for new information about the web features that interest you the most.
 And More
 For more information on what’s in WebKit for Safari 15.5, read the Safari 15.5 release notes.
 These features were first released in Safari Technology Preview: 140, 141, 142, 143, 144, and 145.</content>
     </entry>
     <entry>
       <title>Omnivorous Analysis</title>
         <link href="https://logicmag.io/clouds/omnivorous-analysis"/>
       <updated>2022-05-16T13:35:33.000Z</updated>
       <content type="text">Satellite imagery has woven itself into the fabric of the internet. We recognize these crisp, high-definition, bird’s-eye-view images most commonly from Google Earth—but we employ them in much more besides: from reporting on stuck shipping containers to getting directions to a friend’s house, to tracking forest fires in real time and scrolling through real estate listings. Given their ever-widening range of commercial, consumer, and civic uses, it won’t surprise most people to hear that the industry that produces them (also known as Earth Observation, or EO) is growing at an exponential rate, and is only expected to expand further in the coming years.
 Yet despite the prominence of satellite imagery in the geographical imagination of the internet, the imperatives of the industry are much less clear. The corporations that produce them are much less well known, and the military interests that back them remain as murky as ever. The highly visible commercial side of the industry is still deeply intertwined with its classified counterpart, and two companies, Maxar and Planet, have emerged to dominate the industry—supporting civilian functions with one hand, while supplying US defense needs with the other. 
 Indeed, the ubiquity of commercial satellite imagery gives nearly anyone godlike powers of reconnaissance and surveillance not that far removed from those enjoyed by militaries and intelligence agencies—a fact that causes no small amount of anxiety within the Pentagon. The pervasiveness and power of their imagery compels us to ask: Where do they come from? And how are they being put to use?
 Launching the Industry
 The story of satellite imagery begins with military surveillance. The CORONA satellite program was launched in secret by the NRO (the National Reconnaissance Office, whose existence wasn’t declassified until the 1990s) in response to the USSR’s Sputnik-1 in 1958. CORONA ultimately put over 144 satellites into orbit over twelve years. Satellites were soon found to be useful for surveying purposes as well: the oil, gas, and mining industries, as well as climate researchers could make use of satellite imagery in their work. The academic, commercial, and national-security interest in satellite imagery of natural resources culminated in the launch of the Earth Resources Technology Satellite, now called Landsat 1, in 1972. Landsat remains the longest-running satellite imagery program to date.
 Following the end of the Cold War in 1992, private companies were permitted to enter the satellite business in the United States, kickstarting the industry that we know today. But the newfangled EO industry never drifted far from its origins in the military-industrial complex. In 1994, defense contractor Lockheed Martin was granted a license to sell commercial satellite high-resolution imagery. In 1995, the first commercial imaging satellite, OrbView-1, was launched by Orbital Sciences Corporation, in partnership with NASA. Soon afterwards, WorldView Imaging Corporation (later called DigitalGlobe) was given the first contract to build and operate a commercial satellite system. As the industry grew during the 2000s, it created new markets and many of the companies we know today. In 2004, Google acquired three geospatial companies that formed the basis for Google Maps. (One of them, Keyhole, had received funding from the CIA.) As the industry has expanded, the number of satellites orbiting the planet has grown: from one in 1958 to over 3,300 in 2020.
 Despite the staggering number of satellites, the business of capturing satellite imagery is dominated by a small number of major players. DigitalGlobe and Orbital Sciences (by then called GeoEye) merged in 2013; the resulting corporation, Maxar Technologies, became the largest satellite imagery company in the United States—a monopoly, in effect. Planet, founded by ex-NASA scientists in 2010, initiated a new chapter for the industry, launching small-scale micro satellites that could capture imagery of the entire planet at least once a day. The miniature satellites themselves are called “doves,” ironic given their recently renewed contract with the NRO in late 2021. Planet, which went public on the New York Stock Exchange just weeks later in 2021, has been hailed as an industry disruptor for years. Maxar and Planet have emerged as twin giants of the industry: one supplies high resolution, the other, speed. 
 The granularity of satellite imagery can be divided into three categories: low resolution (over 60m/pixel), medium resolution (10–30m/pixel), and high resolution (30cm–5m/pixel). The precise resolution of NRO satellites remains classified but continues to occupy the highest rung, while public research satellites like Landsat 1 capture the medium to low resolution needed for climate science. Historically, commercial satellites have been restricted to selling imagery up to 50cm/pixels (lowered to 40cm in 2014, then 30cm in 2015) despite their capacity to produce much higher resolution, as is the case with Maxar’s satellites, in particular. While Planet satellites aren’t capable of capturing the same sort of resolution (they max out at 50cm/pixel), the sheer number of micro-satellites they have launched means that Planet has the largest constellation of satellites ever assembled. As of 2022, Planet’s doves can capture and transmit imagery at least once a day, and this “guaranteed collection” business model is, in part, responsible for their recent IPO. 
 In other words, if Landsat imagery can capture a retreating glacier, Maxar can capture every crack—and Planet can capture its movement day-in, day-out. The NRO satellites? Who knows what they can do.
 The GEOINT Singularity
 Thanks to their size and technological advantages, Maxar and Planet have become the go-to suppliers of satellite imagery used to document everything from tornado damage to the 2021–2022 military actions in Ukraine. According to its own statements, Maxar “provides 90 percent of the foundational geospatial intelligence used by the US government,” and was initially the sole supplier of imagery to the US government. Since 2019, the NRO has subscribed to Planet’s services (a contract that was recently expanded). Meanwhile, other companies like Satellogic (which recently partnered with Palantir in early 2022) and BlackSky (contracted to the NRO and NASA) have emerged with similar capabilities.
 At the same time, the growth of “open source intelligence” (OSINT) and open data initiatives has enabled satellite imagery to be co-opted as both an investigative tool and public good, sometimes even used against the very states and corporations that released their imagery in the first place. Organizations like Amnesty International and Forensic Architecture have used satellite imagery to document human rights abuses, while Bellingcat has done the same for US military bases (both using Planet imagery). OpenStreetMap, an open data project, uses Maxar imagery to create a crowdsourced map of the world (often called the “Wikipedia of maps”). Such civic use of satellite imagery has shifted public access as well, as when Maxar released high-resolution imagery of Israel-Palestine in 2021—imagery that has been historically blurred and kept classified due to US government restrictions. 
 This fact is not lost on US intelligence professionals, many of whom fear a looming “GEOINT singularity,” in which public geospatial knowledge may equal that which is known by “experts.” But even the Pentagon understands that OSINT is here to stay—and may even be a resource for the military to exploit, as a DoD strategy report from August 2021 suggests. Similarly, the National Geospatial Intelligence (NGI) agency has emphasized the role of private companies in the geospatial intelligence community, and more such government-commercial partnerships are expected to develop in the coming years. 
 For all its pervasiveness online, the increased production of satellite imagery by companies like Maxar and Planet is not necessarily leading to an increased commercial demand to use it. From a business perspective, the thousands of satellites circling overhead are producing an excess of supply, and demand is still dominated by defense agencies. 
 Indeed, given how much more advanced NRO satellites must be compared to the commercial industry as a whole (something we have the right to assume, given their classified status—the NRO, for instance, was able to unexpectedly donate two high-resolution telescopes of the same quality as Hubble to NASA), it’s worth asking why the government continues to acquire lower-resolution commercial imagery in the first place. The recent declassification of “Sentient,” an “omnivorous analysis tool” being developed by the NRO, points to their need for vast quantities of satellite imagery to train AI-driven imagery analyses. Could the exigencies of AI be driving the DoD’s continuing support of the industry? On the other hand, buying up imagery could also be a means of controlling the flow of information, pushing the images into the wrong (or right) hands. Could their support be a means of ensuring that satellite imagery is steered in the direction of US military interests?
 In either case, we may not ever know for sure—at least not in the near future. But the questions are a reminder that the commercial satellite imagery industry remains impossible to separate from the military applications from which it arose. They are a reminder that the “supply chain” of satellite imagery—the set of companies and institutions that bring the images from above the atmosphere to the apps on our phones—is not necessarily as straightforward as the notion of “surveillance capitalism” might make it seem. In the meantime, this convoluted mix of civil, military, and commercial actors will continue to fill our skies—and our screens—with satellite imagery.</content>
     </entry>
     <entry>
       <title>Rethinking Server-Timing As A Critical Monitoring Tool</title>
         <link href="https://smashingmagazine.com/2022/05/rethinking-server-timing-monitoring-tool/"/>
       <updated>2022-05-16T10:00:00.000Z</updated>
       <content type="text">In the world of HTTP Headers, there is one header that I believe deserves more air-time and that is the Server-Timing header. To me, it’s a must-use in any project where real user monitoring (RUM) is being instrumented. To my surprise, web performance monitoring conversations rarely surface Server-Timing or cover a very shallow understanding of its application — despite it being out for many years.
 Part of that is due to the perceived limitation that it’s exclusively for tracking time on the server — it can provide so much more value! Let’s rethink how we can leverage this header. In this piece, we will dive deeper to show how Server-Timing headers are so uniquely powerful, show some practical examples by solving challenging monitoring problems with this header, and provoke some creative inspiration by combining this technique with service workers.
 Server-Timing is uniquely powerful, because it is the only HTTP Response header that supports setting free-form values for a specific resource and makes them accessible from a JavaScript Browser API separate from the Request/Response references themselves. This allows resource requests, including the HTML document itself, to be enriched with data during its lifecycle, and that information can be inspected for measuring the attributes of that resource!
 The only other header that’s close to this capability is the HTTP Set-Cookie / Cookie headers. Unlike Cookie headers, Server-Timing is only on the response for a specific resource where Cookies are sent on requests and responses for all resources after they’re set and unexpired. Having this data bound to a single resource response is preferable, as it prevents ephemeral data about all responses from becoming ambiguous and contributes to a growing collection of cookies sent for remaining resources during a page load. 
 Setting Server-Timing
 This header can be set on the response of any network resource, such as XHR, fetch, images, HTML, stylesheets, etc. Any server or proxy can add this header to the request to provide inspectable data. The header is constructed via a name with an optional description and/or metric value. The only required field is the name. Additionally, there can be many Server-Timing headers set on the same response which would be combined and separated via a comma.
 A few simple examples:
 Server-Timing: cdn_process;desc&#x3D;”cach_hit&quot;;dur&#x3D;123
 
 Server-Timing: cdn_process;desc&#x3D;”cach_hit&quot;, server_process; dur&#x3D;42;
 
 Server-Timing: cdn_cache_hit
 
 Server-Timing: cdn_cache_hit; dur&#x3D;123
 
 Important Note: For cross-origin resources, Server-Timing and other potentially sensitive timing values are not exposed to consumers. To allow these features, we will also need the Timing-Allow-Origin header that includes our origin or the * value.
 For this article, that’s all we’ll need to start exposing the value and leave other more specific articles to go deeper. MDN docs.
 Consuming Server-Timing
 Web browsers expose a global Performance Timeline API to inspect details about specific metrics/events that have happened during the page lifecycle. From this API we can access built-in performance API extensions which expose timings in the form of PerformanceEntries.
 There are a handful of different entry subtypes but, for the scope of this article, we will be concerned with the PerformanceResourceTiming and PerformanceNavigationTiming subtypes. These subtypes are currently the only subtypes related to network requests and thus exposing the Server-Timing information.
 For the top-level HTML document, it is fetched upon user navigation but is still a resource request. So, instead of having different PerformanceEntries for the navigation and the resource aspects, the PerformanceNavigationTiming provides resource loading data as well as additional navigation-specific data. Because we are looking at just the resource load data, we will exclusively refer to the requests (navigational docs or otherwise) simply as resources.
 To query performance entries, we have 3 APIs that we can call: performance.getEntries(), performance.getEntriesByType(), performance.getEntriesByName(). Each will return an array of performance entries with increasing specificity.
 const navResources &#x3D; performance.getEntriesByType(&#x27;navigation&#x27;);
 const allOtherResources &#x3D; performance.getEntriesByType(&#x27;resource&#x27;);
 
 Finally, each of these resources will have a serverTiming field which is an array of objects mapped from the information provided in the Server-Timing header — where PerformanceEntryServerTiming is supported (see considerations below). The shape of the objects in this array is defined by the PerformanceEntryServerTiming interface which essentially maps the respective Server-Timing header metric options: name, description, and duration.
 Let’s look at this in a complete example.
 A request was made to our data endpoint and among the headers, we sent back the following:
 Server-Timing: lookup_time; dur&#x3D;42, db_cache; desc&#x3D;”hit”;
 
 On the client-side, let’s assume this is our only resource loaded on this page:
 
 const dataEndpointEntry &#x3D; performance.getEntriesByName(&#x27;resource&#x27;)[0];
 
 console.log( dataEndpointEntry.serverTiming );
 
 // outputs:
 // [
 //   { name: “lookup_time”, description: undefined, duration: 42 },
 //   { name: “db_cache”, description:”hit”, duration: 0.0 },
 // ]
 
 That covers the fundamental APIs used to access resource entries and the information provided from a Server-Timing header. For links to more details on these APIs, see the resources section at the bottom.
 Now that we have the fundamentals of how to set and use this header/API combo, let’s dive into the fun stuff.
 It’s Not Just About Time
 From my conversations and work with other developers, the name “Server-Timing” impresses a strong connection that this is a tool used to track spans of time or a detail about a span of time exclusively. This is entirely justified by the name and the intent of the feature. However, the spec for this header is very flexible; allowing for values and expressing information that could have nothing to do with timing or performance in any way. Even the duration field has no predefined unit of measurement — you can put any number (double) in that field. By stepping back and realizing that the fields available have no special bindings to particular types of data, we can see that this technique is also an effective delivery mechanism for any arbitrary data allowing lots of interesting possibilities.  
 Examples of non-timing information you could send: HTTP Response status code, regions, request ids, etc. — any freeform data that suits your needs. In some cases, we might send redundant information that might be in other headers already, but that’s ok. As we will cover, accessing other headers for resources quite often isn’t possible, and if it has monitoring value, then it’s ok to be redundant.
 No References Required
 Due to the design of web browser APIs, there are currently no mechanisms for querying requests and their relative responses after the fact. This is important because of the need to manage memory. To read information about a request or its respective response, we have to have a direct reference to these objects. All of the web performance monitoring software we work with provide RUM clients that put additional layers of monkey patching on the page to maintain direct access to a request being made or the response coming back. This is how they offer drop-in monitoring of all requests being made without us needing to change our code to monitor a request. This is also why these clients require us to put the client before any request that we want to monitor. The complexities of patching all of the various networking APIs and their linked functionality can become very complex very quickly. If there were an easy access mechanism to pull relevant resource/request information about a request, we would certainly prefer to do that on the monitoring side.
 To make matters more difficult, this monkey-patching pattern only works for resources where JavaScript is directly used to initiate the networking. For Images, Stylesheets, JS files, the HTML Doc, etc. the methods for monitoring the request/response details are very limited, as usually there is no direct reference available. 
 This is where the Performance Timeline API provides great value. As we saw earlier, it quite literally is a list of requests made and some data about each of them respectively. The data for each performance entry is very minimal and almost entirely limited to timing information and some fields that, depending on their value, would impact how a resource’s performance is measured relative to other resources. Among the timing fields, we have direct access to the serverTiming data.
 Putting all of the pieces together, resources can have Server-Timing headers in their network responses that contain arbitrary data. Those resources can then be easily queried, and the Server-Timing data can be accessed without a direct reference to the request/response itself. With this, it doesn’t matter if you can access/manage references for a resource, all resources can be enriched with arbitrary data accessible from an easy-to-use web browser API. That’s a very unique and powerful capability!
 Next, let’s apply this pattern to some traditionally tough challenges to measure.
 Solution 1: Inspecting Images and Other Asset Responses
 Images, stylesheets, JavaScript files, etc. typically aren’t created by using direct references to the networking APIs with information about those requests. For example, we almost always trigger image downloads by putting an img element in our HTML. There are techniques for loading these assets which require using JavaScript fetch/xhr APIs to pull the data and push it into an asset reference directly. While that alternate technique makes them easier to monitor, it is catastrophic to performance in most cases. The challenge is how do we inspect these resources without having direct networking API references?
 To tie this to real-world use cases, it’s important to ask why might we want to inspect and capture response information about these resources? Here are a few reasons:
 
 We might want to proactively capture details like status codes for our resources, so we can triage any changes.For example, missing images (404s) are likely totally different issues and types of work than dealing with images returning server errors (500s).
 Adding monitoring to parts of our stack that we don’t control.Usually, teams offload these types of assets to a CDN to store and deliver to users. If they are having issues, how quickly will the team be able to detect the issue?
 Runtime or on-demand variations of resources have become more commonplace techniques.For example, image resizing, automatic polyfilling of scripts on the CDN, etc — these systems can have many limits and reasons for why they might not be able to create or deliver a variation. If you expect that 100% of users retrieve a particular type of asset variation, it’s valuable to be able to confirm that.This came up at a previous company I worked at where on-demand image resizing was used for thumbnail images. Due to the limitations of the provider, a significant number of users would get worse experiences due to full-size images loading where thumbnails are supposed to appear. So, where we thought &gt;99% of users would get optimal images, &gt;30% would hit performance issues, because images did not resize.
 
 Now that we have some understanding of what might motivate us to inspect these resources, let’s see how Server-Timing can be leveraged for inspection. 
 Image HTML:
 
 &lt;img src&#x3D;&quot;/user-rsrc/12345?resize&#x3D;true&amp;height&#x3D;80&amp;width&#x3D;80&amp;format&#x3D;webp&quot; alt&#x3D;&quot;...&quot;/&gt;
 
 
 Image response headers:
 
 Status: 200
 …
 Server-Timing: status_code; dur&#x3D;200;, resizing; desc&#x3D;”failed”; dur&#x3D;1200; req_id; desc&#x3D;”zyx4321”
 
 
 Inspecting the image response information:
 
 const imgPerfEntry &#x3D; performance.getEntriesByName(&#x27;/user-rsrc/12345?resize&#x3D;true&amp;height&#x3D;80&amp;width&#x3D;80&amp;format&#x3D;webp&#x27;)[0];
 
 // filter/capture entry data as needed
 console.log(imgPerfEntry.serverTiming);
 
 // outputs:
 // [
 //   { name: “status_code”, description: undefined, duration: 200 },
 //   { name: “resizing”, description:”failed”, duration: 1200 },
 //   { name: “req_id”, description:”zyx4321”, duration: 0.0 },
 // ]
 
 
 This metric was very valuable because, despite returning “happy” responses (200s), our images weren’t resized and potentially not converted to the right format, etc. Along with the other performance information on the entry like download times, we see the status was served as 200 (not triggering our onerror handlers on the element), resizing failed after spending 1.2s on attempting to resize, and we have a request-id that we can use to debug this in our other tooling. By sending this data to our RUM provider, we can aggregate and proactively monitor how often these conditions happen.
 Solution 2: Inspect Resources That Return Before JS Runs
 Code used to monitor resources (fetch, XHR, images, stylesheets, scripts, HTML, etc.) requires JavaScript code to aggregate and then send the information somewhere. This almost always means there’s an expectation for the monitoring code to run before the resources that are being monitored. The example presented earlier of the basic monkey patching used to automatically monitor fetch requests is a good example of this. That code has to run before any fetch request that needs to be monitored. However, there are lots of cases, from performance to technical constraints, where we might not be able to or simply should not change the order in which a resource is requested to make it easier to be monitored.
 Another very common monitoring technique is to put event listeners on the page to capture events that might have monitoring value. This usually comes in the form of onload or onerror handlers on elements or using addEventListener more abstractly. This technique requires JS to have been set before the event fires, or before the listener itself is attached. So, this approach still carries the characteristic only monitoring events going forward, after the monitoring JS is run, thus requiring the JS to execute before the resources requiring measurement.
 Mapping this to real-world use-cases, e-commerce sites put a heavy emphasis on “above the fold” content rendering very quickly — typically deferring JS as much as possible. That said, there might be resources that are impactful to measure, such as successful delivery of the product image. In other situations, we might also decide that the monitoring library itself shouldn’t be in the critical path due to page weight. What are the options to inspect these requests retroactively?
 The technique is the same as Solution #1! This is possible because browsers automatically maintain a buffer of all of the Performance Entries (subject to the buffer size limit that can be changed). This allows us to defer JS until later in the page load cycle without needing to add listeners ahead of the resource.
 Instead of repeating the Solution #1 example, let’s look at what both retroactive and future inspection of performance entries looks like to show the difference of where they can be leveraged. Please note that, while we are inspecting images in these examples, we can do this for any resource type.
 Setting up context for this code, our need is that we have to ensure our product images are being delivered successfully. Let’s assume all website images return this Server-Timing header structure. Some of our important images can happen before our monitoring script and, as the user navigates, more will continue to load. How do we handle both?
 Image response headers:
 
 Status: 200
 …
 Server-Timing: status_code; dur&#x3D;200;, resizing; desc&#x3D;&quot;success&quot;; dur&#x3D;30; req_id; desc&#x3D;&quot;randomId&quot;
 
 
 Our monitoring logic. We expect this to run after the critical path content of the page.
 Inspecting the image response information:
 
 function monitorImages(perfEntries){
   perfEntries.forEach((perfEntry)&#x3D;&gt;{
   // monitoring for the performance entries
 
 console.log(perfEntry.serverTiming);
 })
 }
 
 const alreadyLoadedImageEntries &#x3D; performance.getEntriesByType(&#x27;resource&#x27;).filter(({ initiatorType })&#x3D;&gt; initiatorType &#x3D;&#x3D;&#x3D; &#x27;img&#x27;);
 
 monitorImages( alreadyLoadedImageEntries );
 
 const imgObserver &#x3D; new PerformanceObserver(function(entriesList) {
 const newlyLoadedImageEntries &#x3D; entriesList.getEntriesByType(&#x27;resource&#x27;).filter(({ initiatorType })&#x3D;&gt; initiatorType &#x3D;&#x3D;&#x3D; &#x27;img&#x27;);
   monitorImages( newlyLoadedImageEntries );
 });
 imgObserver.observe({entryTypes: [&quot;resource&quot;]});
 
 
 
 Despite deferring our monitoring script until it was out of the critical path, we are capturing the data for all images which have loaded before our script and will continue to monitor them, as the user continues using the site.
 Solution 3: Inspecting the HTML Doc
 The final example solution we will look at is related to the ultimate “before JS can run” resource — the HTML document itself. If our monitoring solutions are loaded as JS via the HTML, how can we monitor the delivery of the HTML document?
 There is some precedence in monitoring HTML doc delivery. For monitoring response data, the most common setup is to use server logs/metrics/traces to capture this information. That is a good solution but depending on the tooling, the data may be decoupled from RUM data causing us to need multiple tools to inspect our user experiences. Additionally, this practice could also miss metadata (page instance identifiers for example) that allows us to aggregate and correlate information for a given page load — for example, correlating async requests failing when the doc returns certain document response codes.
 A common pattern for doing this work is putting the content inside of the HTML content itself. This has to be put into the HTML content, because the JS-based monitoring logic does not have access to the HTML request headers that came before it. This turns our HTML document into a dynamic document content. This may be fine for our needs and allows us to take that information and provide it to our RUM tooling. However, this could become a challenge if our system for HTML delivery is out of our control, or if the system has some assumptions into how HTML delivery must function. Examples of this might be, expecting that the HTML is fully static, such that we can cache it downstream in some deterministic manner — “partially dynamic” HTML bodies are much more likely to be handled incorrectly by caching logic. 
 Within the HTML delivery process, there could also be additional data that we want to understand, such as what datacenters processed the request throughout the chain. We might have a CDN edge handler that proxies a request from an origin. In this case, we can’t expect each layer could/should process and inject HTML content. How might Server-Timing headers help us here?
 Building on the concepts of Solution #1 and Solution #2, here’s how we can capture valuable data about the HTML document itself. Keep in mind that any part of the stack can add a Server-Timing header to the response, and it will be joined together in the final header value.
 Let’s assume we have a CDN edge handler and an origin which can process the document:
 CDN added response headers:
 
 Status: 200
 …
 Server-Timing: cdn_status_code; dur&#x3D;200;, cdn_cache; desc&#x3D;”expired”; dur&#x3D;15; cdn_datacenter; desc&#x3D;”ATL”; cdn_req_id; desc&#x3D;”zyx321abc789”; cdn_time; dur&#x3D;120;
 
 
 
 Origin added response headers:
 
 Status: 200
 …
 Server-Timing: origin_status_code; dur&#x3D;200;, origin_time; dur&#x3D;30; origin_region; desc&#x3D;”us-west”; origin_req_id; desc&#x3D;&quot;qwerty321ytrewq789&quot;;
 
 
 
 Inspecting the HTML response information:
 
 // as mentioned earlier, the HTML document is a &#x27;navigation&#x27; type of Performance Entry
 // that has a superset of information related to the resource and the navigation-specific info
 const htmlPerfEntry &#x3D; performance.getEntriesByType(&#x27;navigation&#x27;)[0];
 
 // filter/capture entry data as needed
 console.log(htmlPerfEntry.serverTiming);
 
 // outputs:
 // [
 //   { name: “cdn_status_code”, description: undefined, duration: 200 },
 //   { name: “cdn_cache”, description:”expired”, duration: 0.0},
 //   { name: “cdn_datacenter”, description:”ATL”, duration: 0.0 },
 //   { name: “cdn_req_id”, description:”zyx321abc789”, duration: 0.0 },
 //   { name: “cdn_time”, description: undefined, duration: 120 },
 //   { name: “origin_status_code”, description: undefined, duration: 200 },
 //   { name: “origin_time”, description: undefined, duration: 30 },
 //   { name: “origin_region”, description:”us-west”, duration: 0.0 },
 //   { name: “origin_req_id”, description:”qwerty321ytrewq789”, duration: 0.0 },
 // ]
 
 
 From this information, our monitoring JavaScript (which could have been loaded way later) can aggregate where the HTML processing happened, status codes from the different servers (which can differ for legit reasons — or bugs), and request identifiers if they need to correlate this with server logs. It also knows how much time was taken on the “server” via the cdn_time duration — “server” time being the total time starting at the first non-user proxy/server that we provide. Using that cdn_time duration, the already accessible HTML Time-To-First-Byte value and the origin_time duration, we can determine latency sections more accurately, such as the user latency, the cdn to origin latency, etc. This is incredibly powerful in optimizing such a critical delivery point and protecting it from regression.
 Combining Server-Timing with Service Workers
 Service Workers are scripts that are initialized by the website to sit between the website, the browser, and the network (when available). When acting as a proxy, they can be used to read and modify requests coming from and responses returning to the website. Given service workers are so feature rich, we won’t attempt to cover them in depth in this article — a simple web search will yield a mountain of information about their capabilities. For this article, we will focus on the proxying capability of a service worker — it’s ability to process requests/responses. 
 The key to combining these tools is knowing that the Server-Timing header and its respective PerformanceEntry is calculated after service worker proxying takes place. This allows us to use the service workers to add Server-Timing headers to responses that can provide valuable information about the request itself.
 What type of information might we want to capture within the service worker? As mentioned before, service workers have lots of capabilities, and any one of those actions could produce something valuable to capture. Here are a few that come to mind:
 
 Is this request served from the service worker cache?
 Is this served from the service worker while off-line?
 What service worker strategy for this request type is being used?
 What version of the service worker is being used?This is helpful in checking our assumptions about service worker invalidation.
 Take values from other headers and put them into a Server-Timing header for downstream aggregation.Valuable when we don’t have the option to change the headers on the request but would like to inspect them in RUM — such is usually the case with CDN providers.
 How long has a resource been in service worker cache?
 
 Service workers have to be initialized on the website which is an asynchronous process itself. Furthermore, service workers only process requests within the defined scope. As such, even the basic question of, “is this request processed by the service worker?” can drive interesting conversations on how much we are leaning on its capabilities to drive great experiences.
 Let’s dive into how this might look in the code.
 Basic JS logic used on the site to initialize the service worker:
 
 if (&#x27;serviceWorker&#x27; in navigator) {
 navigator.serviceWorker.register(&#x27;/service-worker.js&#x27;).then(function (registration) {
 registration.update(); // immediately start using this sw
  });
 }
 
 
 Inside of /service-worker.js, basic request/response proxying:
 
 const CACHE_NAME &#x3D; &#x27;sw-cached-files-v1&#x27;;
 
 self.addEventListener(&#x27;fetch&#x27;, function (event) {
   event.respondWith(
     // check to see if this request is cached
     caches.match(event.request)
       .then(function (response) {
 
         // Cache hit - return response
         if (response) {
           const updatedHeaders &#x3D; new Headers(response.headers);
           updatedHeaders.append(&#x27;Server-Timing&#x27;, &#x27;sw_cache; desc&#x3D;&quot;hit&quot;;&#x27;);
           const updatedResponse &#x3D; new Response(response.body, {
             ...response,
             headers: updatedHeaders
           });
           return updatedResponse;
         }
 
         return fetch(event.request).then(function (response) {
 
             // depending on the scope where we load our service worker,
             // we might need to filter our responses to only process our
             // first-party requests/responses
             // Regex match on the event.request.url hostname should
 
             const updatedHeaders &#x3D; new Headers(response.headers);
             updatedHeaders.append(&#x27;Server-Timing&#x27;, status&amp;#95;code;desc&#x3D;${response.status};, sw&amp;#95;cache; desc&#x3D;&quot;miss&quot;;)
 
             const modifiableResponse &#x3D; new Response(response.body, {
               ...response,
               headers: updatedHeaders
             });
 
             // only cache known good state responses
             if (!response || response.status !&#x3D;&#x3D; 200 || response.type !&#x3D;&#x3D; &#x27;basic&#x27; || response.headers.get(&#x27;Content-Type&#x27;).includes(&#x27;text/html&#x27;)) {
               return modifiableResponse;
             }
 
             const responseToCache &#x3D; modifiableResponse.clone();
 
             caches.open(CACHE_NAME).then(function (cache) {
               cache.put(event.request, responseToCache);
             });
 
             return modifiableResponse;
           }
         );
       })
   );
 });
 
 
 Requests that are processed from the service worker, now will have a Server-Timing header appended to their responses. This allows us to inspect that data via the Performance Timeline API, as we demonstrated in all of our prior examples. In practice, we likely didn’t add the service worker for this single need — meaning we already have it instrumented for handling requests. Adding the one header in 2 places allowed us to measure status codes for all requests, service worker-based cache-hit ratios, and how often service workers are processing requests.
 Why Use Server-Timing If We Have Service Workers?
 This is an important question that comes up when discussing combining these techniques. If a service worker can grab all of the header and content information, why do we need a different tool to aggregate it? 
 The work of measuring timing and other arbitrary metadata about requests is almost always, so that we can send this information to a RUM provider for analysis, alerting, etc. All major RUM clients have 1 or 2 windows for which we can enrich the data about a request — when the response happens, and when the PerformanceEntry is detected. For example, if we make a fetch request, the RUM client captures the request/response details and sends it. If a PerformanceEntry is observed, the client sends that information as well — attempting to associate it to the prior request if possible. If RUM clients offer the ability to add information about that requests/entries, those were the only windows to do it. 
 In practice, a service worker may or may not be activated yet, a request/response may or may not have processed the service worker, and all service worker data sharing requires async messaging to the site via postMessage() API. All of these aspects introduce race conditions for a service worker to be active, able to capture data, and then send that data in time to be enriched by the RUM client.
 Contrasting this with Server-Timing, a RUM client that processes the Performance Timeline API will immediately have access to any Server-Timing data set on the PerformanceEntry.
 Given this assessment of service worker’s challenges with enriching request/response data reliably, my recommendation is that service workers be used to provide more data and context instead of being the exclusive mechanism for delivering data to the RUM client on the main thread. That is, use Server-Timing and, where needed, use service worker to add more context or in cases where Server-Timing isn’t supported — if required. In this case, we might be creating custom events/metrics instead of enriching the original request/response data aggregation, as we will assume that the race conditions mentioned will lead to missing the windows for general RUM client enrichment.
 Considerations for Server-Timing Usage
 As uniquely powerful as it is, it’s not without important considerations. Here’s a list of considerations based on the current implementation at time of writing:
 
 Browser Support — Safari does not support putting the Server-Timing data into the Performance Timeline API (they do show it in DevTools).This is a shame, however, given this is not about functionality for users, but instead it’s about improved capabilities for performance monitoring — I side with this not being a blocking problem. With browser-based monitoring, we never expect to measure 100% of browsers/users. Currently, this means we’d look to get ~70-75% support based on global browser usage data. Which is usually more than enough to feel confident that our metrics are showing us good signals about the health and performance or our systems. As mentioned, Server-Timing is sometimes the only way to get those metrics reliably, so we should feel confident about leveraging this tool.As mentioned previously, if we absolutely have to have this data for Safari, we could explore using a cookie-based solution for Safari users. Any solutions here would have to be tested heavily to ensure they don’t hinder performance. 
 If we are looking to improve performance, we want to avoid adding lots of weight to our responses, including headers. This is a trade-off of additional weight for value added metadata. My recommendation is that if you’re not in the range 500 bytes or more to your Server-Timing header, I would not be concerned. If you are worried, try varying lengths and measure its impact!
 When appending multiple Server-Timing headers on a single response, there is a risk of duplicate Server-Timing metric names. Browsers will surface all of them in the serverTiming array on the PerformanceEntry. It’s best to ensure that this is avoided by specific or namespaced naming. If it can’t be avoided, then we would break down the order of events that added each header and define a convention we can trust. Otherwise, we can create a utility that doesn’t blindly add Server-Timing entries but will also update existing entries if they are already on the Response.
 Try to avoid the mistake of misremembering that responses cache the Server-Timing values as well. In some cases you might want to filter out the timing-related data of cached responses that, before they were cached, spent time on the server. There are varying ways to detect if the request went to the network with data on the PerformanceEntry, such as entry.transferSize &gt; 0, or entry.decodedBodySize &gt; 0, or entry.duration &gt; 40. We can also lean into what we’ve learned with Server-Timing to set a timestamp on the header for comparison.
 
 Wrapping Up
 We’ve gone pretty deep into the application of the Server-Timing Header for use cases that aren’t aligned to the “timing” use case that this header is generally associated with. We’ve seen its power to add freeform data about a resource and access the data without needing a reference to the networking API used to make it. This is a very unique capability that we leveraged to measure resources of all types, inspect them retroactively, and even capture data about the HTML doc itself. Combining this technique with service workers, we can add more information from the service worker itself or to map response information from uncontrolled server responses to Server-Timing for easy access.
 I believe that Server-Timing is so impressively unique that it should be used way more, but I also believe that it shouldn’t be used for everything. In the past, this has been a must-have tool for performance instrumentation projects I’ve worked on to provide impossible to access resource data and identify where latency is occuring. If you’re not getting value out of having the data in this header, or if it doesn’t fit your needs — there’s no reason to use it. The goal of this article was to provide you with a new perspective on Server-Timing as a tool to reach for, even if you’re not measuring time. 
 Resources
 
 W3C Server Timing
 Server-Timing MDN
 “Measuring Performance With Server Timing”, Drew McLellan
 Performance Timeline MDN
 </content>
     </entry>
     <entry>
       <title>Doge&#x27;d a bullet</title>
         <link href="https://daverupert.com/2022/05/doge-d-a-bullet/"/>
       <updated>2022-05-13T15:06:00.000Z</updated>
       <content type="text">It’s a secret to everyone! This post is for RSS subscribers only.
 Read more about RSS Club.
 
 Two years ago, Easter of 2020, I was chatting with some friends about crypto and one friend was starting to set aside some money and invest. Seemed harmless to do with “fun money”. We were in a pandemic after all, so no one was doing anything anyways. This was the height of the Gamestop and $DOGE meme stock market drama. I dismissed crypto at the time, but I used my Stocks app to follow the prices of $BTC, $ETH, $SOL, $DOGE, $SHIB, $LUNA, $XRP, $XTZ, and a some others that crossed my periphery field of view.
 You see, it’s not hard to sell me on crypto. “Big banks, payday loans, and fees are problematic” &#x3D; yes. “We can decentralize identity and web logins” &#x3D; yes, cool. “We can decentralize payment” &#x3D; yes, even cooler. “DAOs let communities have ownership in the projects they like” &#x3D; yes, in theory1. At its core, even the basic idea of a sci-fi reality where we zlorp our space credits to each other on our holo-visors seems like a probable eventuality.
 From Easter 2020 to Easter 2021 I watched in my Stocks app as the price of $ETH went from $180 to $2230 (+1,288%)  and $SOL went from $0.60 to $31.70 (+5,221%). Dang. Regret. I should have bought. But two weeks later I watched as both of those coins lost 50% of their value overnight. Phew. Vindication.
 The price of course rebounded and climbed 10× around November 2021. Dang. Regret. This was the peak NFT era, it seemed crypto was approaching a critical mass. Actual smart people I knew were holding crypto. Actual smart people I know were leaving their FAANG jobs to join Web3 projects. Every conversation I had ended up talking about crypto. The FOMO was building.
 I somehow convinced myself that I needed to go in and go in big. If my $500 turned into $600 or even $1000, that doesn’t materially change my life that much. It’s not “Buy a boat” money. It had to be a large number. So I started pooling together about $10,000 to spread over a handful of coins that I thought had a strategic advantage:
 
 $ETH - smart contracts are pretty smart
 $SOL - a greener alternative to $ETH
 $LUNA - backed by a “stablecoin” and that sounded nice
 $DOGE - for the memes but it goes up every time Elon tweets about it and you can buy a lot of it for cheap
 
 At the pace crypto was going I was confident I could double my money over the course of a couple months. I needed to pull the trigger.
 After gut-checking with friends (who were like “Sure, man”), my wife (who to this day says, “I don’t get it”), my financial advisor (who was like 😬), and doing my own research… I decided not to pull the trigger. The lack of consumer protections, the surface area for criminal activity (scams, theft, laundering, wash trading), the way your cash-on-hand is also your investment portfolio, the fake scarcity, the rich getting richer, the low-quality art in the form of monkey JPGs selling at inflated prices, gas fees, and the carbon costs… those all bothered me immensely. Ultimately I felt like crypto was a 2010 solution (electricity is free!) that didn’t fit 2021 problems (we are soon approaching the 1.5ºC thresh hold!).
 Instead of investing actual money, I installed a Stock Market Simulator on my iPad and played pretend crypto. If my imaginary numbers went up, I could revisit the idea. But over the course of a couple months, I watched the numbers turn redder and redder.
 
 As of writing this: $ETH is down 58%, $SOL is down 82%, $LUNA has lost 99.97% of its value. I’m not some genius investor that shorted the entire crypto market and made millions, but I keep thinking about how I dodged a bullet and saved about $8000 by not investing in crypto. Phew. Vindication.
 It’s all relative. If I had bought two years ago, I’d still be up a bit. A lotta bit actually. But back then I would have invested something small like $500. Still, my $500 of imaginary $SOL would be worth $44,000. Dang. Regret. That’s incredible, yet all feels so made up. It’s like how Uber is a taxi company with no cars, crypto is an economy with no tangible goods and services.
 I could still end up with egg on my face here. The minute the market drops 10~20% and I feel vindicated about my decision to hold off, some pump’n’dump happens to spike the price up +20%. And for some reason my lizard brain still says “Dang. Regret.” Weird how that works.
 Anyways. I reserve the right to change my mind at any time.
 
 
 
 DAOs we’re almost the selling point for me on crypto, but I now sort of feel like most DAOs reinforce oligarchies rather than enable democracies. ↩
 
 
 </content>
     </entry>
     <entry>
       <title>Top Tasks: To Focus On What Matters You Must De-Focus On What Doesn’t</title>
         <link href="https://smashingmagazine.com/2022/05/top-tasks-focus-what-matters-must-defocus-what-doesnt/"/>
       <updated>2022-05-13T11:30:00.000Z</updated>
       <content type="text">Let us start with a simple explanation of the word “task”. A task is something someone wants to do using your website or app. If you have a technology website, then tasks might include pricing, installation, and troubleshooting. If you have a university website, then tasks might include courses, lecturer/professor profiles, and accommodation. If you have a hospital website, then tasks might include what to do: before treatment, during treatment, and after treatment. If you are running an intranet, then tasks might include training, finding people, pay, and benefits. If you have a website for vaccines, then tasks might include immunity, side effects, and availability.
 Now, “Top Tasks” focus on the task itself. Once identified, the next steps include measuring whether people are successful at completing the Top Tasks, and how long it takes them. You’re always trying to improve completion rates and reduce completion times for Top Tasks. It’s about focusing on the outcome from a user’s perspective, rather than the input (content, code, design).   
 When Should You Consider Using Top Tasks?
 
 There’s no agreement about what’s most important, and it has been decided that it’s time for some consensus, some unity of purpose and design.
 There’s a need to consolidate and simplify. Too many websites, too many features, too much content. Top Tasks will help you create a single, streamlined, unified approach.
 There is a lot of out-of-date, low-quality content. This tiny task content is cluttering the search and navigation, and you need evidence in order to remove it.
 The navigation is terrible, because it is not focused on top tasks, and instead it is focused on organizational units, systems, ego projects, and vague meaningless terms, such as “Resources”.
 Nobody really knows why they’re doing what they’re doing, whether it really has any value or not. The metrics are all about volume, and chasing volume is not delivering quality or value. Top Tasks is about focusing on what really matters. 
 
 It Starts With a Survey
 The most essential element of Top Tasks is a survey that identifies a prioritized list of tasks, from the most important tasks (the top tasks) to the least important tasks (the tiny tasks). 
 
 The results from a Top Tasks survey can be used to:
 
 prioritize features and content on a website or app;
 de-prioritize or remove tiny task features and content;
 design a more effective classification and navigation;
 implement a metrics model based on measuring the success rate and the time it takes to complete the top tasks.
 
 Designing for Top Tasks is about designing for the long term because tasks last. As humans, we do the same core things day in and day out, year in and year out. The first major Top Tasks project I did was back around 2003. It was for a national tourism website, and we ran surveys in multiple countries. The Top Tasks were: 
 
 Special Offers,
 Accommodation,
 Things to Do &amp; See,
 Planning a Trip,
 Getting Here &amp; Around.
 
 In 2020, people still want special offers. They still want a five-star hotel at a two-star price. The tools to help you find that great offer have changed a lot, but the basic top task of wanting a great deal hasn’t and will not change, because deep down we’re all cheap. Another core characteristic of humans is, of course, that we don’t like to be seen as cheap. 
 When I was carrying out the Top Tasks tourism survey research, I noticed some interesting patterns when it came to search. People weren’t searching so much for “special offers”. Rather, they were searching for “deals”. This is an interesting aspect of search behavior. Sometimes, the words that bring you to a website are not the words that bring you through a website. We found that while people searched for “deals”, they clicked far more on links that said “special offers” once they were on the website. Makes sense. You may search for a “cheap hotel”, but you don’t necessarily want to be greeted on the hotel homepage with a message saying: “Welcome to our dirt cheap hotel!”
 Many organizations have said to me that they already know their top tasks because they’ve got search and website analytics. Search and website analytics are simply not sufficient to give you true and comprehensive insights into the top tasks. For starters, they will rarely give you insight on tasks that you have no content on. 
 Once we did a Top Tasks for a university that specialized in business master’s degrees. A key task that emerged was that potential students wanted to know how doing the master’s would “advance their career”. Nobody was searching to “advance my career”. 
 The Microsoft Excel Web team noticed that lots of people were searching for “remove conditional formatting”. So, they created a page for this. But the page always got huge dissatisfaction, no matter how much they edited it. Finally, after much research and discussion, they discovered that removing conditional formatting was a symptom. The real task — the true task — was about how to use conditional formatting properly. Top Tasks is about getting to these deeper, truer tasks. Doing it well requires a lot of research, a lot of thinking, and a lot of talking and discussing. 
 The Top Tasks methodology is a holistic, 360º examination of the entire physical and digital environment. It goes deep and comprehensive, looking at the entire environment that the user exists within, not simply a website or app. So, if you’re looking for a quick fix, if you’re thinking about the low-hanging fruit, stop reading. Top Tasks is most definitely not for you.
 Still here? Top Tasks is hard, boring, systematic, rigorous, comprehensive, highly collaborative, time-consuming, and profound work. A typical Top Tasks survey takes about 12 weeks to complete:
 
 3-4 weeks to gather an initial list of tasks (typically 200-400).
 3-4 weeks of collaborative discussions to get to a final list of 50-80 tasks.
 2-4 weeks to run the survey. Ideally, you want 400-plus responses to get statistically reliable results, but definitely need a minimum of 100.
 2 weeks to analyze and present results.
 
 Secrets To Success
 The more diverse, the more cross-functional, the more cross-departmental, the more user-facing the group of people you get to do a Top Tasks project, the better. If things are working well, you’re going to have deep, almost existential conversations about what is a task, which tasks are relevant, and which are not. If, for example, you were doing a Top Tasks about COVID-19, you’d be examining the total environment, not simply the stuff you have now on your website or app about COVID-19. You’d be trying to get a total map of a pandemic through a series of tasks. 
 What’s the best group?
 
 Between three and eight people ideally.
 Broad group of representative stakeholders.
 Genuine experienced people who truly understand human needs.
 If you’re doing an intranet Top Tasks survey, you should have representatives from Support, Marketing, Sales, Products, Web, IT, HR, and Communications.
 If you’re doing a public Top Tasks survey, you should have representatives from Support, Marketing, Sales, Products, Web, and Communications.
 Aim to have the same group throughout the entire Top Tasks process.
 
 I know. I know. I know. Getting this group together is time-consuming and very complicated. If you’re finding it almost impossible, then that’s a message. Your organization is probably not ready for Top Tasks, because if people won’t join the group, then the chances of them or their departments taking the results seriously are very, very low.
 Here are the benefits of spending the time and effort to get a great shortlisting group together:
 
 more rounded and comprehensive task list;
 a much deeper understanding of the organization;
 less likely that tiny tasks from any one department will be over-represented;
 bridge-building and future collaboration;
 identifying tasks that have different names in different departments;
 identifying duplicate content and tools across departments/units;
 greater chance of buy-in and real change when the results are delivered.
 
 When we did a Top Tasks for WHO about COVID-19 in May 2020, here are the top tasks that emerged:
 
 vaccine (development, availability, safety);
 latest news, latest research (alerts, directives, updates);
 transmission, spread, epidemiology;
 immunity, antibody testing (criteria, availability, accuracy);
 WHO guidelines, standards, decisions;
 symptoms, signs;
 research papers, studies;
 end date, new normal, safe again;
 virus survival/viability/persistence on surfaces, in air.
 
 How did we get to a result like that? The first step is to gather the potential list of tasks. Here are some sources:
 
 existing website / app;
 top 50 annual search (internal, external);
 top 50 website pages / app sections most visited;
 surveys and research on customers in the last two to three years;
 top 50 annual support, help, feedback;
 competitor / peer websites / apps;
 social media, blogs, communities.
 
 For example, when we did Microsoft Visual Studio, we got lots of great tasks by visiting the independent developer communities. Depending on the complexity of the environment, the initial longlist of tasks can range from 200 to 400. In exceptional circumstances, such as when we did a Top Tasks for the European Commission, it ran into thousands. 
 Below is a tiny sample of these early tasks for a healthy environment:
 
 access to my journal;
 advice in cases of physical disability;
 advice on treatment or hospitals abroad;
 advice, guidance, rights, and practical information to relatives;
 aids (prostheses, wigs, etc.);
 aids in the hospital/treatment site;
 alternative treatment;
 analysis and testing of organic food;
 analysis of food safety (pesticide, dioxin...).
 
 You can see that there’s a lot of work to be done to get from this rough list of hundreds to a refined list of 50-80.
 Shortlisting: the Hardest Part
 Words are by far the most powerful — and often most underrated — invention humans have ever made. Without words, there is no Web. The Web is built from words, from the first search to the last click. Getting the words exactly right, and organizing them in the most intuitive way can prove so challenging that many organizations simply ignore the challenge or address it in the most flippant, derisory manner. 
 Tasks are the things people want to do on your website or app. More than anything else, they’ll use words to help them complete that task. The shortlisting process will be super intense because it’s a process of editing words. But if done right, it will be super rewarding. It may, in fact, be the first time your organization has come together to discuss and agree on what it is you are about, and what it is you are genuinely useful for. This is a quite profound process.
 Getting from a long list to a short list will usually take between two and four weeks. That’s roughly five to eight sessions with the shortlisting group, each session lasting 90 minutes. For each of these sessions, you will have to do an equivalent preparatory session. Have no more than three sessions a week, because this is intense work. Also, it’s good to let the list settle — it will mature and become clearer over time. The longlist shouldn’t be more than 150 for the first shortlisting group session. What this means is that you will probably have to do quite a bit of preparatory work cleaning up the initial list of 200-400 tasks, so that it is ready for that first session.
 The art of shortlisting is about slowly stripping away words until all that’s left are the most essential words that describe the most essential tasks. Here are some shortlisting tips:
 
 Avoid verbs (get, find). Nouns make the best tasks. Verbs are waffly and vague. You don’t need “Find a job”. All you need is “Jobs”.
 Avoid statements (I want to…).
 Avoid questions (Can I…).
 Remove conjunctions. From “Installation and configuration” to “Installation, configuration”.
 Use brackets where necessary: “Conservation (ecology, nature, woodlands)”.
 Keep each task under 65 characters (8-10 words).
 Avoid first-word repetition (no more than four in any list).
 European Commission at work
 European Commission competition
 European Commission directory
 
 
 Delete exact duplicates.
 Remove overlaps. This gets hard. Aim for a list of unique tasks, each one separate and distinct from the others. 
 Remove brands, products, departments, and subjects.
 University: no subjects, courses (English, Computer Science, Law, etc.)
 Intranet: no departments (HR, IT, Accounting)
 Government: no departments, brands
 Company: no products, brands
 Healthcare: no diseases, conditions
 
 
 No audiences, demographics.    
 Don’t have:  
 Women’s health
 Men’s health
 
 
 Don’t have:  
 Development policy for Austria
 Development policy for Ireland  
 
 
 Don’t have:  
 Training for developers
 Training for testers
 
 
 
 
 Avoid formats and channels.
 No formats: reports, newsletters, documents, tools, videos, forms, templates
 No channels: Twitter, Facebook, YouTube, Instagram
 Avoid Web conventions: search, navigation, pages
 
 
 No Dirty Magnets. A dirty magnet is a word or phrase that can mean different things to different people. It is vague and meaningless and can draw attention because it seems like it might be useful. Examples of dirty magnets include Resources; Knowledge Base; Quick Links, and Useful Links. By far, the worst dirty magnet of all is Frequently Asked Questions, which in my experience is the single worst Web design feature I have come across in more than 25 years of helping design websites.
 
 Preparing the Survey
 No. The answer is no, you can’t. Just no. You can’t break up the list. You’re not allowed. It’s not how it works. One single list. One. Randomly presented. You ask people to quickly scan the list and choose no more than five of their top tasks from the list. 
 The amount of times designers and researchers have said to me, “But people aren’t going to vote on a list that long. It’s too long. We have to break it up.” No. The whole idea and concept of Top Tasks is a single list that delivers a single table of tasks ranked from the one that got the most votes to the one that got the least. If you break up the list, it’s no longer Top Tasks. It’s worked over 600 times. More than half a million people have voted in multiple languages in over 120 countries in the world. Since 2003, I have never had one single instance where it’s failed. Not one. 
 You will need a framing statement to introduce the task list. When we did a survey on COVID-19 vaccination, the framing statement for the list was:
 In relation to the COVID-19 vaccine, select up to 5 things that are MOST IMPORTANT to you.
 
 When you’re writing the framing statement, try not to focus on your website or app. Focus on the world of people. Make it about the vaccine and what’s important to them, not your website and what’s important to you.
 The survey typically has three parts:
 
 the top tasks voting;
 segmentation and demographic questions (role, gender, location);
 website/app experience questions.
 
 
 Even after 30 voters, you should expect to see the top tasks begin to emerge. For example, the largest survey we ever did was for the European Commission. We had over 107,000 people voting. The top three tasks were:
 
 EU law, rules, treaties, judgments;
 research and innovation;
 funding, grants, subsidies.
 
 
 Amazingly, after just 30 voters, these top three tasks had emerged, and they were the same three tasks after 107,000 voters.
 However, the more people you get to vote, the more the table stabilizes. We have found that having 400 voters gets us good solid results. The best way to get people to vote is with a popup on the website/app that is the main focus of the tasks.
 Solve Deep Needs. Design for the Long Term
 There’s nothing cool or fashionable about Top Tasks. It’s a boring, deliberate, methodical process. It’s very much focused on the needs of the user, not the ego of the organization. 
 When a tiny task goes to sleep at night, it dreams of being a top task. Websites are flooded with tiny task content, and apps are flooded with tiny task features. Invariably, when we do a Top Tasks survey, the ego of the organization is at the bottom of the table — it gets the least votes.
 When we did Liverpool City Council in 2009, the top tasks included the following: 
 
 find a job,
 leisure,
 waste,
 libraries,
 schools.
 
 
 
 The team analyzed the publishing activities within the council and found that there was an inverse relationship between the importance of the task to a Liverpudlian and the amount of content being published on that task. The more important the task was to citizens, the less content was being published on it. The less important a task was to citizens, the more content was being published on it.
 Out of some 4,000 pages, 200 were getting 85% of traffic, and these 200 were not getting the proper care and attention, because the Web team’s time was taken up with dealing with tiny tasks. Top Tasks gave the team the evidence to delete over 80% of their website. The result was a much better experience for citizens.
  Top Tasks is about designing for the long term.
 My passion now is sustainable digital design, and that means designing things that will be as light as possible and will last for the longest possible time. The same tasks we identified in 2009 (waste, leisure, schools) are still on the Liverpool homepage in 2022. Top Tasks can give you a classification, navigation, and information architecture that will stand the test of time because top tasks last. The most important things people want to do tend not to change much over time.
 It’s about stripping away and removing the unnecessary, and about defocusing on the tiny tasks. Top Tasks gives you real, quantifiable evidence to make tough decisions that will create a better experience for your users.</content>
     </entry>
     <entry>
       <title>Improved Process Isolation in Firefox 100</title>
         <link href="https://hacks.mozilla.org/2022/05/improved-process-isolation-in-firefox-100/"/>
       <updated>2022-05-12T15:09:10.000Z</updated>
       <content type="text">Introduction
 Firefox uses a multi-process model for additional security and stability while browsing: Web Content (such as HTML/CSS and Javascript) is rendered in separate processes that are isolated from the rest of the operating system and managed by a privileged parent process. This way, the amount of control gained by an attacker that exploits a bug in a content process is limited. 
 Ever since we deployed this model, we have been working on improving the isolation of the content processes to further limit the attack surface. This is a challenging task since content processes need access to some operating system APIs to properly function: for example, they still need to be able to talk to the parent process. 
 In this article, we would like to dive a bit further into the latest major milestone we have reached: Win32k Lockdown, which greatly reduces the capabilities of the content process when running on Windows. Together with two major earlier efforts (Fission and RLBox) that shipped before, this completes a sequence of large leaps forward that will significantly improve Firefox’s security.
 Although Win32k Lockdown is a Windows-specific technique, it became possible because of a significant re-architecting of the Firefox security boundaries that Mozilla has been working on for around four years, which allowed similar security advances to be made on other operating systems.
 The Goal: Win32k Lockdown
 Firefox runs the processes that render web content with quite a few restrictions on what they are allowed to do when running on Windows. Unfortunately, by default they still have access to the entire Windows API, which opens up a large attack surface: the Windows API consists of many parts, for example, a core part dealing with threads, processes, and memory management, but also networking and socket libraries, printing and multimedia APIs, and so on.
 Of particular interest for us is the win32k.sys API, which includes many graphical and widget related system calls that have a history of being exploitable. Going back further in Windows’ origins, this situation is likely the result of Microsoft moving many operations that were originally running in user mode into the kernel in order to improve performance around the Windows 95 and NT4 timeframe. 
 Having likely never been originally designed to run in this sensitive context, these APIs have been a traditional target for hackers to break out of application sandboxes and into the kernel.
 In Windows 8, Microsoft introduced a new mitigation named PROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY that an application can use to disable access to win32k.sys system calls. That is a long name to keep repeating, so we’ll refer to it hereafter by our internal designation: “Win32k Lockdown“.
 The Work Required
 Flipping the Win32k Lockdown flag on the Web Content processes – the processes most vulnerable to potentially hostile web pages and JavaScript – means that those processes can no longer perform any graphical, window management, input processing, etc. operations themselves. 
 To accomplish these tasks, such operations must be remoted to a process that has the necessary permissions, typically the process that has access to the GPU and handles compositing and drawing (hereafter called the GPU Process), or the privileged parent process. 
 Drawing web pages: WebRender
 For painting the web pages’ contents, Firefox historically used various methods for interacting with the Windows APIs, ranging from using modern Direct3D based textures, to falling back to GDI surfaces, and eventually dropping into pure software mode. 
 These different options would have taken quite some work to remote, as most of the graphics API is off limits in Win32k Lockdown. The good news is that as of Firefox 92, our rendering stack has switched to WebRender, which moves all the actual drawing from the content processes to WebRender in the GPU Process.
 Because with WebRender the content process no longer has a need to directly interact with the platform drawing APIs, this avoids any Win32k Lockdown related problems. WebRender itself has been designed partially to be more similar to game engines, and thus, be less susceptible to driver bugs. 
 For the remaining drivers that are just too broken to be of any use, it still has a fully software-based mode, which means we have no further fallbacks to consider.
 Webpages drawing: Canvas 2D and WebGL 3D
 The Canvas API provides web pages with the ability to draw 2D graphics. In the original Firefox implementation, these JavaScript APIs were executed in the Web Content processes and the calls to the Windows drawing APIs were made directly from the same processes. 
 In a Win32k Lockdown scenario, this is no longer possible, so all drawing commands are remoted by recording and playing them back in the GPU process over IPC.
 Although the initial implementation had good performance, there were nevertheless reports from some sites that experienced performance regressions (the web sites that became faster generally didn’t complain!). A particular pain point are applications that call getImageData() repeatedly: having the Canvas remoted means that GPU textures must now be obtained from another process and sent over IPC. 
 We compensated for this in the scenario where getImageData is called at the start of a frame, by detecting this and preparing the right surfaces proactively to make the copying from the GPU faster.
 Besides the Canvas API to draw 2D graphics, the web platform also exposes an API to do 3D drawing, called WebGL. WebGL is a state-heavy API, so properly and efficiently synchronizing child and parent (as well as parent and driver) takes great care. 
 WebGL originally handled all validation in Content, but with access to the GPU and the associated attack surface removed from there, we needed to craft a robust validating API between child and parent as well to get the full security benefit.
 (Non-)Native Theming for Forms
 HTML web pages have the ability to display form controls. While the overwhelming majority of websites provide a custom look and styling for those form controls, not all of them do, and if they do not you get an input GUI widget that is styled like (and originally was!) a native element of the operating system.
  Historically, these were drawn by calling the appropriate OS widget APIs from within the content process, but those are not available under Win32k Lockdown. 
 This cannot easily be fixed by remoting the calls, as the widgets themselves come in an infinite amount of sizes, shapes, and styles can be interacted with, and need to be responsive to user input and dispatch messages. We settled on having Firefox draw the form controls itself, in a cross-platform style. 
 While changing the look of form controls has web compatibility implications, and some people prefer the more native look – on the few pages that don’t apply their own styles to controls – Firefox’s approach is consistent with that taken by other browsers, probably because of very similar considerations.
 Scrollbars were a particular pain point: we didn’t want to draw the main scrollbar of the content window in a different manner as the rest of the UX, since nested scrollbars would show up with different styles which would look awkward. But, unlike the rather rare non-styled form widgets, the main scrollbar is visible on most web pages, and because it conceptually belongs to the browser UX we really wanted it to look native. 
 We, therefore, decided to draw all scrollbars to match the system theme, although it’s a bit of an open question though how things should look if even the vendor of the operating system can’t seem to decide what the “native” look is.
 Final Hurdles
 Line Breaking
 With the above changes, we thought we had all the usual suspects that would access graphics and widget APIs in win32k.sys wrapped up, so we started running the full Firefox test suite with win32k syscalls disabled. This caused at least one unexpected failure: Firefox was crashing when trying to find line breaks for some languages with complex scripts. 
 While Firefox is able to correctly determine word endings in multibyte character streams for most languages by itself, the support for Thai, Lao, Tibetan and Khmer is known to be imperfect, and in these cases, Firefox can ask the operating system to handle the line breaking for it. But at least on Windows, the functions to do so are covered by the Win32k Lockdown switch. Oops!
 There are efforts underway to incorporate ICU4X and base all i18n related functionality on that, meaning that Firefox will be able to handle all scripts perfectly without involving the OS, but this is a major effort and it was not clear if it would end up delaying the rollout of win32k lockdown. 
 We did some experimentation with trying to forward the line breaking over IPC. Initially, this had bad performance, but when we added caching performance was satisfactory or sometimes even improved, since OS calls could be avoided in many cases now.
 DLL Loading &amp; Third Party Interactions
 Another complexity of disabling win32k.sys access is that so much Windows functionality assumes it is available by default, and specific effort must be taken to ensure the relevant DLLs do not get loaded on startup. Firefox itself for example won’t load the user32 DLL containing some win32k APIs, but injected third party DLLs sometimes do. This causes problems because COM initialization in particular uses win32k calls to get the Window Station and Desktop if the DLL is present. Those calls will fail with Win32k Lockdown enabled, silently breaking COM and features that depend on it such as our accessibility support. 
 On Windows 10 Fall Creators Update and later we have a fix that blocks these calls and forces a fallback, which keeps everything working nicely. We measured that not loading the DLLs causes about a 15% performance gain when opening new tabs, adding a nice performance bonus on top of the security benefit.
 Remaining Work
 As hinted in the previous section, Win32k Lockdown will initially roll out on Windows 10 Fall Creators Update and later. On Windows 8, and unpatched Windows 10 (which unfortunately seems to be in use!), we are still testing a fix for the case where third party DLLs interfere, so support for those will come in a future release.
 For Canvas 2D support, we’re still looking into improving the performance of applications that regressed when the processes were switched around. Simultaneously, there is experimentation underway to see if hardware acceleration for Canvas 2D can be implemented through WebGL, which would increase code sharing between the 2D and 3D implementations and take advantage of modern video drivers being better optimized for the 3D case.
 Conclusion
 Retrofitting a significant change in the separation of responsibilities in a large application like Firefox presents a large, multi-year engineering challenge, but it is absolutely required in order to advance browser security and to continue keeping our users safe. We’re pleased to have made it through and present you with the result in Firefox 100.
 Other Platforms
 If you’re a Mac user, you might wonder if there’s anything similar to Win32k Lockdown that can be done for macOS. You’d be right, and I have good news for you: we already quietly shipped the changes that block access to the WindowServer in Firefox 95, improving security and speeding process startup by about 30-70%. This too became possible because of the Remote WebGL and Non-Native Theming work described above.
 For Linux users, we removed the connection from content processes to the X11 Server, which stops attackers from exploiting the unsecured X11 protocol. Although Linux distributions have been moving towards the more secure Wayland protocol as the default, we still see a lot of users that are using X11 or XWayland configurations, so this is definitely a nice-to-have, which shipped in Firefox 99.
 We’re Hiring
 If you found the technical background story above fascinating, I’d like to point out that our OS Integration &amp; Hardening team is going to be hiring soon. We’re especially looking for experienced C++ programmers with some interest in Rust and in-depth knowledge of Windows programming. 
 If you fit this description and are interested in taking the next leap in Firefox security together with us, we’d encourage you to keep an eye on our careers page.
 Thanks to Bob Owen, Chris Martin, and Stephen Pohl for their technical input to this article, and for all the heavy lifting they did together with Kelsey Gilbert and Jed Davis to make these security improvements ship.
 
 The post Improved Process Isolation in Firefox 100 appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>How To Use Google CrUX To Analyze And Compare The Performance Of JS Frameworks</title>
         <link href="https://smashingmagazine.com/2022/05/google-crux-analysis-comparison-performance-javascript-frameworks/"/>
       <updated>2022-05-12T09:30:00.000Z</updated>
       <content type="text">In recent years, frameworks have taken over web development, and React is leading the charge. These days it is fairly uncommon to encounter a new website or web app that doesn’t rely on some framework, or a platform such as a CMS.
 And while React’s tagline is “a JavaScript library for building user interfaces” rather than a framework, I think that ship has sailed: most React developers consider it to be a framework and use it as such. Or they use it as a part of a larger application framework such as NextJS, Gatsby, or RemixJS.
 But what price do we pay, as web developers, for the improved developer experience provided by such frameworks? And more importantly what price, if any, do our users pay for this choice that we are making?
 Recently Noam Rosenthal published two articles analyzing the common benefits and capabilities provided by various frameworks, and also their associated costs. I highly recommend checking out these articles. One of the costs that Noam mentions is the increased download size, especially JavaScript bundle sizes, that stem from the use of frameworks and other libraries. In particular, the increase in the amount of downloaded JavaScript can have a direct impact on website performance. And there are other aspects of framework usage that can impact performance as well.
 In this article, I will analyze the performance cost associated with various frameworks, based on field data collected by the Google Chrome User Experience Report, or CrUX for short. I think this information is both interesting and useful, in particular given the wide variety of framework and platform choices currently available to front-end and fullstack developers.
 However, when reviewing this data, it’s important not to conflate correlation and causation. Just because webpages built using a particular framework more often have better or poorer performance than another framework doesn’t mean that the framework itself is always at fault. For example, it could be because certain frameworks are more commonly used for building heavier websites.
 That said, this data can assist in making informed decisions about which framework to choose when implementing front-end projects. When possible, I would prefer frameworks that have a higher good performance ratio.
 Collecting Core Web Vitals From The Field
 As I previously mentioned, my primary data source for this analysis is Google CrUX. CrUX is a cloud-based database into which Real User Measurements (RUM) are collected from live Chrome browser sessions. If you have opted-in to syncing browsing history, have not set up a Sync passphrase, and have usage statistics reporting enabled then whenever you load a webpage using Chrome, information about your session is automatically reported to CrUX. In particular, the collected measurements include the three Core Web Vitals metrics measured for each session.
 In recent years, these metrics have become the cornerstone of modern Web performance analysis:
 
 Largest Contentful Paint (LCP),
 First Input Delay (FID),
 Cumulative Layout Shift (CLS).
 
 For each such metric, Google has defined ranges that can be considered good (green), average / needs improvement (orange), and poor (red).
 
 Starting in June 2021, these metrics have become a ranking factor for Google search. This also increases their importance.
 In addition to collecting field data for each such session, synthetic measurements are performed on the websites, using the WebPageTest tool. These lab measurements are collected into another online repository called the HTTP Archive. This includes analyzing which technologies a webpage uses, including which JavaScript frameworks, using the Wappalyzer service.
 Google makes it possible to execute queries on both these collections using its BigQuery project. However, the easiest way to gain insights from this data is to use the Core Web Vitals Technology Report created by Rick Viscomi. (Rick is Staff DevRel Engineer at Google and manages both CrUX and the HTTP Archive.) This report provides a means of interactively graphing performance-related data for various web-based platforms and technologies and easily compares them to each other.
 For this article, I primarily rely on insights gained from analyzing data presented using the Core Web Vitals Technology Report.
 There are a few caveats to note when analyzing this data:
 
 While field data is collected by page, the Technology Report displays it per origin. The origin value is an aggregate of the values of all the pages for that origin, computed as a weighted average based on page traffic.
 On the other hand, the ratios of good origins are not weighted. This means that an origin that has relatively little traffic, but sufficient to be included in the dataset, is counted equally to a very popular, high-traffic origin. This aspect of the computation can be mitigated by filtering origins by popularity ranking
 HTTP Archive only analyzes the homepage for each origin. This means that the framework determination is only made for the home page, and all other pages belonging to that origin are aggregated for it, regardless of if they use it or not, or even if they use some other framework.
 Subdomains are measured as distinct origins.
 
 Comparing CWV of JavaScript Frameworks
 Let’s start by comparing the performance of various JavaScript frameworks. Specifically the ratio of websites built using each of these frameworks that have good (green) scores for all three CWV metrics out of the total sites built using them. Sites that have good scores for all three CWV metrics should provide a better user experience in terms of performance, and are also eligible for the maximum Google search ranking boost.
 I have filtered the data to include only sessions in the USA in order to eliminate the impact of different geographical distributions between the different frameworks. The ALL line in the graphs refers to all websites in CrUX, not just those that use frameworks, and is used as a reference for comparison.
 
 
 Note: Mobile in this case does not include iOS devices, such as iPhone. This is because Chrome on iOS is just a thin wrapper around a Safari core, and does not measure or report CWV. (This is true for all browsers on iOS — they are all just Safari on the inside.)
 First of all, we can see that different frameworks produce noticeably different results. Moreover, for better or worse, these results are mostly stable over the entire past year. (Preact is an exception to this, and I will explain the cause of this variation shortly.) This indicates that scores are meaningful, and not flukes, or results of statistical anomalies.
 Based on these measurements, as Noam Rosenthal’s article indicates, using frameworks does come at a performance cost: by comparing to the ALL baseline we see that websites built using any of these frameworks are generally less likely to have good CWV than websites built without frameworks. While some frameworks like React, Preact and Svelte do come close to the overall average, it is interesting to note that none of the frameworks provide significantly better performance than the others.
 React and Preact are essentially neck and neck — some may be surprised by this given how much smaller Preact is than React: approximately 4KB download vs 32KB (minified and gzipped in both cases). Apparently that 28KB download difference is not so significant in most cases. Jason Miller, the creator of Preact recently had this to say about it:
 Preact isn&#x27;t associated with any specific SSR or serving solutions, which dominate the impact on CWV. While Preact usage may have some correlation to CWV (really only FID), it is nowhere near as direct as tech choices involved in page delivery.— Jason Miller 🦊⚛ (@_developit) February 11, 2022
 
 I’ll inspect the impact of Server-Side Rendering (SSR) and also Static Site Generation (SSG) as page generation/delivery options in more detail later on in this article.
 Vue and AngularJS are both in a second-tier — approximately 20-25% less likely to get good CWV on mobile, and 15-20% less likely on desktop. (Again, excluding iOS.) That said, Vue appears to be gaining ground and slowly reducing the gap with React in terms of performance.
 The big dip in Preact performance is not related to any change in the framework itself or its usage. Rather it has to do with Preact’s identification by Wappalyzer. Unfortunately, this service missed most uses of Preact prior to November 2021. As a result, the earlier results for Preact should be ignored as being invalid.
 Checking Top Sites
 When we limit our view to only the top 1,000,000 sites in the USA (based on traffic) an interesting change is that Vue almost catches up with React because the ratio of sites having good performance using Vue goes up and for React it surprisingly goes down:
 
 
 And we see the same behavior with the top 100,000 sites, with React’s ratio actually dipping lower so that Vue slightly surpasses it. I didn’t try to limit the results even more because usage numbers for some frameworks dropped so low that they were no longer sufficiently statistically significant.
 But looking at the numbers we do have, the fact that Vue performance improves for higher-traffic sites perhaps indicates that achieving good performance with Vue requires greater expertise in that framework than simply being able to use it? This is because higher traffic sites are more likely to be built by organizations that can afford to employ developers that have greater expertise. Moreover, larger organizations can afford to spend more on infrastructure that is optimized for performance.
 On the other hand, React sites actually go down when limiting the number of sites measured by traffic.
 Why is it that React developers with greater expertise are apparently less likely to produce fast loading websites?
 
 Well, that’s a very interesting mystery that I will try to solve. 
 Analyzing Per Metric
 In addition to examining CWV as a whole, the Technology Report also makes it possible to examine each metric individually. Let’s start by looking at FID:
 
 You might have expected that websites using frameworks would pay a penalty in the responsiveness metric, as it’s the one that should be the most impacted by the significant use of JavaScript. But, in practice, this is not the case. In fact, the FID metric is essentially meaningless, with all frameworks achieving a nearly perfect score.
 The same is true even when looking at the aggregation of all websites in the collection. For this reason, Google is researching a better responsiveness metric and is requesting feedback for the experimental metric they’re already testing.
 Examining the LCP metric is much more meaningful:
 
 Interestingly, LCP scores are a strong match for CWV as a whole, but more spread out: ALL, React, Preact, and Svelte are approximately 10% higher, while Vue and AngularJS remain essentially the same. But when we limit to the top 1,000,000 sites we see Preact and Svelte improve some more, but React doesn’t. As a result, Vue catches up with it.
 
 Finally let’s look at CLS, starting with all websites:
 
 And for the top 1,000,000 websites:
 
 Here we see both React and Preact, especially React, going substantially down, resulting in Vue surpassing both of them.
 In other words, for Vue, both the ratio of good LCP and CLS improve when we check top sites only. For React, on the other hand, LCP remains mostly the same, while CLS actually degrades.
 This could indicate that a reason for the performance degradation seen for top sites using React is an increase in the amount of ads on pages. Ads often adversely impact CLS because as they appear they push other content down. However, I don’t think that’s the case because it doesn’t explain the difference in behavior between React and the other frameworks. Also, you would expect ads to impact LCP as well, but we see that LCP remains essentially the same.
 To try to better understand this behavior, let’s check other webpage aspects visualized by the Technology Report.
 Analyzing Additional Webpage Aspects
 In addition to performance scores, the Technology Report enables analysis of resource download sizes. It’s well known that the amount of JavaScript used by a page can have a direct impact on its performance because all the code needs to be downloaded, parsed and executed. Let’s compare the amount of JavaScript downloaded by the various frameworks:
 
 Unsurprisingly, websites that use frameworks download significantly more JavaScript than websites in general. This is due to all the functionality required for Single Page Applications (SPA), such as routing and client-side rendering.
 Also unsurprisingly, websites built using Svelte download less JavaScript than any other of these frameworks. This is due to the Svelte compiler handling at build-time a lot of functionality that other frameworks need to perform at run-time. As a result, Svelte doesn’t need to deploy the code required for such functionality. It’s also possible that developers using Svelte place a greater premium on delivering lean-and-mean webpages than developers using other platforms.
 That said, the 226KB difference between the median for Svelte sites and React sites only translates to a 2.4% increase in the amount of sites that have good CWV. This highlights the amazing improvement that browsers have been able to achieve in handling larger JavaScript payloads, for example by parsing the JavaScript off of the main thread, during the download. I assume that caching, both in the browser and CDNs, contributes to this as well.
 It’s also interesting to note that websites and apps using Preact actually download more JavaScript than those using React. Perhaps these sites chose Preact in an effort to reduce their total weight. In any event, this may explain why we didn’t see noticeable performance improvements for Preact over React: whatever benefits Preact provides are offset by the application code itself.
 When we look at the top 1,000,000 we see that the amount of JavaScript increases, with the interesting exception of Vue.
 
 This may explain why we saw such a significant improvement for Vue for the top sites, compared to the other frameworks. In the case of those other frameworks, it appears that whatever greater expertise the developers working on top sites may have, it does not translate to reduced JavaScript download sizes. Actually, the opposite is true — perhaps due to the extra functionality provided by such websites.
 Another very interesting comparison is the amount of image data downloaded:
 
 Here we see that sites built using React, Svelte and Angular download significantly less image data than websites in general. This may have to do with such sites leveraging modern techniques for reducing image downloads, such as lazy loading and newer image formats. Interestingly, Angular sites download significantly less image data than any other framework.
 Looking at the top sites we see a significant increase in image downloads across the board.
 
 This may have to do with top sites being more content-rich. In particular, top sites are likely to have more ads in them, which are primarily images. And, as we will see, there are other possible explanations as well.
 The Impact Of SSR And SSG
 As Jason Miller stated in the tweet that I previously quoted, technical choices involving webpage delivery have the greatest impact on the CWV metrics, in particular on LCP. Techniques such as Server-Side Rendering (SSR) and Static Site Generation (SSG) deliver contentful HTML to the browser from the get-go, enabling it to display the content immediately as it arrives. This is usually much earlier than visual content can be generated by client-side rendering techniques, especially when the contentful HTML is cached in a CDN or the browser itself. However, properly implementing the required infrastructure for this method of operation can be challenging when using a JavaScript framework. As a result, in recent years we’ve witnessed the introduction of multiple web application frameworks that provide this functionality out-of-the-box for the leading JavaScript frameworks and UI libraries.
 Given all this, we expect websites built using these web application frameworks to have a noticeably higher ratio of good CWV metrics than websites built using just the JavaScript frameworks or libraries.
 To determine if this is indeed the case, I used the Core Web Vitals Technology Report to compare the ratio of websites having good CWV for React, Vue and Svelte in general with the same ratio for the leading web application frameworks that are built on top of them:
 
 We immediately notice that SvelteKit is able to provide much higher performance than everything else. That being said, there are only 33 websites in this dataset that use SvelteKit. This number is too low to draw conclusions regarding its ability to consistently deliver good performance. But it will be interesting to see if its performance holds up as its usage increases.
 We can see that while the Gatsby framework does indeed provide a significantly higher ratio of good CWV than React in general, this is not the case for NextJS. And while NuxtJS does provide a better ratio than Vue in general, that ratio is still lower than for React.
 Also, why did I include Wix in this graph? After all, Wix is not a web application framework built on top of a JavaScript framework. Or is it?
 Actually, Wix is implemented using React, and as a result, every Wix website is also identified as a React website, just like Gatsby and NextJS. In other words, even though you don’t explicitly write React code when using Wix — after all, it’s a drag-and-drop website builder — it does generate a React website for you. Moreover, Wix also employs SSR like Next.js and uses CDN like both NextJS and Gatsby. And it has a higher good performance ratio than either of them.
 Now let’s consider the number of websites built using each one of these technologies:
 
 Not only does Wix significantly outpace the popular web application frameworks, but in fact about a third of React websites in the USA are actually Wix websites!
 This is significant because, at such a high ratio, Wix performance noticeably impacts the performance measured for React websites as a whole. Fortunately, as we saw in the performance graph, Wix actually has a higher ratio of good CWV than React sites in general. In other words, Wix actually raises the performance scores measured for React.
 But when we limit to the top 1,000,000 websites in the USA the ratios change substantially:
 
 The ratio of Wix and all other web application frameworks out of the total React websites drop significantly when looking only at the top 1,000,000 websites. In this dataset, only 14% of React sites are built with Wix. This means that Wix’s impact on React’s general performance is much reduced when looking only at top sites. This is a significant reason why React’s good performance ratio actually degrades when inspecting only the top sites, unlike the other JavaScript frameworks.
 In other words, Wix is the solution to the mystery of React’s unexpected performance profile. 
 Performance Metrics For Web Application Frameworks
 But what about NextJS and NuxtJS? Why don’t they provide the expected performance benefits across the board that we see with Gatsby (and also with Wix)? Analyzing each metric individually may reveal the root causes for this behavior.
 As before, the FID metric has essentially no impact on the overall performance ratio as all frameworks achieve a good FID ratio above 97%.
 Things get more interesting when we compare CLS ratios:
 
 We can see that NextJS actually surpasses React, but that Gatsby is still ahead. Also, NuxtJS is smack in the middle between Vue and React.
 Since all these frameworks have essentially the same good FID ratios and close good CLS ratios, this indicates that the main cause of the difference between these frameworks is LCP:
 
 As expected we see that Gatsby is significantly ahead of React, and also that React in general is ahead of Next.js. Given that NextJS utilizes SSR / SSG and modern image formats, this is surprising. Certainly, this is something to watch out for when utilizing that framework.
 NuxtJS has made significant progress in this regard in recent months and has surpassed NextJS for good LCP which is essentially the same as React in general.
 Let’s see if the LCP differences can be explained by the image download sizes since larger images are often detrimental to LCP:
 
 It’s interesting to see that websites using NuxtJS and Vue download significantly more image data than websites that use React, and that NuxtJS is able to achieve a fairly good LCP ratio despite this.
 Gatsby and NextJS are both efficient in terms of the amount of the downloaded image data. This means that the improved good LCP ratio that Gatsby provides doesn’t stem just from reduced image sizes. To me, this indicates that Gatsby is likely able to start the download of the largest image sooner and to better prioritize it versus other page resources.
 Interestingly, the Gatsby homepage uses WebP for images and the NextJS homepage uses AVIF.
 Summary
 All of the frameworks that I reviewed in this article have good performance ratios higher than 0%, which means that you can build fast-loading sites, as measured by CWV, using any of them. Likewise, all these frameworks have good performance ratios that are lower than 100%, which means that you can also build slow-loading sites using any of them.
 That said, we saw significant differences between the good performance ratios of the various frameworks. This means that the likelihood that a website built using a specific framework will have good performance can be very different than for another framework. Certainly, this is something to consider when deciding which framework to use.
 On the other hand, we also saw that such differences can be misleading — for example, it initially appeared that React has a noticeably higher-good performance ratio than Vue. But when we effectively excluded most Wix websites, which are included in React’s statistics, by looking only at higher traffic sites, Vue actually catches up with React.
 In addition, certain frameworks that have a reputation for greater emphasis on performance, such as Preact and Svelte, don’t necessarily have an advantage for CWV. It will be interesting to see if their impact increases when Google introduces a new responsiveness metric to replace FID in CWV.
 Even more surprisingly, some Web Application frameworks don’t have an advantage either, despite their built-in support for SSG / SSR and the use of CDNs. In other words, using a Web Application framework is not a silver bullet for performance.
 In particular, it appears that NextJS and NuxtJS have a ways to go in terms of increasing the probability that sites built using them have good CWV. Their graphs are on a significant upward trend, especially NuxtJS, but are still noticeably lower than Gatsby or Wix or even the ratio for all websites in general. Hopefully, their improvements will continue and they’ll succeed in catching up.</content>
     </entry>
     <entry>
       <title>How Even Small UX Changes Can Result In An Increase In Conversion (A Case Study)</title>
         <link href="https://smashingmagazine.com/2022/05/sunuva-case-study-ux-changes-result-increase-conversion/"/>
       <updated>2022-05-11T10:00:00.000Z</updated>
       <content type="text">Sunuva is a global fashion brand for kids established in 2007. Today, their products are featured in famous luxury store brands, such as Bloomingdale’s, Harrods, Barneys, Harvey Nichols, and other luxury department store chains. This is a great example of how — after conducting a UX audit — the team and I at Turum-burum established changes based on analytics data that can have significant results.
 
 UX Audit and Support Over the Implementation of Recommendations
 Since our customer, Sunuva, already had a development team, our task was to conduct a thorough UX Audit with further support over the implementation of the recommendations.
 Turum-burum was hired by the swimwear company to do the following:
 
 Perform a thorough site usability audit with each site detail deeply researched;
 Provide optimal solutions and explanations for the users’ problems;
 Be on hand to advise the customer’s development team for any further clarifications.
 
 We conducted a thorough audit of the website by using data analytics tools, Hotjar recordings and click maps and data of known user snag points provided by the customer. 
 The process of the project review consisted of the following three main stages:
 
 UX auditing of the website (mobile and desktop);
 Interviewing business representatives;
 Providing further support and advice on any further clarifications.
 
 After the audit was completed, we have done the following:
 
 Created a document listing the problems for each key page;
 Prioritized all those issues;
 Provided a full detailed proposal of amendments with documented examples and indications of how each amendment could improve the user experience.
 
 Key Problems and Recommendations
 The analysis of the website metrics showed that the website had engagement indicators, traffic composition, and the conversion rate for both new and returning users along the bounce and exit rate were within the normal range for this segment.
 Yet, we found several issues that needed to be addressed, such as the interface mistakes that adversely affected the key metrics and prevented customers from converting. The main points of growth include:
 
 Increase the conversion rate on the mobile;
 Increase the number of views of the product detail pages;
 Increase the add-to-basket rate on the desktop;
 Minimize the shopping cart abandonment rate;
 Increase the checkout completion percentage rate.
 
 The most critical points that we found were the following:
 
 Catalog structure,
 Absence of prevention of users’ mistakes,
 The checkout page pitfalls,
 Absence of priority in header elements,
 Absence of “Recently viewed” blocks,
 Issues with category blocks.
 
 They are all outlined and described in detail below. Let’s dive in!
 Catalog Structure
 Problem
 The catalog structure resulted in the product list containing a wide range of mixed products. In turn, it became more difficult for users to find the products they needed. According to heat maps, users often used filters — size (age) and gender, if it’s not specified in the selected category. Those who have never used a filter had difficulty finding items on the list. According to our analytics data, only 37% of users who viewed the product list found the product they needed, became interested in them, and decided to go to the product details page.
 
 Recommendation
 To ease the search for the products, fast tags under the product list with common groups (for example, Boys, Girls, Baby, Kids, Teen; Shorts, Suits, Vests) should be added. Also, it’s recommended to show an expanded list of sizes and gender filters by default, so that users can find them quickly and use them. This should shorten the time needed for the search. In the mobile, the filter button should always stay in a fixed position, while the user scrolls down the page.
 
 
 Users’ Mistakes
 Problem
 No error notification would pop up whenever users clicked on the “Add to basket” button without selecting a size beforehand. It made them go back, select a size, and click the button again.
 Recommendation
 Whenever users don’t select a size and click on the “Add to basket” button, the size list would open automatically, and users tend to shift their focus to it.
 
 
 Recommendation
 It would be better not to show the notification that some amount is missing from free shipping on the checkout page. Adding a progress bar in the basket or adding an info message about free shipping and all the details should work much better. 
 
 
 No Priority In Header Elements And Information
 Problem
 There was no priority in the information and sufficient differences in the visual accent in the header elements. The incorrect placement of the accent confused many users. This issue ruined the customer experience.
 
 Recommendation
 
 Visually separate elements, such as Currency, Wishlist, Search, My Account, and Basket.
 Place more emphasis on the catalog categories. 
 Align the currency, wishlist, search, account elements, and so on with the logo. It helps users focus on the more important parts of the site and shortens the time to search for the necessary products.
 
 
 Problem
 It was difficult to notice the catalog categories in the burger menu and differentiate them from the info categories. Users had to spend more time finding the necessary category and its items.
 
 Recommendation
 It’s better to visually differentiate the catalog categories from the info categories. Make less emphasis on the info categories. It will help users to focus on the product categories and faster find the needed items.
 
 Absence Of “Recently Viewed” Blocks
 Problem
 There were no blocks or lists with recently viewed items and no quick “Add to basket” button. According to the analytics data, users would be 2.5 times more likely to return than to buy goods during the next session. In this case, they have already seen some goods and may be ready to buy them quicker. Users might not be able to find the items again or spend too much time doing that. This drastically affected the conversion rate.
 
 Recommendation
 Show the viewed item list. Add the “Add to basket” buttons on the product list cards, hover on desktop, and fixed on the mobile. It will help users find the products they are interested in much quicker.
 
 Not So Obvious Category Blocks
 Problem
 Users never clicked on the category blocks, because the labels were too hard to read, and it seemed to block part of the banner. That’s why it was hard for users to find the necessary items quickly and took more time to search for them.
 
 
 Recommendation
 Change the appearance of the title and the blocks. Visually detach them from the banner, so that users know they are clickable and lead to the category listing.
 
 Results of the Work Done on the Sunuva Website
 Our team has utilized analytic tools, session recordings, heat maps, and other customer data to execute their tasks and provide recommendations. As a result, our customer has reported an increase in the conversion rate and site traffic after nearly implementing every recommendation.
 “Turum-burum’s work drove an increase in the client’s conversion rate and site traffic. Proactive and detail-oriented, they took the time to understand all of the available data to provide optimal solutions and explanations for the client’s problems. They were supportive and transparent throughout.”— Jennifer Tully, E-Commerce Manager, Sunuva
 
 How to Get Started with UX/UI Changes to Increase the Conversion Rate?
 These are seven simple steps that you can follow in order to conduct a UX audit on your own that will help you find and eliminate some interface mistakes: 
 
   
     
       Steps
       
     
   
   
     
       1. Follow the user’s footsteps.
       Make a purchase like you are the customer
     
     
       2. Analyze micro and macro conversions.
       Using GTM and GA
     
     
       3. Conduct audience research.
       What is your target audience portrait?
     
     
       4. Conduct a technical audit.
       Is everything working on your website?
     
     
       5. Check heatmaps and scroll maps.
       Hotjar and Plerdy could be helpful.
     
     
       6. Watch session recordings.
       
     
     
       7. Analyze feedback.
       (Hotjar, Survio)
     
   
 
 
 After these steps are taken, establish several hypotheses on how your website could be improved and prioritize them according to their impact on conversion. Following that, start checking these hypotheses with A/B testing or measuring the changes in analytics. This process can be endless.  
 Converting a user who has already visited your site can cost your business much less than attracting new ones with the help of marketing tools. That’s why you should constantly improve the interface to meet new business goals, new market demand, and new user behavior patterns. If you can’t do this on your own, hire a team of UX/UI experts regularly.
 Find Out More About UX/UI Agency Turum-burum
 🇺🇦 Turum-burum, a UX/UI and CRO company from Ukraine, has over 12 years of UX/UI experience, and its primary focus is on e-commerce and SaaS projects. They influence key website metrics to increase clients’ revenue with the help of design.
 They are Google UX Partners and take part in Retail Development Programs from Google in Ukraine. They cooperate with Baymard Institute, CXL, Hotjar, etc.
 If you need any UX/UI or CRO services, check this Turum-burum’s presentation where you can find their contacts.</content>
     </entry>
     <entry>
       <title>Release Notes for Safari Technology Preview 145</title>
         <link href="https://webkit.org/blog/12629/release-notes-for-safari-technology-preview-145/"/>
       <updated>2022-05-11T09:24:44.000Z</updated>
       <content type="text">Safari Technology Preview Release 145 is now available for download for macOS Big Sur and of macOS Monterey. If you already have Safari Technology Preview installed, you can update in the Software Update pane of System Preferences on macOS.
 This release covers WebKit revisions 291957-293023. This release of Safari Technology Preview does not support versions of macOS Monterey prior to 12.3. Please update to macOS Monterey 12.3 or later to continue using Safari Technology Preview.
 Note: Tab Groups do not sync in this release.
 Web Inspector
 
 Sources tab
 
 Allowed Response Local Overrides to map to a file on disk (r292084, r292120)
 
 
 
 :has() pseudo-class
 
 Added invalidation support for the pseudo-classes :autofill (r292531); :placeholder-shown (r292523); :indeterminate,  :read-only, :read-write, :required and :optional (r292466, r292582)
 
 Container Queries and Containment
 
 Added CSSOM support (r292045)
 Added support for contain: inline-size (r292394, r292416, r292465)
 Added support for containment to disable the special handling of the HTML body element for overflow viewport propagation (r292127, r292157)
 Corrected container selection for pseudo-elements (r292819)
 Corrected container selection for ::slotted and ::part rules (r292635)
 Disallowed invalid query range syntax (r292816)
 Updated container shorthand order (r292759)
 
 CSS Grid
 
 Added support for transitions and animations on grid-template-columns and grid-template-rows (r292432)
 Fixed grid items that establish an independent formatting context to not be subgrids (r292524)
 Implemented support for aligning baselines through subgrids (r292973)
 
 CSS
 
 calc() functions
 
 Added NaN propagation for min, max, clamp, and hypot (r292732)
 Serialized top level min, max, hypot as calc() (r292893)
 
 
 resize property
 
 Added support for block/inline CSS values (r292222)
 Corrected minimum size computation to allow resizing below initial size (r292559)
 
 
 Added support for rendering url(), CSS basic shapes other than path(), and coord-box for offset-path (r292382)
 Fixed scrollIntoView with scroll-snap-type on root element (r292812)
 Fixed drop-shadow filter to work correctly in tiled backing layer (r292059)
 Fixed issue with position: sticky within contain: paint (r292155)
 Implemented units for CSS Typed OM (r292150)
 
 Dialog Element
 
 Dialog element now adapts to dark mode by default (r292029)
 
 JavaScript
 
 Allowed Wasm import from a JS Worker module behind the feature flag (r292799)
 Changed ShadowRealm global object to have a mutable prototype (r292895)
 
 Media
 
 Fixed full screen video progress bar flickering after dragging it (r292572)
 Fixed MSE video not drawing onto canvas (r292811)
 Fixed muted video that sometimes becomes paused when entering fullscreen (r292049)
 
 WebAuthn
 
 Added support for all CTAP transports and remove gesture requirement for virtual authenticators (r292593)
 Implemented getTransports() and getAuthenticatorData() (r292913)
 
 Web API
 
 Removed the 1ms minimum for setTimeout (r291998)
 
 Content Security Policy
 
 Improved compatibility of source matching (r292266)
 Fixed WASM failing to execute after window.open (r292229)
 
 Security
 
 Fixed incorrect CORP and COEP check in 304 responses (r292595)
 
 Service Workers
 
 Added support for ServiceWorkerClients.openWindow (r291979)
 Implemented ServiceWorkerWindowClient.navigate (r292459)
 Exposed workers as service worker clients and implemented registration matching for dedicated workers (r292861)
 Fixed ensuring the document gets controlled by its matching service worker registration during a COOP-based process swap (r292468)
 Fixed Service-Worker-Navigation-Preload header not being sent when Navigation Preload is enabled (r292296)
 Fixed ServiceWorker.postMessage() not working from inside iframes (r292905)
 
 WebRTC
 
 Reduced perceived audio latency on streaming via WebRTC (r292563)
 </content>
     </entry>
     <entry>
       <title>The Barn</title>
         <link href="https://logicmag.io/clouds/the-barn"/>
       <updated>2022-05-10T22:46:29.000Z</updated>
       <content type="text">I wonder what’s taken me so long to pop the question, and why this ritual that’s been performed countless times by so many before me bears down on me like I’m the first woman in history to put a knee to the floor and offer the ring and say the words. She’ll say yes. We talked about it. When we lay beside each other, Aluna gains the power to ease the edge around momentous things, and it’s all in how she finds the right tenor, right whisper, the right circles of pressure to trace along the dips of my neck. I’ve brought all sorts of crises to her before sleep. VR recordings of tragedy filmed that very morning, from hundreds of miles away and through the dissociative lens of a drone’s 360 degree camera. Roaming tent cities trudging across the Sahara that siphon power from abandoned solar panel farms. Ocean slums sprawling off the coasts of Italy, Spain, France, sinking as their fate is decided in boardrooms graced with AC. Sea spray is everywhere it shouldn’t be, and scatters the footage into clouds of discordant pixels—that’s when Aluna pauses our replay, blinks our retinas clear. While the bedroom is at its darkest, we shift to face each other and she tells me what she thinks comes next. Never lessening the blows, but making it all feel approachable, which is exactly how she responded when I asked if we might actually spend the rest of our lives together. And she said yes.
 It feels like I lost my chance. Tonight marks our first week sleeping on the shelter’s temporary foldout beds. We’d considered my SUV but it’s packed tight with all the remaining fragments of our apartment, the things that survived the flood. We gave it a shot, wedging ourselves between piles of clothing in the front seat, but Aluna couldn’t fall asleep. She didn’t complain once. She made constant shifts in her seat through the night, awkwardly reaching for my body only to be thwarted by the packed detritus of our old home.
 Video floats at the shelter tent’s apex above all our heads. It plays the breakdown from every angle, accompanied by live commentary. First comes a deep whine, then a spear of pressured mist breaking through Boston’s seawall. Rapid growth from needle thin spikes of water to spiderweb cracks. A network of black spread over concrete, and freeze-frame there.
 They’re calling it unprecedented. It’s become our country’s favorite word. A guy walks between our cots, offering each of us a refill from his chrome thermos. He looks confused yet determined, young enough to be a Harvard undergrad, coming out to volunteer and help the new climate refugees that huddle in the mandated shelters on his campus. We’re a couple blocks from where the flooding occurred and yet Cambridge is absolutely dry.
 “She’s sad again,” Aluna says, like I can’t hear the dog’s cries for myself. It’s an incessant whimper from a few cots down, battling the mutters of a human trying to shut their pet up. If Aluna had her way she’d know the puppy’s name, who is looking out for her, the neighbors beside them who will have to endure her cries. She peers about the tent and watches as much as she can.
 Since our apartment flooded, my work instruments have been reduced to a loan computer from the company. By the time Aluna and I realized what was happening, our floor swam beneath saltwater. She splayed her arms across the dining table and scooped up a swath of our electronics like a squirrel. Saving what she thought was most important to us, though in real-time, as crisis plays out, that’s so much harder to gauge. You can’t ever be sure of what’s supposed to matter. You grow up with all those educational vids about what to do, how to save yourself, and yet when evacuation sirens blare across the neighborhood all thoughts vacate. Most of us aren’t built for catastrophe.
 I unroll my computer, laying it on my lap. My supervisor has given me some leeway with sign-on times for work due to, well, everything—but I still feel uncomfortable catching up on team messages thirty minutes late. I never imagined that being a climate victim would be so embarrassing. Aluna and I lived in a good neighborhood, made decent money, and were much more used to watching disaster rather than being a part of it. After the seawall breach, I didn’t know what to tell my boss except that it happened, that I wouldn’t wait and burn PTO, that I’d log on as always and work admin. Not out of loyalty to the brand, but to a lifestyle I wasn’t prepared to abandon. Besides, Aluna and I need the money. We haven’t touched our savings yet and we wanna keep it that way, so the work pays for the little things that patch our days together—like walking off campus to purchase snacks from the grocery store. Aluna buys cheap things to distribute among the children here. Her way of keeping up routine as a teacher, since most local schools are temporarily closed and she wouldn’t be much help conducting class from a refugee tent regardless.
 My team’s been talking about the disaster all morning, filling up our text channel with questions and reports on how bad Boston’s been hit. Bickering everywhere, Garrett says, maintenance pointing fingers at the original construction firm, mayor’s scrambling. Good thing is no fingers pointed our way yet but.. Bottom line is the wall should’ve never failed.
 Unprecedented, unprecedented.
 Manager claims he doesn’t have much for me to do right now. Our division at Centra isn’t taking the brunt of the heat, none of the inquiries about damaged property or when the neighborhoods might be “fixed”—as if we can cast a spell and suck the ocean back in place. It’s hard to not glance at other refugees (a word I still find so strange to use here, so near my home) who either tap away on their own devices or blink through screens on their internal retina displays. I wonder which of them are filing the next complaint to the company I work for, and if I might swallow enough pride to do so myself. We all seem the type to be ashamed of this all, how we’re good enough to own multiple devices and line the cots with sets of clothes saved from the water, and yet none of us are second-home rich. Not enough resources to avoid the tents, but just enough to feel bad using them.
 Admin tasks are the majority of my workload at Centra. Garrett and I are specifically in charge of benefits paperwork for the employees working under our relocation division, a reasonable job since the company keeps that section small. While Centra brands itself in all the climate net corporation tropes—commercials aglow with green fields, wind turbines swaying against sunset—the refugee relocation programs we helm are a side gig. That’s how it is with climate nets, a bit of a trend for corporations eager for a brand cleanup. Shove a random percentage of the budget to securing rent-controlled housing in northern, inland cities shielded from the typical forms of climate catastrophe, then offer up leases to a portion of the newly homeless. Great PR, makes the government happy that we do their work for them, and it provides everyone forced to live in tents a shard of hope.
 A couple hours into my work day, Aluna rests a small bag of potato chips on my shoulder and kisses my cheek. She’s fond of a greeting with food involved because I’d probably forget to eat otherwise. As she takes a spot on the bed next to me, Aluna glances at the computer in my lap.
 “You think Centra’s gonna put us up in a loft or something a little more? Maybe a whole house. They’ve got space out in Colorado or wherever for giving us some land, don’t you think? Full patio, a yard neither of us will know what to do with.”
 “No thinking for me, love.” I dance my fingertips across the screen. “Only typing.”
 Aluna grabs the bag of chips before they slide from my shoulder, opening them as she scoots closer. “Liar. You just don’t wanna tell me our spot in the queue, don’t deny it.”
 Wherever we’re at, it’s probably so far down we shouldn’t think about it. Company gossip has gotten more frantic as it seems that our relocation system has ground to a halt. Centra’s been skimming people off the climate net division and nobody’s left the Cambridge tents with a golden ticket. You’d know if someone got a way out, the excitement and rush they’d fail to hide, but nobody here has escaped.
 Something we’ve said snared the attention of the man who sleeps a cot away from us. He leans forward, locking eyes with me. His blazer hasn’t come off the whole week and it’s acquired a film of dust.
 “Hey, I heard ‘queue.’ Can you check my status? Is there a way you can look at that?”
 With a grimace, I shake my head.
 “Sorry, doesn’t work that way. I couldn’t show you the odds if I wanted to. There’s no access to relocation logs from my department.”
 “Damn. Come on, really?” He hugs himself. “If you want me to pay for it, I will. Name your price. Money’s not a problem, trust me. I’m supposed to be out of here in a few days anyway, I just want to see the queue for a plan B.”
 “I don’t work relocation, has nothing to do with me. Like I said, I’m sorry—I really would, but there’s nothing I can do. We all gotta wait.”
 “Everyone’s fucking waiting. That all we’re supposed to do now? Those guys who got flooded in Miami, too, just sitting there.” The man squats, eyes locked on the rubber mat below us, a kindergarten sky blue. “Nobody’s moving anymore. When’s the last time you heard someone get the golden ticket? Who’s being whisked away to safe cities? Because I don’t know anybody.”
 It’s not that he’s wrong, it’s just that I have nothing to say about it. There’s nothing I could say that would fix this. As the man leaves us with the ghost of his cologne, Aluna gives my hand a brief squeeze, a pulse.
 And at night, that dog starts whining more. I could turn on my night vision, but instead I lay in the dark with Aluna wedged by me, still dreaming, sharing the cot’s limited real estate. Around me are rustling blankets, murmurs. Pointless, flitting. These limp sounds rise with the dog’s whimpers, a cross-species call and response.
 I think I can find a relative peace again. At least one strong enough to drift back asleep, salvage some scraps of rest before the day starts. My eyes are shut against the tent’s collective discomfort. It’s the most solitude I’m capable of carving out for myself, and for the past week it has been enough. Enough against the same sighs, the coughs, the sobs that should’ve never traveled so well across this space.
 The dog’s whining swoops up an octave. I’ve never heard her so distressed—beyond distressed. A shimmering yelp scraping against my eardrums, forcing me upright. Aluna’s hands scramble up my forearm and feel out my skin. There’s nothing left except for the dog and that howl, a guttural pain.
 Then she’s done. One more bark, cut short and high. She’s left us in silence undercut by shifting blankets. Our neighbors freeze and look about for the source of disruption, as if pinpointing it all might let us drift back to sleep. As if we won’t all sink back to our cots, turn along the taut stretches of nylon, curl around our partners and ourselves to find a steady beat again, something like the mattress you lost back home under tons of seawater, something that was once warm with you and whoever shared life with you. But it won’t be enough to sleep.
 When the morning comes, Aluna returns to bed. I hadn’t realized she left.
 “She’s okay,” Aluna says, “but shaken. Someone tried to strangle her last night, right here. We think it’s one of us.”
 Everyone is much too awake. People glare at their neighbors, at the ceiling. In their little movements is stored all the dread that has no legitimate avenue for escape. Our urge for relief isn’t new, though it’s become painfully visible.
 My work chat is a repeat of yesterday, gossip about whose heads are on the chopping block. Talk of a “rapid retreat” from upper echelons that none of us know how to interpret. I can’t vent to Aluna no matter how much I’d like to, not in front of the other refugees.
 She drifts back and forth, out into town, buying snacks for the children and a treat for the dog. This is the first time her offers are rejected. From afar, I see parents push her away. Gentle shakes of the head, hands rising up. She’s too far for me to hear how she responds.
 ***
 I suggest we leave at sunset. She doesn’t know how to take that. We’re exchanging messages over internal displays, eating the rations they’ve started handing out for lunch. Aluna would wait and see as she put it, staying in the shelter, till the next disaster forced us to relocate again. This is no home and yet she can barely imagine leaving it. Maybe it’s the proximity to what we lost that keeps her tethered. Cambridge’s parks, rowhouses, and stores provide a familiar urban texture, and when you drift far enough from the tents it succeeds in lulling you back to normality. It’s delusional. I work where the miracles are made, and the only thing Centra is concerned about is laying people off and minimizing damage for the PR fallout of the broken seawall—a tragedy that they refuse to claim as their responsibility, taking all distancing measures available. It will grow clearer over the coming days that there is no help on the way. Not for any of us. We have to leave the city.
 And where would we go? Aluna asks. She already mentioned us hitting up friends who live near Cambridge, ones who haven’t experienced climate failure outside of the dearth of food lining the shelves. We’ve held off because that shame flares up. None of our community has the bandwidth for that type of support no matter how much we wish they could provide it. They ask us what happened, if we’re alright, how they might help, and we dodge each question enough to allow them an out. They always take it.
 We can’t go to them, not now. While Boston as a whole might be running relatively fine, the pockets where they’ve shoved us will soon boil over. I sigh and send Aluna a message. Even though we’re sitting across from each other, I find a way to avoid her eyes.
 There’s a spot little less than an hour out of town owned by Centra. Part of their data infrastructure, and it’s quiet. We don’t need to be there for long… Just give it time for things to settle over here, a few days at most—the SUV will hold its charge, so we won’t have any problems coming in and out. A mini-retreat?
 She doesn’t even crack a smile. Right before dinner, we get up and leave. Just like that. There’s nothing for us to bring along. All that’s left of our submerged home is in our SUV, sitting beneath platinum LED streetlights.
 Autopilot’s enabled for this region, though I’m quick to shut it off. The highway’s riddled with flooding issues now, and I doubt the car could keep up.
 As we exit the parking lot and get on the road, Aluna and I pass the low, inflated lozenges of the refugee tents, pinned to the soil with near-invisible lines of rope. Nearly biological in their aversion to clean, sharp angles, growing out of the flat campus lawns like massive fungi. Logos adorn the sides, all the entities responsible for erecting this crisis architecture. Centra’s branding appears on the tent as a circle adorned with light rays, a flat design vision of a glowing sun. In the encroaching murk, it can’t be easily defined. The sun seems to waver in the shadows, like a pinned spider flat against the outer wall.
 ***
 There was once a time—probably late ’50s, early ’60s—when vehicular windshield HUDs were an inescapable trend of automotive design. Glowing, transparent skins hemmed glass borders, providing location-specific updates, weather, and navigation tips. By then, every other American had a retina display, though the AR novelty of these car HUDs was pushed as if it were cutting edge. At its core, the overlays are candy-colored, highly restricted web browsers tacked to the front of our vehicles. “Futuristic” was the word that nobody wanted to use, and yet guided every step of the design process. The HUDs exist because, as we near the end of the century, they seem like they should.
 The glyphs and readouts bordering my windshield begin to disappear. First it’s the weather icon, a cloud eclipsing a crescent moon, winking out. Then all the stats about the car battery. That one’s especially ironic since you’d assume the SUV’s drawing on local, in-vehicle data. Just like all the other visuals drifting away, it warns me of NO INTERNET CONNECTION before vanishing.
 We’ve split from I-90 onto a lone road that twists away from the city, away from the gleam of retina-enhanced billboards that outshine tent villages sprawled beneath overpasses. Density gets a lot lighter out here. Autumn foliage flanks us on either side, swimming up to high focus in the titanium pools of our headlights. It’s been almost half an hour since we passed another car.
 “If you don’t know where we’re going,” Aluna mutters, “you might want to turn back, Imani. GPS is about to go out.”
 That’s concerning. It’s one of the few visuals remaining on the windshield. I squeeze her thigh.
 “It’s not far love. I’ll get us there. Trust me, it’s exactly the right place for us to lay low. There’ll be better connection in the barn too, so we can keep track of how things play out in town. When it seems a bit safer we can drive back in.”
 “Why would the barn have good connection if it’s abandoned?”
 “These things never go fully dead.” I keep an eye out for our turn. “That said, though, Centra’s moved most of their servers elsewhere, further inland. They think they’re future proofing. All this one’s used for now is cold storage, backups of backups.”
 I’ve got no doubt it has to do with the sunk-cost fallacy. Decades ago Centra spent millions to get the server farm up and running, and they can’t bear wiping the whole thing out. Funny how risk averse these corporations get in the face of unstable environmental conditions.
 “Look,” I say, pointing at the growing lights on the horizon, a fluorescent wash against overcast. “That’s the suburb around the corner from it. Not much longer.”
 “Good ’cause we got no reception at all now.”
 Aluna’s stare hits that middle space, out of focus. She’s accessing her retinal screen and judging from the grimace, she’s not liking what she sees.
 “Imani? This isn’t right. No connection whatsoever, we’re completely in the dark. Is there a dampener or something?”
 “I’m… not sure. I don’t know why there’d be one active around here.”
 Rising over the canopy is a gas station sign, blaring out to the night. I can’t help slowing past, getting a closer look at the metal bars meshing the windows and door. They’ve got a drone hovering at the entrance, hardened edges and matte black finish implying combat. The few cars in the parking lot give off the same energy. Tints and acute angles. I’m half-convinced we’ll glimpse an insignia of some sort, hopefully one of a private military firm instead of a paramilitary group, like the ethnostate bands that crawl through the Pacific Northwest.
 The homes out here are different too. Most of us try to avoid traveling via highway, so the suburbs have become a rare occurrence in our lives. Last time I passed through was maybe a year and a half ago, and the two-story homes were adorned with the typical New England fanfare of ivy and wrought iron. A handsome weariness. Last time I came through, the internet worked. Now it’s all dark. Bars mesh over every window. It’s a stretch to call this a neighborhood, this collection of fortresses tucked away at the end of winding driveways, peeking through the forest in utter silence. If it were daytime we might even see the antennae they erected to shut down the network. All we’ve got is their absence.
 “We’re okay,” I say, and Aluna doesn’t respond. I turn down the side street marked with the Centra logo, and we creep down a long gravel path.
 ***
 It’s called the barn because it’s a massive, ugly, hulking structure that carves up the forest with gray paneling and harsh floodlights. The electrified gate swings open once it recognizes me and allows us to approach the dead server farm. Maybe not the right word there, dead. The building’s humming with vibrations and light. While there are potholes eating their way across the asphalt, and there’s not a single other car in the area, the place is still on.
 Once we drive past the threshold, the car’s HUD returns in a flicker. Our connection piggybacks on Centra’s network. Aluna settles in the passenger seat.
 “I’m checking the news.”
 I park near the barn’s main entrance, two glass padlocked doors. Judging from the imagery of server farms I’ve seen back in the day, the aesthetics of these buildings refuse to change. They remain one step abstracted from a warehouse.
 “We don’t have to stay here for too long,” I say. “Just a night or two. Just to see if things heat up in town or if we’re good.”
 She nods along, but I’m not sure if it’s enough. This feels right to me, getting out, so I have to find a way to swallow the guilt. The last thing Aluna wants is to be far from the community, disengaged with helping in all those small ways. I don’t operate like that. Not to say I don’t care, but when I exchange snacks or provide some clothing, I only feel dread. I’m never doing enough. Aluna might find fulfillment in those acts, or she might hide her fear better than I do. It’s not something I’ve had the urge to figure out until now.
 We lean our seats back as far as they go, crushing the remains of our apartment in the back. Each readjustment brings another plastic creak.
 “Look,” Aluna says, and shares video of Southie. More drone footage, smooth like the camera’s on rails in the sky. The water has already started to lessen. All of our neighborhood’s detritus has mixed in the flood to make the ground invisible. Frothing, dark water, yet so low, getting lower… It feels odd to think of it this way, but the destruction feels pathetic. A quilt drifts from what was once a window. It’s matted with glass and other things, objects I can’t identify no matter how close the zoom gets. Whatever’s been abandoned wades through ruins that we want more than anything to return to.
 I blink away the footage and turn to face Aluna.
 “What do you think?”
 And at first, it seems she might respond as she used to when we slept in a bed together. Aluna opens her mouth and hesitates. Her hand reaches across the cup holders and up my arm, to my collarbone, and finally the dips of my neck.
 “I think we should learn how to be lost. It’s okay, or it’ll have to be. Know what I mean?”
 “Want an honest answer?”
 She nods. I catch her hand in my own, feel where the ring should be. I should’ve asked her already. I bought her a ring, and I don’t know where it might be buried. It could be in this car with us or tumbling down a sewer drain.
 “We shouldn’t be fucked over so fast, Aluna. It’s not supposed to all end so quickly. I always thought there’d be a… I don’t know, a warning? A heads up of some sort. Like, yes, the end’s coming, as it’s been since forever, but here’s a week head start. Or even a day. I don’t know how we wake up and face all of it gone, and whatever. That’s it. We keep going.”
 “I mean, you just said it. It’s always been coming. The warning call’s blared for decades, and we watched it every night. We just thought we’d be lucky.”
 We can’t live off luck. One day soon, we’ll have to find a path forward. It’s not something I can think of now. Now, all I see is Aluna, her cheekbones carved through the data center’s security floodlights. For an hour more, we stay up and watch more footage from back home. The tents are not faring well tonight—actual skirmishes popping up, people who never imagined fighting for survival forced to fend for any resources they can get a handle on. I wish a wake up call didn’t involve people getting hurt.
 By the time we start drifting off, we agree it’s not as bad as it could’ve been. Even from the scattered news footage, it’s clear there are people like Aluna among the tents. Lots of little things resulting in countless points of de-escalation. Though we’ll need more to recreate our homes, it’s a start. I think, next morning, we’ll head back.
 ***
 I check the back while Aluna’s fast asleep. The trunk, too. Moving as quietly as possible through years and years of our shit, all of it bursting at the seams and threatening to fall across the parking lot. Every exhale is a puff of mist, and I blink to clear my eyes.
 Journals, clothes, random pieces of silverware, actual physical books, toothpaste, blankets—I bring one out, drape it over Aluna’s body in the passenger seat, then go back to my search.
 I find it wedged on the trunk floor, beneath the corner of a box. It must’ve fallen out of the case. Unmarred, a thin silver ring.
 After scooting back in the driver’s seat, I hover with the ring tucked in my palm. It grows warm there, in the safety of my hand. I don’t ever want to let it go until Aluna’s ready to take it. So I’ll wait till next morning, and see what she says.</content>
     </entry>
     <entry>
       <title>Agile and the Long Crisis of Software</title>
         <link href="https://logicmag.io/clouds/agile-and-the-long-crisis-of-software"/>
       <updated>2022-05-10T22:46:07.000Z</updated>
       <content type="text">I first encountered Agile when I got a job in a library. I’d been hired to help get a new digital scholarship center off the ground and sometimes worked with the library’s software development team to build tools to support our projects. There were about six members of this team, and I noticed right away that they did things differently from the non-technical staff. At meetings, they didn’t talk about product features, but “user stories”—tiny narratives that described features—to which they assigned “story points” that measured the effort involved in completing the associated tasks. They met every morning for “standup,” a meeting literally conducted standing up, the better to enforce brevity. A whiteboard had pride of place in their workspace, and I watched the developers move Post-it notes across the board to signify their state of completion. They worked in “sprints,” two-week stretches devoted to particular tasks.
 At meetings with the rest of us on the library staff, the head of the development team reported on progress using software that included a dashboard indicating the state of every project. The manager could also show us a graph of the team’s “velocity,” the rate at which the developers finished their tasks, complete with historical comparisons and projections. 
 This was Agile, I learned, a method for managing software development that had achieved enormous popularity in technical workplaces of all kinds—and, increasingly, even non-technical workplaces (including, as one TED speaker would have it, the family home). Honestly, I was impressed. In my own work, I often felt as though I was flailing around, never quite sure if I was making progress or doing anything of real value. The developers, in contrast, seemed to know exactly what they were doing. If they ran into a roadblock, it was no big deal; they just dealt with it. They expected requirements to change as they progressed, and the two-week time horizons allowed them to substitute one feature for another, or adopt a new framework, without starting all over from scratch.
 That’s the beauty of Agile: designed for ultimate flexibility and speed, it requires developers to break every task down into the smallest possible unit. The emphasis is on getting releases out fast and taking frequent stock, changing directions as necessary.
 I was intrigued; Agile was different from anything I’d experienced before. Where had it come from, and why?
 I began to explore the history of Agile. What I discovered was a long-running wrestling match between what managers want software development to be and what it really is, as practiced by the workers who write the code. Time and again, organizations have sought to contain software’s most troublesome tendencies—its habit of sprawling beyond timelines and measurable goals—by introducing new management styles. And for a time, it looked as though companies had found in Agile the solution to keeping developers happily on task while also working at a feverish pace. Recently, though, some signs are emerging that Agile’s power may be fading. A new moment of reckoning is in the making, one that may end up knocking Agile off its perch.
 Turning Weirdos into Engineers
 Software development was in crisis even before the word “software” was coined. At a 1954 conference convened by leaders in industry, government, and academia at Wayne State University in Detroit, experts warned of an imminent shortage of trained programmers. The use of the term “software” to mean application programming first appeared in print in an article by statistician John W. Tukey four years later. By the mid-1960s, at least a hundred thousand people worked as programmers in the United States, but commentators estimated an immediate demand for fifty thousand more.
 In the first decades of the programming profession, most experts assumed that formulating computer-readable directions would be a relatively trivial job. After all, the system analysts—the experts who specify the high-level architecture—had already done the hard intellectual work of designing the program and hardware. The job of the coder was simply to translate that design into something a computer could work with. It was a surprise, then, when it turned out that this process of translation was in fact quite intellectually demanding. 
 The nature of these intellectual demands, along with the larger question of what kind of work software development actually is, continues to baffle managers today. In the computer’s early years, it seemed to some people that coding was, or should be, a matter of pure logic; after all, machines just do what you tell them to do. There was, self-evidently, a formally correct way to do things, and the coder’s job was simply to find it. 
 And yet, the actual experience of programming suggested that coding was as much art as science. Some of the most advanced programming, as Clive Thompson notes in his 2019 book Coders, was pushed forward by long-haired weirdos who hung around university labs after hours, hackers who considered themselves as much artisans as logicians. The fact that one couldn’t physically touch a piece of software—its storage media, perhaps, but not the software itself—made software development more abstract, more mysterious than other engineering fields. Where other fields could be expected to obey the laws of physics, the ground seemed to be constantly shifting under software’s feet. Hardware was perpetually changing its parameters and capabilities. 
 Nevertheless, the field of electronic data processing—the automation of office functions, like time cards and payroll—was growing rapidly. The hulking machines designed for the purpose, leased from IBM, quickly became the hallmark of the technologically forward-thinking operation. But they required teams of operators to design the programs, prepare the punch cards, and feed data into the system. Established managers resented the specialized expertise and professional eccentricity of the growing ranks of “computer boys.” They resented, too, that software projects seemed to defy any estimation of cost and complexity. The famed computer scientist Frederick Brooks compared software projects to werewolves: they start out with puppy-like innocence, but, more often than not, they metamorphose into “a monster of missed schedules, blown budgets, and flawed products.” You could say, then, that by the late 1960s, software development was facing three crises: a crying need for more programmers; an imperative to wrangle development into something more predictable; and, as businesses saw it, a managerial necessity to get developers to stop acting so weird. 
 It was in this spirit of professionalization that industry leaders encouraged programmers to embrace the mantle of “software engineer,” a development that many historians trace to the NATO Conference on Software Engineering of 1968. Computer work was sprawling, difficult to organize, and notoriously hard to manage, the organizers pointed out. Why not, then, borrow a set of methods (and a title) from the established fields of engineering? That way, programming could become firmly a science, with all the order, influence, and established methodologies that comes with it. It would also, the organizers hoped, become easier for industry to manage: software engineers might better conform to corporate culture, following the model of engineers from other disciplines. “In the interest of efficient software manufacturing,” writes historian Nathan Ensmenger, “the black art of programming had to make way for the science of software engineering.”
 Chasing Waterfalls
 And it worked—sort of. The “software engineering” appellation caught on, rising in prominence alongside the institutional prestige of the people who wrote software. University departments adopted the term, encouraging students to practice sound engineering methodologies, like using mathematical proofs, as they learned to program. The techniques, claimed the computer scientist Tony Hoare, would “transform the arcane and error-prone craft of computer programming to meet the highest standards of the engineering profession.” 
 Managers approached with gusto the task of organizing the newly intelligible software labor force, leading to a number of different organization methods. One approach, the Chief Programmer Team (CPT) framework instituted at IBM, put a single “chief programmer” at the head of a hierarchy, overseeing a cadre of specialists whose interactions he oversaw. Another popular approach placed programmers beneath many layers of administrators, who made decisions and assigned work to the programmers under them. 
 With these new techniques came a set of ideas for managing development labor, a management philosophy that has come to be called (mostly pejoratively) the “waterfall method.” Waterfall made sense in theory: someone set a goal for a software product and broke its production up into a series of steps, each of which had to be completed and tested before moving on to the next task. In other words, developers followed a script laid out for them by management. 
 The term “waterfall,” ironically, made its first appearance in an article indicting the method as unrealistic, but the name and the philosophy caught on nevertheless. Waterfall irresistibly matched the hierarchical corporate structure that administered it. And it appealed to managers because, as Nathan Ensmenger writes, “The essence of the software-engineering movement was control: control over complexity, control over budgets and scheduling, and, perhaps most significantly, control over a recalcitrant workforce.” This was precisely the kind of professional that waterfall development was designed to accommodate.
 But before long, software development was again in crisis—or crises. Part of the problem was keeping up with the need for new computer scientists. Universities in 1980 couldn’t fill the faculty positions necessary to train the huge number of students with ambitions to become software engineers. “This situation seriously threatens the ability of Computer Science departments to continue developing the skilled people needed both by our information processing industry and by an increasingly technological society,” warned the Association for Computing Machinery. 
 The industry’s dearth of qualified developers wasn’t its only problem. Software development itself seemed to be struggling. Waterfall’s promise of tightly controlled management was a mirage. No amount of documentation, process, or procedure seemed capable of wrestling development into predictability. Software projects were big, expensive, and they seemed to be spiraling out of control—huge initiatives foundered unfinished as requirements changed and warring project teams bickered about details. Despite managers’ efforts to make software development reliable and predictable, it seemed, if anything, to have only grown more unwieldy. As the computer scientist Jeff Offutt put it, “In the 1960s, programmers built ‘tiny log cabins,’” while “in the 1980s, teams of programmers were building office buildings”—and by the 1990s, skyscrapers. Yet teams of technologists seemed unable to coordinate their work. Peter Varhol, a technology industry consultant, estimates that in the early 1990s, the average application took three years to develop, from idea to finished product. Technology was supposed to make American business smarter, faster, and more profitable, and yet the most respected corporations couldn’t seem to get their projects off the ground.
 The designation of “engineer,” the administrative hierarchies, the careful planning and documentation: all of this had been intended to bring order and control to the emerging field of software development. But it seemed to have backfired. Rather than clearing the way for software developers to build, waterfall gummed up the works with binders of paperwork and endless meetings. 
 For their parts, engineers complained of feeling constrained by heavy-handed management techniques. They just wanted to build software. Why were they hamstrung by paperwork? The typical picture of corporate programming in the 1990s is of the existentially bored twenty-somethings in Douglas Coupland’s novel Microserfs, or the desperate developers in Mike Judge’s movie Office Space, whose rage lurks just below the surface.
 Khakis and Dad Jeans
 Enter what may be the world’s most unlikely group of rock stars: seventeen middle-aged white guys, dressed in khakis and dad jeans, all obsessed with management. The now-legendary authors of what came to be called the Agile Manifesto gathered at Utah’s Snowbird ski resort in February 2001 to hammer out a new vision for the software development process. This wasn’t their first meeting; they’d been gathering in various configurations to discuss software development for some time, though, until the 2001 meeting, they hadn’t come up with much to show for it. This time was different. Scrawled on a whiteboard was the Agile Manifesto, a set of values that, in the following years, would become nearly ubiquitous in the management of programmers, from fledgling startups to huge corporations. It’s pleasantly concise:
 We are uncovering better ways of developing software by doing it and helping others do it.Through this work we have come to value:Individuals and interactions over processes and toolsWorking software over comprehensive documentationCustomer collaboration over contract negotiationResponding to change over following a planThat is, while there is value in the items on the right, we value the items on the left more.
 The manifesto, supplemented by twelve additional principles, targeted the professional frustrations that engineers described. Waterfall assumed that a software application’s requirements would be stable, and that slowdowns and logjams were the result of deviating from management’s careful plan. Agile tossed out these high-level roadmaps, emphasizing instead the need to make decisions on the fly. This way, software developers themselves could change their approach as requirements or technology changed. They could focus on building software, rather than on paperwork and documentation. And they could eliminate the need for endless meetings. 
 It’s an interesting document. Given the shortage of qualified developers, technology professionals might have been expected to demand concessions of more immediate material benefit—say, a union, or ownership of their intellectual property. Instead, they demanded a workplace configuration that would allow them to do better, more efficient work. Indeed, as writer Michael Eby points out, this revolt against management is distinct from some preceding expressions of workplace discontent: rather than demand material improvements, tech workers created “a new ‘spirit,’ based on cultures, principles, assumptions, hierarchies, and ethics that absorbed the complaints of the artistic critique.” That is, the manifesto directly attacked the bureaucracy, infantilization, and sense of futility that developers deplored. Developers weren’t demanding better pay; they were demanding to be treated as different people.
 Organizational Anarchists
 It seems likely that changes in opinions about the nature of software development didn’t take place in 2001 exactly, but in the decade leading up to the authorship of the Agile Manifesto. Consensus was growing—among developers, but also among managers—that software development couldn’t be made to fit the flow-charts and worksheets in which analysts had placed so much hope. Software, as the historian Stuart Shapiro wrote in 1997, is complex in a particularly complex way: the problems are “fuzzy, variable, and multifaceted, and thus rarely proved amenable to any one approach; instead, they demanded hybrid and adaptive solutions.” Not, then, forms and timecards. Moreover, as the workforce of programmers grew by leaps and bounds in the 1990s, companies hired, of necessity, people without formal computer science training. These younger workers likely had less invested in the drive of the 1970s and 1980s to turn software development into a science. The manifesto wasn’t really a shot across the bow: it was more of a punctuation mark, emphasizing years of discontent with prevailing models of corporate management.
 Nevertheless, while Agile had a devoted following, its mandate—the removal of top-down planning and administrative hierarchy—was a risk. It meant management ceding control, at least to some extent, to developers themselves. And most large companies weren’t willing to do that, at least not until the 2010s. Between 2012 and 2015, though, according to the Agile consultancy Planview, more than 50 percent of practicing development teams characterized themselves as “Agile.” 
 Doubtless, some of this popularity had to do with the growth of high-speed internet connections, which drastically altered the way software got released. Before, it wasn’t unusual for software to be updated once a year, or at even longer intervals. The fact that updates had to be distributed on physical media like CD-ROMs and floppy disks limited the speed of new releases. But high-speed internet made it possible to push out fixes and features as often as a company wanted, even multiple times a day. Agile made a lot of sense in this environment.
 Facebook’s famous former motto, “Move fast and break things,” captured the spirit of the new era well. It was an era that rewarded audacity, in software development as much as in CEOs. Venture capital firms, on the hunt for “unicorns,” poured record amounts into the technology sector during the 2010s, and they wanted to see results quickly. Competing with startups required the ability to change on a dime, to release constantly, and to develop at breakneck speed. The risk calculus shifted: it now seemed dangerous to stick with waterfall, when Agile promised so much speed.
 Equally, it seems, what it meant to be a software developer had changed. In the 1970s and 1980s, experts held up the systems-minded, logic-loving scientist as the ideal software worker. But over the years, this ideal had failed to take root. The programmers of the 1990s read Wired, not Datamation. If their characteristics can be intuited from the Agile Manifesto, they were intently committed to the highest standards, working quickly and confidently because managers “trust them to get the job done.” They refused to do things just because they’ve always been done that way, turning their minds to “continuous attention to technical excellence.” They weren’t thrown by fluid, fast-moving requirements; instead, they embraced them as an opportunity to “harness change for the customer’s competitive advantage.” 
 The image of the free-thinking nonconformist fits the philosophy of Agile. The manifesto’s authors may have looked like textbook engineers, in button-downs with cell-phone holsters, but “a bigger group of organizational anarchists would be hard to find,” according to Jim Highsmith, one of their number. Particularly in the early days, there was a lot of talk about the challenge Agile posed to the traditional management paradigm. Agile’s proponents were proud of this nonconformity: the framework “scares the bejeebers out of traditionalists,” wrote Highsmith in 2001. “Agile was openly, militantly, anti-management in the beginning,” writes the software developer and consultant Al Tenhundfeld. “For example, Ken Schwaber [a manifesto author] was vocal and explicit about his goal to get rid of all project managers.” 
 Anti-management, maybe, but not anti-corporate, not really. It’s tempting to see the archetypal Agile developer as a revival of the long-haired countercultural weirdo who lurked around the punch card machines of the late 1960s. But the two personas differ in important respects. The eccentrics of computing’s early years wanted to program for the sheer thrill of putting this new technology to work. The coder of Agile’s imagination is committed, above all, to the project. He hates administrative intrusion because it gets in the way of his greatest aspiration, which is to do his job at the highest level of professional performance. Like the developers in Aaron Sorkin’s The Social Network, he wants most of all to be in “the zone”: headphones on, distractions eliminated, in a state of pure communion with his labor.
 Management’s Revenge
 Back at my library job, I kept an eye on the developers, admiring their teamwork and pragmatism. As time went by, though, I couldn’t help but notice some cracks in the team’s veneer. Despite the velocity chart and the disciplined feature-tracking, the developers didn’t seem to be making all that much progress. They were all working hard, that was clear, but there was a fatal flaw: no one really knew what the project was ultimately supposed to look like, or exactly what purpose it was supposed to serve. The team members could develop features, but it wasn’t clear what all these features were being tacked on to. Maybe that problem came from my workplace’s own dysfunction, which was considerable. Still, I began to wonder whether the Agile methodology had some limitations.
 And, in fact, anyone with any proximity to software development has likely heard rumblings about Agile. For all the promise of the manifesto, one starts to get the sense when talking to people who work in technology that laboring under Agile may not be the liberatory experience it’s billed as. Indeed, software development is in crisis again—but, this time, it’s an Agile crisis. On the web, everyone from regular developers to some of the original manifesto authors is raising concerns about Agile practices. They talk about the “Agile-industrial complex,” the network of consultants, speakers, and coaches who charge large fees to fine-tune Agile processes. And almost everyone complains that Agile has taken a wrong turn: somewhere in the last two decades, Agile has veered from the original manifesto’s vision, becoming something more restrictive, taxing, and stressful than it was meant to be. 
 Part of the issue is Agile’s flexibility. Jan Wischweh, a freelance developer, calls this the “no true Scotsman” problem. Any Agile practice someone doesn’t like is not Agile at all, it inevitably turns out. The construction of the manifesto makes this almost inescapable: because the manifesto doesn’t prescribe any specific activities, one must gauge the spirit of the methods in place, which all depends on the person experiencing them. Because it insists on its status as a “mindset,” not a methodology, Agile seems destined to take on some of the characteristics of any organization that adopts it. And it is remarkably immune to criticism, since it can’t be reduced to a specific set of methods. “If you do one thing wrong and it’s not working for you, people will assume it’s because you’re doing it wrong,” one product manager told me. “Not because there’s anything wrong with the framework.”
 Despite this flexibility in its definition, many developers have lost faith in the idea of Agile. Wischweh himself encountered a turning point while describing a standup meeting to an aunt, a lawyer. She was incredulous. The notion that a competent professional would need to justify his work every day, in tiny units, was absurd to her. Wischweh began to think about the ways in which Agile encourages developers to see themselves as cogs in a machine. They may not be buried under layers of managers, as they were in the waterfall model, but they nevertheless have internalized the business’s priorities as their own. “As developers, IT professionals, we like to think of ourselves as knowledge workers, whose work can’t be rationalized or commodified. But I think Agile tries to accomplish the exact opposite approach,” said Wischweh. 
 Al Tenhundfeld points out that the authors of the Agile Manifesto were working developers, and that the manifesto’s initial uptake was among self-organizing teams of coders. Now, however, plenty of people specialize in helping to implement Agile, and Agile conferences notoriously tend to be dominated by managers, not developers. The ubiquity of Agile means that it is just as likely to be imposed from above as demanded from below. And Agile project managers, who are generally embedded in the team as the “product owner,” find themselves pulled in two directions: what’s best for the developers on the one hand, and what they’ve promised to deliver to management on the other. 
 Even as the team is pulled in multiple directions, it’s asked to move projects forward at an ever-accelerating pace. “Sprinting,” after all, is fast by definition. And indeed, the prospect of burnout loomed large for many of the tech workers I spoke to. “You’re trying to define what’s reasonable in that period of time,” said technical writer Sarah Moir. “And then run to the finish line and then do it again. And then on to that finish line, and on and on. That can be kind of exhausting if you’re committing 100 percent of your capacity.”
 Moreover, daily standups, billed as lightweight, low key check-ins, have become, for some workers, exercises in surveillance. Particularly when work is decomposed into small parts, workers feel an obligation to enumerate every task they’ve accomplished. There’s also pressure for every worker to justify their worth; they are, after all, employees, who need to be perceived as earning their salaries. 
 “Story points”—the abstraction that teams use to measure the effort involved in particular development tasks—have also lost some of their allure. They began as a way to give engineers some control over the amount and substance of their work. And yet, in practice, they often serve as a way to assess engineers’ performance. “Once you’ve put something in a digital tool, the amount of oversight that people want goes up, right?” said Yvonne Lam, a software engineer based in Seattle.
 The problem isn’t just with surveillance, but with the way the points calcify into a timeline. John Burns, an engineer at a platform company, recalled a former workplace that simply multiplied story points by a common coefficient, in order to get a rough estimate of how long a project would take. Despite the points’ avowed status as an informal, internal measure, managers used them as a planning device. 
 The Next Crisis
 Underlying these complaints is a deeper skepticism about the freedom that Agile promises. Agile’s values celebrate developers’ ingenuity and idiosyncratic ways of working. But there are distinct limits to the kinds of creativity workers feel authorized to exercise under Agile, particularly because problems tend to be broken down into such small pieces. “It is clear that Agile dissolves many of the more visible features of hierarchical managerial control,” writes Michael Eby. “But it does so only to recontain them in subtle and nuanced ways.” Yvonne Lam notes that autonomy under Agile has distinct parameters. “People say you have the autonomy to decide how you’re going to do the work. And it’s like, yeah, but sometimes what you want is the autonomy to say, this is the wrong work.” There are so many choices to be made in the course of any software development project—about languages, frameworks, structure—that it’s possible to lose sight of the fact that developers often don’t get to weigh in on the bigger questions. 
 And, in the last few years, those bigger questions have taken on greater importance and urgency. We’ve seen numerous examples of tech workers organizing to change the direction of their companies’ business strategies: Google developers agitating to kill an AI contract with the Department of Defense, game developers agitating to end sexual harassment. These demands go beyond Agile’s remit, since they aim not to create conditions for workers to do a better job, but to change the nature of that job altogether. 
 It’s also worth considering how Agile might have played a role in creating a work culture that is increasingly revealed to be toxic for women, people of color, and members of gender minority groups. It’s an inescapable fact that the authors of the Agile Manifesto were a very specific group of people: white men who, whatever their varying experiences, have probably not spent much time in workplaces where they composed the minority. The working group has since acknowledged the deficit in the team’s diversity and vowed to incorporate a larger set of voices in the Agile Alliance, a nonprofit associated with the manifesto. 
 But when you survey a list of Agile-affiliated methodologies, alarm bells might go off if you’re the kind of person who’s faced discrimination or harassment at work. Many people testify to the utility of “pair programming,” for example, but the practice—in which two developers code together, each taking turns looking over the other’s shoulder—assumes that the two coders are comfortable with each other. Similarly, the warts-and-all breakdown of Agile “retrospectives” seems healthy, but I’ve watched them descend into a structureless series of accusations; everything depends on who’s leading the team. And Coraline Ada Ehmke, former community safety manager at GitHub, has described how fellow developers used the code review—ideally a low-stakes way for developers to check each other’s work—as an instrument of harassment. We’ve long known that eliminating bureaucracy, hierarchy, and documentation feels great, until you’re the person who needs rules for protection.
 Could Agile even have played a role in some of the more infamous failures of the tech industry? The thought occurred to me as I watched Frances Haugen, the former Facebook manager turned whistleblower, testifying before Congress in October 2021. If a company sets a goal of boosting user engagement, Agile is designed to get developers working single-mindedly toward that goal—not arguing with managers about whether, for example, it’s a good idea to show people content that inflames their prejudices. Such ethical arguments are incompatible with Agile’s avowed dedication to keeping developers working feverishly on the project, whatever it might be.
 This issue becomes especially pressing when one considers that contemporary software is likely to involve things like machine learning, large datasets, or artificial intelligence—technologies that have shown themselves to be potentially destructive, particularly for minoritized people. The digital theorist Ian Bogost argues that this move-fast-and-break-things approach is precisely why software developers should stop calling themselves “engineers”: engineering, he points out, is a set of disciplines with codes of ethics and recognized commitments to civil society. Agile promises no such loyalty, except to the product under construction.
 Agile is good at compartmentalizing features, neatly packaging them into sprints and deliverables. Really, that’s a tendency of software engineering at large—modularity, or “information hiding,” is a critical way for humans to manage systems that are too complex for any one person to grasp. But by turning features into “user stories” on a whiteboard, Agile has the potential to create what Yvonne Lam calls a “chain of deniability”: an assembly line in which no one, at any point, takes full responsibility for what the team has created. 
 The Agile Manifesto paints an alluring picture of workplace democracy. The problem is, it’s almost always implemented in workplaces devoted to the bottom line, not to workers’ well-being. Sometimes those priorities align; the manifesto makes a strong case that businesses’ products can be strengthened by worker autonomy. But they’re just as likely to conflict, as when a project manager is caught between a promise to a client and the developers’ own priorities. 
 “There’s a desire to use process as a way to manage ambiguity you can’t control,” said Mark Matienzo, a software engineer for an academic institution. “Especially in places where you’re seen as being somewhat powerless, whether that’s to the whims of upper management or administration. So you may not be able to influence the strategic direction of a project at a high level, but Agile allows that certain conception of developer free will.” The product manager I spoke to put it more bluntly: “Agile tricks people into thinking they have ownership over their work, but from a labor perspective, they literally do not have ownership, unless they have, like, significant stock options or whatever.” 
 Software development has never fit neatly into the timelines and metrics to which companies aspire. The sheer complexity of a modern application makes its development sometimes feel as much alchemical as logical. Computers may have emerged as military equipment, but completely subordinating programming work to the priorities of capital has been surprisingly difficult. When software engineering failed to discipline the unwieldiness of development, businesses turned to Agile, which married the autonomy that developers demanded with a single-minded focus on an organization’s goals. That autonomy is limited, however, as developers are increasingly pointing out. When applied in a corporate context, the methods and values that Agile esteems are invariably oriented to the imperatives of the corporation. No matter how flexible the workplace or how casual the meetings, the bottom line has to be the organization’s profits.
 There’s another angle on Agile, though. Some people I talked to pointed out that Agile has the potential to foster solidarity among workers. If teams truly self-organize, share concerns, and speak openly, perhaps Agile could actually lend itself to worker organization. Maybe management, through Agile, is producing its own gravediggers. Maybe the next crisis of software development will come from the workers themselves.
 A previous version of this article incorrectly identified Al Tenhundfeld as a co-author of the Agile Manifesto. The present version has been corrected.</content>
     </entry>
     <entry>
       <title>Cable’s Last Laugh</title>
         <link href="https://stratechery.com/2022/cables-last-laugh/"/>
       <updated>2022-05-10T14:42:51.000Z</updated>
       <content type="text">If there is one industry people in tech are eternally certain is doomed, it is cable. However, the reality is that cable is both stronger than ever and poised for growth; the reasons why are instructive to not just tech industry observers, but to tech companies themselves.
 The Creation of Cable
 Robert J. Tarlton was 29 years old, married with a son, when he volunteered to fight in World War II; thanks to the fact he owned a shop in Lansford, Pennsylvania that sold radios, he ended up repairing them all across the European Theater, learning about not just reception but also transmission. After the war Tarlton re-opened his shop, when Motorola, one of his primary suppliers, came out with a new television.
 Tarlton was intrigued, but he had a geography problem; the nearest television station was in Philadelphia, 71 miles away; in the middle lie the Pocono Mountains, and mountains aren’t good for reception:
 
 It turned out, though, that some people living in Summit Hill, the next community over, could get the Philadelphia broadcast signal; that’s where Tarlton sold his first television sets. Of course Tarlton couldn’t demonstrate this new-fangled contraption; his shop was in Lansford, not Summit Hill. However, it was close to Summit Hill: what if Tarlton could place an antenna further up the mountain in Summit Hill and run a cable to his shop? Tarlton explained in an interview in 1986:
 
   Lansford is an elongated town. It’s about a mile and a half long and there are about eight parallel streets bisected with cross streets about every 500 feet. There are no curves; everything is all laid out in a nice symmetrical pattern. Our business place was about three streets from the edge of Summit Hill and Lansford sits on kind of a slope. The edge of Lansford inclines from about a thousand feet above sea level to about fifteen hundred feet above sea level in Summit Hill. So to get television into our store, my father and I put an antenna partly up the mountain. No, we didn’t go all the way up, but we put up an antenna, kind of a crude arrangement, and then from tree to tree we strung a twin lead that was used in those days as a transmission line. We ran this twin lead, crossed a few streets, and into our store. And we had television.
 
 The basic twin lead was barely functional, but that didn’t stop everyone from demanding a television with a haphazard wire to their house; Tarlton realized that new coaxial cable amplifiers designed for a single property could be chained together, re-amplifying the signal so it could reach multiple properties. After getting all of the other electronics retailers on board — Tarlton knew that a clear signal would sell more TV sets, but that everyone needed to use the same system — the first commercial cable system was born, and it sold itself. Tarlton reflected:
 
   You didn’t have to advertise. You had to keep your door locked because the people were clamoring for service. They wanted cable service. You certainly didn’t have to advertise.
 
 People couldn’t get enough TV; Tarlton explained:
 
   Cable is dependent upon advances in technology because people who originally saw one channel wanted to see the second channel, wanted to see the third, and after you had five, they wanted more. So it was a case of more begets more. At one time three channels seemed to be quite sufficient but when we added one more channel, it created a new interest in the cable system. People then had variety. They had alternatives. At one time later five channels seemed enough. As a matter of fact, a man who is often quoted, former FCC Commissioner Ken Cox, said that five channels was enough, and that’s quite a story in itself. The engineers were able to continually refine equipment to add more channels…All these technical advances‑‑continuing advances, automatic gain control, automatic temperature compensation, etc. have made cable what it is today.
 
 One of the most important technological developments was satellite: now cable systems could get signals both more reliably and from far further away; this actually flipped geography on its head. Tarlton said:
 
   At that time we went to the highest possible point to look to the transmitter. Today with satellites, we go to the lowest possible point because we don’t want the interference from other signals. So it is ironic that we have changed so much from what we used to do.
 
 Within these snippets is everything that makes the cable business so compelling:
 
 Cable is in high demand because it provides the means to get what customers most highly value.
 Cable works best both technologically and financially when it has a geographic monopoly.
 Cable creates demand for new supply; technological advances enable more supply, which creates more demand.
 
 It’s that last bit about satellites being better on lower ground that stands out to me, though: as long as you control the wires into people’s houses you can and should be pragmatic about everything else.
 Cable’s Evolution
 Tarlton would go on to work for a company called Jerrold Electronics, which pivoted its entire business to create equipment for cities that wanted to emulate Lansford’s system; Tarlton would lead the installation of cable systems across the United States, which for the first two decades of cable mostly retransmitted broadcast television.
 The aforementioned satellite, though, led to the creation of national TV stations, first HBO, and then WTCG, an independent television station in Atlanta, Georgia, owned by Ted Turner. Turner realized he could buy programming at local rates, but sell advertising at national rates via cable operators eager to feed their customers’ hunger for more stations. Turner soon launched a cable only channel devoted to nothing but news; he called it the Cable News Network — CNN for short (WTCG would later be renamed TBS).
 Jerrold Electronics, meanwhile, spun off one of the cable systems it built in Tupelo, Mississippi to an entrepreneur named Ralph Roberts; Roberts proceeded to systematically buy up community cable systems across the country, moving the company’s headquarters to Philadelphia and renaming it to Comcast Corporation (Roberts would eventually hand the business off to his son, Brian). Consolidation in the provision of cable service proceeded in conjunction with consolidation in the production of content, an inevitable outcome of the virtuous cycle I noted above:
 
 Cable companies acquired customers who wanted access to content
 Studios created content that customers demanded
 
 The more customers that a cable company served, the stronger their negotiating position with content providers; the more studios and types of content that a content provider controlled the stronger their negotiating position with cable providers. The end result were a few dominant cable providers (Comcast, Charter, Cox, Altice, Mediacom) and a few dominant content companies (Disney, Viacom, NBC Universal, Time Warner, Fox), tussling back-and-forth over a very profitable pie.
 Then came Netflix, and tech industry crowing about cord cutting.
 Netflix and other streaming services were obviously bad for television: they did the same job but in a completely different way, leveraging the Internet to provide content on-demand, unconstrained by the linear nature of television that was a relic of cable’s origin with broadcast TV. Here, though, cable’s ownership of the wires was an effective hedge: the same wires that delivered linear TV delivered packet-based Internet content.
 Moreover, this didn’t simply mean that cable’s TV losses were made up for by Internet service: Internet service was much higher margin because companies like Comcast didn’t need to negotiate with a limited number of content providers; everything on the Internet was free. This has meant that the fortunes of cable companies has boomed over the last decade, even as cord-cutting has cut the cable TV business by about a third.
 Cable companies today, though, are yet another category down from their pandemic highs, thanks to fear that the broadband growth story is mostly over; fiber offers better performance, 5G opens the door to wireless in the home, and anyone who doesn’t have broadband now is probably never going to get it. I think, though, this underrates the strategic positioning of cable companies, and ignores the industry’s demonstrated ability to adapt to new strategic environments.
 The Wireless Opportunity
 From the Wall Street Journal in 2011:
 
   Verizon Wireless will pay $3.6 billion to buy wireless spectrum licenses from a group of cable-television companies, bringing an end to their years-long flirtation with setting up its own cellphone service. The sellers—Comcast Time Warner Cable Inc. and Bright House Networks—acquired the spectrum in a government auction in 2006 and now will turn it over to the country’s biggest wireless carrier at more than a 50% markup. While cable companies have dabbled with wireless, the spectrum has largely sat around unused, prompting years of speculation about the industry’s intentions…
   Under the deal, Verizon Wireless will be able to sell its service in the cable companies’ stores. The carrier, in turn, will be able to sell the cable companies’ broadband, video and voice services in its stores. Verizon’s FiOS service only reaches 14% of U.S. households, according to Bernstein Research. In four years’ time, the cable companies will have the option to buy service on Verizon’s network on a wholesale basis and then resell it under their own brand.
   The joint marketing arrangement “amounts to a partnership between formerly mortal enemies,” wrote Bernstein analyst Craig Moffett in a research note.
 
 Moffett would later partner with Michael Nathanson to form their own independent research firm (called MoffettNathanson, natch); Moffett released an updated report last month that explained how that Verizon deal ended up playing out:
 
   When Comcast and Time Warner Cable sold their AWS-1 spectrum to Verizon back in 2011, they believed at the time that they were walking away from ever becoming facilities-based wireless players. They therefore viewed it as imperative that the sale come with an MVNO agreement with Verizon to compensate for that forfeiture. They got precisely what they wanted, an MVNO contract with Verizon that was described at the time as “perpetual and irrevocable“…
   It took another six years after that transaction before Comcast finally launched Xfinity Mobile in mid-2017. Charter [which merged with Time Warner Cable, acquiring the latter’s MVNO rights] followed suit a year later…in four short years, the Cable operators have become the fastest growing wireless providers in the country, accounting for nearly 30% of wireless industry net additions. Cable’s 7.7 million mobile lines represent ~2.5% of the U.S. mobile phone market (including prepaid and postpaid phones).
 
 As Moffett notes, this growth is particularly impressive given that most cable companies couldn’t feasibly offer family plans under the terms of the original deal, which was negotiated before such a concept even existed; the deal has been re-negotiated, though, and almost certainly to the cable companies’ advantage: if Verizon is going to lose customers to an MVNO, it would surely prefer said MVNO be on their network; this means that the cable companies have negotiating leverage.
 What, though, makes the cable companies such effective MVNOs? One of the most interesting parts of Moffett’s note was proprietary data about the amount of data used by cellular subscribers; it turns out that cable MVNO customers are far more likely to consume data over WiFi, perhaps because of cable company out-of-home WiFi hot spots. This could become even more favorable in the future as cable companies build out Citizens Broadband Radio Service (CBRS) service, particularly in dense areas where cable companies have wires from which to hang CBRS transmitters. Moffett writes:
 
   In a perfect world, Cable will offload traffic onto their own facilities where doing so is cheap (high density, high use locations) and leave to Verizon the burden of carrying traffic where doing so is/would be expensive. Because the MVNO agreement is “perpetual and irrevocable,” and is based on average prices (i.e., the same price everywhere, whether easy or hard to reach), Cable is presented with a perfect ROI arbitrage; they can take the high ROI parts of the network for themselves, and leave the low ROI parts of the network to their MVNO host… all without sacrificing anything with respect to their national coverage footprint.
 
 One can understand why Verizon gave the cable companies these rights in 2011; the phone company desperately needed spectrum, and besides, everyone knew that MVNOs could never be economically competitive with their wholesale providers, who were the ones that actually made the investments in the network.
 That’s the thing, though: cable companies had their own massive build-out, one that was both much older in its origins and, because it connected the house, actually carried more data. This was to the benefit of Verizon and other cellular providers, of course: a Verizon iPhone uses WiFi at home just as much as a Comcast iPhone does; here, though, it was cable that had internet economics on its side: there is no competitive harm in giving equal access to an abundant resource; it is cellular access that is scarce, which means that the cable companies’ MVNO deal, in conjunction with their out-of-home Internet access options, gives them a meaningful advantage.
 Customer Acquisition
 Moffett spends less time on customer acquisition; anecdotally speaking, it’s clear that Charter’s Spectrum, which provides the cable service I consume (via the excellent Channels application), is pushing wireless service hard: phones are front-and-center in their stores, and Spectrum wireless commercials fill local inventory. Moreover, this is a well-trodden playbook: cable companies came to dominate the fixed line phone service business simply because it was easier to get your TV, Internet, and phone all in the same place (and of those, the most important was TV, and now Internet); it’s always easier to upsell an existing subscriber than it is to acquire a new one.
 Of course cable companies long handled customer acquisition for content creators — that was the cable bundle in a nutshell. What is interesting is how this customer acquisition capability is attractive to the companies undoing that bundle: Netflix, for example, put its service on Comcast’s set-top boxes in 2016, and made a deal for Comcast to sell its service in 2018. I wrote in a Daily Update at the time:
 
   Netflix, meanwhile, is laddering up again: the company doesn’t actually need a billing relationship with end users, it just needs ever more end users (along with the data about what they watch) to spread its fixed content costs more widely; the company said in its shareholder letter that:
 
     These relationships allow our partners to attract more customers and to upsell existing subscribers to higher ARPU packages, while we benefit from more reach, awareness and often, less friction in the signup and payment process. We believe that the lower churn in these bundles offsets the lower Netflix ASP.
   
   What is particularly interesting is that this arrangement moves the industry closer to the endgame I predicted in The Great Unbundling…
 
 “Endgame” was a bit strong: that Article was, as the title says, about unbundling; one of my arguments was that the traditional cable TV bundle would become primarily anchored on live sports and news, while most scripted content went to streaming. That is very much the case today (TBS, for example, is abandoning scripted content, while becoming ever more reliant on sports). What was a mistake was insinuating that this was the “end”; after all, as Jim Barksdale famously observed, the next step after unbundling is bundling.
 To that end, Netflix + Xfinity TV service was a bundle of sorts, but the real takeaway was that Comcast was fine with being simply an Internet provider (which ended up helping with TV margins, since cable companies mostly gave up on fighting to keep cord cutters). Shortly after the Netflix deal Comcast launched Xfinity Flex, a free 4K streaming box for Internet-only subscribers that included a storefront for buying streaming services (which would be billed by Comcast). You can even subscribe to digital MVPDs like YouTube TV!
 
 The first takeaway of an offering like Xfinity Flex ties into the wireless point: cable companies already have a billing relationship with the customer — because they provide the most essential utility for accessing what is most important to said customer — which makes them particularly effective at customer acquisition. That is why Netflix, YouTube, etc. are all willing to pay a revenue share for the help.
 The Great Rebundling?
 The second takeaway, though, is that the cable companies are better suited than almost anyone else to rebundle for real. Imagine a “streaming bundle” that includes Netflix, HBO Max, Disney+, Paramount+, Peacock, etc., available for a price that is less than the sum of its parts. Sounds too good to be true, right? After all, this kind of sounds like what Apple was envisioning for Apple TV (the app) before Netflix spoiled the fun; I wrote in Apple Should Buy Netflix in 2016:
 
   Apple’s desire to be “the one place to access all of your television” implies the demotion of Netflix to just another content provider, right alongside its rival HBO and the far more desperate networks who lack any sort of customer relationship at all. It is directly counter to the strategy that has gotten Netflix this far — owning the customer relationship by delivering a superior customer experience — and while Apple may wish to pursue the same strategy, the company has no leverage to do so. Not only is the Apple TV just another black box that connects to your TV (that is also the most expensive), it also, conveniently for Netflix, has a (relatively) open app platform: Netflix can deliver their content on their terms on Apple’s hardware, and there isn’t much Apple can do about it.
 
 Six years on and Netflix is in a much different place, not only struggling for new customers but also dealing with elevated churn. Owning the customer may be less important than simply having more customers, particularly if those customers are much less likely to churn. After all, that’s one of the advantages of a bundle: instead of your streaming service needing to produce compelling content every single month, you can work as a team to keep customers on board with the bundle.
 The key point about a bundle, as longtime YouTube executive and Coda CEO Shishir Mehrotra has written, is that it minimizes SuperFan overlap while maximizing CasualFan overlap; in other words, effective bundles have more disparate content that you are vaguely interested in, instead of a relatively small amount of focused content that you care about intensely. This makes the bundle concept even more compelling to new entrants in the streaming wars, who may not have as large of libraries as Netflix, and certainly don’t have as many subscribers over which to spread their content costs.
 Moreover, the fact that the streaming services have largely done their damage to traditional TV, leaving the cable TV bundle as the sports and news bundle, means it is actually viable to create a lower-priced bundle than what was previously available (if you don’t want sports). After all, you’re not cannibalizing TV, but rather bringing together what has long since been broken off (unbundling then bundling!).
 
 This isn’t something that is going to happen overnight: despite the fact that bundles are good for everyone it is hard to get independents into a bundle as long as they are growing; Netflix’s recent struggles are encouraging in this regard, particularly if other streaming services start to face similar headwinds. Moreover, cable companies are not the only entities that will seek to pull something like this off: Apple, Amazon, Google, and Roku already make money from revenue shares on streaming subscriptions they sell; all of them sell devices that can be used as interfaces for selling a bundle. And, of course, there are more Internet providers than just the cable companies: there is fiber, wireless, and even Starlink.
 The breadth of the cable company bundle, though, is unmatched: not only might it include streaming services, but also linear TV; more than that, this is the company selling you Internet access, and increasingly wireless phone service. That gives even more latitude for discounts, and perks like no data caps on streamed content, not just at home but also on your phone.
 All of these advantages go back to Robert J. Tarlton and Lansford, Pennsylvania. A recurring point on Stratechery this past year has been the durability and long-term potential inherent in technology rooted in physical space. I wrote in Digital Advertising in 2022:
 
   Real power in technology comes from rooting the digital in something physical: for Amazon that is its fulfillment centers and logistics on the e-commerce side, and its data centers on the cloud side. For Microsoft it is its data centers and its global sales organization and multi-year relationships with basically every enterprise on earth. For Apple it is the iPhone, and for Google is is Android and its mutually beneficial relationship with Apple (this is less secure than Android, but that is why Google is paying an estimated $15 billion annually — and growing — to keep its position). Facebook benefited tremendously from being just an app, but the freedom of movement that entailed meant taking a dependency on iOS and Android, and Apple has exploited that dependency in part, if not yet in full.
 
 For cable companies, power comes from a wire; it would certainly be ironic if the cord-cutting trumpeted by tech resulted in cable having even more leverage over customers and their wallets than the pre-Internet era.</content>
     </entry>
     <entry>
       <title>Magical SVG Techniques</title>
         <link href="https://smashingmagazine.com/2022/05/magical-svg-techniques/"/>
       <updated>2022-05-10T10:00:00.000Z</updated>
       <content type="text">SVGs have become more and more popular in the past few years. For good reasons. They are scalable, flexible, and, most importantly, lightweight. And, well, they have even more to offer than you might think. We came across some magical SVG techniques recently that we’d love to share with you. From SVG grids and fractional SVG stars to SVG masks, fancy grainy SVG gradients, and handy SVG tools. We hope you’ll find something useful in here.
 By the way, a while ago, we also looked at SVG Generators — for everything from shapes and backgrounds to SVG path visualizers, cropping tools, and SVG → JSX generators. If you’re tinkering with SVG, these might come in handy, too.
 Generative SVG Grids
 Generative art is a wonderful opportunity for everyone who would love to create art but feels more at home in code. Let’s say you want to create geometric patterns, for example. Generative art will take away the difficult decisions from you: What shapes do I use? Where do I put them? And what colors should I use? If you want to give it a try, Alex Trost wrote a tutorial on creating generative art with SVG grids that is bound to tickle your creativity — and teach you more about SVG.
 
 The generative art that Alex creates is a grid of blocks with a random number of rows and columns. Each block has a randomly chosen design and colors from a shared color palette. Alex takes you step by step through the process of coding this piece: from setting up the grid and creating isolated functions to draw SVGs to working with color palettes, adding animations, and more. A fun little project — not only if you’re new to generative art and creative coding.
 Generative Landscape Rolls
 An awe-inspiring project that bridges the gap between a century-old tradition and state-of-the-art coding is {Shan, Shui}. Created by Lingdong Huan and inspired by traditional Chinese landscape rolls, it creates procedurally generated, infinitely-scrolling Chinese landscapes in SVG format. The mountains and trees in the landscape are modeled from scratch using noise and mathematical functions. Fascinating!
 
 Now, if you’re asking yourself how something as complex might work, you’re not alone. Victor Shepelev wanted to get behind the secret of {Shan, Shui}* and made it his advent project to understand how it works. And, indeed, it took him 24 days to fully dig into the code. He summarized his findings in a series of articles.
 SVG Paths With Masks
 SVGs have a lot of benefits compared to raster images. They are small in size, scalable, animatable, they can be edited with code, and a lot more. You can’t get the textured feel that raster graphics can provide, though. However, we can combine the strengths of vector and raster to create some charming effects. Like Tom Miller did in his Silkscreen Squiggles demo.
 
 Silkscreen Squiggles is an animation where squiggles fill a rectangular canvas. What makes the squiggles special is that they appear to have a paintbrush texture. The secret: a mask with an alpha layer that gives the simple squiggly paths their texture. Alex Trost dissects how it works. Inspiring!
 Grainy Gradients
 Noise is a simple technique to add texture to an image and make otherwise solid colors or smooth gradients more realistic. But despite designer’s affinity for texture, noise is rarely used in web design. Jimmy Chion explores how we can add texture to a gradient with only a small amount of CSS and SVG.
 
 The trick is to use an SVG filter to create the noise, then apply that noise as a background. Layer it underneath your gradient, boost the brightness and contrast, and that’s already it. Potential use cases could be light and shadows or holographic foil effects, for example. The core of this technique is supported by all modern browsers. A clever visual effect to add depth and texture to a design.
 Adding Texture And Depth
 “Analog” materials like paint and paper naturally add depth to an artwork, but when working digitally, we often sacrifice the organic depth they provide for precision and speed. Let’s bring some texture back into our work! George Francis shares three ways to do so.
 
 The techniques that George explores are quite simple but effective. Tiny random shapes added to a canvas at random points, solid shape fills with lines, and non-overlapping circles distributed evenly but randomly with an algorithm. Inspiring ideas to tinker with.
 Cut-Out Effects With CSS And SVG
 In a recent front-end project that Ahmad Shadeed was working on, one of the components included a cut-out effect where an area is cut out of a shape. And because there are multiple ways to create such an effect in CSS or SVG, he decided to explore the pros and cons that each of the solutions brings along.
 
 In his blog post “Thinking About The Cut-Out Effect”, Ahmad takes a look at three different use cases for a cutout effect: an avatar with a cut-out status badge that indicates that a user is currently online, a “seen avatar” that consists of overlapping circle avatars that are indicators that a message has been seen in a group chat, as well as a website header with a cut-out area behind a circular logo. Ahmad presents different solutions for each use case — SVG-only, CSS-only, and a mix of both — and explains the pros and cons of each one of them. A comprehensive overview.
 Fractional SVG Stars
 Are you building a rating component and you want it to support fractional values like 4.2 or 3.7 stars but without using images? Good news, you can achieve fractional ratings with only CSS and inline SVG. Samuel Kraft explains how it works.
 
 The component basically consists of two parts: a list of star icons based on the max rating and an “overlay” div that will be responsible for changing the colors of the stars underneath. This is the magic that makes the fractional part work. The technique is supported in all modern browsers; for older browsers, you can fall back to opacity instead. Clever!
 Generative Mountain Ridge Dividers
 When Alistair Shepherd built his personal website, he wanted to have section dividers that match the mountain theme of the site. But not any mountain dividers, but dividers with unique ridges for every divider.
 
 Instead of creating a variety of different dividers manually, Alistair decided to use a combination of SVG and terrain generation, a technique that is usually used in game development, to generate the dividers automatically. In a blog post, he explains how it works.
 If you’re up for some more horizontal divider inspiration, also be sure to check out Sara Soueidan’s blog post “Not Your Typical Horizontal Rules” in which she shows how she turned a boring horizontal line into a cute “birds on a wire” divider with the help of some CSS and SVG.
 Flexible Repeating SVG Masks
 Sometimes it’s a small idea, a little detail in a project that you tinker with and that you can’t let go off until you come up with a tailor-made solution to make it happen. Nothing that seems like a big deal at first glance, but that requires you to think outside the box. In Tyler Gaw’s case, this little detail was a flexible header with a little squiggle at the bottom instead of a straight line. The twist: to make the component future-proof, Tyler wanted to use a seamless, horizontal repeating pattern that he could color with CSS.
 
 To get the job done, Tyler settled on flexible repeating SVG masks. SVG provides the shape, CSS handles the color, and mask-image does the heavy lifting by hiding anything in the underlying div that doesn’t intersect with the shape. A clever approach that can be used as the base for some fun experiments.
 Swipey Image Grids
 When you think of “SVG animation”, what comes to your mind? Illustrative animation? Well, SVG can be useful for much more than pretty graphics. As Cassie Evans points out, a whole new world of UI styling opens up once you stop looking at SVG purely as a format for illustrations and icons. One of her favorite use cases for SVG: responsive animated image grids.
 
 Cassie doesn’t build her image grid on CSS Grid but uses SVG’s internal coordinate system (which is responsive by design) to design the grid layout. She then adds images to the grid and positions them with preserveAspectRatio. clipPath “swipes” the images in. The final animation relies on GreenSock to ensure that the transforms work consistently across browsers. If you want to dig deeper into the code, be sure to check out Cassie’s blog post in which she explains each step in detail.
 Animated SVG Debit Card Illustrations
 What if you could animate a debit card design? Probably not on an actual physical card, but rather for a landing page where you’d like to drive interest towards the card’s design or features? Well that’s an unusual challenge to tackle, and Tom Miller decided to take it on.
 
 In a series of SVG debit card animations, Tom uses GreenSock to animate SVG paths and shapes smoothly, so every card literally comes to life on its own, transforming, rotating, and scaling beautifully, alongside just a few lines of JavaScript. A wonderful inspiration for your next landing page design!
 Raster Image To SVG Converter
 You need to quickly convert a raster image into an SVG? Then SVGcode is for you. The progressive web app converts image formats like JPG, PNG, GIF, WebP, and AVIF to vector graphics in SVG format.
 
 To convert an image, drop your raster image into the SVGcode app, and the app will trace the image, color by color, until a vectorized version of the input appears. You can choose between color SVG and monochrome SVG and there also are a number of customization settings to improve the output further, by suppressing speckles and adjusting the color, for example. If you install the PWA, you can even use it as a default file handler on your machine. A real timesaver.
 Download SVGs From Any Site
 A handy little tool to enhance your SVG workflow is SVG Gobbler. The browser extension finds the vector content on the page you’re viewing and gives you the option to download, optimize, copy, view the code, or export it as an image.
 
 When you click the browser extension, it shows you all SVGs detected on the site. You can quickly download the ones you like or copy them to your clipboard. When you view the code, you can toggle optimization options from SVGO — to beautify the markup or clean up attributes or numeric values, for example. And if you need a PNG version of an SVG, you can export it in any size you want. A fantastic addition to any developer’s toolkit.
 Scaling SVGs Made Simple
 Scaling svg elements can be a daunting task, since they act very differently than normal images. Amelia Wattenberger came up with an ingenious comparison to help us make sense of SVGs and their special features: “The svg element is a telescope into another world.”
 
 Based on the idea of the telescope, Amelia explains how to use the viewBox property to zoom in or out with your “telescope”, and, thus, change the size of your &lt;svg&gt;. A small tip that works wonders.
 Wrapping Up
 We hope that these techniques will tickle your curiosity and inspire you to try some SVG magic yourself. If you came across an interesting SVG technique that left you in awe, please don’t hesitate to share it in the comments below. We’d love to hear about it. Happy creating!
 More On SVG
 
     SVG Generators
     A Practical Guide To SVG And Design Tools
     SVG Circle Decomposition To Paths
     Accessible SVGs: Perfect Patterns For Screen Reader Users
     Also, subscribe to our newsletter to not miss the next ones.
 </content>
     </entry>
     <entry>
       <title>Thoughts on spectrums</title>
         <link href="https://daverupert.com/2022/05/spectrums/"/>
       <updated>2022-05-09T13:14:00.000Z</updated>
       <content type="text">One principle I’ve come to embrace is that the answer is rarely ever a spectrum. At first glance, a single-axis spectrum might make perfect sense, a reduction of the problem, a right and a left, a binary dichotomy but with a grey area in the middle. But as I grow older a spectrum falls short for me in a couple different dimensions.
 Take color for instance. As kids we learned about the ROYGBIV visible spectrum of light, the zone between infrared light and ultraviolet light that most of us humans can see.
 
 The rainbow, a naturally occurring ubiquitous concept. Children love rainbows. A light spectrum makes for a great science experiment or album cover. All the hues of visible light rolled into one gradient stripe.
 In design tools the different hues are often represented in a 360º color wheel, an infinitely wrapping spectrum of rainbow colors.
 
 A beautiful rainbow circle, I hope my computer is okay.
 Then… we learn about saturation. Colors can be vibrant and saturated or duller and unsaturated shades. You can tune color wheels to provide this effect, but for the best representation of color saturation, we need another axis as if color exists on an XY chart.
 
 Thus color pickers! The horizontal X-axis reflects the hue across the ROYGBIV spectrum and the vertical Y-axis reflects the saturation. Adding another dimension helps explain color and saturation in a single image. Two axes are better than one.
 But look! On this particular color picker there’s a slider to control another aspect of color: lightness, a way to tint a color lighter and darker. This hints that our XY plot isn’t enough.
 This is where math and algorithmic representations of colors and lightwaves start coming into play. On the web, we can express over 16 million colors using a mix of red, green and blue pixels or expressing a color in an alternate method of combining hue, saturation and lightness; each three properties, three dimensions of differentiation. We actually need three axes to visualize our color possibilities which we can do in a three dimensional space.
 
   
   
 
 The RGB color space looks like a gay Rubik’s cube and the HSL color space looks like a birthday cake drenched in icing. Each color-space (RGB, HSL, CMYK, Lab, etc) has its own different space of representable colors! The algorithm for each color-space gives us a different perspective. David Briggs has a collection of renderings of color spaces next to each other.
 
 The shape of the color space is somewhat dependent on the math and algorithmic shorthand used by the color profile to derive the colors. Steve Eddins generated a beautiful rendering of a rotation of the Lab color-space and it’s one of my favorite renderings I’ve come across.
 
   
   
   
 
 There’s more color spaces to explore! I found an old app for Windows called Gamutvision that tries to visualize these color spaces. John Austin explains the mathematical history of color spaces. Craig Hockenberry —who wrote a book on color—  wrote about Apple’s new wide-gamut displays and links to an even more in-depth look at color gamuts on Mac. There’s newer and wider gamuts on the horizon too, colors we’ve never seen on the screen before!
 People see color differently. Not just that they bought a nice TV or phone with a wider-gamut and more accurate algorithm, but also due to a variety of cultural or physiological differences as well.
 
 In Japan they call the color of their verdant bamboo mountains, leafy green vegetables, green apples, and the color of the “go” signal on a traffic light; 青 (blue). I learned the history of this in college, but it always caught me by surprise an entire nation could be so collectively wrong… or was I the one who was wrong in that particular cultural setting?
 Personal perspective factors in too, even factoring out colorblindness. My wife calls the international walk signal pictogram “the blue walking man” and while it does have a blue-ish tint, I guffaw because it’s clearly white. And don’t get me started about the color of a tennis ball; she insists it’s “fluorescent green” and I recognize it as the color “fluorescent yellow” —as in “optic yellow”, the literal color name for a tennis ball— but it’s hard to say who’s right. We’re not alone in our disagreement and the Wikipedia page for tennis ball is now locked after hundreds of edit wars over dozens of years.
 So that’s color. It can be a spectrum and you wouldn’t be wrong, but the possibilities of color are more accurately modeled in a multi-dimensional space. I’m not anti-spectrum, they’re an improvement over binaries, but they rarely sum up the entire issue at hand. Factor in your culture’s understanding of color or your personal understanding of color via the rods and cones in your eyeballs… color is both extremely objective and extremely subjective. And on a computer, the hardware algorithm that is showing you the colors might also limit your perception of color.
 Anyways, those are my thoughts on spectrums. I feel you can apply multi-dimensional thinking to a lot of ideas: good vs. evil, gender, politics, disabilities, technology choices, kanbans, video game character balance, and the list goes on. Your mileage may vary, but I think there’s one thing we can all agree on; most dichotomies are false. 😉</content>
     </entry>
     <entry>
       <title>Performance Game Changer: Browser Back/Forward Cache</title>
         <link href="https://smashingmagazine.com/2022/05/performance-game-changer-back-forward-cache/"/>
       <updated>2022-05-09T10:30:00.000Z</updated>
       <content type="text">First of all, it would be remiss of me to give the Chrome browser all the credit here, when the other two main browsers (Safari and Firefox) have had this concept for quite some time now, though there are some subtle differences in all three implementations.
 So, Chrome was playing catch-up here. However, as the world’s most popular browser and the only browser feeding back the Core Web Vitals information for any search ranking boasting, Chrome getting this (finally, some might say) is important. Plus, they’ve created some more transparency about this, both in documentation and tooling. Not to mention the many other Chromium-based browsers (e.g. Edge, Opera, Brave) will now also have gotten this functionality too.
 With that caveat out of the way, let’s get to the guts of the article: What is the Back/Forward Cache and why does it matter so much? As its name implies, this is a special cache used to remember the web page as you browse away from that web page, so if you browse back to it later, it can load a lot quicker.
 Think about the number of times you visit a home page, click on an article, then go back to the home page to view another article? Or you click back, then realize you forgot to make a note of something on that article, so click forward again. Similarly from cross-site navigation — think Google search results or the like and then clicking back. All those navigations can benefit from the Back/Forward Cache to instantly restore the page.
 Didn’t The HTTP Cache Do All That Anyway?
 Browsers have lots of caches, the most well-known of which is the HTTP Cache that stores copies of downloaded resources on a local drive for later reuse. Ensuring your website caches most of its assets for future uses has long been touted as essential for web performance. Sadly, however, it is not yet universal, but there are plenty of other articles written about that.
 Using the HTTP Cache can avoid the need to download the same resources over and over again. Going back to the example of visiting a home page and then going to an article page. There are likely lots of shared resources (site-wide CSS and JavaScript, logos, other images, etc.), so reusing them from that home page visit is a good saving. Similarly, before the Back/Forward Cache came along, being able to reuse these assets if going back to the home page was a good gain — even if it wasn’t instant.
 Downloading resources is a slow part of browsing the web no doubt, but there is a lot more to do as well as that: parse the HTML, see and fetch what resources you need (even if they can be gotten relatively quickly from the HTTP Cache), parse the CSS, layout the page, decode the images, run the oodles of JavaScript we so love to load our pages with… etc.
 WebPageTest is one of the few web performance testing tools that actually tests a reload of the page using a primed HTTP Cache — most of the other tools just flag if your HTTP resources are not explicitly set to be cached. So, running your site through WebPageTest will show the initial load takes some time, but the repeat view should be faster. But it still takes a little time, even if all the resources are being served from the cache. As an aside, that repeat view will likely not be fully representative anyway. Loading with an empty cache or a completely primed cache are the two extremes, but the reality is that you might have a semi-primed cache with some but not all of the resources cached already. Especially if you navigate from another page on the site.
 Here’s an experiment for you. In another tab, load https://www.smashingmagazine.com. Loads pretty quickly, doesn’t it? The Smashing team has done a lot to make a fast website, so even a fresh load (though this experiment may not be a completely fresh load if you came to this article from the home page). Now close the tab. Now open another new tab and load https://www.smashingmagazine.com again. That’s the reload with a fully primed HTTP cache from that initial load. Even faster yeah? But not quite instant. Now, in that tab, click on an article link from https://www.smashingmagazine.com and, once it’s loaded, click back — much faster yeah? Instant, some might say.
 You can also disable the Back/Forward Cache in Chrome at chrome://flags/#back-forward-cache if you want to experiment more, but the above steps should hopefully be a sufficient enough test to give a rough feel for the potential speed gains.
 It might not seem like that much of a difference, but repeat the experiment over a bad mobile connection, or using a heavier and slower website, and you’ll see a big difference. And if you browse a few articles on this site, returning to the home page each time, then the gains multiply.
 Why is the Back/Forward Cache so much faster?
 The Back/Forward Cache keeps a snapshot of the loaded page including the fully rendered page and the JavaScript heap. You really are returning to the state you left it in rather than just having all the resources you need to render the page.
 Additionally, the Back/Forward Cache is an in-memory cache. The HTTP Cache is a disk cache. A disk cache is faster than having to fetch those resources from the network (though not always, oddly enough!), but there’s an extra boost from not even having to read them from disk. If you ever wondered why browsers need so much memory just to display a simple page — it’s partly for optimizations like this. And mostly, that’s a good thing.
 Finally, not all resources are allowed to be cached in the HTTP Cache. Many sites don’t cache the HTML document itself, for example, and only the resources. This allows the page to be updated without waiting for a cache expiry (subresources normally handle this with unique cache-busting URLs, but that can’t be used on the main document, as you don’t want to change the URL each time), but at the cost of the page having to be fetched each time. Unless you’re a real-time news website or similar, I always recommend caching the main document resource for at least a few minutes (or even better hours) to speed up repeat visits. Regardless, the Back/Forward Cache is not limited to this as it is restoring a page, rather than reloading a page — so again another reason why it can be faster. However, this isn’t quite a clear cut, and there is still work going on in this space in Chrome, though Safari has had this for a bit now.
 OK, But Is It Really That Much Faster For Most Web Browsing Or Are We Just Nitpicking Now?
 The Core Web Vitals initiative gives us a way of seeing the impact on real user web browsing, and the Core Web Vitals Technology Report allows us to see these figures by site based on HTTP Archive monthly crawls. Some in the e-commerce web performance community noticed the unexplained improvement in January’s figures:
 CRUX data for Jan-22 is out. Let&#x27;s see how various eCommerce platforms did this month. #webperf #perfmatters. Change in % of Origins worldwide having good CWVs (compared to last month)@LightspeedHQ - 18% up@ShopifyEng - 9% up@opencart - 9% up@squarespace - 5% up pic.twitter.com/wbnDdGeRWl— Rockey Nebhwani (@rnebhwani) February 9, 2022
 
 Annie from the Chrome team confirmed this was due to the Back/Forward Cache rollout and was a bit surprised just how noticeable it was:
 This is a bit surprising, but the improvement is user-visible; it is caused by the bfcache rollout (https://t.co/9raiXQaYwU).What&#x27;s happening is that when a web page is restored from cache instead of fully loading, it skips all the layout shifts from load. Big CLS improvement!— Annie Sullivan (@anniesullie) February 9, 2022
 
 Although it is restoring the page from a previous state, as far as Core Web Vitals, as measured by Chrome, is concerned, this is still page navigation. So, the user experience of that load is still measured and counts as an additional page view — just with a much faster and more stable load!
 Be aware, however, that other analytics may not see this as a page load and need to take extra efforts to also measure these. So for some of your tooling you may even have noticed a drop in visitor count and performance as these fast repeat views (which were measured), became even faster restores (but which potentially aren&#x27;t now measured), so your overall average page loads measured appear to have got slower. Again, I’ll refer you to Philip Walton’s article including a section on how bfcache affects analytics and performance measurement, for more information.
 I’ve another more dramatic example, closer to home to share, but first a little more background.
 Do I Need To Do Anything, Or Does My Site Just Get A Free Boost?
 Well, this isn’t just some puff piece for the Chrome team — especially since they were late to the party! There is an action for sites to take to ensure they are benefiting from this speed gain because there are a number of reasons you may not be.
 Implementing a Back/Forward Cache sounds simple, but there are a million things to consider that could go wrong. I’ve kept it reasonably light on technical details here (check out Philip Walton’s Back/forward cache article for more technical information), but there are many things that could go wrong with this.
 Browser makers have to make sure it’s safe to just restore pages like this, and the problem is many sites are not written with the expectation that a page can be restored like this and think (not entirely unreasonably!) that when the page is navigated away from that the user isn’t coming back without a full page reload.
 For example, some pages may register an unload event listener in JavaScript to perform some clean-up tasks under the assumption the user is finished with this page. That doesn’t work well if the page is then restored from the Back/Forward Cache. Now, to be honest, the unload event listener has been seen as a bad practice for a while now as it is not reliably fired (e.g. if the browser crashes, or the browser is backgrounded on a mobile and then killed), but the Back/Forward cache gives a more pressing reason why it should be avoided because browsers will not use the Back/Forward Cache if your page or any of its resources uses an unload event listener. 
 There are a number of other reasons why your page might not be eligible for the Back/Forward cache. Counterintuitively, for a web performance feature, some of these reasons are due to other web performance features:
 
 Unload listeners were (and in some cases still are) often used log data when the page was finished with. This is particularly useful for Real User Monitoring web performance tools — used by sites that obviously have an interest in ensuring the best performance! There are other better events now that should now be used instead that are compatible, including pagehide and freeze.
 Using a dedicated worker to offload work from the main thread is another performance technique that at the moment makes the website ineligible for the Back/Forward cache. This is used by some popular platforms like Wix, but a fix for sites using Web Workers is coming soon!
 Using an App Install Banner also makes a site ineligible, which affected smashingmagazine.com (we’ll get back to that momentarily), and again supporting App Banners is being actively worked on.
 
 Those are some of the more common ones, but there are lots of reasons why a site may not be eligible. You can see the complete list of reasons for the Chrome source code, with a bit more explanation in this sheet.
 Back/Forward Cache Testing Tool
 Helpfully, rather than having to examine each one, the Chrome team added a test to Chrome Dev Tools under the Application tab:
 
 Clicking on that inviting little blue button will run the test and should hopefully give you a successful message:
 
 If you are getting an ineligibility message, with a reason then it’s well worth investigating if that reason can be resolved.
 Third-party scripts might be making your page ineligible, even if you don’t use those features directly in your code. As I mentioned above, unload event listeners are particularly common to some third-parties. As well as the above test showing this, there’s a Lighthouse test for this. Running a query on the HTTP Archive data yielded a list of popular third-party scripts used by many sites that use unload event listeners.
 
 I’ve heard the Chrome team has been reaching out to these companies to encourage them to solve this, and many of the scripts logged in the above spreadsheet are older versions, as many of the companies have indeed solved the issues. Facebook pixel, for example, is used by a lot of sites and has apparently recently resolved this issue, so I am expecting that to drop off soon if that is indeed the case.
 Site owners may also have power here: if you are using a third-party service that uses an unload event listener, then reach out to them to see if they have plans to remove this, to stop making your website slow — especially if it’s a service you are paying for! I know some discussions are underway with some of the other names on that list for precisely this reason, and I’ve helped provide some more information to one of the companies on that list to help prioritize this work, so am hopeful they are working on a fix.
 As I mentioned earlier, each browser has implemented the Back/Forward Cache separately, and the above test is only for Chromium-based browsers, so even if you pass that, you may still not be benefiting completely in the other browsers (or maybe you are, and it’s just Chrome that’s not using it!). Unfortunately, there is no easy way to debug this in Firefox and Safari, so my advice would be to concentrate on Chrome first using their tool and then hope that’s sufficient for the other browsers as they often are more permissive than Chrome. Manual testing may also show this, especially if you can slow down your network, but that is a little subjective, so can be prone to false positives and false negatives.
 Impact On SmashingMagazine.com
 As readers undoubtedly have noticed, the smashingmagazine.com website is already fast, and they’ve published a number of articles in the past on how they achieve this level of performance. In the last one, which I wrote, we documented how we spent a huge amount of time investigating a performance issue that was holding us back from meeting the Core Web Vitals.
 Thanks to that investigation we had crossed into the green zone and stayed there, so we were reasonably happy, as we were consistently under the 2.5-second limit for LCP in CrUX for at least 75% of our visitors. Not by much admittedly, as we were getting 2.4 seconds, but at least we were not going over it. But never ones to rest on our laurels, we’re always on the lookout for further improvements. They delayed some of their JavaScript in January leading to some further improvements to an almost 2.2-second CrUX number.
 This website was initially failing that test due to the fact it had an App Install Banner. SmashingMazgazine.com is a PWA that prompts you to install it on the home screen for browsers that support that (Chrome on Android devices primarily). When I highlighted to the team in early March that this was holding them back from benefiting from the Back/Forward Cache that had recently been launched, they decided to remove some key parts of manifest.json to prevent the App Install Banner from showing, to see if this feature was costing them performance and the results were dramatic:
 
 We can see the impact of the December and January improvements were noticeable but started to tail off by the start of March, and then when we implemented the Back/Forward Cache fix the LCP numbers plummeted (in a good way!) all the way down to 1.7 seconds — the best number Smashing Magazine has ever seen since the Core Web Vitals initiative was launched.
 In fact, if we’d known this was coming, and the impact it would have, we may not have spent so much time on the other performance improvements they did last year! Though, personally, I am glad they did since it was an interesting case study.
 The above, was a custom chart created by measuring the CrUX API daily, but looking at the monthly CrUX dashboard (you can load the same for any URL) showed a similarly drastic improvement for LCP in the last two months, and the April numbers will shop a further improvement:
 
 Across the web, CLS had a bigger improvement with the rollout of the Back/Forward Cache in Chrome, but Smashing saw the improvement in LCP as they had no CLS issues. When investigating the impact on your site look at all available metrics for any improvement.
 Of course, if your site was already eligible for Back/Forward Cache then you will have already got that performance boost in your Core Web Vitals data when this was rolled out in Chrome to your users — likely in December and January. But for those that are not, there is more web performance here available to you — so look into why you are not eligible.
  I honestly believe that sites that are ineligible for the Back/Forward Cache are giving up free web performance for their users, and making passing Core Web Vitals needlessly tough on themselves.
 Was there a downside to disabling the App Install Banner that was preventing this? That’s more difficult to judge. Certainly, we’ve had no complaints. Perhaps it’s even annoying some users less (I’m not an Android user, so can’t comment on whether it’s useful or annoying to have pop-ups on a site you visit encouraging you to install it). Hopefully, when Chrome fixes this blocker, Smashing can decide if they want both, but for now the clear benefits of the Back/Forward Cache mean they are prepared to give up this feature for those gains.
 Other sites, that are more app-like, may have a different opinion and forgo the Back/Forward Cache benefits in favor of those features. This is a judgment call each site needs to make for any Back/Forward Cache blocking features. You can also measure Back/Forward Cache navigations in your site Analytics using a Custom Dimension to see if there are a significant number of navigation and so an expected significant gain. Or perhaps A/B test this? There are also a couple of interesting proposals under discussion to help measure some of this info to help websites make some of these decisions.
 Test Your Website Today!
 I strongly encourage sites to run the Back/Forward Cache test, understand any blockers leading to an unsuccessful test and seek to remove those blockers. It’s a simple test that literally only takes a couple of seconds, though the fix (if you are not eligible) might take longer! Also, remember to test different pages on your website in case they have different code and thus eligibility.
 If it’s due to your code, then look at whether it’s giving the benefits over this. If it’s third-party code, then raise it to them. And if it’s for some other reason, then let the Chrome team know it’s important to you for them to solve it: Brower makers want user feedback to help prioritize their work.
 If you don’t do this, then you’re leaving noticeable web performance improvements on the table and making your site slower for users, and failing to capitalize on any SEO benefit at the same time.
 Many thanks to Philip Walton, Yoav Weiss, Dan Shappir, Annie Sullivan, Adrian Bece, and Giacomo Zecchini for reviewing the article for technical accuracy.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 299: Parent selectors, JS concepts, Custom Elements, debugging page reloads and too smart SPAs</title>
         <link href="https://wdrl.info/archive/299"/>
       <updated>2022-05-07T05:30:00.000Z</updated>
       <content type="text">Hey,
 
 this week’s links are quite interesting: A couple of web standard (proposals) that would make CSS easier again like CSS Toggles, easy progressive enhancement for form data using the web platform. But also things like a huge list of JavaScript concepts to enhance your knowledge, or simple multi-select fields without a UI library. And then, self-hosted apps seem to become more popular again as rally or planby, two open source planning/meeting/schedule tools show us. It’s always fun to collect the links for my newsletter and shows how diverse web development is. And finally, the tips from Kevin Kelly on how to see and live life are a great inspiration to become a calmer, nicer and more focused person valued in a community.
 Generic
 
 	Taylor Hunt says I’m not smart enough for a SPA and brings up great points on when an SPA isn’t well suited or too complex, too slow, too insecure and when to still consider it for a project. I really like the article because it’s honest and well documented but at the same time open for options, individual choice and not too subjective or suggestive.
 
 UI/UX
 
 	Elizabeth Alli shares how to think in a conditional logic when planning, designing and coding UI components to improve the user experience. It’s applicable to every blog but even more important once you try to collect user data for your website.
 	Mega menus are quite common and improved navigation for users a lot on many bigger websites. But Vitaly Friedman now comes up with a new approach with better guidance: Navigation queries.
 	Google presents its new Material Symbols, a nice icon set. Weirdly, it seems only available as webfont.
 
 Tooling
 
 	Regex-Vis is a super nice visual editor and tester for your regular expressions.
 
 JavaScript
 
 	Planby is a React based component for a quick implementation of EPG, schedules, and event timelines that works interactively and very smooth.
 	Bramus Van Damme shares how to code better form validations and interactions in JavaScript using the native FormData method which progressively enhances the form automatically and with little effort. A great example how great native browser/platform features are often overlooked by developers.
 	This is an interesting collection of JavaScript features all engineers who work with the language should at least be aware of or know. So it’s a great reminder or learning go to to improve your JS-skills: 33 JavaScript concepts you should know.
 	The folks from Google Chrome labs published an open source custom element that allows you to easily put a Dark Mode 🌒 toggle or switch on your site. It initially adheres the users&#x27; preferences according to prefers-color-scheme, but allows them  override their system setting.
 	With more and more interactive and JS-driven web apps out there this piece of code made me cheer: performance.navigation.type &#x3D;&#x3D;&#x3D; 1 can be used to find out when a user reloaded your website in the browser. This may be a hint that something broke and the user had to »turn it off and on again«. Of course it’s just one metric but an important and helpful step to debug while easy to implement.
 	Rallly is a free group meeting scheduling tool built with Next.js, Prisma &amp; TailwindCSS. Great self-hosted alternative to the not very cheap services out there.
 
 CSS
 
 	Ahmad Shadeed shares insights into when and how to use the new :has() parent selector in CSS. It’s a great blink into the future of CSS coding but right now only Safari ships it (and Chrome behind flag).
 	Oof, what happens lately in CSS is crazy: There’s a proposal for CSS Toggles which would become incredibly useful with modern platform features such as dark/light mode or reduced motion and more toggle settings. While we currently have media queries for this, it can get quite complex and annoying to write. Toggles try to solve this in an elegant way.
 	Adrian Roselli shares how to easily create an accessible, flexible, collapsible multiselect field with simple HTML and CSS only.
 
 Go beyond…
 
 	
 Today is my birthday. I turn 70. I’ve learned a few things so far that might be helpful to others. For the past few years, I’ve jotted down bits of unsolicited advice each year and much to my surprise I have more to add this year. So here is my birthday gift to you all: 103 bits of wisdom I wish I had known when I was young. — Kevin Kelly
 
 
 
 If you like this newsletter, you can contribute here. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>The different kinds of notes</title>
         <link href="https://www.baldurbjarnason.com/2022/the-different-kinds-of-notes/"/>
       <updated>2022-05-06T12:56:06.000Z</updated>
       <content type="text">This is the second entry of three where I go over what I learned from the user research I’ve been doing for Colophon Cards.
 
 My oldest notebook
 
 The post where I outline my general theory of notetaking for all to disagree with
 At the end of my last post, I mentioned that I had switched to pen and paper notetaking in the early 2010s, after years of serious digital notetaking using plain text files.
 I wrote about how much more enjoyable the analogue notetaking was but what I didn’t mention was why I abandoned my plain text system. Nor did I write about my first foray into serious notetaking.
 The notebook above, from 1994, is the oldest notebook of mine I’ve found. It’s the first proper notebook I kept. It wasn’t a journal, sketchbook, drafts, or workbook for school. Just notes.
 And it petered out pretty quickly. I had forgotten all about it until I found it in a box a few months ago. For a good reason: it was a random mess that I didn’t take seriously.
 Notetaking seriousness didn’t come for me until college, which makes sense. College is when I first started to test myself with more structured projects. Essays, theses, and my early blogging, all required a little bit more thought and planning than a teenager’s haphazard writing hobby.
 Serious writing required serious notetaking, both in terms of scheduling and structure. For years, I had a strict notetaking system. Each day I made a new text file with the current date as a file name. Each file was filled with whatever notes I needed for whatever I was working on at the time. Every day I wrote something, I crossed it off on the calendar with a black marker. On the days when I didn’t write something, I marked in red.
 The goal was to keep the run of black unbroken. It worked. Sort of. This regimented system was based on a random quote from Jerry Seinfeld, of all people, and a misunderstanding on my part of the purpose of Julia Cameron’s Morning Pages from her book The Artist’s Way.
 I’d fallen into one of the oldest process trap of all: I had cargo culted methods whose principles, concepts, and motivations I did not understand. I had copied the ritual without understanding the mechanism.
 Over time this turned into a slog, _no matter what, write three pages!, and by the time I got to my PhD, it made everything about a bad experience worse.
 When I began my PhD in 2001, we were all still recovering from the dot-com crash. It made sense to work on a thesis that was grounded in an evolutionary, small-c conservative, view of media. Not only was web- and new media scepticism abundant at the time, it was very strong in the specific academic environment where I was working. Over the years that I worked on my PhD, that view became untenable to me. Interactive media—the web—was and is fundamentally different from its predecessors, in a way that promised to be completely revolutionary to both culture and society. But I was locked in a reactionary thesis. Starting from scratch would have almost inevitably led to me having to abandon the PhD. This meant that I had to write a thesis I utterly disagreed with, making an argument I didn’t believe in, using references that I felt were borderline nonsense in the context I was pulling them into, and making the words I wrote feel like ash.
 The regimented daily pages notetaking routine made everything worse. It turned the writing process into a multi-year death march, filling the folders of my hard drive with unusable nonsense that I didn’t believe in then and don’t believe in now.
 I managed to complete and defend my PhD and swore to leave academia behind for the rest of my life.
 It took me years to fully recover.
 To get there—to recover from my unrelenting thesis death march—I had to get more thoughtful and serious about writing and my notetaking. As is the way of my people (overthinkers, that is), this meant research. Lots and lots of research. On writing. On creativity. On notes and notetaking. On planning. Even research on skills development, the cognitive component in expertise, as well as memory formation. I dug through the entire Cambridge Handbook of Expertise and Expert Performance (over 900 pages). I read fluffy, self-help books on creativity. I read serious studies on the effects stress has on memory formation. I read old books. I read new books. Some of them dated. Some of them timeless. Some worked. Some didn’t.
 I also began to understand the mechanism behind Julia Cameron’s morning pages, how it was supposed to work, and what it had in common with other approaches, even the ones that superficially look completely different.
 I began to understand, at least for my own purposes, what notetaking was for.
 And I think—based on the research I’ve been doing—that I’m not alone.
 It’s all about boxes and maps.
 
 The different kinds of notes
 I recently interviewed about a dozen people on their notetaking habits. Before that, I had done a survey.
 There’s a lot to cover as notes seem to come in an endless variety. Even from a group as small as twelve people you already got more than twelve different kinds of notetaking:
 
 Household management (shopping lists, chores, etc.)
 Task management
 Mnemonic notes (write to help you remember)
 Various kinds of annotations
 Shared work notes (meeting minutes, documentation, etc.)
 Personal work notes
 Idea exploration
 Journalling
 Scrapbooking (digital and analogue)
 Research management
 Outlining
 Information management (bookmarks and the like)
 Project management
 Client management
 Concept database (save your ideas)
 
 
 The above list in blackboard form
 
 This is not an exhaustive list. The closer you look, the more varieties of notetaking you find, even in such a small group of people.
 There’s a temptation there to dig in, get into even more detail, and generally act as if you’ve hit a motherlode, a vein that’s rich and deep and rewarding. Just really get into it, interview dozens or hundreds of people, do study after study and spend years cataloguing all the varieties of notetaking
 But that would be a mistake.
 When you’re in a domain where increasing the resolution and detail only reveals an escalating amount of previously undiscovered complexity, odds are you are tackling what E.F. Schumacher called a ‘divergent problem’. These problems aren’t technical. They can’t be broken down into parts to solve each part step by step. A problem like this becomes more complex and hard to solve under close analysis, not simpler and easier to solve. The more detail you see, the more conflicting data points you encounter and the harder it gets to reconcile the diversity you’re witnessing with a cohesive mental model.
 From A Guide for the Perplexed, page 126:
 
 I have said that to solve a problem is to kill it. There is nothing wrong with “killing” a convergent problem, for it relates to what remains after life, consciousness, and self-awareness have already been eliminated. But can—or should—divergent problems be killed? (The words “final solution” still have a terrible ring in the ears of my generation.)
 Divergent problems cannot be killed; they cannot be solved in the sense of establishing a “correct formula”; they can, however, be transcended. A pair of opposites—like freedom and order—are opposites at the level of ordinary life, but they cease to be opposites at the higher level, the really human level, where self-awareness plays its proper role.
 
 So, when I step back, squint a bit, and stare at my blackboard and my notes, it strikes me that most of the notetaking methods the interviewees described are trying to answer at least one of the three following questions:
 
 How do I manage my creativity?
 How do I manage my knowledge?
 How do I manage my understanding?
 
 Or, more to the point, the notetaking methods are a way for the writer to continuously ask themselves these questions and adjust their tactics as needed. None of the methods are prescriptive or rigid; they are all constantly being adapted.
 The job of notes for creativity is to:
 
 Generate ideas in a structured way through research and sketching.
 Preserve those ideas.
 Explore the ideas until they have gelled into a cohesive plan or solved a problem.
 
 The job of notes for knowledge is to:
 
 Extend your memory to help you keep track of useful information (client data, meeting notes, references).
 Connect that information to your current tasks or projects so that you can find it when you need it.
 
 The job of notes for understanding is to:
 
 Break apart, reframe, and contextualise information and ideas so that they become a part of your own thought process.
 Turn learning into something you can outline in your own words.
 
 In terms of software, that means we have these three broad approaches:
 
 Notes for creativity tend to favour loosely structured workspaces. Scrivener and Ulysses probably come the closest, though, in practice, I have doubts that either of them is loosely structured enough.
 Notes for knowledge favour databases, ‘everything-buckets’ (apps that expect you to store everything in them), and hypertextual ‘link-everything’ note apps.
 Notes for understanding tend to favour tools that have powerful writing or drawing features (which you favour will depend on your skill set and comfort).
 
 I don’t think you can make software that perfectly serves all three of these kinds of notetaking. Knowledge bases become too rigid to serve as workspaces for creativity. The creative spaces are too loosely structured to work well as knowledge bases. You can integrate writing and drawing tools in either, but that serves notetaking for understanding only up to a point. Most knowledge bases preserve too much detail and context, which gets in the way of reframing and contextualization. And too fully featured writing or drawing tools could make the creativity tools too complex to use.
 In the parlance of roleplaying games, these are the three character statistics of your notetaking app, and you only get to assign 15 points between them. You could choose to be average in all three (5, 5, 5, uninteresting). You could choose to dump all of the points in one stat and be just plain bad at the rest, which would simplify the design a bit. Or, you could think of it as a priority list: most important, second important, unimportant (dump stat!).
 
 (If you aren’t familiar with RPGs, you can think of them as three sliders with a fixed sum of values between themselves. Every time you push a slider above ‘5’, another slider has to go down a corresponding amount.)
 How would I divvy up the points for Colophon Cards? What would the Colophon Cards character sheet look like?
 At the moment, I’m leaning towards this:
 
 Creativity: 7
 Knowledge: 2 (dump stat!)
 Understanding: 6
 
 
 A workspace that:
 
 Favours lightweight and flexible structure over detailed organisation and connections.
 Prioritises speed over accuracy when creating and finding notes.
 Has better-than-average writing tools.
 
 Now, I’m not going to pretend that I’m basing those priorities on the comparatively little user research I’ve been doing. They instead represent the kind of app I want to use and enjoy. Because it turns out that enjoyment is essential for long-term notetaking. The research helped me clarify what I wanted, give it focus, and place it properly in the larger context. But, ultimately, I want to make an app I enjoy.
 Also, it made me drop a bunch of smaller ideas that research revealed to be less likely to work, but that’s honestly a little less interesting. Though possibly a topic for a future blog post.
 
 Boxes and Maps
 You don’t have to copy the methods of a German academic with a tendency for the abstruse to take notes
 In my relentless daily writing during my PhD, I had missed the point of it. By using a regimented notetaking structure I drove my creativity into the ground, my joy and motivation cratered with it, and burnout inevitably followed.
 What I had missed was that the point of these techniques wasn’t to continuously force march forward according to a predetermined master plan. Just writing or getting things done isn’t the purpose, at least not in any of the writing methods I’ve looked at.
 And I’ve looked at a few—they all cover some form of notetaking or journaling as a core part of their process:
 Umberto Eco’s How to Write a Thesis is a book about writing that contains a fully-formed (and very usable) notetaking system based on index cards. It’s structured so that everything hangs off the Table of Contents or outline. The thesis’s ToC becomes the map you use to structure your notes.
 (I wish that I had read Eco’s book before I did my thesis. Alas.)
 Anne Lamott’s Bird by Bird describes an ad hoc and improvised system of index cards. She presents it as an almost random process but then proceeds to spend an entire chapter explaining the value of a loosely structured notetaking system. Her notes are more of a box full of unsorted items that prod the work forward and guide it.
 A good chunk of David Hewson’s Writing: A User Manual is about how various kinds of notes help you write your book. His method has elements of both the Box and the Map. He recommends plotting and planning in advance, but he also uses an unstructured daily journal and a ‘box’ of notes on characters, concepts, places, and ideas to guide the process.
 The Story Grid by Shawn Coyne is pretty much just an excel-based, fiction-oriented version of Eco’s “How to Write a Thesis”. Instead of Eco’s table of contents, the story grid uses a list of scenes. But it’s otherwise a very similar principle: the map guides the project and structures your notes. The map evolves as you go but no matter how much it changes it remains the primary organizing principle.
 Brenda Ueland, in her 1938 book If You Want to Write recommends keeping a “slovenly, headlong, impulsive, honest diary”. The way she puts it:
 
 Write every day, or as often as you possibly can, as fast as and as carelessly as you possibly can, without reading it again, anything you happened to have thought, seen or felt the day before.
 
 In her amazing The Creative Habit Twyla Tharp talks about The Box. A literal box. Each project of hers gets a box which is full of items that document the active research and work on that project. It’s filled with objects, notes, files, clippings, toys and just stuff. Some people store things in files. She prefers boxes.
 And, last but not least, the idea I had misunderstood so badly years ago, seems so clear today when I reread Julia Cameron’s words in The Artist’s Way:
 
 There is no wrong way to do morning pages. These daily morning meanderings are not meant to be art. Or even writing. I stress that point to reassure the nonwriters working with this book. Writing is simply one of the tools. Pages are meant to be, simply, the act of moving the hand across the page and writing down whatever comes to mind. Nothing is too petty, too silly, too stupid, or too weird to be included.
 
 It’s about re-contextualising your experiences, reading, thoughts, ideas, and goals in words. This serves both as a mnemonic and to increase understanding as the act of reframing it all in words, in black and white, serves to help your mind integrate it. It turns external ideas and information into integrated thoughts that you understand and can act on.
 Flipping to the next page, I can also immediately see the one paragraph that later led to all of my notetaking misery:
 
 Morning pages are nonnegotiable. Never skip or skimp on morning pages. Your mood doesn’t matter.
 
 If you ever read her book, ignore this paragraph. Your mood does matter in the long run. Don’t beat yourself up about it. There are other ways to accomplish the same thing without torturing yourself. What’s especially galling is that I clearly didn’t finish reading the book at the time as its entire purpose is to prevent people from making the mistake that I made.
 Once you take in these many approaches to writing and study them as wholes, as opposed to just teasing out individual components, you begin to spot a few common threads running through them, among both the Box and Map creators.
 The first one is that the difference between the Box and the Map isn’t as great as you think.
 Those who favour the Box maintain an evolving internal map of where they are going with the project. The purpose of the items in the box is to both make it easy for you to maintain that internal representation and to give it enough context to adapt it to changing circumstances.
 Those who favour the Map instead flip this around. The map is external and evolving and serves to make it easy for you to maintain the context internally so that you can adapt the project to changing circumstances. The goal is the same. The methods are remarkably similar. It’s just a question of which bit is serving as a mnemonic for the other bits.
 Evidence boards are a compromise between the two that shows just how little the difference between the two is in actual practice. Depending on how you look at it, the evidence board is either the items of the box pinned to a board or a map that has been loosely transposed over to a wall.
 Which general approach you should choose is entirely personal. As the difference is primarily in mnemonic tactics, the optimal approach depends on how your own memory and other cognitive processes work. Some people’s memory is “out of sight, out of mind.&#x27; Or, you might find it easy to remember details but have a harder time maintaining a big picture context. Some have an internalised heuristic or rule of thumb for how a project should work, so they only need reminders and bookmarks that they check against their mental rubric. Some have a hard time visualising the final product that is the goal of their project and so need a tactic that keeps them motivated and more observant of the bigger picture. People vary and so must their methods.
 The tactic to use depends on how your mind works. Experiment. Try different approaches out. But, once you’ve found your process, trust that the overall process is roughly the same:
 Collect
 All the things you fancy. Note down ideas and thoughts. Take notes on what you read. Take pictures. Sketch when the whim strikes. Collect what interests you in life using whatever method is the most convenient for you. Anne Lamott uses index cards. Twyla Tharp uses whatever can be collected in her boxes. David Hewson uses a notetaking app (Ulysses, last I heard). Some use pens and notebooks. Whatever you use, remember that these notes are necessarily only an intermediate form. They don’t become understanding or thoughts until you integrate them. You need to be able to review them, and storing them safely is always useful, but, paradoxically, notes aren’t the be all or end all of the notetaking process.
 Contextualise
 The purpose of the various journalling tactics as well as some of the mapping tactics (like the evidence board) is to get you into the habit of reframing and contextualising your thoughts, your reading, and your ideas. By reframing them in words (or sketches) you integrate them which makes them available to your thinking and decision-making processes. If you don’t integrate what you collect, are just building an ever more intimidating database of opaque words and alien ideas.
 An external brain is a brain that can’t drive your thoughts and actions and will just make your work harder.
 A daily written journal is a common way to accomplish this integration. But people have also used daily sketches, voice memos (on tape in the olden days), or video diaries. The goal isn’t to preserve or convince but to reframe and understand in your own terms what you’ve experienced, read, and thought. Blogs work great. Twitter might be stretching it because of the character limit. Use whatever works for you.
 Rule of thumb: let the contextualisation method match the bones of the project. A writing project benefits from writing tools.
 Map
 The final part is the map, the bird’s eye picture of where you are, what lies ahead, and where you want to end up. It’ll start off vague, whatever tactic you use. As you work it’ll become clearer, no matter whether you’re maintaining it in your mind, on paper, or in an Excel spreadsheet. The map might end up being an outline, a diagram, a building plan, a list of tasks in Trello, or a list of issues on GitHub. It will change over time, but don’t worry if it doesn’t. It’s important to keep in mind that the map isn’t the end product, just one more tool among many to help you build the end product.
 What does this all have to do with a notetaking app?
 The first, most obvious conclusion is that there is no one best method. The above meta-framework for notetaking is broad enough to encompass both Zettelkasten and Twyla Tharp’s boxes filled with stuff. If I’m right about notetaking methods being especially sensitive to individual variation in neurocognition, then there will never be one method or one app to rule all of notetaking.
 It becomes a question then of deciding on what my own particular take on this software genre might be. And, if you’ve been following my earlier posts, you probably have some idea of the direction I’m taking. At least in broad strokes.
 But the meta-framework doesn’t answer one, very important, question. One that people have been tackling since the dawn of humanity.
 What happens when you need to collaborate with other people?
 Because nobody, not even your lone wolf novelist, truly works alone.
 So…
 Next: the final entry in the Colophon Cards research blog series, where I write about how collaboration and sharing fit into all of this.</content>
     </entry>
     <entry>
       <title>How To Give Effective Feedback Remotely</title>
         <link href="https://smashingmagazine.com/2022/05/give-effective-feedback-remotely/"/>
       <updated>2022-05-06T09:30:00.000Z</updated>
       <content type="text">We need more good feedback in our world, and I don’t just mean the type of feedback that celebrates your good work. I’m talking about feedback that is actionable, specific, and kind; feedback that does not set us on edge or make us fall into an anxious spiral; feedback that helps us collaborate more effectively. The kind of feedback that is actually really hard to do when working remotely.
 In this article, I’ll discuss a few ways to get around that difficulty. We’ll start by learning what causes feedback sessions to get off track, how to prevent this from happening, and what to do when this happens.
 This article is targeted toward designers and developers who are currently working remotely or are planning to switch to remote work.
 Imagine...
 You’re part of a remote design team in a small company that is currently having you focus on revising their to-do app’s design based on some customer feedback and some ideas that the company wants to validate. During your presentation to the company, someone interrupts you. With their camera off, they say: “Wait, why are we wasting time doing this? The existing page is fine. The older colors and layout were better and this new thing feels clunky. Just put the sign-up call to action at the top and be done with it.”
 You’ve been doing this for a few years, so this isn’t new to, you but you still feel like you got the wind knocked out of you. You get flushed and a little angry, because this person hasn’t always been so unfriendly to you before, and you don’t want to look bad in front of the leadership. You start to stumble over your words, which you think makes you look bad. Now you’re in a downward spiral. 
 Why Does Negative Feedback Elicit Such Strong Responses?
 Mostly for two reasons:
 
 We put a lot of ourselves into what we do, and so it’s natural, that when confronted with negative feedback, we would take it personally. That’s totally understandable and human.
 Our brain perceives critical feedback and arguments in the same way it did thousands of years ago. We’ve gone from a decentralized, hunter-gatherer society to a modern-day society so quickly (evolutionarily speaking, of course) that our brain hasn’t been able to keep pace with the changes. So, while we’re generally a lot less likely to be physically attacked at work today, our brains still perceive disagreements just as if we’re about to be. This is the reason why oftentimes the person who disagrees with you on a really hot design issue at work may appear to you in the exact same way as a caveman about to bean you with a rock to take your food.
 
 Our Brain’s Structure
 Understanding how we route around this problem requires a brief discussion on how our brains perceive conflict. So, let’s dive into some neuroscience! 
 We’ll first discuss the three main levels of our brain:
 
 the neocortex (where our rational, thinking brain lives),
 the limbic brain (where our emotions and feelings live),
 the reptilian brain (where our basic biological, survival programs live).
 
 When we are threatened, something called the amygdala (which is our brain’s “smoke detector”) sounds the alarm and does two things: it produces cortisol (a stress hormone) and diverts the blood flow from the neocortex to the lower levels of our brain. This means at this moment the part of our brain that thinks and reasons isn’t getting enough blood, so we become the real-life equivalent of the Incredible Hulk: just pure fight, flight, or freeze (only without the torn clothes and green skin). In his book Emotional Intelligence, Daniel Goleman calls this “amygdala hijacking.” It’s responsible for almost all of our conflicts going awry.
 “An amygdala hijack is an emotional response that is immediate, overwhelming, and out of measure with the actual stimulus, because it has triggered a much more significant emotional threat. The term was coined by Daniel Goleman in his 1996 book Emotional Intelligence: Why It Can Matter More Than IQ.”— Wikipedia
 
 Psychological Safety
 You want the receiver of the feedback to feel comfortable enough to hear what you have to say without triggering their amygdala response, and you need to feel comfortable enough to say it. This is called psychological safety, and it is defined like this:
 “Psychological safety is being able to show and employ one’s self without fear of negative consequences of self-image, status or career (Kahn 1990, p. 708). It can be defined as a shared belief that the team is safe for interpersonal risk-taking. In psychologically safe teams, team members feel accepted and respected. It is also the most studied enabling condition in group dynamics and team learning research.”— Wikipedia
 
 The tools that follow will help build and maintain psychological safety by signaling that it’s not a harmful situation. Remember, when you lose safety, it’s because someone’s survival circuits kicked in. Better put, psychological safety is the underpinning of all good conversations.
 When there isn’t safety, resentment builds, feedback gets conveniently ignored, and people tend to clam up and not give real, helpful feedback.
 How Can We Avoid It Going Sideways?
 We’ll learn how to give high-fidelity feedback, share actionable and specific insights, as well as how to give a little conversational “first aid” by making refocusing statements.
 Give High Fidelity Feedback
 Having a conflict when collaborating remotely causes us to lose what I call “high fidelity conversation,” leaving everyone at a disadvantage. Here’s my definition:
 High fidelity conversationIt is a conversation in which all participants have access to the full range of human communication, including tone of voice as well as non-verbal cues, such as body language and tone.
 
 As more and more teams work remotely, we lose out on vital conversational cues, such as body language and tone of voice, leaving us in the dark about what someone truly means when they speak. Let’s dig into why this is difficult so that we can frame up an appropriate response.
 To start, let’s go over the different levels of conversational fidelity we can have and discuss what we lose in remote work communications:
 
 In-person, which gives us the full range of verbal and non-verbal cues, such as body language, tone of voice, and the full range of their voice.
 Video, which loses most of the body language due to cropping and the fidelity of people’s voices (video chat apps often compress audio).
 Phone, which loses out on all non-verbal cues.
 Text, which loses out on all non-verbal cues and the tone of voice.
 Email, which loses out on everything else, including the synchronous communication (read: Slack, various chat apps, SMS messages, etc.) and leaves you only with the transcript of what people say.
 
 The goal for these tough conversations is to have them in as high a fidelity as you possibly can. While email and Slack have dramatically reduced the friction for communication, it’s come at the expense of clarity and thoughtfulness. It’s far easier to send a half-baked email that leaves too much open to interpretation (remember the last email — or email thread — you got that was a forwarded email with just “thoughts?” included on a single line?). Plus, you’re at the mercy of the reader’s mood, state of mind, or distractions they experience, which could contribute to the missing key parts of your message.
 When we lose conversational fidelity, our brain’s survival circuits activate and fill in all the gaps in the communication with negative assumptions, leaving us prone to misinterpret someone’s message. 
 There are times, however, when real-time communication isn’t an option, especially for fully remote companies with people spread all across the world. In that case, consider using a tool like Loom to give your feedback, or you could even record an audio message on your phone and upload that. In doing so, you still retain your tone and give them some body language to work off of. 
 Failing that, you’ll need to work a little harder to ensure things are taken well. Remember, you’re losing a lot of fidelity here, so you’ll need to compensate. Here are two of the best tips:
 
 Be very clear with your language.Humans’ brains like to fill in the gaps with negative assumptions.
 Use emoji. 😊Because our brains perceive emoji in the same way that it does a real human’s reaction. Don’t overuse them, but know they’re a helpful tool if people can’t see your real face.
 
 Ask For (And Give) Actionable Feedback On Specific Areas
 When we start projects, we establish objectives and goals we want to achieve. Feedback sessions have exactly the same needs. So, when you gather people for feedback, be specific. Here’s a helpful guide:
 Don’t ask:
 
 What do you think?
 Thoughts?
 
 Do ask:
 
 I’d like feedback on the grid system, and particularly on the following elements of...
 This technically passes our contrast guideline tests, but it feels somewhat wrong. What do you think?
 
 We often mistake a client’s, a manager’s, or a stakeholder’s desire to contribute to your discussion for direction. But it’s not always so — generally, these people just want to help you solve the user experience or user interface design problems.
 I’ve found Asana’s method to be particularly helpful here — you need to bucket the feedback into three buckets: do, try, and consider.
 “A while back at Asana we noticed teams were laser-focused on shipping and would carefully ask if each piece of feedback was “launch blocking” at our launch reviews. Often non-blocking feedback would be brushed aside even if it was relatively cheap and would really improve the quality of the product. We reflected on what was happening and realized that we didn’t have clear language or norms on how to give or respond to feedback. And so, the Do, Try, Consider framework was born.”— “Do, Try, Consider — How we give product feedback at Asana” by Jackie Bavaro
 
 
 When giving feedback, you should give the person something to explore or try. For example, instead of “Put the sign-up call to action here,” try “What other layouts might help us achieve our goal?”
 Every piece of feedback should also be directly related to a goal, whether that’s the design being on-brand, responding to someone’s feedback, and so on. And if you aren’t seeing a way to be more specific, consider asking the other person to ask what specific type of feedback they’re looking for. 
 Here’s how a feedback session could work:
 Invite people to have several minutes of quiet ideation, adding sticky notes to a virtual board. Once the solo time expires, group the similar sticky notes and then discuss each group individually, bucketing them accordingly.
 Here’s a little template that I made:
 
 And here’s how this could look like in action:
 Person A: “Hey Person B, here’s the latest prototype. I’d like to get feedback on the navigation structure for this app because I’m feeling a little tension with where the account settings are currently located. I’m also not sure about whether or not this design iteration is fully in line with the new brand look that we’re rolling out next quarter.”Person B: “The account settings should definitely be not so front-and-center, I think we have to put them under ‘Profile’ to match the website UX. Also, I expected to see the buttons in our brand’s shade of blue and not the shade that you have used. Can we change the color?”Person A: “Okay, let’s talk about the blue first. We didn’t have a matching shade that was also accessible (not enough color contrast), so I made a new one that was. Is this comment a must-do, try, or consider? Also, it sounds like repositioning the account settings is a must-do, is that right?”
 
 
 
 Be Kind
 Before we continue, let’s discuss what I mean by being kind. I don’t mean that you should go around giving empty compliments to people or avoiding telling them things that aren’t quite right — both of these are ultimately harmful. What I intend by saying “be kind” is that you are direct with the other person. You have hard, direct conversations because you care. That’s true kindness. (And of course, be polite.)
 More people need to learn that a polite suggestion as feedback is far more likely to get action than a snarky or unprofessional comment. And don&#x27;t insult the people you&#x27;re giving feedback to. We&#x27;re all human.— Tim Misiak (@timmisiak) March 11, 2022
 
 How you frame the feedback has a direct influence on how it will be received. For example, which do you think is the better type of feedback? 
 
 “This blue is lame. Just not digging it, man.”
 “This blue isn’t in line with our brand guidelines. Good thinking on the accessibility angle, though. You should chat with marketing to make sure that we use the right color shade, and at the same time, we have enough color contrast to keep this UI element accessible.”
 
 I hope you chose the second option. 😄
 When It Goes Sideways, Refocus
 Despite our best efforts, conversations will still go sideways sometimes. Here, we’ll discuss the best way to perform some conversational “first aid” when things feel a little dicey. Enter what I call refocusing statements.
 A refocusing statement is a statement that addresses the misinterpretation, reestablishes focus on your goal, and asks open-ended questions to ensure clarity.
 
 Here’s what they look like:
 
 What you aren’t saying (the misunderstanding or misrepresentation).
 What you are saying (your personal or shared goal).
 An open-ended question that puts the conversation back in their court.
 
 Here are a few refocusing statements in action:
 “I don’t intend to imply you’re not a skillful enough designer, I’m saying that this isn’t up to our team’s standards. We can &amp;mdash, and we want — to help you get there though.”“I’m not saying you have to do it my way. I know that you’ve got a lot more expertise in designing interfaces than I do. I just wanted to say that we need to consult with one another before sending the prototypes to the stakeholders because I’m responsible for doing the accessibility audits and don’t want any unnecessary back and forth to happen.”“It’s not that you aren’t welcome to contribute to the user interviews or you aren’t a part of our team, your expertise in this is essential. Rather I would say that the type of questions you asked could taint our user research. Can we talk about how to reframe those questions?”
 
 By framing the statements like in these examples, we actively address someone’s humanity and experience, and at the same time, we shift the conversation in a way that overcomes the objection or misunderstanding.
 Note: Please, avoid ending these with “does this make sense?” These non-questions only serve to open the door for condescension.
 And of course, give credit where credit is due — the inspiration for the refocusing technique came from Crucial Learning’s excellent “contrasting statement”, but I shaped it further to better fit my own practice.
 Conclusion
 In closing, let’s briefly recap some of the key points that I made in the article:
 
 Society evolved quickly over the last few millennia, but our brains still “lag behind”, so we need to adopt a few new techniques to make giving and receiving feedback easier.
 If we can build and kindle psychological safety with our peers, giving feedback will be a much more effective process.
 We build psychological safety in part by having a high fidelity conversation — a conversation in which all participants have access to the full range of human communication, including tone of voice as well as non-verbal cues, such as body language and tone. (And if you cannot have a high fidelity conversation, then you’ll need to work a little harder to ensure things are taken well when communicating your feedback using text alone — remember to be very clear with your language and do use emoji.)
 Ask for (and give) feedback that’s actionable and specific. Use Asana’s methods to bucket the feedback into three separate “buckets”: do, try, and consider.
 Be kind. How you frame the feedback has a direct influence on how it’s received.
 When it goes sideways, try using a refocusing statement — a statement which addresses the misinterpretation, reestablishes focus on your goal, and asks open-ended questions to ensure clarity.
 </content>
     </entry>
     <entry>
       <title>Resilience, Flexibility And Immediacy: Working With Headless Systems</title>
         <link href="https://smashingmagazine.com/2022/05/resilience-flexibility-immediacy-headless-systems/"/>
       <updated>2022-05-05T09:30:00.000Z</updated>
       <content type="text">This article is a sponsored by Storyblok
 In the last couple of years, our industry has figured out how to make use of cloud infrastructure and flexible deployments in the best way possible. We use services that give us continuous integration without headaches and serve static files without us managing anything. And adding the right framework to the mix, those services blur the line between static files, dynamic updates, and serverless APIs.
 In all of this, one fundamental piece is often left out: storing data. This raises the question, “Where does our content come from?” Well, a headless content management system might just be what you need.
 The Mighty Monolith And Opinions
 In order to understand why we get so many benefits out of architectures that involve headless systems, we need to understand how things worked with a more traditional approach: monolithic architectures.
 Not so long ago, monolithic content management systems were the jack-of-all-trades for your web content delivery concerns. They came along with:
 
 a database (or required a very specific one),
 the logic to read, change, and store data,
 user interfaces for content administration,
 logic to render all your content in deliverable formats such as HTML or JSON,
 the ability to upload, store, and deliver assets like images and static files,
 sometimes even an editor for design,
 a routing logic to map readable URLs to actual data in your system.
 
 That’s a lot of tasks! Furthermore, a monolithic CMS more often than not was very opinionated in its choice of tools and flavors. For example, if you were lucky, you got a template engine to define the markup you wanted to produce and had absolute control over it. It might not have been as featureful as you wanted it to be, but it at least was better than mixing PHP code with HTML tags.
 Being this opinionated left you as a developer with two choices:
 
 Submit to the system.Learn all its intricacies inside out, become an expert, and don’t deviate from the standard. Treat a CMS as a framework with all the benefits and downsides that come with them. Be super productive if you follow the rules. Avoid anything that doesn’t play well with the opinions of your tool of choice. This can go well for a long time. But you become very inflexible if there are changes in requirements. Not only if you need a different front-end to deploy that is to be designed differently than your tool allows, but also if you end up needing to scale out your servers because of increased traffic, or you need to establish stricter security because of new attack vectors you can’t mitigate. Don’t get me wrong, people who produce and develop monolithic CMS know about those challenges and work on them. It’s still not your choice if you want change.
 Work around the system.This is arguably worse than following a framework blindly. If you work around a system, try to integrate functionality via plug-ins, shift responsibility to attached systems on the side, try to hack something in the core to get things done your way (yes, this happens as well), you end up frankensteining your solution until it’s unrecognizable anymore. While this gives you flexibility when your product managers want to crank out features, it will fall back on you sometime in the future. Don’t do this.
 
 None of these choices is something we want to have. And it all happens because we mix responsibilities in one single solution: How we define, maintain, and store content is mixed with the creation of views.
 The following chart shows what happens if we want to see content produced by a traditional, server-side rendered system.
 
 
 A client, maybe a browser, requests a website from a server via a URL;
 A router resolves the URL and maps it to some input parameters that the system can understand;
 It asks a rendering component to render content based on said input parameters;
 Then, in return, it needs to ask the data storage if this content is available;
 One pierce through the layer cake, and you get your content back;
 The renderer renders it into HTML;
 The routing layer can send a response to the client.
 
 Every communication between the components of this layered architecture is subject to the opinions of its creator. In monolithic architectures, these are the content management system’s opinions. If you want to change them, you have to go the extra mile.
 The Promise Of Headless Architectures
 And this is exactly where headless content management systems come in. A headless CMS gives you easy reading access to the data. Everything else can be designed according to your opinions. 
 You decide how to resolve routing. On the client? On a web proxy? In a Node.js server? Your choice. You decide how and where to render content. Again, on the client? With blade templates in PHP on the server? A serverless function on AWS Lambda written in Go that spits out JSON? Your choice.
 
 Chopping of the head gives you as a developer the freedom to decide what is the right technology for your situation. From a single page application to a traditional server-side rendered website or pre-built sites in continuous integration. 
 Suddenly you also get the freedom to update “the head” as many times as you like and as often as you need. You can deploy it to different servers with different scaling possibilities. Remix, Nuxt, Laravel, 11ty… They couldn’t be more different in how they work, and how they’re supposed to be deployed. Your CMS stays the same.
  Headless CMS not only solve the technology lock-in problem, but they are also a chance to raise the availability and stability of your website. With a traditional, monolithic architecture, everything is tightly coupled.
 An unknown error in your render logic might cause the same kind of problems as if your database has no room to write anymore. By attaching a headless content management system moves a potential source of problems away from others. Suddenly, the boundary becomes clearer — and we can even go further.
 What if we cannot only define the error boundary much better? What if we can react to potential errors much better because of all the ways we are able to consume data?
 Jamstack And Static Sites
 Jamstack means publishing static sites. It’s the opposite of a pull architecture, where each request means fresh content from the database, instead, we push out all our content at once, pre-rendered, in static files, ready to be served.
 This comes with a ton of benefits. Jamstack sites are easy to deploy. All your web server needs to be able to do is serve static files. Web servers are really good at serving static sites! Security-wise, Jamstack pages are a fortress. You actively cut all ties to systems that require authentication or handle sensitive data. Web servers serve in read-only mode. Some setups from popular providers even create an immutable copy of your site. No possibility to change anything on this disk.
 
 All you need is a static site generator, and if you want to scale, a build server. Services like Netlify and Vercel build static sites for you. Content updates happen at build time. The static site generator pulls the entire necessary content from a headless CMS once.
 See this example with Storyblok and Next.js:
 import {
   useStoryblokApi,
   StoryblokComponent,
 } from &#x27;@storyblok/react&#x27;;
 
 export default function Home({ story }) {
   // Let&#x27;s show what we got.
   return &lt;StoryblokComponent blok&#x3D;{story.content} /&gt;;
 }
 
 export async function getStaticProps({ preview &#x3D; false }) {
   // Here, we are loading all data for static generation.
   const storyblokApi &#x3D; useStoryblokApi();
   let { data } &#x3D; await storyblokApi.get(&#x60;cdn/stories/react&#x60;, {
     version: &#x27;draft&#x27;,
   });
 
 
   return {
     props: {
       story: data ? data.story : false,
       preview,
     },
   };
 }
 
 Once the website is done building, all ties are cut. Your website runs without any connection to a content management system. From an operations point of view, this is fantastic. Not only did we introduce an error boundary and reduced channels to other systems, but we also cut off communication entirely because we don’t need it anymore. 
 Updates to your content require a rebuild. A CMS like Storyblok offers you to call a webhook once the publish button has been pressed. Thankfully, hosts like Vercel offer to create webhooks that rebuild your site.
 
 It’s not all hunky-dory, though. While Jamstack sites have undeniable benefits, they also come with some problems. The bigger your site is, the slower your builds can be. If you have to have mission-critical updates, and some companies do, it might be unacceptable to wait a couple of minutes until you see an update. Heck, I’ve seen projects taking up to 30 minutes to publish new content. This is way too long if you need to react quickly.
 Deployment Free Updates
 If we think back to how monolithic content management systems dealt with retrieving content, we see that there was just one spot where the content was actually queried. Thanks to a headless CMS that gives you APIs to fetch data, we can now fetch content from multiple places.
 We can use this to maneuver around some limitations we get when using Jamstack when we need to wait for a build to get an update. We can see the deployed static files as a robust, available baseline to serve our content from. For critical parts of our website, where real-time updates are of essence, we fetch data dynamically from our CMS.
 This can be parts of your website, but also the entire content of an entire page. The following example shows how this can be done using Storyblok and Next.js:
 import {
   useStoryblok,
   useStoryblokApi,
   StoryblokComponent,
 } from &#x27;@storyblok/react&#x27;;
 
 export default function Home({ story: initialStory }) {
   // We are fetching new data from our CMS.
   let story &#x3D; useStoryblok(&quot;react&quot;, { version: &quot;draft&quot; });
 
   // If data isn&#x27;t available, yet, we show the original static data.
   if (!story.content) {
     story &#x3D; initialStory;
   }
 
   return &lt;StoryblokComponent blok&#x3D;{story.content} /&gt;;
 }
 
 export async function getStaticProps({ preview &#x3D; false }) {
   // Here, we are loading all data for static generation.
   const storyblokApi &#x3D; useStoryblokApi();
   let { data } &#x3D; await storyblokApi.get(&#x60;cdn/stories/react&#x60;, {
     version: &#x27;draft&#x27;,
   });
 
 
   return {
     props: {
       story: data ? data.story : false,
       preview,
     },
   };
 }
 
 Next.js ergonomic APIs allow us to pinpoint the exact locations where we want to have data dynamically with a static fallback. And we increase our data accessing points.
 
 Press releases, news articles, and everything that needs to be published within a certain time frame can be done dynamically on the client-side — without losing the benefits of static site generation. You still can redeploy your website, but publishing at an exact point in time becomes an afterthought. Let the build run as long as it needs.
 Not only do you get fast updates. If the connection to your CMS breaks for whatever reason, users are still able to see the old content. There’s no arguing that this is definitely better than seeing no content at all.
 There are many more use cases for a model like this. Updates on time-critical pages are one thing but think of different modes for your site. You publish statically generated pages fetching all recent data at once at build time. You have a preview mode for your content creators and editors to see what they’re currently working on hosted someplace else.
 import {
   useStoryblok,
   useStoryblokApi,
   StoryblokComponent,
 } from &#x27;@storyblok/react&#x27;;
 
 export default function Home({ story: initialStory, preview }) {
   if (preview) {
     // Load draft articles in preview mode.
     let story &#x3D; useStoryblok(&quot;react&quot;, { version: &quot;draft&quot; });
     return &lt;StoryblokComponent blok&#x3D;{story.content} /&gt;;
   }
 
   return &lt;StoryblokComponent blok&#x3D;{initialStory.content} /&gt;;
 }
 
 export async function getStaticProps({ preview &#x3D; false }) {
   // Load published data in build mode.
   const storyblokApi &#x3D; useStoryblokApi();
   let { data } &#x3D; await storyblokApi.get(&#x60;cdn/stories/react&#x60;, {
     version: &#x27;published&#x27;,
   });
 
 
   return {
     props: {
       story: data ? data.story : false,
       preview,
     },
   };
 }
 
 Nice! Give your editors some taste of what they’re actually working on!
 Incremental Static Updates
 Fetching time-sensitive data on the client-side is one idea to update your page without a full rebuild. It’s great if you want to have fast updates, but you might see a huge switch between the pre-built. If your hosting solution allows it, tools like Next.js allow you to incrementally update content after a certain retention period.
 This article on Smashing Magazine by Lee Robinson goes into incremental static regeneration in full detail. The story in a nutshell: The web server serves statically generated content. You define how long this content is supposed to be valid. If at the time of serving this period has elapsed, Next.js triggers a serverless function that renders the desired page new and overwrites the original page with the newly generated HTML. Quite a task, for you it’s just one line of code you need to add.
 import {
   useStoryblok,
   useStoryblokApi,
   StoryblokComponent,
 } from &#x27;@storyblok/react&#x27;;
 
 export default function Home({ story: initialStory }) {
   // We are fetching new data from our CMS.
   let story &#x3D; useStoryblok(&quot;react&quot;, { version: &quot;draft&quot; });
 
   // If data isn&#x27;t available, yet, we show the original static data.
   if (!story.content) {
     story &#x3D; initialStory;
   }
 
   return &lt;StoryblokComponent blok&#x3D;{story.content} /&gt;;
 }
 
 export async function getStaticProps({ preview &#x3D; false }) {
   // Here, we are loading all data for static generation.
   const storyblokApi &#x3D; useStoryblokApi();
   let { data } &#x3D; await storyblokApi.get(&#x60;cdn/stories/react&#x60;, {
     version: &#x27;draft&#x27;,
   });
 
 
   return {
     props: {
       story: data ? data.story : false,
       preview,
     },
     revalidate: 3600 // Check back after 1 hour if there&#x27;s new content.
   };
 }
 
 Now we have three points where we access our headless CMS from:
 
 Once a build for all available content.
 For time-sensitive data on the client-side. Fallback to the original content.
 In a serverless function for every page we want to update incrementally.
 
 
 Soft Coupling
 All three connection points work for different time frames.
 
 Real-time client-side data fetching has the shortest life span and gives you the most recent content.
 Incrementally generated static pages have a longer life span and give you content that is of a certain age.
 Build-time generated static pages have the longest life span and are as recent as you configure them. Their life span is significantly shorter if you rebuild on every content update, it’s longer if you only rebuild at code changes.
 
 
 But by adding all the connection points to our CMS, something else has changed. We coupled ourselves more often to an external system than before, but every connection is allowed to fail.
 If a real-time update on the client-side fails for whatever reason, we still see the content of the incrementally generated page. If this connection didn’t work, we see what we generated on build time.
 Note: The examples above use Next.js mostly because Next.js allows us to get all that benefit with very little to do for us developers. Especially with the hooks from Storyblok data fetching becomes a single line of code. The same principles of soft coupling can be applied to other sites and frameworks as well.
 Let’s say you generate all your content on the server-side using Express.js. The following route would fetch content with every request to the server:
 app.get(&#x27;/*&#x27;, function (req, res) {
   var path &#x3D; url.parse(req.url).pathname;
   console.log(path);
   path &#x3D; path &#x3D;&#x3D; &#x27;/&#x27; ? &#x27;home&#x27; : path;
 
   Storyblok.get(&#x60;cdn/stories${path}&#x60;, {
     version: &#x27;draft&#x27;,
   })
     .then((response) &#x3D;&gt; {
       // Render content to HTML.
       res.render({
         story: response.data.story,
       });
     })
     .catch((error) &#x3D;&gt; {
       res.send(error);
     });
 });
 
 If the bridge to our CMS fails, so does delivering our content. Adding another connection point serves cached content first and does updates in the background.
 app.get(&#x27;/*&#x27;, function (req, res) {
   var path &#x3D; url.parse(req.url).pathname;
   console.log(path);
   path &#x3D; path &#x3D;&#x3D; &#x27;/&#x27; ? &#x27;home&#x27; : path;
 
   // Loading the cached content.
   let content &#x3D; contentMap.get(path);
   if (content) {
     Storyblok.get(&#x60;cdn/stories${path}&#x60;, {
       version: &#x27;draft&#x27;,
     }).then((response) &#x3D;&gt; {
       // Update in the background.
       contentMap.set(path, response);
     });
 
     return res.send(content);
   }
 
   // Fetching the real content.
   Storyblok.get(&#x60;cdn/stories${path}&#x60;, {
     version: &#x27;draft&#x27;,
   })
     .then((response) &#x3D;&gt; {
       contentMap.set(path, response);
       res.send({
         story: response.data.story,
       });
     })
     .catch((error) &#x3D;&gt; {
       res.send(error);
     });
 });
 
 And with some added metadata on revalidation, we can even check for updates after a certain period of time:
 app.get(&#x27;/*&#x27;, function (req, res) {
   var path &#x3D; url.parse(req.url).pathname;
   console.log(path);
   path &#x3D; path &#x3D;&#x3D; &#x27;/&#x27; ? &#x27;home&#x27; : path;
 
   // Loading the cached content.
   let content &#x3D; contentMap.get(path);
   if (content) {
     // Check if the revalidation window has elapsed.
     if (content.fetchedDate + content.revalidate &lt; Date.now())
       Storyblok.get(&#x60;cdn/stories${path}&#x60;, {
         version: &#x27;draft&#x27;,
       }).then((response) &#x3D;&gt; {
         contentMap.set(path, {
           response,
           fetchedDate: content.fetchedDate,
           revalidate: content.revalidate,
         });
       });
 
     return res.send(content.response);
   }
 
   // Fetching the real content.
   Storyblok.get(&#x60;cdn/stories${path}&#x60;, {
     version: &#x27;draft&#x27;,
   })
     .then((response) &#x3D;&gt; {
       // Store with some metadata for revalidation.
       contentMap.set(path, {
         response,
         fetchedDate: Date.now(),
         revalidate: 3600000, //in ms
       });
       res.send({
         story: response.data.story,
       });
     })
     .catch((error) &#x3D;&gt; {
       res.send(error);
     });
 });
 
 Add this with some client-side caching using stale-while-revalidate to add an extra fallback for your revisiting users.
 Bottom Line
 Headless content management systems allow you to add a clear boundary between the content you deliver and the content that is being maintained by others. This clear separation lets a CMS focus on content management again, instead of blurring the line between content and its design.
 But it’s not only that. When deploying websites, there’s rarely a one-size-fits-all solution. Some websites benefit from server-rendered pages, some prefer statically generating content upfront. A headless CMS works with all approaches, allowing you to mix and match depending on your needs.
 We just played through one possibility of adding a headless CMS to our stack and connecting it from various sources to make our site more resilient without losing the flexibility to deliver time-relevant content. And that’s just one of many.</content>
     </entry>
     <entry>
       <title>Notes from a gopher:// site</title>
         <link href="https://daverupert.com/2022/05/notes-from-a-gopher-site/"/>
       <updated>2022-05-04T21:06:00.000Z</updated>
       <content type="text">Long time readers of the blog will know I’m a fan of esoteric content delivery protocols. While compulsively checking for updates on my Playdate (a small, yellow, Gameboy-like device with a crank) and I came across this tweet:
 When Playdate shipped, Panic co-founder Steven wrote a little about what it meant to him. And in the spirit of doing things differently, posted it to a Gopher (!) site.If you&#x27;re up for it, find it here:  gopher://stevenf.com:70/0/journal/2022/04/18/first-playdates-shipping.txt— Playdate (@playdate) April 29, 2022 
 A gopher:// site in the year 2022!? Minutes prior I was reading a Polygon piece about Playdate’s 8-year process behind the crank and I needed more Playdate development stories. Now I find out the co-founder of Panic has a non-http gopher site where he blogs about Playdate development? How cool. I couldn’t resist, but first I needed a gopher client.
 brew install xvxx/code/fetch
 phetch gopher://stevenf.com:70/0/journal/2022/04/18/first-playdates-shipping.txt
 
 With that, Steven Frank’s article appeared in my command line terminal. In the post Steven illuminates some of the core principles and lessons learned from developing the Playdate. You should gopher it and read it, but I plucked my favorite highlights.
 
 Platform owners need not be the gatekeepers of all content and money. A platform can thrive without needing to have their finger in every single pie.  This is, in fact, the way it used to be by default in the industry for a very long time and everyone did quite well.  Anyone can make a Playdate game and distribute it or sell it however they choose.
 
 I love it and it feels somewhat anti-Capitalist, anti-AppStore, or (in a gaming context) anti-Metaverse when most platforms want to insert themselves in the middle of every transaction, taking a cut.
 
 When you buy hardware, you own it and have the right to do what you want with it, including developing and installing your own software.
 
 A wonderful ethos about hardware ownership, especially coming from a traditionally Mac-centered company.
 
 I know everything is kind of depressing recently, but color is OK, shapes are OK, and joyfulness can be a feature.
 
 I wish I could take Sharpies to my screen and underline these characters in my terminal. “Joyfulness can be a feature.” 🤩
 
 Limitations like monochrome displays and a minimum of buttons and controls are not only rocket fuel for developer creativity, but also broaden Playdate’s appeal to people who may find modern games intimidating or out of reach due to their complexity.
 
 This reminds me of Gunpei Yokoi’s “Lateral thinking with withered technology” or the interplay between play (”fun”) and rules. Rules, somewhat counterintuitively, are what make games fun. Thinking of limitations as “rocket fuel for developer creativity” is something I identify with as I look for simple tools in my own development practices.
 The result of adding more constraints means that the products have a broader appeal due to their simple interface. It reminds me of a Jeremy Keith talk I heard last month about programming languages like CSS which have a simple interface pattern: selector { property: value }. Simple enough anyone can learn. But simple doesn’t mean it’s simplistic, which gives me a lot to think about.
 The end of the post turns towards thoughts about modern technology…
 
 Maybe I’m attempting to build a grandiose mythology where none exists,
 
 I’m okay with people building grandiose mythologies… keep going…
 
 we don’t have to just sit by and watch as 2 or 3 massive corporations consume each other and gradually become the sole arbiters of what you can and cannot do with technology.
 
 This is a violent, pointed finger. And yes, I agree.
 
 Maybe crushing every competitor needn’t be the goal of every business.  Maybe, just maybe, it is good for alternatives to popular ideas to exist.
 
 My heart is pounding in sympathetic applause as we approach the end of the post…
 
 So, at the risk of sounding trite: look forward, but study history. Have goofy ideas and don’t measure their value exclusively in dollars. Never stop playing.
 
 Yes. I’m here for it.</content>
     </entry>
     <entry>
       <title>Designing A Better Language Selector</title>
         <link href="https://smashingmagazine.com/2022/05/designing-better-language-selector/"/>
       <updated>2022-05-04T10:00:00.000Z</updated>
       <content type="text">Imagine that you’ve just arrived in Tokyo. Full of impatience and excitement, you are just about to hit the road, yet there it comes: an urgent warning from your mobile provider, nudging you to top up your dwindling balance. With some justified concern, you go to the website, just to be redirected to the Japanese version of the site. You can’t read Japanese just yet, yet there is no obvious option to change the location, and there is no option to change the language either.
 As the data keeps dwindling, you juggle between auto-translation and your limited VPN options — just to run out of data in the middle of the session. The language selector is somewhere there, yet it’s disguised between cryptic symbols and mysterious icons, nowhere to be found on the spot.
 
 Chances are high that at some point you had a similar experience as well. As designers, we can of course make language selectors more obvious and noticeable, yet most of the time, the appearance of a component is only a part of the problem.
 Too often, when we design interfaces, we subconsciously embed our personal assumptions, biases and expectations into our work. Of course, we can’t possibly consider all exceptions and all edge cases, along with all happy or unhappy coincidences. So we focus on the most common situations, eventually breaking a beautifully orchestrated user experience entirely for some of our disgruntled users.
 Can we fix it? Absolutely! We just need to decouple presets, allow for overrides and allow users to specify their intent. But before we dive in, let’s explore what options we have in front of us.
 The Fine Little Details Of A Language Selector
 Usually, we know when we need a language selector. Every multi-lingual website will need one, and this definitely holds true for public services and companies residing in countries with multiple national languages. It is also necessary for global brands, organizations and the hospitality industry — as well as eCommerce where goods might be paid in various currencies and shipped to various destinations around the world.
 
 Where do we place a language selector? Well, users have their usual suspects of course. In my experience, when asked to change a country or language, a vast majority of users will immediately head to the header of the page first, and if they can’t find it there, they’ll jump all the way to the bottom of the page and scout the footer next.
 As for indicators of country selection, flags actually do work fairly well, and if users can’t spot them, they seek other icons that might represent a language in one way or the other — such as the globe icon or a “translation” icon. Obviously, when it comes to translations of articles or specific pages, users rely on the laws of locality and search for a selection of language next to the title of the article. So far so good.
 
 Design-wise, however, there are plenty of intricate details that we need to account for. Surely the selector on its own will live somewhere in the footer of the page, and it is also very likely to make its appearance in the header as well. However, we could also auto-redirect users based on their location and auto-detect language based on the browser’s preferences, or prompt a modal window and ask users to select a region first. We could be using text labels or abbreviations, icons or flags, native or custom dropdowns, preferences panes or sidebars, toggles, or standalone pages.
 As we will see, many of these solutions have usability issues on their own; and if we want to maximize clarity and reduce ambiguity, we need to come up with a proper strategy of how to label and group languages, how to present them, and how to make the language selector obvious to users — without running into a wild mixture of accessibility and auto-translation problems down the line.
 Let’s start with something that is probably obvious, but worth stating nevertheless — auto-redirects might be helpful, but they often cause more frustration and annoyance than a help.
 Avoid Auto-Redirects
 Many websites rely on redirects based on user’s location (IP) or browser’s language. However, if a person is located in Tokyo, it doesn’t necessarily imply that they fluently read Japanese. And if their preferred locale is Dutch, it doesn’t mean that they want to deliver physical items to the Netherlands. In the same way, if the preferred locale is French, yet it isn’t available on the site, a user might encounter a fallback language that isn’t necessarily the language that they are most comfortable with.
 We can’t confidently infer users’ preferences without asking them first. That doesn’t mean that we should avoid redirects at all costs though. If a user happens to be connecting to a US website from Germany, it‘s perfectly reasonable to nudge them towards a German website. But if they happen to be connecting to a German website with an English locale preferred, it would be confusing to redirect them to the UK or US version of the site — even though it might very well be the user’s intent in some rare cases.
 In general, redirects based on location are probably more instructive than redirects based on the browser’s language, but they are error-prone, too.
 
 On the very first visit, Dyson.com nudges visitors to select the preferred region and language in the header on the page. Users can dismiss the bar and locate the language selector in the footer of the page again.
 
 Backcountry, a US company for outdoor gear and clothing, automatically redirects its users to another site. Since 2018, the website is no longer available outside the U.S. as an answer to the GDPR regulations. Without a VPN, it’s impossible to reach the website, for example, to purchase and deliver a gift for a friend located in the U.S.
 
 Audi automatically redirects users to a country deemed as a best fit. However, users can choose their country by clicking on the language selector in the right upper corner. On click, a modal shows up with autocomplete and a disabled “Continue” button.
 
 
 A global BMW website doesn’t automatically redirect users to any website. Instead, you can locate the “BMW in your country” option in the right upper corner of the header. It opens a modal with all the options listed, along with the prominent button at the top “BMW in your country”, which, on click, redirects users to the website considered to be the best fit for them. 
 
 IKEA, without an automatic redirect, but with a very large country selector that understands domains, endonyms and languages of the largest countries in the world. The “Go shopping” button might be the biggest button in the world and might deserve a spot in the World Guinness Records Book. Unfortunately, on the site, you can change the country, but not always the language.
 While polite nudging is reasonable, automatic redirects are not. Once we start moving users from one site to another without asking them at all, we start baking our assumptions into the design, and that’s usually a red flag. We shouldn’t be surprised by increased levels of frustration and abandonment as a result. Ironically, this data is rarely tracked or known as the abandonment is happening on the “other” website, with different departments and teams on the other side of the globe.
 Either way, whether we want to nudge users towards a different website, or we absolutely need to use an auto-redirect, it’s definitely a good idea to always allow users to override redirects with manual preferences. This requires us to tame our assumptions and decouple our presets.
 Decouple Location and Language Presets
 Many websites rely on an assumption that location, language and currency are usually tightly coupled. After all, if a user chooses a location in Germany, they are very likely to prefer the German language and see prices in Euro. However, this is based on assumptions that work for many people, but break the experience entirely for others.
 
 For example, if you want to purchase sneakers on Adidas from Germany but deliver them to your friend in Poland, you need to be able to make sense of the Polish language when checking out. You can either choose the German language with delivery to Germany or the Polish language with delivery to Poland. It’s impossible to select the English language with delivery options to Poland, for example. In other words, both language and location are tightly coupled.
 As it turns out, there are plenty of scenarios where this assumption doesn’t work:
 
 A person is using a German VPN, but not be located in Germany nor understands German;
 A person is connecting from Germany, but be visiting for a few days, and they might not speak nor read German at all;
 A person is living in Germany, access a website in German, but prefers to pay with a company’s credit card in USD, rather than in EUR;
 A person is living in Germany might want to deliver an item from an American store to an American friend, but keeps getting redirected to a German website;
 A person is connecting from the USA but needs to be able to provide a VAT number because the product will be purchased by a German office with a German credit card.
 
 Of course, we might consider all these situations to be very rare edge cases and dismiss them. But first, we need to track how many people actually experience such issues and end up leaving as a result. In practice, especially for global brands, these numbers might be more significant than one might think.
 These problems appear because we frame common situations in tightly coupled and rather inflexible presets. Surely presets are useful as default options, but they break down when defaults aren’t good enough. That’s why it’s usually a good idea to decouple all presets, and allow users to make standalone choices.
 
 On Mondraker, users select location and language separately. All countries are grouped into tabs, and at the bottom users can choose the language of their preference. A very similar design, but a quite different approach. A downside: labeling all countries in a selected language is probably not as effective as using corresponding native labels instead.
 
 Monese shows two tabs in the right upper corner of the header. Users can switch between language and country, defining preferences for each separately.
 User preferences don’t have to be limited by country and language alone. We can allow users to customize further parts of the UI, from currency and auto-translation to units of measurement and date formatting.
 Allow Users To Set Custom Preferences
 For many sites, language and location are just the first important attributes that convey what website might be a good fit for a customer. However, to deliver value to users, we might want to go a little bit beyond that.
 
 
 Revolve.com uses language, country and currency presets based on the user’s IP and their browser’s locale. However, users can override these presets with custom preferences. They can choose a country for shipping, the language on the site and the currency. The hint for preferences is located in the header, with a combination of a language abbreviation, flag and a currency indicator.
 These details are enough to show all products with the final price that includes delivery costs to their country and in the currency that’s most familiar to them. That’s what the perfect decoupling of location, language and currency is.
 
 
 AirBnB suggests languages and regions in groups, but also allows users to adjust their preferences and choose a language and region of their choice. Additionally, users can opt-in to automatically translate descriptions and reviews to English. The modal is prompted by a tap on the globe icon in the right upper corner of the header.
 Once the settings are set, users can jump from one location to another, compare prices in the same currency and see reviews automatically translate to a language that they might understand better. That’s undoubtedly a win for the users.
 
 iHerb goes the extra mile by providing a whole range of additional preferences for their users. Not only can users choose their language, preferred currency and shipping destination (and specify it with ZIP code for US destinations) — they can also choose preferred units of measure and check available payment methods and available shipping methods. Bonus points for smart autocomplete input rather than a not-so-good old-fashioned &lt;select&gt;-dropdown. 
 
 Some slightly different preferences can be defined on the State Street of Global Advisors. On the first visit, a modal window appears explaining to users some of the assumptions that the interface is making about the location and their interests. Within the modal window, users can change a location, specify their role and choose a preferred type of site for their visit. 
 In general, these are some of the useful adjustments that we could allow users to specify to customize the entire experience for them:
 
 Shipping location
 Preferred currency
 Units of measure
 Time/date formatting
 Time zones preferences
 Level of experience
 
 The question, of course, is how to surface all these settings to the user — in a separate settings page, as a sidebar, in the header, or the footer? One disputable option is to show the settings in a modal or non-modal window upon entry to the site.
 A Case For Non-Modal Dialogs
 Admittedly, modal windows are rarely a good idea. They are disruptive and annoying as they require immediate attention. However, they are appropriate when we need to draw the user’s attention to important details — be it loss of data, mutually exclusive preferences or critical errors.
 Some of the websites listed above prompt a modal window on the very first visit, asking users to specify their intent and their preferences before using the site. On others, default presets are silently applied, with an option to adjust them if needed — sometimes in a modal, and sometimes on a dedicated page.
 
 While modal windows will always be noticed, usability tests show that they are often instinctively dismissed, sometimes even before users realize what content they contain. On the other hand, users often don’t pay attention to any accessory navigation such as choice of currency, measurements or shipping location as they are very much focused on products. It’s only if the change of the language is necessary that they might notice that further settings can be adjusted as well.
 
 
 
 Rather than using one modal with tabs, Booking uses separate buttons in the header for currency and language. The interface infers some settings from the user and applies them directly, with an option to override these settings. Rather than using a &lt;select&gt;-dropdown, which is often the slowest form component, all options are displayed in plain text, hence being searchable by in-browser search.
 
 For comparison, Skyscanner allows users to prompt all customization options with one large button, grouping all options in a few drop-downs. Also, the interface always allows users to fall back to the English language if they’ve accidentally made a mistake.
 Which option is better? Ultimately, this will of course be decided by usability tests. In this particular case, showing a modal window upon entry might not be a bad idea since it provides tangible value to users — a value that they might not be able to spot otherwise. However, there might be an alternative option that could work even better — using a non-modal dialog instead.
 
 Upon website entry on Patagonia, a sticky non-modal dialog appears in the left bottom corner. Users can choose location and language and save their preferences as a cookie. They can also always bring the selection back by accessing the preferences bar in the footer. 
 
 In the mock-up above, the important content isn’t blocked by the modal; users can scroll, navigate, select and copy-paste. However, the preference pane appears in the bottom right section of the screen. It can also be collapsed or minimized, but it does require an action from the user. It is a little bit more intrusive than when silently placed in the global navigation, but is easier to discover as well.
 If you aren’t certain about the best option for your project, consider adding a link in the navigation bar first. Measure design KPIs and test how they change with a non-modal option — a much less intruding and more friendly option — and ultimately a modal. Chances are high that the modals might perform better than one might think.
 Click-Through Menus For Countries
 Large corporations know the problem too well: navigating dozens of options in a small overlay, or even a large modal is quite cumbersome and requires a healthy dose of scrolling. So it’s not very surprising that often websites present all available options on separate pages, broken down by regions and sometimes illustrated with country flags.
 
 Revolut displays all available options on a separate, dedicated page. The countries are written in the English language, organized in groups and listed alphabetically. However, the page doesn’t only showcase available locations, but also locations that aren’t available yet. For this particular case, it might be a good idea to allow users to filter — e.g. hide all unavailable locations, perhaps with a toggle or tab above the list.
 
 Logitech displays most languages in their local format — e.g. “Deutschland” for Germany, and “中文” for China. This eliminates the assumption that the user needs to understand English to find the country or language of their choice. On the page, all countries (and available languages) are grouped by geography and displayed across columns, making it easier for users to discover them.
 
 Rather than displaying all available options on one long page, Dell breaks countries by regions within tabs. No flags are being used, making the scanning a bit more difficult. Countries and languages are combined. In this case, less scrolling is required to find a location that would fit users best.
 
 Not all tabs are alike though. Cisco uses a small overlay with vertical tabs, rather than horizontal ones. This makes the selection very compact, and the solution very straightforward. It’s worth noting that the main disadvantage of tabs is that they make the content inaccessible with an in-browser search (well, for now). The user always has to select a region first.
 
 Another option, of course, is to group the countries with a vertical accordion, as it’s done on eDreams, for example. You might need a bit more vertical space as a result, but all options can be scanned from top to bottom in one go.
 
 A slightly different kind of country selector on Oracle: a click-through overlay menu, with all countries grouped, rather than displaying a standalone page. That’s a very compact and straightforward solution.
 If you need to display a wide range of languages, explore if you can group and display them on a single page. If it’s getting too overwhelming, consider grouping them within accordions or tabs — assuming that tabs appear like tabs and don’t contain cryptic labels. Or even better: provide users with poignant autocomplete suggestions.
 Show Autocomplete Suggestions
 Getting autocomplete right isn’t an easy task. This is especially difficult if we are dealing with multiple pieces of information at once, i.e. both country and language. For it to work well, we need to support frequent abbreviations, endonyms, and shorthands for all available options. And then, of course, our autocomplete suggestions should display both countries and languages, with an option to choose one option or another. Plus, we also might want to consider the support of multiple “primary” languages (English, French, Spanish, to name a few). Thats’ not easy at all!
 
 On Framework, users can select country and language separately, both with autocomplete, with the most frequent options highlighted on focus. There is no need to scroll through the list of countries to find the preferred option. While this might be perfectly enough for some scenarios, it might not be sufficient in a situation when the user’s country isn’t available in the list. Instead, we could indicate the closest locations to the preferred option, rather than guiding a user to a dead end.
 
 In the mock-up above, “4 locations nearby” could open an accordion, highlighting the closest locations next to Lithuania (Lietuva), indented.  This pattern might not be applicable when a user is trying to open a new bank account, but it might be useful when a user is looking for a particular office in their country, but can’t locate it.
 
 Wise also includes autocomplete for language settings. If the same language appears in multiple items, the autocomplete specifies what country it refers to. All language options are presented in their local format.
 
 Porsche uses an accordion along with autocomplete as a page overlay. The interface supports abbreviations and indicates available options with flag icons.
 Undoubtedly autocomplete is a great addition to language selection. However, when testing it, explore how people use autocomplete, and what they are actually typing to find their country. Sometimes the fine-tuning of making autocomplete work for many different languages might be an effort way too underestimated and way too time-consuming. 
 Grouping Countries
 Not every location or language has to represent with a separate entry in the language selector. If multiple countries are speaking the same language, what if indicated by grouping countries within one option?
 
 Daniel Marchini has come up with an interesting concept of grouping flags within a single selection. If the content will appear exactly the same for multiple countries, is it really necessary to show them separately? For example, the Portuguese language is displayed as an option for Portugal and Brazil, while the Spanish language is highlighted for Mexico and Spain. Obviously, not all countries could be grouped this way, but if you target users from specific countries, this might be worth a shot.
 
 Airwallex’s country selector groups all European countries as “European Union”. The service is available in the entire European Union, so it’s not necessary to select an individual country. However, if you have a slightly longer list of options, and you are looking for an option to open a bank account in the Netherlands, you might need a bit of time to realize that the Netherlands is assumed as a country within the European Union.
 Use Flags For Countries, Text Labels For Languages
 When designing a country selector, it feels almost natural to think about the flags they are represented by. After all, compared to just plain text, it should be much easier for users to locate the icon that they can immediately recognize. This is indeed true, however, as James Offer has suggested in his wonderful blog on Flags are not languages, flags are specific to countries, but languages often cross borders.
 We can find French-speaking people in Canada, Vietnam, Senegal, Switzerland, and many other countries. It would be inaccurate to assume that all users from these countries associate their choice of language with a French flag. 
 
 In the article “My Take On Language Selectors”, Zsolt Szilvai shows an interesting example of such a conundrum. During the usability tests of an application designed for the UAE, many people found the fact that the Arabic language was visualized with one single flag, as it is used in many countries and cannot be identified with any particular flag alone.
 
 Curve.com opts in for a default international version which is available in English. There are a few other options available as well but one might wonder about the difference between “International (English)” and “English (United States)”. When flags are used to indicate languages, it can quickly become a little bit confusing.
 
 Backmarket includes a list of flags in the footer of the page to indicate local sites of the marketplace. When we want to drive users to specific local websites, we can safely use flags that best represent countries, rather than languages. Many sites also just add links in the footer instead, making language labels easier to find with an in-browser search.
 
 Flags for countries, and plain text for languages. Everything seems to be about right on Bol.com. The country selector (with a flag) is located in the right upper corner, where most users expect it.
 To avoid misunderstandings, make sure that you use flags if your users need to select a specific country. However, if you’re providing users with a choice of a specific language, then flags are probably not going to work well. There, an autocomplete with all available countries and labels for languages written next to them might work better.
 This of course brings up a question: how should these labels actually be written? In English or in a language’s local format?
 Label Languages Locally
 Assumptions are error-prone, and if it goes for combinations of currency, language and location, this holds true for how we label languages as well. We shouldn’t assume that a user will be speaking one of the languages we choose to see as a default option. Instead, when users select a language, usually it’s better to always use the name of the language in its local format.
 So rather than offering a choice of German and Chinese, assuming that users understand English, we can label these options as Deutsch and 中文.
 
 But if the languages are labeled locally, somebody who happens to be in China might be experiencing issues switching to a slightly more familiar language. Surely flags would help to locate a button that would allow for that, but we could also prefix the selected language with a label, for example, “Language” to make it easier to spot the selector. Or we could just add a link saying “English” in the header. This of course relies on assumptions we are making, but it might be easier than hopping through the navigation bar and view-source with fingers crossed.  
 
 Booking provides a hint to indicate that users can change the language in a local language. If you prefer to show a hint on hover, that’s one of the very few cases where one might consider using a language that many users would understand, and it could be English.
 
 The Globe and Translate Icons
 Since flags can be somewhat problematic, what would be a reasonable alternative to them? As briefly mentioned at the beginning of the article, we can also use icons such as “Globe” or “Translate” to indicate the choice of locales. There is as well an official language icon, which is free to use, but unfortunately is still not as recognizable as the other icons.
 
 Surely not everybody will be able to understand the icon in combination with a word that they can barely decipher, but if it’s prominently located in the header or the footer, the chance to be discovered are significantly higher.
 
 Tomorrow.one displays a large drop-down selector with a globe icon in the footer of each page. It’s not available in the header on the site. Because pages aren’t very long, that’s probably not a big problem, but users might give up if they have to embark on a long-running scrolling marathon, or if the infinite scroll prevents them from reaching the footer.
 
 On Atlassian, the language selector is tucked at the very end of the page in the footer, with a globe icon indicating the selection. However, if the user with a different browser language preference enters the site, it suggests changing the language at the very top of the page, with a globe icon appearing there, too.
 
 Monday.com keeps the language selection at the very top of the page, in the left upper corner. All options are presented in three columns, with the current selection highlighted in blue.
 
 While flags are easier to recognize, icons can work as an alternative option as well, especially if you need to provide users with language options, rather than choices for location. Even if the selection is provided in the header, it’s a safe bet to also place it at the bottom to ensure that users can find it when they need to. 
 Avoid Language Shorthands or Initials
 Another interesting problem that Zsolt Szilvai has discovered in testing is related to the use abbreviations, initials or shorthands to indicate a particular language. When we are running out of space in navigation, we could be using “EN” for English, or “DE” for Germany, or “UA” for Ukraine. Indeed, these shorthands are often well-understood, but they bring surprising results when a user’s browser auto-translates all websites in a particular language.
 
 Not only does it often result in broken menus and surprising layouts; browsers also translate language shorthands, producing an interface that might be very difficult to make sense of. However, were the shorthands avoided in favor of the full local name of the language, the user wouldn’t have to deal with these issues at all. Instead, the translator would help them find a language that would work better for them.
 
 N26.de uses a shorthand “EN” for “English”. The selected language is disabled in the list of options, but it’s probably a good idea to increase the color contrast a little. As users scroll down the page, the header remains sticky, so there is really no need to display the language selector in the footer as well.
 
 Wise uses shorthands for language selection in the right upper corner, but displays the languages in full on click, with a noticeable focus style to indicate where a user currently is. This avoids the problem of auto-translation that often turns abbreviations into seemingly random strings.
 Wrapping Up
 The country and language selector might appear like a quite trivial design challenge, but there are plenty of fine details that make or break the experience. When designing one, always decouple presets and reduce assumptions about groups that are likely to go together. Users expect the language selector to be located in the header or the footer of each page, and they often watch out for flags, “Globe” or “Translate” icons to find it.
 If you have just a few languages, a drop-down overlay might be perfectly enough. If you need 10–15 languages, perhaps it’s worth exploring the option of a non-modal overlay with autocomplete. If there are even more options to display, consider using a standalone page, with countries grouped into tabs or accordions.
 Language Selector Checklist
 As usual, here’s a general checklist of a few important guidelines to consider when designing a better language selector:
 
 Nudge users, but avoid auto-redirects.
 Decouple presets, be it location, language, or anything else.
 Allow users to set custom preferences (currency, time zones, units of measure).
 Consider using a non-modal dialog.
 Organize countries and languages in sections, tabs, and accordions.
 Provide input with autocomplete suggestions.
 Use flags for countries, but avoid them for languages.
 Consider the Globe and Translate icons instead of flags.
 Label languages locally, e.g. Deutsch instead of German.
 Avoid language shorthands or initials.
 For accessibility reasons, make sure the country selector appears in the header as well as in the footer, and is keyboard-accessible.
 
 Meet Smart Interface Design Patterns
 If you are interested in similar insights around UX, take a look at Smart Interface Design Patterns, our shiny new 7h-video course with 100s of practical examples from real-life projects. Plenty of design patterns and guidelines on everything from accordions and dropdowns to complex tables and intricate web forms — with 5 new segments added every year. Just sayin’! Check a free preview.
 Meet Smart Interface Design Patterns, our new video course on interface design &amp; UX.
 
 Jump to the video course →
 
 100 design patterns &amp; real-life 
 examples.7h-video course + live UX training. Free preview.
 Resources
 
 Flags are not languages, a blog by James Offer
 “My take on language selectors” + accessible implementation details, by Zsolt Szilvai
 UX practice: Skyscanner’s language selector
 Language switching UI/UX on multilingual sites, by Robert Jelenic
 Best practices for presenting website language selection
 UI/UX design of a language selector
 Interesting language selector patterns on Siemens Design System
 Designing a language switch: Examples and best practices, by Thomas Peham
 
 Related Articles
 If you find this article useful, here’s an overview of similar articles we’ve published over the years — and a few more are coming your way.
 
 Designing A Better Infinite Scroll
 Designing Better Breadcrumbs
 Designing A Better Carousel UX
 Designing A Better Accordion
 Designing A Better Responsive Configurator
 Designing A Better Birthday Picker
 Designing A Better Date and Time Picker
 Designing A Better Feature Comparison
 Designing A Better Slider
 “Form Design Patterns Book,” written by Adam Silver
 </content>
     </entry>
     <entry>
       <title>Vibe Check  №16</title>
         <link href="https://daverupert.com/2022/05/vibe-check-16/"/>
       <updated>2022-05-03T15:23:00.000Z</updated>
       <content type="text">April was a busy month of new happenings and finishing projects. I did a new workshop, we got a new lawn, my kids are learning new skills, my wife got a new (short term) job, new developments at my new company, a new blog design, and to cap it all off I turned a new age number which is for sure what people call having a birthday.
 A new lawn
 
 My wife and I have wanted to redo the yard for awhile now. In Austin it’s hard to find people to do the work. We even offered someone an obscene amount of money to help us and they didn’t even call us back. Then our neighbor Jim said he was getting his bushes replaced by a guy named José. We asked José, not expecting to hear back, and he said he could start that week.
 Surprise remodeling project! Setting the landscape (heh), while the existing landscaping was wonderful and charming, in practice it didn’t work well for our family. My wife and I worked hard to narrow the scope and priority of the projects we needed done.
 Project #1: The front yard
 The front yard zeroscape treatment featured two amoeba-shaped rings of grass surrounded by crushed granite rocks. Modern, low maintenance, but the slope of the front yard causes all those rocks wash into the street every time it rains. I get the joy of looking like an asshole once a month shoveling the rocks from the street back into my yard. Project #1: Replace rocks with grass.
 We also needed some concrete steps, but I didn’t want to blow the scope of the project and delay it further by mixing trades, so this was #4 on our list of priorities.
 Project #2: The rock pit
 In the backyard I worked hard to remove a big area of english ivy and revealed a rock pit. Yikes. A haven for snakes, roaches, and mosquitos. I removed the rocks and thankfully my other neighbor Jason took some of those rocks, but I still had two mounds of dirt surrounding the pit that must have been a pond at some point. Now every kid loves a rock pit to play in, but a flat patch of grass would be more useful to our family. Project #2: Fill pit with dirt and cover with grass.
 Project #3: The side yard
 On the side yard, we had another english ivy zone that didn’t do much other than eat lost toys. Ideally this could be usable space where we could park our garbage cans or possibly another camper one day. This was a stretch goal. Project  #3: Convert ivy side yard to a crushed granite pad.
 I love it when a plan comes together
 I was happy to do this all as separate projects over different months, but José and his crew made some on the fly decisions that tied the projects together and made everything more efficient.
 They removed the rocks from the front yard (Project #1) and dumped them in the side yard (Project #3). Then they took the dirt mounds from the rock pit (Project #2) and used that as fill for the front yard where the rocks were (Project #1). After about two days of work, we were ready for the four pallets of sod.
 As we were preparing for the grass, José said he knew a concrete guy. A couple of quick conversations and we were pouring concrete within the week. The total turnaround time on the entire project was about 2 weeks. In the blink of an eye, I had a new front and back yard.
 St. Augustine is a hearty stubborn grass that grows well here in Texas. It’s a bit water-hungry and I know that’s going to bite me in the upcoming Water Wars, but I think grass was a good choice and my microbiome seems to support it (we were the only rock yard on the street). And the results seem to have had a cooling effect on the whole property. The other grass seems happier, the trees seem happier, I’m happier; it works.
 There’s a sense of relief and pride that comes with finishing a project that’s been on your dockett for years. We now have a usable front yard that doesn’t stab your feet as you run out to grab something from the car barefoot. With the shed, the flattened lawn, and the trampoline, we now have the backyard we’ve envisioned for awhile. It’s all come together. That project’s done. Next we battle the mosquitos.
 My web component workshop and my web component talk
 The second week of April I headed up to Minneapolis to do my Frontend Masters workshop. My whole year so far had been working towards that, so there’s a sense of relief to have that checked off my to-do list. A months worth of nights and weekends went into preparing the course, building an entire online guidebook for the course, and making slides. Then for that workshop day you have to upload an internet’s worth of know-how into your “Brain RAM”, it’s a bit overwhelming. But I think it went well, I learned a lot about giving workshops, and I look forward to people’s feedback when the course gets released.
 The next week I did my web components talk at An Event Apart and a LIVE ShopTalk as well. The crew at An Event Apart work hard to put on a good conference. Wil Reynolds’s talk on SEO keeps rattling around in my brain the most. He spoke about how the context of words change when they’re next to other words, and if you manage to win the keyword lottery how do you meet customers expectations by delivering on those words. You should find this talk, I’m not doing it justice. Thanks again to AEA for having me.
 Thus ends this spring conference season for me. I haven’t made any hard decisions about speaking at future conferences, but I am acknowledging to myself they are a tough fit for me and my life right now while starting a new company (Luro). As I understand my chaos brain a bit more, diverting extra brain cycles to prepare a talk while doing my job, life, kids, etc was a skill… but now it’s a pretty risky maneuver to add to the Jenga tower of obligations without careful consideration.
 Baseball and the suburban life
 We are deep into the shuttling kids around to their activities lifestyle. While my wife does most of this, I spent a lot of April evenings down at the baseball fields with my son Otis. I didn’t know if he’d every want to try baseball again after COVID cancelled his first season of baseball. A weird side effect of parenting small kids thru COVID is its not uncommon that they might have missed some critical socialization windows or —in our case— learning how to play group sports. Thankfully they have a cohort of kids in the same boat.
 As each week progresses Otis is getting better at hitting, throwing, and fielding. It’s great to see him start to figure out the mechanics of the game and his body. Although it wreaks havoc on bedtime routines, it’s been a fun ritual to be out watching these kids learn and play the game under the lights of the field. It’s a good little taste of Americana.
 Baseball is a bit of a flash mob event though and the season ends the first week of May. He’s excited his team has a chance to go to the championship, but I can’t tell if that’s a real thing or a thing he and his teammates made up. I think we’re already looking at signing up “fall ball”, my daughter Emi wants to play too. Looking forward to that if it happens.
 I turned 42
 At the end of the month, I added another year to the sum of my lifespan. This happen most years so it wasn’t a shock. Somewhere Neil deGrasse Tyson is saying “Space doesn’t care about your birthday”, but birthdays are still nice to have sometimes.
 To celebrate this year, we kept it a bit low-key and had groups of friends from different corners of my life gathered at my house in my new backyard. We ate BBQ and drank swampwater while herds of kids migrated between the house and trampoline and back, zig-zagging around us like the mosquitos. I had a great afternoon/evening and I’m thankful to know such wonderful people.
 
 Stats
 What you all actually came for. Ugh, nerds.
 
 💪 Fitness - Mediocre performance.
 
 Closed my rings 9 times.
 I was under 240lbs (Goal! 🙌) but only for one day. I think a single HIIT workout did the trick. So will try more of those.
 
 
 🧶 Crafting - No big crafts.
 📖 Reading - Two books about the apocalypse.
 
 Station Eleven - The book that inspired the HBO show (I haven’t watched). Great book. A little too close to home with the central premise being a pandemic and whatnot.
 The Ends of the World - Recommended by Rob Dodson, The Ends of the World a scientific retelling of the five mass extinction events that have already happened on the earth and a look how humans are catapulting ourselves towards another event. Ironically, not as anxiety inducing as the apocalyptic fiction book.
 
 
 📺 TV, Movies, and Anime - Mostly just YouTubes.
 
 Al Bladez - extreme tall grass lawncare
 Speak the Truth - a look at the Ukrainian war from a tactical military perspective
 Went on a weird bender about how Egyptians built the pyramids.
 Lots of “wet fart prank” videos. I ain’t proud of it, but it gave me some joy this month.
 
 
 👾 Video games
 
 Joined the Overwatch 2 Beta
 Stacklands by Sokpop
 Knotwords by Zach Gage (and that got me back on my Good Sudoku addiction)
 Couple of Wordle clones: Melodle, 言葉で遊ぼう
 
 
 🎙 Podcasts and YouTubes - A pretty standard month
 
 Recorded a workshop for Frontend Masters
 A LIVE ShopTalk at An Event Apart
 No ShopTalk videos, but will resume in May.
 
 
 📝 Blogging - Good month of blogging. 7 new posts. I’m ahead on my blogging OKRs for the quarter. And, as you can see, I’ve started on an open redesign project. May be doing some streams soon.
 
 What if… one day everything got better? - The most important post I wrote this month.
 7 Web Component Tricks - Possibly my highest traffic post.
 My Weekly Engineering Report - A quaint little process post.
 Productivity-sniped by PARA - My favorite post of the month.
 An unplanned open redesign - Behind the scenes of the redesign.
 Inspiration in the Tall Grass - An inspiration post
 Server-side vs Client-side Analytics - See my blog stats in this comparison post!
 
 
 </content>
     </entry>
     <entry>
       <title>Release Notes for Safari Technology Preview 144</title>
         <link href="https://webkit.org/blog/12621/release-notes-for-safari-technology-preview-144/"/>
       <updated>2022-05-02T20:19:11.000Z</updated>
       <content type="text">Safari Technology Preview Release 144 is now available for download for macOS Big Sur and of macOS Monterey. If you already have Safari Technology Preview installed, you can update in the Software Update pane of System Preferences on macOS.
 This release covers WebKit revisions 291506-291957. This release of Safari Technology Preview does not support versions of macOS Monterey prior to 12.3. Please update to macOS Monterey 12.3 or later to continue using Safari Technology Preview.
 Note: Tab Groups do not sync in this release.
 Web Inspector
 
 Fixed page reloading and showing an empty inspector on pages with container queries (r291824)
 Elements Tab
 
 Fixed $0 not being displayed for the selected node after switching to another tab (r291729)
 Fixed unwanted extra dash when autocompleting CSS variable names in the Styles panel (r291740)
 Fixed inline swatch popovers not being hidden when the inline swatch is removed (r291628)
 
 
 Console Tab
 
 Fixed console.screenshot to no longer have extra transparent pixels at the bottom of viewport screenshots (r291519)
 
 
 
 CSS
 
 Added Typed OM support for container units (r291524)
 Fixed CSS cascade regarding logical properties (r291546)
 Fixed incorrect handling of NaN inside calc() for top-level calculation (r291911)
 Let revert-layer roll back to presentational hints (r291594)
 Implemented border-image serialization (r291537)
 Preserved repeat() notation when serializing grid-templates (r291956)
 Reduce memory usage for large, sparse grids (r291952)
 Handled finite value with infinite step in round() for calc() (r291841)
 Fixed incorrect resolution of percentage grid-gaps within subgrids (r291953)
 
 Web Animations
 
 Enabled support for mutable timelines by default (r291868)
 
 JavaScript
 
 Changed Date.parse to stop returning numbers with fractional part (r291603)
 Fixed class field initializer with extra parentheses (r291577)
 
 WebAuthn
 
 Added getAssertion support for virtual HID authenticators (r291624)
 Specified correct ASCPublicKeyCredentialKind in configureAssertionOptions (r291761)
 Updated to pass along timeout to ASA and ignore timeout for conditional mediation requests (r291625)
 
 Web API
 
 Added support for focused and visible ServiceWorkerWindowClient states (r291888)
 Added a check for whether the origin can access storage in the Storage API (r291726)
 Disabled custom storage paths for IndexedDB and LocalStorage by default (r291909)
 Fixed PointerEvent.movementX to not always be 0 (r291886)
 Fixed Context2D drawImage(img, x, y, w, h) to not throw IndexSizeError when width or height are 0 (r291748)
 Fixed fetching a Blob URL with an unbounded Range header to correctly generate a Content-Range response header (r291622)
 Implemented CSSNumericValue.mul, CSSNumericValue.div, CSSNumericValue.add, CSSNumericValue.sub, CSSNumericValue.max, and CSSNumericValue.min (r291597)
 Implemented ServiceWorkerWindowClient.focus (r291938)
 
 Accessibility
 
 Included initial accessibility support for display: contents (r291570)
 
 Media
 
 Fixed a bug where clicking anywhere on the progress bar pauses some MSE video implementations (r291629)
 Fixed video playback for HEVC-encoded video with a lot of b-frames and a wide sliding window (r291813)
 
 Security Policy
 
 Fixed website policies not being respected when doing COOP-based process swap (r291606)
 
 Web Extensions
 
 Fixed a crash clicking on Safari App Extension toolbar items
 </content>
     </entry>
     <entry>
       <title>Building a Musical Instrument with the Web Audio API</title>
         <link href="https://www.taniarascia.com/musical-instrument-web-audio-api/"/>
       <updated>2022-05-01T00:00:00.000Z</updated>
       <content type="text">It&#x27;s been a while since I&#x27;ve written anything due to some personal concerns that I might write about later, but don&#x27;t worry, I&#x27;m still around and I&#x27;m still coding. Recently, I went to Texas and bought a three-row diatonic button accordion. Diatonic accordions are popular for a lot of different types of folk music, which is generally learned by ear. This is good for me, because I don&#x27;t really know how to read music anyway.
 The accordion has 34 buttons on the treble side and 12 buttons on the bass side. Unlike a piano accordion, which has the same logical, chromatic layout as a piano, the diatonic accordion just has a bunch of buttons and I didn&#x27;t really know where to start. Also, every note is different whether you&#x27;re pulling the bellows out or pushing them in, so there are actually 68 notes on the treble side (albeit some are repeated). Also, as I&#x27;m sure you might be aware, accordions are loud. Very loud. In order to not piss off my neighbors too much, and to learn how the layout of this box works, I decided to make a little web app.
 
       
     
   
   
     
 That web app is KeyboardAccordion.com, which like everything else I create in my free time is open source. I noticed that there are just enough keys on a computer keyboard to correspond to the accordion layout, and they&#x27;re arranged in a similar pattern. With this, I can keep track of the notes, scales, and chords and start figuring out how to put it all together.
 Here&#x27;s what one of the accordions looks like:
 
       
     
   
   
     
 I decided to make this app in Svelte, because I&#x27;ve used React and Vue professionally but have no experience with Svelte whatsoever and wanted to know what everyone loves about it.
 Web Audio API
 KeyboardAccordion.com only has one dependency, and that&#x27;s Svelte. Everything else is done using plain JavaScript and the built-in browser Web Audio API. I&#x27;d never really used the Web Audio API before, so I figured out what I needed to to get this working.
 The first thing I did was create an AudioContext and attach a GainNode, which controls the volume.
 const audio &#x3D; new (window.AudioContext || window.webkitAudioContext)()
 const gainNode &#x3D; audio.createGain()
 gainNode.gain.value &#x3D; 0.1
 gainNode.connect(audio.destination)
 As I was figuring everything out, I was experimenting with making new AudioContext for every note because I was trying to fade out the sound, but then I kept realizing that after 50 notes, the app would stop working. Fifty is apparently the limit for the browser, so it&#x27;s better to just make one AudioContext for the entire app.
 I&#x27;m using waves with the Audio API and not using any sort of audio sample, and I used the OscillatorNode to make each note. There are various types of waves you can use - square, triangle, sine, or sawtooth, which all have a different type of sound. I went with the sawtooth for this app because it worked out the best. Square makes an extremely loud, chiptune-esque sound like an NES which is kind of nice in its own way. Sine and triangle were a bit more subdued but if you don&#x27;t fade the sound out properly, it makes a really unpleasant kind of cutting sound due to how your ear reacts when a wave gets cut off.
 Waveforms
 
       
     
   
   
     
 So for each note, I&#x27;d make an oscillator, set the wave type, set the frequency, and start it. Here&#x27;s an example using 440, which is a standard tuning for &quot;A&quot;.
 const oscillator &#x3D; audio.createOscillator()
 oscillator.type &#x3D; &#x27;sawtooth&#x27;
 oscillator.connect(gainNode)
 oscillator.frequency.value &#x3D; 440
 oscillator.start()
 If you do that, the note will just play until infinity, so you have to make sure you stop the oscillator when you want the note to end.
 oscillator.stop()
 For me, this meant event listeners on the DOM that would listen for a keypress event to see if any button was pressed, and a keyup event to determine when any button was no longer being pressed. In Svelte, that&#x27;s handled by putting event listeners on svelte:body.
 &lt;svelte:body
   on:keypress&#x3D;&quot;{handleKeyPressNote}&quot;
   on:keyup&#x3D;&quot;{handleKeyUpNote}&quot;
   on:mouseup&#x3D;&quot;{handleClearAllNotes}&quot;
 /&gt;
 So that&#x27;s really everything there is to the Web Audio API itself when it comes to setting up the app - creating an AudioContext, adding a Gain, and starting/stopping an Oscillator for each note.
 You could paste this into the console and it&#x27;ll play a note. You&#x27;ll have to either refresh or type oscillator.stop() to make it stop.
 const audio &#x3D; new (window.AudioContext || window.webkitAudioContext)()
 const gainNode &#x3D; audio.createGain()
 gainNode.gain.value &#x3D; 0.1
 gainNode.connect(audio.destination)
 
 const oscillator &#x3D; audio.createOscillator()
 oscillator.type &#x3D; &#x27;sawtooth&#x27;
 oscillator.connect(gainNode)
 oscillator.frequency.value &#x3D; 440
 oscillator.start()
 Data Structure
 I had to figure out how I wanted to lay out the data structure for this application. First of all, if I&#x27;m going to be using the Web Audio API with frequencies directly, I had to collect all of them.
 Frequencies
 Here&#x27;s a nice map of notes to frequencies with all 12 notes and 8-9 octaves for each note, so I can use A[4] to get the 440 frequency.
 tone
 export const tone &#x3D; {
   C: [16.35, 32.7, 65.41, 130.81, 261.63, 523.25, 1046.5, 2093.0, 4186.01],
   Db: [17.32, 34.65, 69.3, 138.59, 277.18, 554.37, 1108.73, 2217.46, 4434.92],
   D: [18.35, 36.71, 73.42, 146.83, 293.66, 587.33, 1174.66, 2349.32, 4698.64],
   Eb: [19.45, 38.89, 77.78, 155.56, 311.13, 622.25, 1244.51, 2489.02, 4978.03],
   E: [20.6, 41.2, 82.41, 164.81, 329.63, 659.26, 1318.51, 2637.02],
   F: [21.83, 43.65, 87.31, 174.61, 349.23, 698.46, 1396.91, 2793.83],
   Gb: [23.12, 46.25, 92.5, 185.0, 369.99, 739.99, 1479.98, 2959.96],
   G: [24.5, 49.0, 98.0, 196.0, 392.0, 783.99, 1567.98, 3135.96],
   Ab: [25.96, 51.91, 103.83, 207.65, 415.3, 830.61, 1661.22, 3322.44],
   A: [27.5, 55.0, 110.0, 220.0, 440.0, 880.0, 1760.0, 3520.0],
   Bb: [29.14, 58.27, 116.54, 233.08, 466.16, 932.33, 1864.66, 3729.31],
   B: [30.87, 61.74, 123.47, 246.94, 493.88, 987.77, 1975.53, 3951.07],
 }
 Button layout
 Figuring out exactly how to arrange all the buttons into a data stucture took a couple of tries for me. The data that had to be captured was:
 
 The row on the accordion
 The column on the accordion
 The direction of the bellows (push or pull)
 The name and frequency of the note at that row, column, and direction
 
 This means that there are different combinations for all three sets of these things. I decided to make an id that corresponds to each possible combination, such as 1-1-pull being row 1, column 1, direction pull.
 This way, I could create an array that holds the data for any note that is currently being played. If you press the button to reverse the bellows, it would take all the currently playing notes and reverse them, thus changing 1-1-pull and 1-2-pull to 1-1-push and 1-2-push.
 So ultimately I had an object that contained the data for all three treble rows like so:
 layout
 const layout &#x3D; {
   one: [],
   two: [],
   three: [],
 }
 My particular accordion is tuned to FB♭Eb, meaning the first row is tuned to F, the second row is tuned to B♭, and the third row is tuned to E♭. The example for the first row looks like this:
 layout
 const layout &#x3D; {
   one: [
     // Pull
     { id: &#x27;1-1-pull&#x27;, name: &#x27;D♭&#x27;, frequency: tone.Db[4] },
     { id: &#x27;1-2-pull&#x27;, name: &#x27;G&#x27;, frequency: tone.G[3] },
     { id: &#x27;1-3-pull&#x27;, name: &#x27;B♭&#x27;, frequency: tone.Bb[3] },
     { id: &#x27;1-4-pull&#x27;, name: &#x27;D&#x27;, frequency: tone.D[4] },
     { id: &#x27;1-5-pull&#x27;, name: &#x27;E&#x27;, frequency: tone.E[4] },
     { id: &#x27;1-6-pull&#x27;, name: &#x27;G&#x27;, frequency: tone.G[4] },
     { id: &#x27;1-7-pull&#x27;, name: &#x27;B♭&#x27;, frequency: tone.Bb[4] },
     { id: &#x27;1-8-pull&#x27;, name: &#x27;D&#x27;, frequency: tone.D[5] },
     { id: &#x27;1-9-pull&#x27;, name: &#x27;E&#x27;, frequency: tone.E[5] },
     { id: &#x27;1-10-pull&#x27;, name: &#x27;G&#x27;, frequency: tone.G[5] },
     // Push
     { id: &#x27;1-1-push&#x27;, name: &#x27;B&#x27;, frequency: tone.B[3] },
     { id: &#x27;1-2-push&#x27;, name: &#x27;F&#x27;, frequency: tone.F[3] },
     { id: &#x27;1-3-push&#x27;, name: &#x27;A&#x27;, frequency: tone.A[3] },
     { id: &#x27;1-4-push&#x27;, name: &#x27;C&#x27;, frequency: tone.C[4] },
     { id: &#x27;1-5-push&#x27;, name: &#x27;F&#x27;, frequency: tone.F[4] },
     { id: &#x27;1-6-push&#x27;, name: &#x27;A&#x27;, frequency: tone.A[4] },
     { id: &#x27;1-7-push&#x27;, name: &#x27;C&#x27;, frequency: tone.C[5] },
     { id: &#x27;1-8-push&#x27;, name: &#x27;F&#x27;, frequency: tone.F[5] },
     { id: &#x27;1-9-push&#x27;, name: &#x27;A&#x27;, frequency: tone.A[5] },
     { id: &#x27;1-10-push&#x27;, name: &#x27;C&#x27;, frequency: tone.C[6] },
   ],
   two: [
     // ...etc
   ],
 }
 There are notes 1 through 10 in row one, and each one has a name and frequency associated with it. Repeating this for two and three, I now have all 68 notes on the treble side.
 Keyboard layout
 Now I had to map each key on the keyboard to a row and column of the accordion. Direction doesn&#x27;t matter here, since z will correspond to both 01-01-push and 01-01-pull.
 keyMap
 export const keyMap &#x3D; {
   z: { row: 1, column: 1 },
   x: { row: 1, column: 2 },
   c: { row: 1, column: 3 },
   v: { row: 1, column: 4 },
   b: { row: 1, column: 5 },
   n: { row: 1, column: 6 },
   m: { row: 1, column: 7 },
   &#x27;,&#x27;: { row: 1, column: 8 },
   &#x27;.&#x27;: { row: 1, column: 9 },
   &#x27;/&#x27;: { row: 1, column: 10 },
   a: { row: 2, column: 1 },
   s: { row: 2, column: 2 },
   d: { row: 2, column: 3 },
   f: { row: 2, column: 4 },
   g: { row: 2, column: 5 },
   // ...etc
 }
 Now I have all the keys from z to /, a to &#x27;, and w to [ mapped out. Very auspicious that the computer keyboard and accordion keyboard are so similar.
 Pressing keys, playing notes
 As you might recall, I have an event listener on the entire page listening for the key press event. Any key press event that happens will go through this function.
 First, it has to check both lowercase and uppercase keys in case shift or caps lock are pressed, otherwise the keys won&#x27;t work at all. Then, if you&#x27;re pressing the button to toggle the bellows (which I made q), it has to handle that separately. Otherwise, it will check the keyMap, and if one exists, it will find the corresponding id by checking the current direction and getting the row and column from the keymap.
 handleKeyPressNote
 let activeButtonIdMap &#x3D; {}
 
 function handleKeyPressNote(e) {
   const key &#x3D; &#x60;${e.key}&#x60;.toLowerCase() || e.key // handle caps lock
 
   if (key &#x3D;&#x3D;&#x3D; toggleBellows) {
     handleToggleBellows(&#x27;push&#x27;)
     return
   }
 
   const buttonMapData &#x3D; keyMap[key]
 
   if (buttonMapData) {
     const { row, column } &#x3D; buttonMapData
     const id &#x3D; &#x60;${row}-${column}-${direction}&#x60;
 
     if (!activeButtonIdMap[id]) {
       const { oscillator } &#x3D; playTone(id)
 
       activeButtonIdMap[id] &#x3D; { oscillator, ...buttonIdMap[id] }
     }
   }
 }
 The way I&#x27;m tracking each currently playing note is putting them in the activeButtonIdMap object. In Svelte, in order to update a variable you just reassign it, so instead of what you might do in React with useState:
 React
 const [activeButtonIdMap, setActiveButtonIdMap] &#x3D; useState({})
 
 const App &#x3D; () &#x3D;&gt; {
   function handleKeyPressNote() {
     setActiveButtonIdMap(newButtonIdMap)
   }
 }
 You have to declare it as a let and reassign it:
 Svelte
 let activeButtonIdMap &#x3D; {}
 
 function handleKeyPressNote() {
   activeButtonIdMap &#x3D; newButtonIdMap
 }
 This was mostly easier, except when all I wanted to do was delete a key from the object. As far as I could tell, Svelte only rerenders when a variable is reassigned, so just mutating some value within wasn&#x27;t enough and I had to clone it, mutate it, the reassign it. This is what I did in the handleKeyUpNote function.
 handleKeyUpNote
 function handleKeyUpNote(e) {
   const key &#x3D; &#x60;${e.key}&#x60;.toLowerCase() || e.key
 
   if (key &#x3D;&#x3D;&#x3D; toggleBellows) {
     handleToggleBellows(&#x27;pull&#x27;)
     return
   }
 
   const buttonMapData &#x3D; keyMap[key]
 
   if (buttonMapData) {
     const { row, column } &#x3D; buttonMapData
     const id &#x3D; &#x60;${row}-${column}-${direction}&#x60;
 
     if (activeButtonIdMap[id]) {
       const { oscillator } &#x3D; activeButtonIdMap[id]
       oscillator.stop()
       // Must be reassigned in Svelte
       const newActiveButtonIdMap &#x3D; { ...activeButtonIdMap }
       delete newActiveButtonIdMap[id]
       activeButtonIdMap &#x3D; newActiveButtonIdMap
     }
   }
 }
 Maybe someone knows a better way to delete an item from an object in Svelte, but this is the best I could come up with.
 I also made a few functions that will play through the scales, starting with F, B♭ and E♭ being the main diatonic keys of the accordion, but there are more options. To play the scales, I simply looped through all the ids that correspond to the notes in the scale and used a JavaScript &quot;sleep&quot; command of 600ms between each note.
 Rendering
 Now that I have all the data structures set up and the JavaScript, I just need to render all the buttons. Svelte has #each blocks for looping logic, so I just looped through the three rows of buttons and rendered a circle for each button.
 &lt;div class&#x3D;&quot;accordion-layout&quot;&gt;
   {#each rows as row}
     &lt;div class&#x3D;&quot;row {row}&quot;&gt;
       {#each layout[row].filter(({ id }) &#x3D;&gt; id.includes(direction)) as button}
         &lt;div
           class&#x3D;{&#x60;circle ${activeButtonIdMap[button.id] ? &#x27;active&#x27; : &#x27;&#x27;} ${direction} &#x60;}
           id&#x3D;{button.id}
           on:mousedown&#x3D;{handleClickNote(button.id)}
         &gt;
           {button.name}
         &lt;/div&gt;
       {/each}
     &lt;/div&gt;
   {/each}
 &lt;/div&gt;
 Each circle has its own mousedown event so you can click on them in addition to using the keyboard, but I didn&#x27;t put the mouseup event on the circle itself. This is because if you move your mouse somewhere else before lifting it up, it won&#x27;t correctly determine the mouseup and the note will play forever.
 And of course, I just used plain CSS because I don&#x27;t usually feel like anything fancier is necessary for small projects.
 .circle {
   display: flex;
   align-items: center;
   justify-content: center;
   border-radius: 50%;
   height: 60px;
   width: 60px;
   margin-bottom: 10px;
   background: linear-gradient(to bottom, white, #e7e7e7);
   box-shadow: 0px 6px rgba(255, 255, 255, 0.4);
   color: #222;
   font-weight: 600;
   cursor: pointer;
 }
 
 .circle:hover {
   background: white;
   box-shadow: 0px 6px rgba(255, 255, 255, 0.3);
   cursor: pointer;
 }
 
 .circle.pull:active,
 .circle.pull.active {
   background: linear-gradient(to bottom, var(--green), #56ea7b);
   box-shadow: 0px 6px rgba(255, 255, 255, 0.2);
 }
 
 .circle.push:active,
 .circle.push.active {
   background: linear-gradient(to bottom, var(--red), #f15050);
   box-shadow: 0px 6px rgba(255, 255, 255, 0.2);
   color: white;
 }
 Conclusion
 I hope you liked my write-up for the Keyboard Accordion app! Of course, the full code is available on GitHub.
 There are a few little bugs here and there, such as if you use keyboard shortcuts while also pressing other keys, it will get stuck on a note forever. I&#x27;m sure if you try to find more bugs you&#x27;ll be able to.
 This app was fun to make, I learned how to use both Svelte and the Web Audio API, and it&#x27;s helping me and hopefully some other afficionados to understand the squeezebox a little better. Maybe it&#x27;ll inspire you to build your own little online instrument, or make an app for one of your hobbies. The best part about coding is that you can make anything you want!</content>
     </entry>
     <entry>
       <title>Common Voice dataset tops 20,000 hours</title>
         <link href="https://hacks.mozilla.org/2022/04/common-voice-dataset-tops-20000-hours/"/>
       <updated>2022-04-28T15:23:57.000Z</updated>
       <content type="text">The latest Common Voice dataset, released today, has achieved a major milestone: More than 20,000 hours of open-source speech data that anyone, anywhere can use. The dataset has nearly doubled in the past year.
 
 Why should you care about Common Voice?
 
 Do you have to change your accent to be understood by a virtual assistant? 
 Are you worried that so many voice-operated devices are collecting your voice data for proprietary Big Tech datasets?
 Are automatic subtitles unavailable for you in your language?
 
 Automatic Speech Recognition plays an important role in the way we can access information, however, of the 7,000 languages spoken globally today only a handful are supported by most products.
 Mozilla’s Common Voice seeks to change the language technology ecosystem by supporting communities to collect voice data for the creation of voice-enabled applications for their own languages. 
 Common Voice Dataset Release 
 This release wouldn’t be possible without our contributors — from voice donations to initiating their language in our project, to opening new opportunities for people to build voice technology tools that can support every language spoken across the world.
 Access the dataset: https://commonvoice.mozilla.org/datasets
 Access the metadata: https://github.com/common-voice/cv-dataset 
 Highlights from the latest dataset:
 
 The new release also features six new languages: Tigre, Taiwanese (Minnan), Meadow Mari, Bengali, Toki Pona and Cantonese.
 Twenty-seven languages now have at least 100 hours of speech data. They include Bengali, Thai, Basque, and Frisian.
 Nine languages now have at least 500 hours of speech data. They include Kinyarwanda (2,383 hours), Catalan (2,045 hours), and Swahili (719 hours).
 Nine languages now all have at least 45% of their gender tags as female. They include Marathi, Dhivehi, and Luganda.
 The Catalan community fueled major growth. The Catalan community’s Project AINA — a collaboration between Barcelona Supercomputing Center and the Catalan Government — mobilized Catalan speakers to contribute to Common Voice. 
 Supporting community participation in decision making yet. The Common Voice language Rep Cohort has contributed feedback and learnings about optimal sentence collection, the inclusion of language variants, and more. 
 
  Create with the Dataset 
 How will you create with the Common Voice Dataset?
 Take some inspiration from technologists who are creating conversational chatbots, spoken language identifiers, research papers and virtual assistants with the Common Voice Dataset by watching this talk: 
 https://mozilla.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id&#x3D;6492f3ae-3a0d-4363-99f6-adc00111b706 
 Share with us how you are using the dataset on social media using #CommonVoice or sharing on our Community discourse. 
  
 The post Common Voice dataset tops 20,000 hours appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>Server-side vs Client-side Analytics</title>
         <link href="https://daverupert.com/2022/04/server-side-vs-client-side-analytics/"/>
       <updated>2022-04-28T13:41:00.000Z</updated>
       <content type="text"> 
   
     .gradient-start stop { stop-color: var(--accent, #2451f5);}
     .gradient-start stop[offset&#x3D;&quot;0&quot;] { stop-opacity: 0.35; }
     .gradient-start stop[offset&#x3D;&quot;1&quot;] { stop-opacity: 0; }
   
   
 For the last 3 years, I’ve been running both client-side analytics and server-side analytics on my site and I’ve noticed some alarming discrepancies. I thought I was alone until Jim Nielsen posted his comparison Comparing Data in Google and Netlify Analytics. My data story and Jim’s data story end up similar (we’re both popular in Germany?); a situation where the more data you have, the less the data seems true.
 The data
 Here’s my data from Mar 25th, 2022 ~ April 25th, 2022. The data collection methods differ like so:
 
 Fathom is a client-side script served from a self-hosted, first-party subdomain (stats.daverupert.com)
 Netlify processes raw server log data.
 
 I think these are both great products that you should totally use over other alternatives.
 Total visits
 
 
 
 
 Fathom (Client-side)
 Netlify (Server-side)
 
 
 
 
 Unique visitors
 12,300
 26,085
 
 
 Pageviews
 18,000
 333,801
 
 
 
 These couldn’t be more different. 2.1× the visitors, 18.5× the pageviews. Obviously the big numbers are the real numbers. 😉
 Top content
 
 
 
 Rank
 Fathom (Client-side)
 Views
 Netlify (Server-side)
 Views
 
 
 
 
 1
 /2022/04/7-web-component-tricks/
 4,483
 /2014/01/4k-rwd/
 101,421
 
 
 2
 /
 3,560
 /2013/04/responsive-deliverables/
 65,491
 
 
 3
 /2022/04/what-if-everything-got-better/
 1,305
 /
 62,486
 
 
 4
 /about/
 665
 /offline
 17,202
 
 
 5
 /2022/04/what-if-everything-got-better/?ref&#x3D;sidebar
 467
 /2014/01/4K-RWD/
 16,686
 
 
 6
 /archive/
 422
 /2022/04/7-web-component-tricks/
 8,433
 
 
 7
 /2012/05/making-video-js-fluid-for-rwd/
 331
 /2022/04/what-if-everything-got-better/
 7,473
 
 
 8
 /2022/04/my-weekly-engineering-report/
 292
 /archive/
 2,106
 
 
 9
 /2022/04/vibe-check-15/
 284
 /about/
 1,605
 
 
 10
 /2022/04/productivity-sniped-by-para/
 265
 /2022/04/vibe-check-15/
 1,412
 
 
 
 As much as I’d love to believe 100,000+ people show up each month to read my 2014 banger about the upcoming advent of 4K displays and its impact on responsive design… I highly doubt that. Even stripping those outliers from the results in the table above, the top 10 posts from each service return wildly different traffic numbers.
 Doing some napkin math comparisons…
 
 
 
 URL
 Discrepancy
 
 
 
 
 /
 17.5×
 
 
 /about/
 2.4×
 
 
 /archive/
 4.9×
 
 
 /2022/04/7-web-component-tricks/
 1.8×
 
 
 /2022/04/what-if-everything-got-better/
 4.2×
 
 
 /2022/04/vibe-check-15/
 4.9×
 
 
 
 The discrepancies are not consistent at all. If these were similar, we might be able to say 75% of people use ad blockers and believe the large numbers, but it’s not so cut and dry.
 Top locations
 
 
 
 Country
 Pageviews
 
 
 
 
 Germany
 75,359
 
 
 Finland
 70,605
 
 
 United States
 60,031
 
 
 Canada
 45,305
 
 
 China
 28,092
 
 
 France
 13,345
 
 
 UK
 5,960
 
 
 
 My version of Fathom doesn’t give me country data, but I’m including my Netlify data here which I’ll use to make a point later. You may have noticed something is a little fishy about this data.
 Before I jump to any conclusions, I wanted to check my data with their sources.
 Feedback from Fathom and Netlify
 I tweeted about this phenomenon in 2019…
 Server-side @netlify stats vs. Client-side @usefathom stats (over the last ~4 weeks)439k pageviews vs 12.5k pageviews44k uniques vs. 7.9k uniques pic.twitter.com/ARoW82BhAm— Dave Rupert (@davatron5000) July 11, 2019 
 After tweeting about it Netlify and Fathom both reached out to me. It’s a bit of a he-said/she-said situation about who’s data is most correct, but some high-level notes add value to the conversation.
 
 Fathom’s point of view was that I was on the outdated community product and the SaaS product is much better and more accurate now. I don’t doubt the paid product is better, but I do doubt it would close the 18× gap on the Netlify numbers.
 Netlify’s take was pretty matter-of-fact. “Yup. This reflects the log data.” They’re not wrong, raw server logs don’t lie. Their investigation into the matter turned up two anomalies I can’t solve:
 
 80% of the hits to my Responsive Deliverables post contain a utm_campaign&#x3D; query param, which tells me people love my footnote about SMACSS. Some bot (in Germany?) must be hammering that campaign URL.
 There’s a Google Cloud Uptime Monitor health check pointed at my site. This is helpful, except that I never set one up! Another rogue bot (in Germany?) is pinging my old content thousands of times a day.
 
 
 
 Fathom’s numbers seem more believable but Netlify’s data eliminates any ad blocking questions. My one small takeaway is that it’d be nice if I could filter out known bot traffic from Netlify’s reporting. That might remove the delta to an acceptable degree.
 Big conclusions about data-driven decision making
 The data tells me I get somewhere between 12k and 26k visitors to my site who open anywhere between 18k and 333k pages. If I stare at the larger numbers, that is impressive but not actionable. Comparing 2019 to 2022, Netlify is showing a 25% drop in pageviews and a 41% drop in uniques, while Fathom is showing almost the inverse.
 This isn’t a judgement on Netlify or Fathom, they are both incredible services that offer incredible analytics products. But it leaves me in a position of wondering who should I believe?
 My trust level in analytics products is low
 My trust in analytics data is at an all-time low. Browser-level privacy improvements, ad blockers, and bot traffic have obliterated the data integrity. Unless I’m a certified statistician, I’m unable to accurately assess traffic to my breakfast table.
 I used to believe people were honest, but seeing all this bot traffic on my little site I could see how someone might point bots at their own site to inflate their traffic stats, gain perceived popularity, which they then pass on to advertisers. My ad vendor’s FAQ page has a note that says “Bots account for as much as 80% of website traffic”, yikes.
 Data-driven decisions can be a blindspot
 If I believed Netlify’s data as gospel, I would be a blogging sensation! I should quit my job and do blogging fulltime. Four million eyeballs per year. Following Netlify’s data to its logical extreme, I should be structuring my new blogging business like so:
 
 Writing a lot of content about responsive design and 4K monitors.
 Investing  a lot in German and Finnish localized content.
 I’d should be in full panic mode about my 41% drop in uniques.
 
 I love Germany (and I hear Finland is great), but I suspect they’re not my #1 and #2 markets. And those vintage 2014 posts are okay, but should not be the core of my content strategy.
 If I, or some hypothetical manager, put too much stock into these metrics I could see it causing a firestorm of reprioritization based on bot traffic1. We’d be chasing the tail of a health check bot somewhere in Germany.
 The point is, you can enter a McNamara Fallacy if you base your business solely on quantitative data.
 You need more than just metrics
 Probably old advice, but you need a mixture of quantitative AND qualitative data to make good decisions. In Just Enough Research, Erika Hall drives home a point that I think applies here:
 
 You want to know what is happening (qualitative), how much it’s happening (quantitative) and why it’s happening (qualitative).
 
 If your goal is to grow your product, you need a mixture of research (academic and applied), user tests, A/B tests, analytics, performance audits2, accessibility audits3, and a plethora of other considerations. Even if you have all those processes in place, I still think you need a human — a sage — who can read the tea leaves and interpret the data in a way that the business understands.
 If your goal is not to grow your site… maybe ignorance is bliss.
 
 
 
 Maybe pointing a bot army at a competitor’s site to skew metrics is some next level Art of War shit. ↩
 
 
 Every 100ms of latency costs Amazon 1% of sales (source) /via WPO Stats ↩
 
 
 1 in 4 adults in the United States have a disability (source) /via A11y Project ↩
 
 
 </content>
     </entry>
     <entry>
       <title>MDN Plus now available in more regions</title>
         <link href="https://hacks.mozilla.org/2022/04/mdn-plus-now-available-in-more-markets/"/>
       <updated>2022-04-28T10:05:35.000Z</updated>
       <content type="text">At the end of March this year, we announced MDN Plus, a new premium service on MDN that allows users to customize their experience on the website.
 We are very glad to announce today that it is now possible for MDN users around the globe to create an MDN Plus free account, no matter where they are.
 Click here to create an MDN Plus free account*.
 The premium version of the service is currently available as follows: in the United States, Canada (since March 24th, 2022), Austria, Belgium, Finland, France, United Kingdom, Germany, Ireland, Italy, Malaysia, the Netherlands, New Zealand, Puerto Rico, Sweden, Singapore, Switzerland, Spain (since April 28th, 2022), Estonia, Greece, Latvia, Lithuania, Portugal, Slovakia and Slovenia (since June 15th, 2022).
 We continue to work towards expanding this list even further.
 Click here to create an MDN Plus premium account**.
 * Now available to everyone
 ** You will need to subscribe from one of the regions mentioned above to be able to have an MDN Plus premium account at this time
 The post MDN Plus now available in more regions appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>Decarbonization as a Service</title>
         <link href="https://logicmag.io/clouds/decarbonization-as-a-service"/>
       <updated>2022-04-27T13:02:55.000Z</updated>
       <content type="text">Net zero has gone viral. Everyone is announcing their net-zero greenhouse gas emissions targets, from Saudi Arabia to Australia. By the final day of COP26—the annual United Nations Climate Change Conference held in Glasgow in 2021—more than 130 countries had made net-zero pledges of one kind or another, representing about 70 percent of the world’s UN-recognized nations. The private sector has also rushed in: more than 30 percent of the world’s 2,000 biggest public companies have committed to net-zero targets, including Amazon, Walmart, and ExxonMobil. 
 A global ambition has coalesced. It’s an achievement of sorts. Limiting warming to 1.5°C requires reaching net zero by midcentury, according to the Intergovernmental Panel on Climate Change (IPCC). This doesn’t mean zero emissions; rather, it means that some remaining amount of positive emissions would be canceled out by “negative emissions”—that is, by carbon removals. Negative emissions can be generated by using ecosystems like forests, farms, and oceans to store more carbon, or by using industrial technologies like carbon capture and storage (CCS) to pull carbon from the atmosphere and store it in rock formations. When these negative emissions balance out the positive emissions—when the amount of carbon being taken out of the atmosphere equals the amount of carbon being put into the atmosphere—then net zero is reached.
 Under such an arrangement, countries are free to continue burning fossil fuels, so long as they offset their emissions. For this reason, many climate advocates have been critical of net zero as a goal. One common critique is that net-zero pledges won’t stop the continued extraction and combustion of fossil fuels. Further, they’re aimed at a future that’s far enough away that present-day leaders won’t be accountable for what happens. “Net zero by 2050. Blah, blah, blah,” Greta Thunberg told a summit of young organizers just prior to COP26. 
 It’s true that net zero is woefully insufficient. We also need to be talking about immediately phasing out fossil fuel production. But net zero is still a worthwhile transitional goal, because we don’t yet have all the technologies we need at scale for true climate repair. Climate change is a problem of stocks, not flows: we need not only to stop emitting carbon by switching to renewables, but to reduce the existing levels of carbon in our atmosphere. Net zero alone won’t get us there. Still, it gives us a way to buy time while we develop and deploy the technologies required for full decarbonization.
 Yet net zero is harder than it looks. The difficulty isn’t just political—compelling countries and companies to make promises and abide by them—but epistemological. At the center of net zero is a knowledge problem: How do we know when we’ve gotten to net zero? Answering this question is surprisingly hard. 
 There are two main challenges. First, there is immense technical complexity involved in accurately measuring both positive and negative emissions. Take positive emissions: how many are embodied in the manufacture of a car? You could measure how much carbon is produced by a single car factory, but a car has around 30,000 parts. Those parts might be sourced from suppliers around the world, each with their own carbon footprint. Further, the parts use different raw materials, and the extraction and transport of those materials carry emissions of their own. And that’s only one factory; there are some 300,000 car manufacturing facilities in the US alone.
 Measuring negative emissions presents headaches of its own. For example, there are emergent methods for measuring how much carbon is stored in soil that hinge on spectroscopy, satellite-based sensors, and machine learning. But knowing what’s actually going on in the soil is complex, as conditions can vary even across a single farm. Knowing what’s going on with carbon in the deep ocean, as required for some carbon removal approaches, is even more challenging. Industrial technologies like CCS that store carbon geologically may seem easier to measure than biological systems, but, even then, understanding what happens to carbon that is stored thousands of feet underground isn’t easy.
 Even if these difficulties are overcome, however, there is still the problem of carbon deception. This is the second main challenge involved in knowing when we’ve gotten to net zero: the presence of dishonest actors in the system. Think about Volkswagen rigging vehicles to cheat on emissions tests, or gas station chain owner Lev Dermen, who was found guilty of stealing $1 billion from US taxpayers through claiming tax credits for renewable biodiesel that didn’t exist. Behind the office-park facade of net zero lies a seedy hotbed of white-collar crime.
 The knowability of net zero can’t be taken for granted, then. All those policymakers and executives are making net-zero pledges without a clue for how to see them through. How can these pledges possibly be fulfilled? How can the knowledge problem of net zero be solved?
 The answer, apparently, is software. Companies large and small have begun to build “platforms” that promise to help organizations meet their net-zero promises. These platforms claim to be able to deliver “decarbonization-as-a-service” through the use of digital technologies that effectively track carbon. They are racing ahead of law and policy and performing de facto governance, creating new proprietary infrastructures for knowing and managing our planet.
 If this new regime fails to reliably measure and monitor carbon—and it very well may—that will be bad for the climate. If it succeeds, that may also be bad for the climate, not to mention ecosystems more broadly as well as human communities. But right now, before the new computational systems are locked in, we may have a chance to intervene and shape them for the better.
 Initial Tree Offering
 In 2012, Derrick Emsley cofounded Tentree, a Vancouver-based clothing company that plants ten trees for every product sold. That sounds simple enough, but managing the planting projects turned out to be a challenge. “The hardest part was monitoring and verifying the work and claims we were making,” Emsley told The Hill’s Saul Elbein. Confirming that the trees they paid to plant were actually planted required costly in-person trips and lots of managerial overhead—it meant “traveling there, auditing them, making sure those trees were in the ground having an impact,” according to Emsley.
 So Tentree started developing software to help streamline the process. That software became the basis of Veritree, a publicly available “planting management platform” unveiled in the fall of 2021. When a company that wants to offset their emissions signs up for Veritree, they get access to a user portal that lets them place orders for new tree plantings and displays metrics on their current tree holdings. These metrics are collected by the planters in the developing countries, who use custom-made “collect devices”—essentially modified smartphones designed to work in internet-limited environments—to take geotagged photographs of the trees. The trees get a unique digital token, and become digital inventory, hosted on the Cardano blockchain. Using a blockchain is supposed to reduce fraud by ensuring that trees are only counted once, so that the same forest can’t be claimed by multiple entities—a well-known problem in the world of carbon credits. 
 Veritree is in its early days, but it hopes to become “an operating system for the restoration economy,” in Emsley’s words. An “Initial Tree Offering” was used to raise money for a “First Edition Forest,” which features trees in Madagascar, Indonesia, Nepal, Kenya, Senegal, and Haiti. The appeal for corporations hoping to make good on their net-zero pledges is obvious: they can invest in a reliable carbon offset program and obtain a real-time picture of precisely how much carbon is being offset, and share that information with the public. 
 Veritree is far from the only platform hoping to dominate the net-zero space. A wide range of digital services is being developed by companies large and small. Established “Big Four” accounting firms like KPMG and Deloitte, as well as tech giants like Salesforce, are creating tools for Environmental, Social, and Governance (ESG) accounting that help measure the carbon footprint of firms, among other things. Some companies combine carbon measurement with a portal for purchasing offsets, such as the Atlanta-based startup Cloverly. Still others try to track negative emissions, like Veritree. Agreena, for example, is a Dutch startup that monitors changes in farmers’ fields after they switch to regenerative agriculture, and issues them e-certificates. Companies can sponsor these carbon reductions, and use Agreena’s platform to track them.
 Atoms into Commodities
 The creators of these new platforms believe that they can solve the knowledge problem of net zero with software. Given the proliferation of net-zero pledges, this is a profitable problem to solve. But the platforms all approach this problem in a particular way: they turn carbon into a tradable commodity. This points to a broader point: net zero, in its current configuration, is a market-based project. It requires creating a global market where offsets can be freely bought and sold. This market already exists, but it has much room to grow; Mark Carney, the former governor of the Bank of England, says it could be worth $100 billion. 
 Creating that value, though, hinges on turning lively carbon atoms into a smooth commodity. And that task, in turn, hinges on code. The new platforms aim to improve and expand carbon markets by packaging carbon into a reliable product that can be easily bought and sold online. Their value proposition isn’t just about using digital tools to do superior carbon monitoring and accounting, but about disrupting traditional carbon markets by disintermediating them.
 In traditional carbon markets, supply and demand is linked by retail traders who purchase carbon credits from suppliers and bundle them into portfolios, to be sold on to brokers or end buyers. This is an inefficient system, with too many middlemen; it is also rife with fraud. The new platforms want to cut the knot by connecting buyers and sellers—that is, the producers of positive emissions with the producers of negative emissions. Think of Veritree: corporations can purchase offsets by directly sponsoring planting projects in the developing world, without having to navigate a tangle of traders and brokers.
 The platforms don’t just want to revolutionize existing carbon markets, however. They also want to create new ones. Climate Impact X is a Singapore-based carbon exchange that plans to build a “forest carbon marketplace” that uses remote sensing, artificial intelligence, and blockchain to “open participation to forests that were previously left out of the climate solution.” The idea is that a data-driven approach will lower the barriers of entry for the producers of negative emissions and enable more of them to participate. And, if given the chance, they probably will: there is a lot of money to be made. After languishing for many years, carbon offsets are trading at record highs.
 Risk Factors
 The rise of a decarbonization-as-a-service sector may make it easier to commodify and exchange carbon. But what if it doesn’t actually help address climate change? There is always the risk that the platforms could be algorithmically flawed—that their software isn’t accurately quantifying emissions, whether positive or negative. But there are also deeper risks, ones that can’t be mitigated by tweaks to the code because they’re inherent in a market-based system.
 First, ask yourself what any of these platforms want. Continued emissions; more exchange. The platform becomes a vested actor. The company is not oriented toward the phaseout of fossil fuels. Rather, it wants to maintain its own life, its own revenue stream. If you were designing a net-zero platform with the goal of working towards full decarbonization, it would be a time-limited project. But that would be incompatible with market imperatives: nobody wants to start a business with an expiration date. This is why the companies running these platforms will have an incentive to encourage a version of net zero with continued residual emissions—not a version of net zero oriented toward a future that would make them obsolete.
 More broadly, a market-based system cultivates a tendency toward cheapness that’s hard to square with the costs of carbon removals. Why buy a carbon offset for $50 or $100 a ton when you can buy one for $10 a ton? How is a sustainability manager at a company going to justify that to higher-ups or shareholders? Yet that $10 offset is much less likely to be trustworthy, since proper monitoring and verification costs money. Moreover, the most permanent kind of carbon removals, performed through industrial technologies like CCS (Carbon Capture and Storage), still cost hundreds of dollars per ton. Yes, some platforms will attempt to introduce boutique or premium removals. But no matter how sophisticated the platforms for carbon management become, market logic will inevitably push companies away from these more expensive and more robust offsetting techniques, and toward cheaper and less effective ones.
 Another issue with a market-based system is that offsets can be purchased by any market actor. But talk to any climate scientist and they’ll tell you that to reach net zero, removals should only be used to compensate for emissions from sectors that are hard to transition to renewables, like aviation or agriculture. In an open market, offsets can be bought up by anyone, including entities that have no future in a post-carbon economy such as fossil fuel companies. That’s bad, because we need to allocate removal capacity to sectors that we want to preserve and whose emissions are truly hard to abate. Platforms can make carbon markets more efficient, but they can’t perform the sorting function needed to prioritize the offsetting of particular activities and industries.
 A final problem with a market-based system is that it turns the platforms themselves into black boxes. As profit-seeking enterprises, they must protect their algorithms and data, as otherwise their competitors might gain an advantage. But this opacity obstructs the learning and experimentation process that’s required to combat climate change. We need a broad-based scientific effort to help figure out what works in terms of curbing emissions and removing carbon from the atmosphere. That can’t happen if all the data is locked away on proprietary platforms, hidden from view.
 We wouldn’t just be entrusting the platforms with data, however. We would also be entrusting them to play a major role in the governance of ecosystems, with significant social consequences. Creating negative emissions is never a neutral, purely technical process—it always involves political choices. For example, communal land title is sometimes seen as a risk to the permanence of carbon credits.  This led to the exclusion of communally held land from one carbon credits project in Cambodia, as research by Sarah Milne and Sango Mahanty has shown. As these calculative practices scale up in digital realms, the repercussions will be turbocharged. 
 Getting to net zero matters, but how we get there is just as important. Put another way, different attempts to quantify carbon produce different kinds of social relations. Ceding unilateral control over these choices to corporate platforms—letting them decide which kinds of net-zero social relations to make—would be a significant mistake. 
 Public and Planetary
 Software is essential for facing the challenge of climate change. We need to build a computational infrastructure for tracking carbon. The impact of humans on ecosystems has been so deep and so complex that digital monitoring and modeling are essential for ecological repair. As Benjamin Bratton writes, “planetary-scale computation” could “contribute to the comprehension, composition, and enforcement of a shared future that is more rich, diverse, and viable.” But right now, we are headed towards a world of proprietary platforms, repeating the same mistakes that we made with the development of the internet. 
 Shouldn’t knowledge about the Earth’s carbon flows belong to communities and the commons? Shouldn’t the benefits of such data go to the people who are working to remove and sequester carbon—tending the trees, soil, seagrass meadows, and injection wells—and to society more broadly? Shouldn’t the political choices about how to quantify carbon—and, by extension, about what kind of social relations to create in pursuit of net zero—be made democratically, rather than by executives and shareholders? 
 We need platforms that monitor our world for the sake of collective management of a shared commons—not platforms that measure our world for the sake of profiting from the data created from it, extracting value from the digital layer the same way that value is extracted from the mines, forests, and soils of our physical world. Plundering the earth and then repeating the plunder in the metaverse is dystopia squared.
 We could imagine platforms that are so much better: better because they actually get us to net zero and address other sustainability and social challenges. The public sector must be central here. It alone can bear the costs of accurate monitoring of both positive and negative emissions—costs that disincentivize companies from buying high-quality carbon offsets, no matter how many advanced digital tools for carbon management emerge. 
 Public platforms can also help dislodge another obstacle to climate action: community resistance. We will have to build an incredible amount of new transmission lines, solar panels, wind farms, geothermal facilities, factories, mines, and more to pull off a wholesale transformation of our energy system. But when people don’t understand the contours of the problem, or don’t benefit from the solution, they will oppose such initiatives. We’ve seen already that technocratic climate projects without public input are likely to face hostility from local communities. If such communities aren’t brought into the design process, the energy transition is imperiled. 
 Better platforms are not a panacea for this challenge, but they could help. Imagine a platform for tracking carbon that isn’t just operating quietly in the background, but is highly visible, easy-to-use, open source, and closely integrated with community planning projects. Such platforms could enable people to collaborate on roadmaps for decarbonizing their town or region. They could integrate data around carbon flows and other ecosystem attributes with real-time visualization tools, in order to anticipate different scenarios and deliberate around possible tradeoffs. The platforms could facilitate a more participatory ecology, and thus generate community buy-in for decarbonization measures.
 More broadly, public planetary data infrastructures would enable the inhabitants of Earth to know their world. Knowledge about one’s environment should be a basic human right. Imagine a world where you could know what’s in your air, what’s in your drinking water, what species are in the forest near you and how much carbon it’s storing, or what the health of your local lake is like. You could use this information not just to gauge the environmental risks you face, but also to discover which companies are degrading the ecological systems you live in and hold them accountable.
 This isn’t to suggest that every piece of data needs to be public. As the Indigenous scholar Stephanie Carroll Rainie and her colleagues have explained, open data can be in tension with the rights of Indigenous peoples to govern their own data. In particular, it may cause tensions for communities that continue to experience data extraction under settler-colonial frameworks, and who are working to establish their own frameworks. When it comes to knowing net zero, the important thing is that the computational systems for monitoring emissions and carbon flows are publicly or community-owned, accessible to community members, and democratically governed. What this actually looks like on local, regional, and planetary scales needs to be established through participatory processes that will require time and trust.
 Across the Binaries
 How do we get there? The answer is not fancy. It involves assembling a coalition for public ecological data infrastructures, drawn from several existing communities. There’s the long-standing open source software movement that could participate. There’s also the open data movement in science, and the movement for Indigenous data sovereignty. Environmentalist NGOs are tracking data about emissions, from larger projects like the Environmental Defense Fund’s methane-tracking satellites to databases like the Carbon Disclosure Project. There are a number of people across these fields who might join a movement for open and public planetary data. So why doesn’t this movement exist in a more mainstream way, or even as a common demand at climate protests? 
 One challenge is the fact that this is an anticipatory mobilization. We’re not mobilizing against something that’s already happened, we’re acting defensively based upon trends that are just beginning to emerge. Proprietary carbon management platforms are still in their infancy; their full impact might not be felt for many years. 
 Then there’s the cultural divide between the worlds of climate activism and tech activism. Organizations concerned with climate change may be inclined to see the digital as outside their core mission; after all, many people became involved in this space because they loved being outside, not because they wanted to think about algorithms. Moreover, such people may be actively opposed to technological interventions, or understandably dismiss net zero as a narrow, technocratic goal. Meanwhile, people with expertise in algorithmic justice issues might not be tracking developments in the environmental sphere. Their attention is likely devoted to a multitude of other concerns.
 But each of these communities has critical things to contribute. Climate activists can help us avoid the trap of “platform determinism”—that is, the risk of fetishizing the platforms as mythically powerful actors, instead of centering the choices made by the humans who design the platforms. Such activists also bring expertise in movement building: they know how to put pressure on investors and companies, and how to form relationships with policymakers. On the other hand, those who are working on the politics of data, whether from socialist, decolonial, or Indigenous perspectives, have valuable experience in identifying the problems with data appropriation, access, and use, as well as in creating just data frameworks. And technologists can put their knowledge of quantification and design to work building platforms that are genuinely usable and inspiring. 
 The knowledge problem of net zero is difficult but not insurmountable. Solving it the right way will require a group effort. We must build data infrastructures that embody multiple ways of knowing and understanding our world, and that help us advance both ecological and social ends, before corporations conquer this space for themselves.</content>
     </entry>
     <entry>
       <title>Inspiration in the Tall Grass</title>
         <link href="https://daverupert.com/2022/04/inspiration-in-the-tall-grass/"/>
       <updated>2022-04-26T04:23:00.000Z</updated>
       <content type="text">I got some landscaping done this month and as a reward the YouTube algorithm brought me a new genre of video called “Tall Grass”. These are the lawns with shoulder high grass, filled with mice and snakes, endlessly consuming buildings, fences, and concrete. It’s complete and utter ASMR for me.
 
 My favorite among them is Al Bladez. Al and his best friend AP take one day a week and offer to mow someone-in-need’s yard for free. To be clear, there’s an economy here, the homeowner gets their yard mowed and Al gets a YouTube video out of the deal which garners a million views. But the homeowners — and more often than not, the neighbors — are incredibly thankful to see the transformations.
 The circumstances differ from yard to yard. Sometimes the homeowner is elderly, ailing, deceased, or in the hospital; unable or unfit to mow their lawn. In those situations Al’s work is a simple act of charity.
 Sometimes, the homeowner is perfectly able but hasn’t mowed their lawn. What I love about Al is that he doesn’t show an ounce of judgement. Al doesn’t care if you’ve been in jail, or if you’re too poor, your mower broke, you’re unemployed, you’re struggling with addiction, or you’re just plain lazy. He’s there to cut grass — tall grass — and bring a bit of pride back to his community and hopefully help people dodge citations from the city.
 To top it off, Al’s personal slogan is “Impact Over Views” and whew, that’s a lot to think about. I love that outlook and wish I could embody it more.
 Whatever religion Al is peddling, I want to be a part of it.</content>
     </entry>
     <entry>
       <title>An unplanned open redesign</title>
         <link href="https://daverupert.com/2022/04/an-unplanned-open-redesign/"/>
       <updated>2022-04-25T22:35:00.000Z</updated>
       <content type="text">Howdy! If you frequent this website often, you’ll notice some changes. This weekend I started on my planned redesign, but as I started nudging my About section, I found myself in a fight with my typography. After some hemming and hawing, I made the drastic decision to nuke my stylesheets and start over.
 ~81 file changes later, I made another decision to yeet my changes live and do an open redesign. Not my original plan, but it’ll be good blog fodder. One thing I want to avoid is killing blogging momentum by getting sunk in a redesign branch.
 Without further ado, as with any redesign, let’s see what we’re dealing with first…
 Website feature inventory
 Here’s a list of all the features on my little website that I maintain:
 
 330+ blog posts
 Art-directed posts with custom CSS, JavaScript, and SVGs
 Open Graph images (handmade)
 sitemap.xml
 RSS feed
 ███ ████
 Code samples
 Codepen, YouTube, and Vimeo embeds
 About Timeline (YAML-powered)
 Bookshelf (YAML-powered)
 Likes (RSS-powered)
 /uses page
 Service worker + offline page
 
 My checks are all manual right now, but not rocket science. I created a “secret” page to roll up all my art-directed posts and I’m happy to report just a few are super fucked up. One day I’ll add proper visual integration tests, but today is not that day.
 Which leads me to the next step, let’s define the goals and non-goals for this project so we can finish on time.
 Goals and non-goals
 The goals I have outlined for myself are pretty simple:
 
 Redo my typography setup - After years and years of subtle tweaks, I ruined it. Time to start over, more fluid this time.
 New homepage - This is the lowest priority, but nothing sings “redesign” like a new homepage.
 Remove Ads - Ads make me ~$20/mo. Originally a motivator to blog consistently, I’ve got a flow now. Ads were also an intentional third-party constraint I used to inform client work. Since I’m moving away from client work, that experiment can end.
 ✅ Remove Fathom Analytics - Fathom is great, but I’m paying $5/mo to self-host and I check it zero times a year. I have Netlify’s server-side analytics running on this as well. There’s a 26× disparity between those two analytics services, enough to make me think there’s no truth to metrics at all. Jim Nielsen covers this phenomenon in his post Comparing Data in Google and Netlify Analytics.
 ✅ Expose tags - I want to do a better job at tagging and categorization of posts for my own sake and I think elevating that will help.
 ✅ Annual stats -  Another feature for me, I like to keep track of my progress over time.
 ✅ Syntax Highlighting - I had this once with client-side Prism.js, then I took it off, now I want it back. I’m using server-side Rouge + Commonmark in Jekyll.
 ✅ Support page - How can you give me $10? If I’m removing ads, might as well sell my own products and services.
 
 My list of non-goals would include:
 
 Do not replatform the blog to Eleventy (or Astro) or whatever. I’d love to not be on Jekyll, but that’s a separate project. Tooling is a lot better on a Node stack.
 Do not go down a testing rabbit hole. Spot checks or automated webperf/accessibility tests would be beneficial, but it’d be easier on a Node stack.
 Do not automate opengraph images - Auto-generated opengraph images would be rad, but again, easier on a Node stack.
 
 With those parameters defined, let’s do a status check…
 Status check!
 What you’re looking at now is the lightly styled bones of my site. No Sass, no JavaScript, nothing but Jekyll static site compilation. I’m pleased with how the website is shaping up with basic browser defaults. I’ve got ~2.37kb of global CSS (926b gzipped) and I don’t see the need for much beyond that.
 RIP Newshammer, long live Newshammer: Astute readers will notice Newshammer is not on the site anymore. That’s not a permanent decision, but I am leaving opportunity/room to do something different.
 Typography: I’m embracing browser defaults for type right now, with two or three exceptions. I’m soft-committing to system-ui for now because I had some recent trouble rendering headings in other languages. The article H1 is the only heading I see that feels a little too small, but I’d be happy if that’s all the typesetting I need to do.
 Spacing: I have seven margin declarations and most of those are resets. Liberal usage of Grid and Flexbox using gap to manage consistent spacing. Super happy about this. Even the posts are one giant grid. I may abstract my grids into some layout utility classes…
 .layout-grid { --gap: 1.5rem; display: grid; gap: var(--gap); }
 .layout-grid &gt; * { min-width: 0; } /* https://daverupert.com/2017/09/breaking-the-grid/ */
 .layout-grid[data-vibe&#x3D;&quot;compact&quot;]     { --gap: 0.75rem }
 .layout-grid[data-vibe&#x3D;&quot;comfortable&quot;] { --gap: 3rem }
 
 This would probably remove half my existing CSS.
 Charts n’ graphs: I followed Josh Collinsworth’s post on CSS Grid bar charts for my Archive page. Happy with this. It needs some work to be fully accessible (context-less labels) but it’s presentational at its core.
 The flagpole: One stylized component I have right now is the “flagpole” look for my Post Archives and About Timeline. I’m not attached to the flagpole treatment, but I do think visually grouping content by years is super helpful.
 Beyond that, I’ve already been able to land some accessibility fixes around my document heading structure which I’m pretty glad about. I noticed my HTML is also “over-classed” with a lot of half-baked .hentry microformats that I brought over from WordPress ten years ago, I’m looking forward to mowing those down.
 That’s all I have for now. Tune in next week for more changes and CSS riffin’!</content>
     </entry>
     <entry>
       <title>Beyond Aggregation: Amazon as a Service</title>
         <link href="https://stratechery.com/2022/beyond-aggregation-amazon-as-a-service/"/>
       <updated>2022-04-25T14:29:00.000Z</updated>
       <content type="text">Five months and $134 billion in market cap ago (before the stock slipped by 68%), Bloomberg Businessweek purported to explain How Shopify Outfoxed Amazon to Become the Everywhere Store. One of the key parts of the story was how Shopify pulled one over on Amazon seven years ago:
 
   An even more critical event came a few months after the IPO. Amazon also operated a service that let independent merchants run their websites, called Webstore. Bang &amp; Olufsen, Fruit of the Loom, and Lacoste were among the 80,000 or so companies that used it to run their online shops. If he wanted to, Bezos surely had the resources and engineering prowess to crush Shopify and steal its momentum.
   But Amazon execs from that time admit that the Webstore service wasn’t very good, and its sales were dwarfed by all the rich opportunities the company was seeing in its global marketplace, where customers shop on Amazon.com, not on merchant websites…In late 2015, in one of Bezos’ periodic purges of underachieving businesses, he agreed to close Webstore. Then, in a rare strategic mistake that’s likely to go down in the annals of corporate blunders, Amazon sent its customers to Shopify and proclaimed publicly that the Canadian company was its preferred partner for the Webstore diaspora. In exchange, Shopify agreed to offer Amazon Pay to its merchants and let them easily list their products on Amazon’s marketplace. Shopify also paid Amazon $1 million—a financial arrangement that’s never been previously reported.
   Bezos and his colleagues believed that supporting small retailers and their online shops was never going to be a large, profitable business. They were wrong—small online retailers generated about $153 billion in sales in 2020, according to AMI Partners. “Shopify made us look like fools,” says the former Amazon executive.
 
 If only we could all make such excellent mistakes; Amazon’s move looks like a strategic masterstroke.
 Shopify’s Revenue Streams
 Three major things have changed, will change, or should change about Shopify’s business in the years since the company made that deal with Amazon.
 What has changed is the composition of Shopify’s business. While the company started out with a SaaS model, the business has transformed into a commission-based one:
 
 “Subscription Solutions” are Shopify’s platform fees, including the cost to use the platform (or upgrade to the company’s Pro offering), commissions from the sales of themes and apps, and domain name registration. “Merchant Solutions”, meanwhile, are all of the fees that are generated from ongoing sales; the largest part of this are payment processing fees from Shopify Payments, but other fees include advertising revenue, referral fees, Shopify Shipping, etc.
 It’s the shipping part that is line for big changes: while Shopify first announced the Shopify Fulfillment Network back in 2019, it is only recently that the company has committed to actually building out important pieces of said network on its own, the better to compete with Amazon’s full-scale offering.
 As for what should change, I argued back in February that Shopify needed to build out an advertising network; this recommendation is more pertinent than ever, in large part because the second item on this list might be in big trouble.
 Buy With Prime
 From the Wall Street Journal:
 
   Amazon.com Inc. is extending some of the offerings of its popular Prime membership program to merchants off its platform with a new service that embeds the online retailing giant’s payment and fulfillment options onto third-party sites. Called Buy with Prime, the service will allow merchants to show the Prime logo and offer Amazon’s speedy delivery options on products listed on their own websites…
   The company said the Buy with Prime offer will be rolled out by invitation only through 2022 for those who already sell on Amazon and use the company’s fulfillment services. Later, Amazon plans to extend Buy with Prime to other merchants, including those that don’t sell on its platform. Participating merchants will use the Prime logo and display expected delivery dates on eligible products. Checkout will go through Amazon Pay and the company’s fulfillment network. Amazon will also manage free returns for eligible orders.
 
 This is a move that you could see coming for a long time; back in 2016 I wrote an article called The Amazon Tax that explained that the best way to understand Amazon as a whole was to understand Amazon Web Services (AWS):
 
   The “primitives” model modularized Amazon’s infrastructure, effectively transforming raw data center components into storage, computing, databases, etc. which could be used on an ad-hoc basis not only by Amazon’s internal teams but also outside developers:
   
   This AWS layer in the middle has several key characteristics:
 
 AWS has massive fixed costs but benefits tremendously from economies of scale
 The cost to build AWS was justified because the first and best customer is Amazon’s e-commerce business
 AWS’s focus on “primitives” meant it could be sold as-is to developers beyond Amazon, increasing the returns to scale and, by extension, deepening AWS’ moat
 
   This last point was a win-win: developers would have access to enterprise-level computing resources with zero up-front investment; Amazon, meanwhile, would get that much more scale for a set of products for which they would be the first and best customer.
 
 As I noted in that article, the AWS model was being increasingly applied to e-commerce as Amazon shifted from being a retailer to being a services provider:
 
   Prime is a super experience with superior prices and superior selection, and it too feeds into a scale play. The result is a business that looks like this:
   
   That is, of course, the same structure as AWS — and it shares similar characteristics:
 
 E-commerce distribution has massive fixed costs but benefits tremendously from economies of scale
 The cost to build-out Amazon’s fulfillment centers was justified because the first and best customer is Amazon’s e-commerce business
 That last bullet point may seem odd, but in fact 40% of Amazon’s sales (on a unit basis) are sold by 3rd-party merchants; most of these merchants leverage Fulfilled-by-Amazon, which means their goods are stored in Amazon’s fulfillment centers and covered by Prime. This increases the return to scale for Amazon’s fulfillment centers, increases the value of Prime, and deepens Amazon’s moat
 
 
 My prediction in that Article was that Amazon’s burgeoning logistics business would eventually follow the same path:
 
   It seems increasingly clear that Amazon intends to repeat the model when it comes to logistics…how might this play out? Well, start with the fact that Amazon itself would be this logistics network’s first-and-best customer, just as was the case with AWS. This justifies the massive expenditure necessary to build out a logistics network that competes with UPS, Fedex, et al, and most outlets are framing these moves as a way for Amazon to rein in shipping costs and improve reliability, especially around the holidays.
   However, I think it is a mistake to think that Amazon will stop there: just as they have with AWS and e-commerce distribution I expect the company to offer its logistics network to third parties, which will increase the returns to scale, and, by extension, deepen Amazon’s eventual moat
 
 Today Amazon’s logistics is massive and fully integrated from the fulfillment center to the doorstep, even though it only serves Amazon; the obvious next step is opening it up to non-Amazon retailers, and that is exactly what is happening.
 Beyond Aggregation
 At first glance, this might seem like a bit of a surprise; after all, Stratechery is well known for describing Aggregation Theory, which is predicated on controlling demand. In the case of Amazon that has meant controlling the website where customers order goods — Amazon.com — even if those goods were sold by 3rd-party merchants. Why would Amazon give that up?
 The reasoning is straightforward: while Amazon has had Aggregator characteristics, the company’s business model and differentiation has always been rooted in the real world, which, by extension, means it is not an Aggregator at all. I noted in 2017’s Defining Aggregators that Aggregators benefit from zero marginal costs, which only describes a certain set of digital businesses like Google and Facebook:
 
   Companies traditionally have had to incur (up to) three types of marginal costs when it comes to serving users/customers directly.
 
 The cost of goods sold (COGS), that is, the cost of producing an item or providing a service
 Distribution costs, that is the cost of getting an item to the customer (usually via retail) or facilitating the provision of a service (usually via real estate)
 Transaction costs, that is the cost of executing a transaction for a good or service, providing customer service, etc.
 
   Aggregators incur none of these costs:
 
 The goods “sold” by an Aggregator are digital and thus have zero marginal costs (they may, of course, have significant fixed costs)
 These digital goods are delivered via the Internet, which results in zero distribution costs
 Transactions are handled automatically through automatic account management, credit card payments, etc.
 
   This characteristic means that businesses like Apple hardware and Amazon’s traditional retail operations are not Aggregators; both bear significant costs in serving the marginal customer (and, in the case of Amazon in particular, have achieved such scale that the service’s relative cost of distribution is actually a moat).
 
 Amazon’s control of demand has been — and will continue to be — a tremendous advantage; Amazon not only has power over its suppliers, but it also gets all of the relevant data from consumers, which it can feed into a self-contained ad platform that is untouched by regulation from either governments or Apple.
 At the same time, limiting a business to customer touchpoints that you control means limiting your overall addressable market. This may not matter in markets where there are network effects (which means you appeal to everyone) and you are an Aggregator dealing with zero marginal costs (and thus can realistically cover every consumer); in the case of e-commerce, though, Amazon will never be the only option, particularly given The Anti-Amazon Alliance working hard to reach consumers.
 A core part of the Anti-Amazon Alliance are companies that have invested in brands that attract customers on their own; these companies don’t need to be on Amazon fighting to be the answer to generic search terms, but can rather drive customers to their own Shopify-powered websites both organically and via paid advertising (Facebook is a huge player in the Anti-Amazon Alliance). Still, every customer that visits these websites has an Amazon-driven expectation in terms of shipping; Shippo CEO Laura Behrens Wu told me in a Stratechery interview:
 
   Consumers have those expectations from Amazon that shipping should be free, it should be two days, and whatever those are expectations are, returns should be free. That is still carried over when I’m buying on this branded website. If the expectations are not met, consumers decide to buy somewhere else…Merchants are constantly trying to play catch up, whatever Amazon is doing they need to follow suit.
 
 Now Amazon has — or soon will have, in the case of Shopify-only merchants — a solution: the best way to get an Amazon-like shipping experience is to ship via Amazon. And, in contrast to the crappy Webstore product, you can keep using Shopify and its ecosystem for your website. Amazon may have given away business to Shopify in 2015, but that doesn’t much matter if said business ends up being a commoditized complement to Amazon’s true differentiation in logistics. That business, thanks to the sheer expense necessary to build it out, has a nearly impregnable moat that is not only attractive to all of the businesses competing to be consumer touchpoints — thus increasing Amazon’s addressable market — but is also one that sees its moat deepen the larger it becomes.
 Shopify’s Predicament and Amazon’s Opportunity
 The reason this announcement is so damaging to Shopify goes back to the transformation in the company’s revenue I charted above: Amazon’s shipping solution requires the customer to pay with Amazon; I love Shop Pay’s checkout process, but it’s not as if I don’t have an Amazon account with all of my relevant details already included, and I absolutely trust Amazon’s shipping more than I do whatever option some Shopify merchant offers me. If I had a choice I’m taking the Amazon option every time.
 Granted, I may not be representative; I’m a bit of a special case given where I live. What is worth noting, though, is that every transaction that Amazon processes is one not processed by Shopify, which again, is the company’s primary revenue driver. Moreover, the more volume that Amazon processes, the more difficult it will be for Shopify to get their own shipping solution to scale. This endangers the company’s current major initiative.
 That is why I think the company needs to think more deeply than ever about advertising: the implication of Amazon being willing to be a services provider and not necessarily an Aggregator is that Amazon is surrendering some number of customer touchpoints to competitors; to put it another way, Amazon is actually making competitive customer touchpoints better — touchpoints that Shopify controls. Shopify ought to leverage that control.
 That noted, there is a potential wrench in this plan: if Shopify doesn’t own payment processing, will they have sufficient data to build a competitive conversion-data-driven advertising product? Amazon — and Apple — would likely argue that that data is Amazon’s, but the merchant will obviously know what is going on (by the same token, will Amazon be able to tie this off-platform conversion data back to its own advertising product? It’s not clear what would stop them).
 Shopify has two additional saving graces:
 
 First, the fact that Amazon will be able to collect data is a big reason for many merchants not to use Amazon’s new offering. Shopify’s offering will always be differentiated in this regard.
 Second, as the announcement noted, this is going to take Amazon a year or two to fully roll out, and lots of stuff can change in the meantime.
 
 Moreover, while AWS has always been excellent at serving external customers — which it did before Amazon.com ever actually moved over — Amazon’s off-Amazon.com merchant offerings have never been particularly successful (including the aforementioned Webstores).
 With that in mind, I think it’s meaningful that “Buy With Prime” is the first major initiative of new CEO Andy Jassy’s regime; I don’t think it’s an accident that it is so clearly inspired by AWS. AWS’s strength is its focus on infrastructure at scale; successfully moving e-commerce beyond aggregation to the same type of service business model would put his stamp on the company in a meaningful way, and, contra that quote in Bloomberg Businessweek, mark Jassy as nobody’s fool.</content>
     </entry>
     <entry>
       <title>Productivity-sniped by PARA</title>
         <link href="https://daverupert.com/2022/04/productivity-sniped-by-para/"/>
       <updated>2022-04-22T15:33:00.000Z</updated>
       <content type="text">In February, I got nerd-sniped by a mention on Twitter about a productivity system called PARA. PARA is a way to organize your digital life in to separate buckets: Projects, Areas, Resources, and Archives.
 
 After watching some videos, I started applying PARA to my second brain in Notion. The timing to choose to redo my entire Notion setup was horrible, of course. I was busy with work, writing a workshop, moving ShopTalk to its own YouTube channel… why not pick up another big project and redo my entire organization system?
 The psychology checks out though. When large tasks loom you don’t feel in control over your schedule and doing something you can control (like organizing your organizer) helps you feel more in control. And it did help. Quantifying the “Projects” I’m committed to over various “Areas” gave me a more solid perspective of what I’m doing, why I’m doing them, and when they’re due… rather than a constant state of overwhelm. I’m pleased with the results so far…
 
 Inspired by Marie Poulin, I took some liberties and created a global “Task” database as well as made a dashboard for some even more meta-level “Objectives” or goals that act like personal OKRs1. There’s a table of Results I want to achieve and Metrics I’m using to gauge that progress. Overkill, I know, but it’s my overkill and it’s already helping me set some sights on personal and professional goals.
 The biggest paradigm shift from PARA is the idea that projects have an end date. This is hard for me; a person who tends to have a lot of open-ended, never-ending projects. I’ve logged those never-ending projects like open source, podcasting, or gaming as “Areas” for now, but that feels a bit mismatched because they do require recurring participation. That said, the process has been illuminating. I’m not making major life changes yet, but it has opened my eyes to the fact that I need more “close-ended projects” in my life.
 My Notion still needs some cleaning up but the broad strokes are working for now. I haven’t rolled this out to every digital corner, but I think this will work for at least until my next annual refactor. If you’re doing PARA in Notion, I’d love to hear about it or how you’ve modified it.
 
 
 
 Pro-tip: Quarterly OKRs are working much better than Annual OKRs. ↩
 
 
 </content>
     </entry>
     <entry>
       <title>Adopting users’ design feedback</title>
         <link href="https://hacks.mozilla.org/2022/04/adopting-users-design-feedback/"/>
       <updated>2022-04-21T15:04:05.000Z</updated>
       <content type="text">On March 1st, 2022, MDN Web Docs released a new design and a new brand identity. Overall, the community responded to the redesign enthusiastically and we received many positive messages and kudos. We also received valuable feedback on some of the things we didn’t get quite right, like the browser compatibility table changes as well as some accessibility and readability issues.
 For us, MDN Web Docs has always been synonymous with the term Ubuntu, “I am because we are.” Translated in this context, “MDN Web Docs is the amazing resource it is because of our community’s support, feedback, and contributions.”
  Since the initial launch of the redesign and of MDN Plus afterwards, we have been humbled and overwhelmed by the level of support we received from our community of readers. We do our best to listen to what you have to say and to act on suggestions so that together, we make MDN better. 
 Here is a summary of how we went about addressing the feedback we received.
 Eight days after the redesign launch, we started the MDN Web Docs Readability Project. Our first task was to triage all issues submitted by the community that related to readability and accessibility on MDN Web Docs. Next up, we identified common themes and collected them in this meta issue. Over time, this grew into 27 unique issues and several related discussions and comments. We collected feedback on GitHub and also from our communities on Twitter and Matrix.
 With the main pain points identified, we opened a discussion on GitHub, inviting our readers to follow along and provide feedback on the changes as they were rolled out to a staging instance of the website. Today, roughly six weeks later, we are pleased to announce that all these changes are in production. This was not the effort of any one person but is made up of the work and contributions of people across staff and community.
 Below are some of the highlights from this work.
 Dark mode
 
 We updated the color palette used in dark mode in particular.
 
 We reworked the initial color palette to use colors that are slightly more subtle in dark mode while ensuring that we still meet AA accessibility guidelines for color contrast.
 We reconsidered the darkness of the primary background color in dark mode and settled on a compromise that improved the experience for the majority of readers.
 We cleaned up the notecards that indicate notices such as warnings, experimental features, items not on the standards track, etc.
 
 Readability
 
 We got a clear sense from some of our community folks that readers found it more difficult to skim content and find sections of interest after the redesign. To address these issues, we made the following improvements:
 
 We implemented a clearly defined type-scale adjusted for mobile to optimize legibility and to effectively use space, especially on smaller screens.
 We made the distinction between the different heading levels clearer.
 We moved away from changing link colors across different areas of MDN Web Docs. We still retain some of the intent of this design decision, but this is now more subtle.
 The font size was bumped up across all pages, including the home page. We have also optimized letter and line spacing for a more effortless reading experience. This has also improved the reading experience for our Asian readers, for whom line heights were much too tight.
 Missing links are clearly distinguishable from other links and content.
 We improved the layout and readability of specifications pages across desktop and small screen devices.
 We addressed a bug in how the highlighting in the table of contents worked and moved to use an IntersectionObserver.
 We made styling for tables consistent across all pages.
 To ensure readers are always oriented regarding which page they are currently viewing, we have made the header (which includes the breadcrumbs) sticky on desktop and mobile.
 We fixed our accessibility skip navigation to now offer skip to content, skip to search, and skip to language selectors.
 We fixed a tricky issue that caused some elements to flicker when interacting with the page, especially in dark mode. Many thanks to Daniel Holbert for his assistance in diagnosing the problem.
 
 Browser compatibility tables
 
 Another area of the site for which we received feedback after the redesign launch was the browser compatibility tables. Almost its own project inside the larger readability effort, the work we invested here resulted, we believe, in a much-improved user experience. All of the changes listed below are now in production:
 
 We restored version numbers in the overview, which are now color-coded across desktop and mobile.
 The font size has been bumped up for easier reading and skimming.
 The line height of rows has been increased for readability.
 We reduced the table cells to one focusable button element.
 Browser icons have been restored in the overview header.
 We reordered support history chronologically to make the version range that the support notes refer to visually unambiguous.
 
 We also fixed the following bugs:
 
 Color-coded pre-release versions in the overview
 Showing consistent mouseover titles with release dates
 Added the missing footnote icon in the overview
 Showing correct support status for edge cases (e.g., omit prefix symbol if prefixed and unprefixed support)
 Streamlined mobile dark mode
 
 We believe this is a big step in the right direction but we are not done. We can, and will, continue to improve site-wide readability and functionality of page areas, such as the sidebars and general accessibility. As with the current improvements, we invite you to provide us with your feedback and always welcome your pull requests to address known issues.
 This was a collective effort, but we’d like to mention folks who went above and beyond. Schalk Neethling and Claas Augner from the MDN Team were responsible for most of the updates. From the community, we’d like to especially thank Onkar Ruikar, Daniel Jacobs, Dave King, and Queen Vinyl Da.i’gyu-Kazotetsu.
  
 The post Adopting users’ design feedback appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>My Weekly Engineering Report</title>
         <link href="https://daverupert.com/2022/04/my-weekly-engineering-report/"/>
       <updated>2022-04-21T13:57:00.000Z</updated>
       <content type="text">Providing visibility into your work is one of the most underrated developer soft skills. Coworkers are often too busy to keep track of what’s going on with engineering work, even if it’s their job to know what’s going on with engineering work. It’s generally not out of malice or lack of interest, but a lack of time and energy to chase down every issue or conversation thread. And to be fair, engineering is an opaque process where we trick the rocks inside a computer to act like a business.
 Standups are a great tool for sharing visibility. Unfortunately, not everyone loves standups. I’ve personally experienced some extremely toxic scrum master situations. Impersonal bots can have the same effect. A daily status report can feel like micromangement leading to a loss of autonomy. Also, that visibility into your work evaporates after attendees leave the call.
 For the past four or five years, I’ve been practicing an alternative to daily standups. Rather than waiting for people to confront you, I find it better to proactively share your progress in a brief bulleted list1 in a public channel (of appropriate level). That’s the secret: share without being asked. The template has evolved a bit over the years, but here’s what it looks like now:
 💪 **Progress**
 - Fixed something project#123
 - Added something project#234
 - Documented something
 
 😖 **Regressions**
 - Found bug on something project#345
 
 🧪 **Prototypes**
 - Experimented with new feature codepen.io/123
 
 💬 **To Discuss**
 - Blocked on something project#456
  
 📆 **This Week**
 - Work on new feature project#567
 - Out of office on Friday
 
 That’s a … uh … standup?
 Yeah. It’s basically a standup in newsletter form. It could be the years working in a Japanese office talking, but I don’t hate standups. They are great for mind-melding. But, as I’ve experienced, not everyone thrives in standups. But the sharing part is valuable, I try to keep doing the sharing part.
 I’ve done this in solo contexts, team contexts, and external client-facing contexts and no one hates it. No one says, “God, Dave, I wish you’d quit summarizing what’s going on.” No one asks for it, I just post it. People —especially those that pay you— enjoy seeing progress made and you (or your team) can avoid a lot of workplace mistrust issues by broadcasting a short progress report.
 Consistency helps. Any weekday works but I’ve found posting on Monday mornings provides a nice recap of last week’s progress and sets a baseline for expectations in the upcoming week. It’s also a good opportunity to publicly celebrate specific people or any team wins as well.
 Thus sums up my entire software development philosophy: Proactively communicate as succinctly as possible what you’re working on, what problems you ran into, and what you’re working on next.
 
 
 
 The bulleted list part is maybe the most important part. ↩
 
 
 </content>
     </entry>
     <entry>
       <title>Seeding the Cloud</title>
         <link href="https://logicmag.io/clouds/seeding-the-cloud"/>
       <updated>2022-04-20T15:00:58.000Z</updated>
       <content type="text">My first encounter with cloud computing was in the early 2010s. It took the form of a problem—a crisis, even—that needed solving. By crisis I don’t mean a serious issue such as climate change, but rather the kind of business problem that’s treated as an existential threat, prompting late nights and “all hands on deck” emails from management.
 In this case, it was a problem caused by a system that couldn’t keep up with demand. I worked for a company that I’ll call Libros, which ran a book-ordering website. The website was powered by servers located on-premises—that is, within our data center. A data center is a kind of specialized warehouse used to house computers known as servers. These computers are called servers because, unlike your personal computer, they are used to serve many people simultaneously. When you use a web application such as a banking website, there are servers in data centers behind the scenes. In a data center, you’ll also find the data storage, networking, and other equipment that together form computational systems. 
 Early in my career, I spent many hours in places like this. They’re kept very cold—think of the bone-deep chill of a February day—to prevent the equipment, which generates heat (lots of heat), from burning out. They’re also easy places to get injured. Although it was years ago, I can still vividly remember cutting my fingers (sometimes very badly) on the rails used to hold servers in place. No matter how many servers you successfully installed, there’d always be a few which were difficult to place, requiring careful work with hard metal and plastic in small spaces, a perfect recipe for cuts and pinches. It was an occupational hazard for many of us in those days, and we considered it a badge of honor.
 Our data center worked, but the servers it hosted could become slow or unresponsive if there was too much customer demand. (“Too much” is a relative term: the amount of work a server can do is based on factors like the amount of memory and processing capacity it has—just like your laptop but on a bigger scale.) That was the source of the crisis at Libros: every year, as the popularity of our website grew, the system became more strained. Customers complained about slow response times, lost orders, and other annoyances. Upset customers meant lost revenue, which made our executives angry, who in turn made our managers demand solutions. 
 The trouble was that solving the problem required installing new servers. And new servers required money the company was unwilling to spend—a few million, not a trivial investment. So there we were, my colleagues and I, facing pressure to fix a problem but not provided with the tools to do it. There was another challenge: even if we were granted the money for the new servers, additional demand might easily outrun the added capacity, placing us back at square one. That is, by the time my colleagues and I finished installing the new servers—shivering and slicing our hands open in the data center—so many more users might be trying to access our website that the new infrastructure wouldn’t make much difference. Think of building a highway: if it fills up with cars right away, the traffic is just as bad as before.
 What to do? The way we solved this problem was by turning to cloud computing, a new offering at the time—so new, in fact, that most techies and managers were unaware of it and, outside of early enthusiasts, those who were familiar with it harbored deep suspicions. (“No cloud” policies were not uncommon at the time.) We had no way of knowing that the technology we used to address our specific challenges, as helpful as it was from a technical point of view, would grow over the next several years to become extraordinarily large—indeed, to become the dominant form of computing, with immense consequences for the world’s political economy and for how the power of computation would be used and abused. What few people realize today is that, in those early days, the move to the cloud mostly wasn’t a top-down decision. It was typically brought into shops quietly, surreptitiously or against management objections, by the engineers themselves, simply because it made their jobs a little easier. We let the behemoth in through the backdoor. 
 Fee Fi Fo Fum
 As our team at Libros sweated, one of my colleagues, always on the lookout for new tech industry trends, tentatively suggested a relatively new service from Amazon called Elastic Beanstalk. In those days, before Amazon’s rise into ubiquity, our first question was: Amazon? The bookseller? Naturally, the second question was: What the hell is an elastic beanstalk? We knew what the words “elastic” and “beanstalk” meant separately, but together it seemed like a nonsensical word sandwich.
 “Hear me out,” my colleague said. “This is a web-based service we can use to host our customer portal—it can automatically expand and contract with growing or shrinking demand, solving our availability and response problems.” 
 Let’s pause here to consider the impact this news had on me and my coworkers. A major problem might just disappear, poof, because of a computational service we could use like water or electricity. In the same way we don’t run our own power or water treatment plants—rather, consuming these resources as a utility—Amazon was offering a utility model of computing. It had first begun doing so in 2006, through a new division called Amazon Web Services (AWS). Rather than running our website from our own data center, we could let AWS do it from theirs. And AWS would be able to increase capacity in response to rising customer demand much faster than we could. 
 Sure, there’d be real software development and networking effort required to get things started. But the promise of using something that was a commodity, rather than bespoke, was enticing. This was the appeal behind the new paradigm of cloud computing that was then emerging: we could rent access to data center resources like processor power, storage, and databases as needed.
 We listened to that colleague and directed our plans, like a conference of nerd generals, to putting Amazon’s Elastic Beanstalk to use. There was, however, a significant hurdle: management did not trust the cloud. 
 Behind the Boss’s Back
 Today, when the cloud is dominant, it’s difficult to understand just how hostile many managers were to the technology. There were a couple main issues. The first was that Amazon, the company that first offered cloud infrastructure services, was known more for book delivery than anything having to do with enterprise computing. The second, larger obstacle was the fear of a loss of control. Capitalist enterprises crave control and predictability—the ability to forecast spending, profits, and the behavior of workers. With on-premises computing, even an executive who didn’t know the difference between a SCSI disk drive and a microwave dinner (only a slight exaggeration) could walk into a company data center and survey the physical reality of all those tens of millions spent—the chill in the air, the blinking lights, the techies busy teching the tech. It was all there. Giving that up was a very hard sell for management in the beginning.
 So my colleagues and I had to be stealthy. We diagrammed how to build an elastic web service—that is, a service that can expand and contract with customer demand—using Amazon’s cloud. We worked with the networking team to devise a strategy for redirecting user requests from the stressed, on-premises system to the cloud service. And we created a fallback plan, just in case our gamble didn’t pay off. 
 The late-night tests when demand was usually low were promising; the redirect and fallback methods behaved to plan. But the only way to really know whether the new service would work was to put it online during crush time. We (metaphorically) pulled a lever and prayed to the gods that it would all come together.
 The customer rush began and the system responded as we’d hoped, automatically adding more capacity as demand grew and removing capacity as demand shrank. Management was pleased. For the first time in years, there were zero customer complaints and many smoothly executed sales. Eventually, after some time had passed, we let the cloud cat out of the bag. Surprise! We solved the problem by going behind everyone’s back and using new technology. At first, management was dubious. But it’s difficult to argue with success—and this particular success took pressure off middle managers, who for years had faced uncomfortable conversations with the C-suite because of crunch season complaints. The old saying that “it’s better to ask for forgiveness than permission” applied, because everyone came out looking good.
 Of course, cloud computing also meant the professional lives of people like me would be utterly transformed. It’s difficult to overstate the effect on those of us who had built our careers working with what in the industry is known as bare metal—those of us who had been able to touch, and engineer into complex systems, the machines that run the software we use. For some, the rise of the cloud was seen as a threat—an unwelcome change that might eliminate their job, a not unreasonable concern, particularly for workers whose jobs were focused on tasks such as server maintenance and systems and database administration. But for many others, myself included, it was seen as a relief. When I thought of the lost weekends, the ruined dates, the late nights caused by server failures, I didn’t miss the old way of working at all. Because of elasticity, we no longer had to intervene when systems were stressed. Because of hosted platform databases (instead of databases running on our own servers) we could focus on using the database, rather than constantly worrying about whether a system would work as needed. 
 What I didn’t realize at the time was that alongside this new capability came the accumulation of a new form of techno-political power. I didn’t realize that the increasing concentration of data center capacity by a few US-based tech firms (primarily Amazon, Microsoft, and Google) would help intensify algorithmic harms and inaugurate a new era of monopoly. While trying to solve an immediate problem, my colleagues and I inadvertently helped this behemoth grow, welcoming the services it provided, failing to foresee the future we were making possible.
 Points of Failure
 What kind of world has the cloud helped create? Cloud data centers, which have grown so large that they now operate at what’s called hyperscale, enable cloud providers like Amazon and their customers to run the types of computationally demanding systems that only a very few companies could have hoped to assemble in the past. These systems have greatly enlarged the command and control capabilities of firms large and small. For example, without the cloud, Amazon would not be able to monitor their drivers or automate aspects of management in their warehouses. Moreover, the recent growth in machine learning startups—some offering dubious and harmful services such as facial recognition to determine hiring decisions—can be directly linked to the growth of the cloud. Hyperscale-level cloud computing makes it possible for these VC-funded companies to build complex software without needing to invest in expensive data center equipment and real estate.
 Using the Marxist theory of base and superstructure, we can view the cloud’s data centers as an element of the mode of production—the combination of tools, machinery, technical expertise, and labor that propels capitalist activity. Computation is an industrial process. Data centers are created by bringing together computers (servers) and associated equipment that are manufactured using the extraction of minerals, the forging of metals, and shaping of hydrocarbons into plastics. The tech industry presents this hard physicality—which makes software useful—as a cloud, an amorphous, unlimited digital resource adding pleasure and efficiency to our lives. But there’s another way to view the cloud era: as the concentration of computational power and, therefore, real power with political consequences, into fewer and fewer hands.
 This concentration means that only a small handful of companies—and more critically, their executives and investors—possess the ability to determine how computing capacity should be deployed. This not only grants them a vast amount of leverage, it also creates a new mode of failure. Consider the AWS outage on December 7, 2021. This event impacted many companies, including global content providers Disney and Netflix, connected devices such as Ring cameras, and even Amazon’s internal processes that utilize their computational infrastructure (“eating their own dogfood,” as we say). Before the cloud era, each organization might have made large investments in maintaining their own data centers to host the computers, storage, and networking equipment required to offer online services. But now, the access to hyperscale capability and the possibility (if not always reality) of reducing the cost of computation have pushed most companies into the cloud, leading to consolidations that have introduced new kinds of fragility.
 At the dawn of the cloud era, none of this was apparent to me. Like my colleagues, I was focused on technical metrics such as speed and elasticity. What few of us realized is that we were facilitating the birth of a new form of power that rose, like Godzilla from the sea, to loom over everything. The cloud is rapidly becoming the computational infrastructure of global capitalism and the basis of immense digital empires that wield significant political power. But it was workers like me, hands on keyboards, toiling behind the scenes, who laid the foundation.</content>
     </entry>
     <entry>
       <title>Back to the Future of Twitter</title>
         <link href="https://stratechery.com/2022/back-to-the-future-of-twitter/"/>
       <updated>2022-04-18T15:21:03.000Z</updated>
       <content type="text">Elon Musk wrote in a letter to Twitter’s board:
 
   I invested in Twitter as I believe in its potential to be the platform for free speech around the globe, and I believe free speech is a societal imperative for a functioning democracy.
   However, since making my investment I now realize the company will neither thrive nor serve this societal imperative in its current form. Twitter needs to be transformed as a private company.
   As a result, I am offering to buy 100% of Twitter for $54.20 per share in cash, a 54% premium over the day before I began investing in Twitter and a 38% premium over the day before my investment was publicly announced. My offer is my best and final offer and if it is not accepted, I would need to reconsider my position as a shareholder.
   Twitter has extraordinary potential. I will unlock it.
 
 The vast majority of commentary about the Musk-Twitter saga has focused on the first three paragraphs: what does Musk mean by making Twitter more free speech oriented? Why doesn’t Musk believe he can work with the current board and management? Does Musk have the cash available to buy Twitter, and would the Twitter board accept his offer (no on the latter, but more on this below)?
 The most interesting question of all, though, is the last paragraph: what potential does Musk see, and could he unlock it? For my part, not only do I agree the potential is vast, but I do think Musk could unlock it — and that itself has implications for the preceding paragraphs.
 What is Twitter?
 It’s popular on Twitter to point to a funny tweet or exchange and marvel that “This website is free“, or alternatively, “This app is free“. That raises the question, though, what is Twitter: is it a website or an app, or something different?
 The answer, of course, is “All of the above”, but it’s worth being clear about the different pieces that make up Twitter; “Jin” made this illustration on Medium:
 
 Twitter is actually a host of microservices, including a user service (for listing a user’s timeline), a graph service (for tracking your network), a posting service (for posting new tweets), a profile service (for user profiles), a timeline service (for presenting your timeline), etc.; the architecture to tie all of these together and operate at scale all around the world is suitably complex.
 The key thing to note, though, is that only the green boxes in the diagram above are actually user-facing; a dramatically simplified version of Twitter, that condenses all of those internal services to a big blue “Twitter” box and focuses on the green interfaces might look something like this:
 
 Again, this is a dramatic oversimplification, but the important takeaway is that the user-facing parts of Twitter are distinct from — and, frankly, not very pertinent to — the core Twitter service.
 Twitter’s API Drama
 The general idea behind a services architecture is that various functionalities are exposed via application programming interfaces, more commonly known as APIs; a “client” will leverage these APIs to build an end user experience. There is no requirement that these clients be owned or managed by the centralized service, and for the first several years of Twitter’s existence, that is exactly how the service operated: Twitter the company ran the service and the Twitter.com website, while third-parties built clients that let you access Twitter first on the desktop and then on smartphones.
 Mobile was an absolute boon for Twitter: the public messaging service, modeled on SMS, was a natural fit for a smartphone screen, and the immediacy of Twitter updates was perfectly suited to a device that was always connected to the Internet. The explosion in mobile usage, though, led to a situation where Twitter didn’t actually control the user experience for a huge portion of its users. This actually led to a ton of innovation: Twitterrific, for example, the earliest third party client, came up with the Twitter bird, the term “tweet” for a Twitter message, and early paradigms around replies and conversations. It also led to problems, the solutions to which fundamentally changed Twitter’s potential as a business.
 The first problem that came from Twitter the service relying on third party clients is that the company, which descended into politics and backstabbing from the board level on down almost immediately, was drifting along without a business model; the obvious candidate was advertising, but the easiest way to implement advertising was to control the user interface (and thus insert ads — ads, including promoted tweets, are another distinct service from Twitter itself). Thus Twitter bought Tweetie, widely regarded as the best Twitter mobile client (I was a user), in April 2010, and rebranded and relaunched it as the official Twitter for iPhone app a month later.
 The second problem is that starting in 2010, a Silicon Valley entrepreneur named Bill Gross (who invented search advertising) started trying to build his own Twitter monetization product called TweetUp; when Twitter acquired Tweetie and made clear they were going to monetize it via advertising, Gross started buying up multiple other third party Twitter clients with the idea of creating a competing network of clients that would monetize independently. Twitter responded in the short term by kicking several of Gross’s clients off of the platform for dubious terms-of-service violations, and in the long term by killing the 3rd party API for everyone. Clients could keep the users they had but could only add 100,000 more users — ever.
 The net result of these two decisions was that Twitter, its architecture notwithstanding, would be a unified entity where Twitter the company controlled every aspect of the experience, and that that experience would be monetized via advertising.
 Twitter’s Reality
 Twitter has, over 19 different funding rounds (including pre-IPO, IPO, and post-IPO), raised $4.4 billion in funding; meanwhile the company has lost a cumulative $861 million in its lifetime as a public company (i.e. excluding pre-IPO losses). During that time the company has held 33 earnings calls; the company reported a profit in only 14 of them.
 Given this financial performance it is kind of amazing that the company was valued at $30 billion the day before Musk’s investment was revealed; such is the value of Twitter’s social graph and its cultural impact: despite there being no evidence that Twitter can even be sustainably profitable, much less return billions of dollars to shareholders, hope springs eternal that the company is on the verge of unlocking its potential. At the same time, these three factors — Twitter’s financials, its social graph, and its cultural impact — get at why Musk’s offer to take Twitter private is so intriguing.
 Start with the financials: Twitter’s business stinks. Yes, you can make an argument that this is due to mismanagement and poor execution — who enjoys seeing a stale promoted tweet about something that happened weeks ago?1 — but I have also made the argument that Twitter just isn’t well suited to direct response advertising in particular:
 
   Think about the contrast between Twitter and Instagram; both are unique amongst social networks in that they follow a broadcast model: tweets on Twitter and photos on Instagram are public by default, and anyone can follow anyone. The default medium, though, is fundamentally different: Twitter has photos and videos, but the heart of the service is text (and links). Instagram, on the other hand, is nothing but photos and video (and link in bio).
   The implications of this are vast. Sure, you may follow your friends on both, but on Twitter you will also follow news breakers, analysts, insightful anons, joke tellers, and shit posters. The goal is to mainline information, and Twitter’s speed and information density are unparalleled by anything in the world. On Instagram, though, you might follow brands and influencers, and your chief interaction with your friends is stories about their Turkey Day exploits. It’s about aspiration, not information, and the former makes a lot more sense for effective advertising.
   It’s more than just the medium though; it’s about the user’s mental state as well. Instagram is leisurely and an escape, something you do when you’re procrastinating; Twitter is intense and combative, and far more likely to be tied to something happening in the physical world, whether that be watching sports or politics or doing work:
   
   This matters for advertising, particularly advertising that depends on a direct response: when you are leaning back and relaxed why not click through to that Shopify site to buy that knick-knack you didn’t even know you needed, or try out that mobile game? When you are leaning forward, though, you don’t have either the time or the inclination.
 
 That article made the argument for Twitter to move towards more of a subscription offering; that may be the wrong idea, but the bigger takeaway is that what Twitter has been trying to build for years just isn’t working, and the challenges aren’t just bad management. To put it another way, when it comes to Twitter’s business, there really isn’t much to lose, but Twitter could only risk losing what there is if it were a private company, free from the glare of public markets who, for very justifiable reasons, give Twitter’s management a very short leash.
 What is valuable is that social graph: while Facebook understands who you know, Twitter, more than any other company, understands what its users are interested in. That is, in theory, much more valuable; said value is diminished by the fact that Twitter just doesn’t have that many users, relatively speaking; the users it has, though, are extremely influential, particularly given the important of Twitter in media, tech, and finance. For this group Twitter is completely irreplaceable: there is no other medium with a similar density of information or interest-driven network effects.
 This, by extension, drives Twitter’s cultural impact: no, most people don’t get their news off of Twitter; the places they get their news, though, are driven by Twitter. Moreover, Twitter not only sets the agenda for media organizations, it also harmonizes coverage, thanks to a dynamic where writers, unmoored from geographic constraints or underlying business realities of their publications, end up writing for other writers on Twitter, oftentimes radicalizing each other in plain sight of their readership. Twitter itself is part of this harmonization, going so far as to censor politically impactful stories in the weeks before an election; it’s no surprise that when Musk says he wants to impose a stronger free speech ethos that the reaction is fierce and littered with motte-and-baileys (“actually we just care about limiting abuse and spam!”).
 Back to the Future
 This is all build-up to my proposal for what Musk — or any other bidder for Twitter, for that matter — ought to do with a newly private Twitter.
 
 First, Twitter’s current fully integrated model is a financial failure.
 Second, Twitter’s social graph is extremely valuable.
 Third, Twitter’s cultural impact is very large, and very controversial.
 
 Given this, Musk (who I will use as a stand-in for any future CEO of Twitter) should start by splitting Twitter into two companies.
 
 One company would be the core Twitter service, including the social graph.
 The other company would be all of the Twitter apps and the advertising business.
 
 TwitterAppCo would contract with TwitterServiceCo to continue to receive access to the Twitter service and social graph; currently Twitter earns around $13/user/year in advertising, so you could imagine a price of say $7.50/user/year, or perhaps $0.75/user/month. TwitterAppCo would be free to pursue the same business model and moderation policies that Twitter is pursuing today (I can imagine Musk sticking with TwitterServiceCo, and the employees upset about said control being a part of TwitterAppCo).
 However, that relationship would not be exclusive: TwitterServiceCo would open up its API to any other company that might be interested in building their own client experience; each company would:
 
 Pay for the right to get access to the Twitter service and social graph.
 Monetize in whatever way they see fit (i.e. they could pursue a subscription model).
 Implement their own moderation policy.
 
 This last point would cut a whole host of Gordian Knots:
 
 Market competition would settle the question about whether or not stringent moderation is an important factor in success; some client experiences would be heavily moderated, and some wouldn’t be moderated at all.
 The fact that everyone gets access to the same Twitter service and social graph solves the cold start problem for alternative networks; the reason why Twitter alternatives always fail is because Twitter’s network effect is so important.
 TwitterServiceCo could wash its hands of difficult moderation decisions or tricky cultural issues; the U.S. might have a whole host of Twitter client options, while Europe might be more stringent, and India more stringent still. Heck, this model could even accommodate a highly-censored China client (although this is highly unlikely).
 
 I strongly suspect that a dramatic increase in competition amongst Twitter client services would benefit TwitterServiceCo, growing its market in a way that hasn’t happened in years. What is most exciting, though, is the potential development of new kinds of services that don’t look like Twitter at all.
 Step back a moment and think about the fundamental infrastructure of the Internet: we have a media protocol in HTTP/web, and a communications protocol in SMTP/email; what is missing is a notifications protocol. And yet, at the same time, if there is one lesson from mobile, it is just how important notifications are; a secondary consideration is how important identity is. If you can know how to reach someone, and have the means to do so, you are set, whether you be a critical service, an advertiser, or anything in-between. Twitter has the potential to fill that role: the ability to route short messages to a knowable endpoint accessible via a centralized directory has far more utility than political signaling and infighting. And yet, thanks to Twitter’s early decisions and lack of leadership, the latter is all the service is good for; no wonder user growth and financial results have stagnated!
 A truly open TwitterServiceCo has the potential to be a new protocol for the Internet — the notifications and identity protocol; unlike every other protocol, though, this one would be owned by a private company. That would be insanely valuable, but it is a value that will never be realized as long as Twitter is a public company led by a weak CEO and ineffective board driving an integrated business predicated on a business model that doesn’t work.
 Twitter’s Reluctance
 The surest evidence of the Twitter board’s lack of imagination and ineffectiveness is that their response to Musk’s proposal is to further dilute existing shareholders as a means of denying Musk control. This is, in my estimation, clearly against the interest of Twitter shareholders (which, for what it’s worth, don’t in any meaningful way include Twitter’s board members); given Twitter’s performance over the last decade, though, this isn’t really a surprise.
 Indeed, when you consider the fact that Twitter’s board members not only don’t own much of Twitter, but famously, barely use Twitter at all, it is easy to wonder if the actual goal is not financial results but rather harnessing that immense cultural impact. This suspicion only intensifies when you consider that the bidder in this case is one of the most successful entrepreneurs of all time: if there was one person in the world who could realize Twitter’s latent value, wouldn’t Musk be at the top of the list? And yet he is anathema, not for his business acumen, but despite it.
 This, more than anything, makes me even more sure that my proposal for competition amongst Twitter client companies is essential: not only do I think that more competition would lead to dramatically more innovation, but it would also solve the problem of who decides what we see by undoing the centralization of that power and subjecting decisions to market forces. That this is unacceptable to some says more about their ultimate motivations than anything else.
 I wrote a follow-up to this Article in this Daily Update.
 A friend sent me this promoted tweet on April 15, almost a full month since Kentucky had been eliminated from March Madness, and well over a week after the entire tournament had ended ↩</content>
     </entry>
     <entry>
       <title>In Defense of the Irrational</title>
         <link href="https://logicmag.io/clouds/in-defense-of-the-irrational"/>
       <updated>2022-04-14T16:17:27.000Z</updated>
       <content type="text">Rationalization is a form of compression that lays a grid over our world and attempts to remake it to fit its shape. The goal of rational thought is to break down a complex and infinite reality into small pieces and reconstitute it in a logical system. In computing, rationalization is the process by which phenomena like actions, identity, and emotions are split, stripped, reduced, standardized, and otherwise converted into computable data and mapped within machines. 
 It is a process of rationalization that allows a company like DoorDash to use computational algorithms to determine “optimal” delivery speeds, and then to discipline its workers for not matching those predetermined outcomes. To believe in the efficacy and reliability of such a system, we must first accept that DoorDash is able to measure and model an intricately layered series of complex relationships—including traffic patterns, consumer desires, worker behaviors, prices, and more—and is then able to draw actionable predictions from all that information. To accept this requires a faith that each part in the complex system is knowable, quantifiable, and fixed. This is the ideology of rationality at work.
 But the world is not rational! The world is, in fact, irrational! It is chaotic, expansive, interrelational, and incalculable. Machines, in particular, are far too rigid in their logic and far too limited in their capacity to meaningfully capture the world. Yet we continue to grant them ever greater power. Across the public and private sector, computer-aided systems of management premised on transactional relationships and the supposed ability to optimize outcomes are being used to guide our interactions. As DoorDash workers will be the first to tell you, these systems shape behavior, remaking the activities and phenomena they are meant to model while radiating innumerable harms—from incentivizing unsafe driving speeds to suppressing wages—as a result. 
 To resist this reconfiguration and mitigate these harms, we must reject rationality and embrace a fundamentally irrational worldview. If rationality says the world is measurable, knowable, optimizable, and automatible, an embrace of the irrational is a rejection of that ideology. Embracing irrationality allows for multiple interpretations, contradiction, inexplicability. It empowers us to reclaim the act of meaning-making as a collaborative, social exercise as opposed to one that can be automated and forgotten. Ultimately, a program of irrationality requires that we harness the power of our machines through a form of democratic oversight that acknowledges the false promise of rational management and insists that, in the absence of certainty, we must work together to organize society. Irrationality celebrates doubt, because only if the future is unknown will it be ours to build. 
 Rational Data from an Irrational World
 Rationalization—the process of abstraction in the service of computational reasoning—has long been a feature of the sciences, math, and philosophy. Of course, tools of theoretical inquiry are often put to practical use by those in power. Beginning in the late nineteenth and early twentieth century, labor processes were mapped out rationally to the great benefit of early industrialists. In the postwar period, cyberneticists and game theorists, many employed by the US military, theorized that they could go beyond simple numerical equations or discrete production processes and rationally describe much more complex phenomena using newly developed computing machines. 
 Around the end of World War II, the first general-purpose electronic computers were introduced. The data being entered into these machines was numerical, it was subjected to mechanically coded mathematical formulae, and the output was a solved equation. An amazing innovation, and one that—provided the inputs were entered accurately, and the machine operations properly encoded—returned an accurate and reliable result. An increased focus on feedback soon enabled a process known as “machine learning” through neural networks—a technique that has been revived in the last decade to propel a new AI boom, driven by breakthroughs in computer vision and natural language processing.
 Machine learning sorts through vast amounts of chaotic data and, using adaptive algorithms, closes in on specific arrangements of information while excluding other possible interpretations. But, in order for any computation to occur, a process of rationalization must first create machine-readable datasets. Real-world phenomena must be “datafied” by sorting it into categories and assigning fixed values. 
 Take, for example, most image recognition software. Whether the goal is identifying handwriting or enemy combatants, a training set of data made up of digital images—themselves encoded arrangements of pixels—is typically created. This initial process of digital image capture is a form of reduction and compression; think of the difference between the sunset you experience and what that same sunset looks like when it is posted to Instagram. This mathematical translation is necessary so the machines can “see,” or, more accurately, “read” the images. 
 But for the purposes of this kind of machine learning, further rationalization must occur to make the data usable. The digital image is identified and labeled by human operators. Maybe it is a set of handwriting examples of the numeral two, or drone images from a battlefield. Either way, someone, often an underpaid crowdworker on a platform like Amazon Mechanical Turk, decides what is meaningful within the images—what the images represent—so that the algorithms have a target to aim for. 
 The set of labeled images is fed into software tasked with finding patterns within it. First, borders are identified in the encoded arrangement of pixels. Then larger shapes are identified. An operator observes the results and adjusts parameters to guide the system towards an optimal output. Is that a 2? Is that an enemy combatant or a civilian? 
 Once this output has been deemed acceptable, the system is fed new, unlabeled images and asked to identify them. This new data, along with feedback on the functional accuracy of its initial output—“yes, that is a 2”—is used to further fine-tune the algorithm, the optimization of which is largely automated. This basic process applies to most machine learning systems: rationalized data is fed in, and through association, feedback, and refinement, the machine “learns” to provide better results. 
 But rational data is an unstable foundation from which to learn. Those initial stages in the machine-learning process when phenomena are translated into code, when the irrational is rationalized and the real world is datafied, demand close scrutiny. And as the phenomena we are asking the software to interpret become more complex—as these systems are tasked with going from recognizing that an image contains a face to recognizing a specific face, to recognizing the emotion on that face, to determining what actions might result from someone in a certain emotional state—the more skeptical we should be of any supposed insights that are generated. 
 The process of translating the world into code is reductive. There is a similar reduction in the labeling of data into certain categories—there will never be sufficient categories available to represent all possibilities. In an infinitely complex and constantly shifting world, any one-to-one representation is impossible. And anything that is lost in that original training data will be invisible to machine learning systems that are built upon it. 
 Far from being a neutral process, the creation of training data is fundamentally social and subjective. It requires human actors to determine the available categories and label the data accordingly. The attendant assumptions, biases, and distinctions made by these human actors are necessary to create “rational” data, and once encoded they define the possibilities and limitations of what machine learning systems can “learn.” 
 To be clear, all forms of knowledge-making are social and subjective, not just machine learning. The difference is that other ways of making sense of the world acknowledge their own fallibility. For instance, in academia, disciplines have developed various techniques for vetting new information, such as peer review. The issues are not always resolved, but there are processes that help create meaning collectively. 
 The Irrational Program
 The making of meaning cannot be automated because an irrational world cannot be coded rationally. Machine learning systems, with their immense computational power, can surface novel arrangements of information and offer new forms of perception. But any claims to objectivity made on behalf of these systems should be disregarded outright. 
 Moreover, these systems are engaged in actively shaping society to fit the models they create. When the options for human activity are reduced to a set of “optimal” choices made available through a machine-generated recommendation, other courses of action—and thus other possible future outcomes—are eliminated. We cannot allow this reduction to put limitations on the world in which we live. Instead, if these systems are to be salvaged, we have a responsibility to relentlessly interrogate who and what constitutes “data,” how it is made, what patterns we seek within it, and what we do with the insights that are surfaced. These questions must be put to the widest public forums available, and the decisions about how to respond must be made democratically. Then those questions must be asked again and again. 
 The process of rationalization, and the technology it enables, are social in origin and have a social impact once deployed. Ultimately, we must embrace their collective nature and respond collectively. This means organizing the workers at the point of rationalization and organizing the subjects of datafication to resist until their demands for input into the development of these systems is met. We make technological systems as they make us, and we can remake or unmake them. When we recognize our role in the co-creation of technological systems, and take collective control over that process, who knows what innovations may result?</content>
     </entry>
     <entry>
       <title>7 Web Component Tricks</title>
         <link href="https://daverupert.com/2022/04/7-web-component-tricks/"/>
       <updated>2022-04-14T15:58:00.000Z</updated>
       <content type="text">This week I finished my Frontend Masters Web Components Course. To market it celebrate that accomplishment, I wanted to share ~7 tips and tricks I’ve learned preparing my course or I feel aren’t super obvious about Web Components.
 1. You can manipulate props right on a Lit element
 This may be something only I would do, but if you make an element with Lit that exposes its properties, you can edit those props externally using querySelector.
 &lt;my-counter counter&#x3D;&quot;3&quot;&gt;&lt;/my-counter&gt;
 
 &lt;script&gt;
 const myCounter &#x3D; document.querySelector(&#x27;my-counter&#x27;)
 myCounter.counter &#x3D; 10
 &lt;/script&gt;
 
 2. :host-context let’s you style an element based on its parent
 You can use :host-context() to style an element based on its parent. Your HTML may look like this:
 &lt;my-element&gt;&lt;/my-element&gt;
 &lt;div class&#x3D;&quot;card&quot;&gt;&lt;my-element&gt;&lt;/my-element&gt;&lt;/div&gt;
 
 In your CSS inside the Web Component, you have something like this:
 :host-context(.card) { background:pink; }
 :host-context(.card)::after { content: &#x27;I’m in a card&#x27; }
 
 See Example
 3. Declarative ShadowDOM
 &lt;my-element&gt;
 	&lt;template shadowroot&#x3D;&quot;open&quot;&gt;
 	  &lt;p&gt;I&#x27;m a spooky skeleton screen 💀&lt;/p&gt;
 	&lt;/template&gt;
 &lt;/my-element&gt;
 
 Declarative Shadow DOM enables server-side rendering of Web Components, but one thing that’s not clear is your inlined template and the components actual template can be totally different.
 See Example
 4. Open WC has a project starter
 If you’re looking for a create-react-app for Web Components, the folks at Open WC have you covered.
 npm init @open-wc
 
 You get so much from this (local server, testing configs, a storybook, production rollup config, etc) but my favorite bit is from the sample component’s test file: it runs an accessibility audit on your Shadow DOM!
 it(&#x27;passes the a11y audit&#x27;, async () &#x3D;&gt; {
   const el &#x3D; await fixture(html&#x60;&lt;custom-element&gt;&lt;/custom-element&gt;&#x60;);
 
   await expect(el).shadowDom.to.be.accessible();
 });
 
 Accessibility out of the box! Nice.
 5. You can “rebrand” other people’s components.
 Want to mix and match components from different design systems but keep a consistent naming structure in your company? You can import a component and “rebrand” it or even add functionality.
 import { CoolButton } from &#x27;cool-design-system&#x27;
 
 class OurButton extends CoolButton {
 	constructor { super() }
 }
 
 customElements.define(&#x27;our-button&#x27;, OurButton)
 
 6. The Open WC Publishing Guides are cool
 The OpenWC group also has some nice community guidelines for publishing Web Components.
 
 ✅ Do publish latest standard EcmaScript
 ✅ Do publish standard es modules
 ✅ Do include &quot;main&quot;: &quot;index.js&quot; and &quot;module&quot;: &quot;index.js&quot; in your package.json
 ✅ Do export element classes
 ✅ Do export side effects separately
 ✅ Do import 3rd party node modules with “bare” import specifiers
 ✅ Do include file extensions in import specifiers
 ❌ Do not optimize
 ❌ Do not bundle
 ❌ Do not minify
 ❌ Do not use .mjs file extensions
 ❌ Do not import polyfills
 
 That’s helpful and hopefully provides a consistent experience, allowing for a consistent bundling story, and preventing weird footguns that might occur when trying to use other people’s Web Components in your project.
 7. You don’t need build tools until the very, very end.
 If you want to write Web Components, you can write vanilla web components and use ES Modules to join them together. You can use a web component library like Lit with an import statement pointed at skypack.dev or unpkg.com. It’s super handy to get started with zero tooling.
 If you want to install packages off of npm … you could try Import Maps … but otherwise you’ll need a local dev server (vite or @web/dev-server) that supports “bare import specifiers”.
 It’s only when going to production that you need tooling specific to your site’s needs. TypeScript is optional, bundling is optional, minifying code is optional. From a Web Component perspective, these are all considered “application-level concerns” that happen at deployment time.
 Rollup build script examples are out there, but Web Components don’t prescribe how to build your application, they don’t hitch you to an architecture. It could be a whole tree-shaken SPA (single page app), but Web Components also work well in a MPA (multi-page app) architecture. It’s up to you and your application to figure out what fits best.
 Stay tuned for more Web Component content
 If you want more Web Component content, stay tuned! I learned a bunch and it feels worth it to share. Also, if you’re a visual learner, my Frontend Masters course should be up soon so keep a look out over there, I’ll also post about it here, don’t worry 😉</content>
     </entry>
     <entry>
       <title>Mozilla partners with the Center for Humane Technology</title>
         <link href="https://hacks.mozilla.org/2022/04/mozilla-partners-with-the-center-for-humane-technology/"/>
       <updated>2022-04-13T15:02:02.000Z</updated>
       <content type="text">We’re pleased to announce that we have partnered with the Center for Humane Technology, a nonprofit organization that radically reimagines the digital infrastructure. Its mission is to drive a comprehensive shift toward humane technology that supports the collective well-being, democracy and shared information environment. Many of you may remember the Center for Humane Tech from the Netflix documentary ‘Social Dilemma’, solidifying the saying “If you’re not paying for the product, then you are the product”. The Social Dilemma, is all about the dark side of technology, focusing on the individual and societal impact of algorithms. 
 It’s no surprise that this decision to partner was a no brainer and supports our efforts for a safe and open web that is accessible and joyful for all. Many people do not understand how AI and algorithms regularly touch our lives and feel powerless in the face of these systems. We are dedicated to making sure the public understands that we can and must have a say in when machines are used to make important decisions – and shape how those decisions are made. 
 Over the last few years, our work has been increasingly focused on building more trustworthy AI and safe online spaces. From challenging YouTube’s algorithms, where Mozilla research shows that the platform keeps pushing harmful videos and its algorithm is recommending videos with misinformation, violent content, hate speech and scams to its over two billion users to developing Enhanced Tracking Protection in Firefox that automatically protects your privacy while you browse, and Pocket which recommends high-quality, human-curated articles without collecting your browsing history or sharing your personal information with advertisers.
 Let’s face it, most, if not all people, would probably prefer to use social media platforms that are safer and technologists should design products that reflect all users and without bias. As we collectively continue to think about our role in these areas — now and in the future, this course from the Center for Humane Tech is a great addition to the many tools necessary for change to take place. 
 The course rightly titled ‘Foundations of Humane Technology’ launched out of beta in March of this year, after rave reviews from hundreds of beta testers! 
 It explores the personal, societal, and practical challenges of being a humane technologist. Participants will leave the course with a strong conceptual framework, hands-on tools, and an ecosystem of support from peers and experts. Topics range from respecting human nature to minimizing harm to designing technology that deliberately avoids reinforcing inequitable dynamics of the past. 
 The course is completely free of charge and is centered towards building awareness and self-education through an online, at-your-own pace or binge-worthy set of eight modules. The course is marketed to professionals, with or without a technical background involved in shaping tomorrow’s technology. 
 It includes interactive exercises and reflections to help you internalize what you’re learning and regular optional Zoom sessions to discuss course content, connect with like-minded people, learn from experts in the field and even rewards a credential upon completion that can be shared with colleagues and prospective employers.
 The problem with tech is not a new one, but this course is a stepping stone in the right direction.
 The post Mozilla partners with the Center for Humane Technology appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>DALL-E, the Metaverse, and Zero Marginal Content</title>
         <link href="https://stratechery.com/2022/dall-e-the-metaverse-and-zero-marginal-content/"/>
       <updated>2022-04-12T14:46:46.000Z</updated>
       <content type="text">Last week OpenAI released DALL-E 2, which produces (or edits) images based on textual prompts; this Twitter thread from @BecomingCritter has a whole host of example output, including Teddy bears working on new AI research on the moon in the 1980s:
 
 A photo of a quaint flower shop storefront with a pastel green and clean white facade and open door and big window:
 
 And, in the most on-the-nose example possible, A human basking in the sun of AGI utopia:
 
 OpenAI has a video describing DALL-E on its website:
 
 While the video does mention a couple of DALL-E’s shortcomings, it is quite upbeat about the possibilities; some excerpts:
 
   Dall-E 2 is a new AI system from OpenAI that can take simple text descriptions like “A koala dunking a basketball” and turn them into photorealistic images that have never existed before. DALL-E 2 can also realistically edit and re-touch photos…
   DALL-E was created by training a neural network on images and their text descriptions. Through deep learning it not only understands individual objects like koala bears and motorcycles, but learns from relationships between objects, and when you ask DALL-E for an image of a “koala bear riding a motorcycle”, it knows how to create that or anything else with a relationship to another object or action.
   The DALL-E research has three main outcomes: first, it can help people express themselves visually in ways they may not have been able to before. Second, an AI-generated image can tell us a lot about whether the system understands us, or is just repeating what it’s been taught. Third, DALL-E helps humans understand how AI systems see and understand our world. This is a critical part of developing AI that’s useful and safe…
   What’s exciting about the approach used to train DALL-E is that it can take what it learned from a variety of other labeled images and then apply it to a new image. Given a picture of a monkey, DALL-E can infer what it would look like doing something it has never done before, like paying its taxes while wearing a funny hat. DALL-E is an example of how imaginative humans and clever systems can work together to make new things, amplifying our creative potential.
 
 That last line may raise some eyebrows: at first glance DALL-E looks poised to compete with artists and illustrators; there is another point of view, though, where DALL-E points towards a major missing piece in a metaverse future.
 Games and Medium Evolution
 Games have long been on the forefront of technological development, and that is certainly the case in terms of medium. The first computer games were little more than text:
 
 Images followed, usually of the bitmap variety; I remember playing a lot of “Where in the world is Carmen San Diego” at the library:
 
 Soon games included motion as you navigated a sprite through a 2D world; 3D followed, and most of the last 25 years has been about making 3D games ever more realistic. Nearly all of those games, though, are 3D images on 2D screens; virtual reality offers the illusion of being inside the game itself.
 Still, this evolution has had challenges: creating ever more realistic 3D games means creating ever more realistic image textures to decorate all of those polygons; this problem is only magnified in virtual reality. This is one of the reasons even open-world games are ultimately limited in scope, and gameplay is largely deterministic: it is through knowing where you are going, and all of your options to get there, that developers can create all of the assets necessary to deliver an immersive experience.
 That’s not to say that games can’t have random elements, above and beyond roguelike games that are procedurally generated: the most obvious way to deliver an element of unpredictability is for humans to play each other, albeit in well-defined and controlled environments.
 Social and User-Generated Content
 Social networking has undergone a similar medium evolution as games, with a two-decade delay. The earliest forms of social networking on the web were text-based bulletin boards and USENET groups; then came widespread e-mail, AOL chatrooms, and forums. Facebook arrived on the scene in the mid-2000s; one of the things that helped it explode in popularity was the addition of images. Instagram was an image-only social network that soon added video, which is all that TikTok is. And, over the last couple of years in particular, video conferencing through apps like Zoom or Facetime have delivered 3D images on 2D screens.
 Still, medium has always mattered less for social networking, just because the social part of it was so inherently interesting. Humans like communicating with other humans, even if that requires dialing up a random BBS to download messages, composing a reply, and dialing back in to send it. Games may be mostly deterministic, but humans are full of surprises.
 Moreover, this means that social networking is much cheaper: instead of the platform having to generate all of the content, users generate all of the content themselves. This makes it harder to get a new platform off of the ground, because you need users to attract users, but it also makes said platform far stickier than any game (or, to put it another way, the stickiest games have a network effect of their own).
 Feeds and Algorithms
 The first iterations of social networking had no particular algorithmic component other than time: newer posts were at the top (or bottom). That changed with Facebook’s introduction of the News Feed in 2006. Now instead of visiting all of your friends’ pages you could simply browse the feed, which from the very beginning made decisions about what content to include, and in what order.
 Over time the News Feed evolved from a relatively straightforward algorithm to one driven by machine learning, with results so inscrutable that it took Facebook six months to fix a recent rankings bug. The impact has been massive: not just Facebook but also Instagram saw huge increases in engagement and increased growth the better their algorithmically-driven feeds became; it was also great for monetization, as the same sort of signals that decided what content you saw also influenced what ads you were presented.
 However, the reason why this discussion of algorithmically-driven feeds is in a different section than social networking is because the ultimate example of their power isn’t a social network at all: it’s TikTok. TikTok, of course, is all user-generated content, but the crucial distinction from Facebook is that you aren’t limited to content from your network: TikTok pulls in the videos it thinks you specifically are most interested in from across its entire network. I explained why this was a blindspot for Facebook in 2020:
 
   What is interesting to point out is why it was inevitable that Facebook missed this: first, Facebook views itself first-and-foremost as a social network, so it is disinclined to see that as a liability. Second, that view was reinforced by the way in which Facebook took on Snapchat. The point of The Audacity of Copying Well is that Facebook leveraged Instagram’s social network to halt Snapchat’s growth, which only reinforced that the network was Facebook’s greatest asset, making the TikTok blindspot even larger.
 
 TikTok combines the zero cost nature of user-generated content with a purely algorithmic feed that is divorced from your network; there is a network effect, in that TikTok needs lots of content to choose from, but it doesn’t need your specific network.
 The Machine Learning Metaverse
 I get that metaverses were so 2021, but it strikes me that the examples from science fiction, including Snow Crash and Ready Player One, were very game-like in their implementation. Their virtual worlds were created by visionary corporations or, in the case of the latter, a visionary developer who also included a deterministic game for ultimate ownership of the virtual world. Yes, third parties could and did build experiences with strong social components, most famously Da5id’s Black Sun club in Snow Crash, but the core mechanic — and the core economics — were closer to a multi-player game than anything else.
 That, though, is exceptionally challenging in the real world: remember, creating games, particularly their art, is expensive, and the expense increases the more immersive the experience is. Social media, on the other hand, is cheap because it uses user-generated content, but that content is generally stuck on more basic mediums — text, pictures, and only recently video. Of course that content doesn’t necessarily need to be limited to your network — an algorithm can deliver anything on the network to any user.
 What is fascinating about DALL-E is that it points to a future where these three trends can be combined. DALL-E, at the end of the day, is ultimately a product of human-generated content, just like its GPT-3 cousin. The latter, of course, is about text, while DALL-E is about images. Notice, though, that progression from text to images; it follows that machine learning-generated video is next. This will likely take several years, of course; video is a much more difficult problem, and responsive 3D environments more difficult yet, but this is a path the industry has trod before:
 
 Game developers pushed the limits on text, then images, then video, then 3D
 Social media drives content creation costs to zero first on text, then images, then video
 Machine learning models can now create text and images for zero marginal cost
 
 In the very long run this points to a metaverse vision that is much less deterministic than your typical video game, yet much richer than what is generated on social media. Imagine environments that are not drawn by artists but rather created by AI: this not only increases the possibilities, but crucially, decreases the costs.
 Zero Marginal Content
 There is another way to think about DALL-E and GPT and similar machine learning models, and it goes back to my longstanding contention that the Internet is a transformational technology matched only by the printing press. What made the latter revolutionary was that it drastically reduced the marginal cost of consumption; from The Internet and the Third Estate:
 
   Meanwhile, the economics of printing books was fundamentally different from the economics of copying by hand. The latter was purely an operational expense: output was strictly determined by the input of labor. The former, though, was mostly a capital expense: first, to construct the printing press, and second, to set the type for a book. The best way to pay for these significant up-front expenses was to produce as many copies of a particular book that could be sold.
   How, then, to maximize the number of copies that could be sold? The answer was to print using the most widely used dialect of a particular language, which in turn incentivized people to adopt that dialect, standardizing languages across Europe. That, by extension, deepened the affinities between city-states with shared languages, particularly over decades as a shared culture developed around books and later newspapers. This consolidation occurred at varying rates — England and France several hundred years before Germany and Italy — but in nearly every case the First Estate became not the clergy of the Catholic Church but a national monarch, even as the monarch gave up power to a new kind of meritocratic nobility epitomized by Burke.
 
 The Internet has had two effects: the first is to bring the marginal cost of consumption down to zero. Even with the printing press you still needed to print a physical object and distribute it, and that costs money; meanwhile it costs effectively nothing to send this post to anyone in the world who is interested. This has completely upended the publishing industry and destroyed the power of gatekeepers.
 The other impact, though, has been on the production side; I wrote about TikTok in Mistakes and Memes:
 
   That phrase, “Facebook is compelling for the content it surfaces, regardless of who surfaces it”, is oh-so-close to describing TikTok; the error is that the latter is compelling for the content it surfaces, regardless of who creates it…To put it another way, I was too focused on demand — the key to Aggregation Theory — and didn’t think deeply enough about the evolution of supply. User-generated content didn’t have to be simply pictures of pets and political rants from people in one’s network; it could be the foundation of a new kind of network, where the payoff from Metcalfe’s Law is not the number of connections available to any one node, but rather the number of inputs into a customized feed.
 
 Machine learning generated content is just the next step beyond TikTok: instead of pulling content from anywhere on the network, GPT and DALL-E and other similar models generate new content from content, at zero marginal cost. This is how the economics of the metaverse will ultimately make sense: virtual worlds needs virtual content created at virtually zero cost, fully customizable to the individual.
 Of course there are many other issues raised by DALL-E, many of them philosophical in nature; there has already been a lot of discussion of that over the last week, and there should be a lot more. Still, the economic implications matter as well, and after last week’s announcement the future of the Internet is closer, and weirder, than ever.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 298: Email tech stacks, less CSS duplicates, simple color systems, a custom highlight API, UUID6 and more.</title>
         <link href="https://wdrl.info/archive/298"/>
       <updated>2022-04-07T13:00:00.000Z</updated>
       <content type="text">Hey,
 
 if you read this email, it’ll be technically different from the previous. When I recently updated my newsletter service server software nearly everything broke into pieces. I was using mailtrain v1 for a long time now but it’s deprecated and using old node versions. Upgrading to v2 wasn’t easy enough for me and I also wanted to have an easier to maintain mailer anyway. The new email is sent via Spatie’s Laravel Mailcoach. So far it’s a pretty manual process again since it doesn’t support RSS-campaigns. At some point in future I want to publish the newsletter via Kirby to the Mailcoach API.
 But at this point I now changed my template as well to a modern MJML based template code. This should hopefully fix all the rendering issues you reported to me recently. I tried to support light and dark mode in the emails but that utterly failed and the new template does not support it, too. It’s a bit sad but mail clients once again do what they believe is right, leading to white on white text in some cases. It’s a good reminder to caniemail and that email templating isn’t coding for a browser.
 Please report any issues to me if you find one. Now enjoy the new content:
 News
 
 	UUID6-8 will be a standard soon for unique identifiers, solving a lot of the common issues with UUID formats (e.g. UUIDv4). Quite nice options ahead!
 	If macOS is your operating system of choice and if you use Docker for your local development environment, there might be good news. The Docker team finally reports promising performance improvements for file synchronization.
 
 Generic
 
 	When creating websites and web applications, we have to define deployment and integration processes, think about different environments, and learn more about tooling. The internet is full of complex methods and heavy approaches. Therefore it is nice to read about an alternative. Lewis Monteith writes about why he is not using staging environments.
 
 UI/UX
 
 	The Open Source Color System is a nice preset for web apps.
 
 HTML &amp; SVG
 
 	»You may not think about images as part of your web dev work, but they can affect your web app&#x27;s performance more than any other part of your code«, says Addy Osmani in Picture perfect images with the modern &lt;img&gt; element. A great summary of what and how to optimize delivery of image content on the web.
 	There is so much we can do by just using HTML. Louis Lazaris created a list with a lot of lesser-known but awesome HTML attributes.
 
 JavaScript
 
 	Mark shares a nice example of how to build a mobile scroll navigation that automatically scrolls active links into view. A menu principle I’ve been using more often lately as it’s super simple and effective.
 	The CSS Custom Highlight API which will be a webstandard soon makes it possible to style arbitrary text ranges from JavaScript. Pretty cool to highlight dynamic search results or similar things.
 
 CSS
 
 	css-checker checks your css for duplications to avoid redundant css.
 	Stephanie Eckles shares how we can animate a newly added element with CSS. And since we cannot animate easily a dynamic height just with CSS, Stephanie has a nice design hack for this to make it look like that.
 	The more experienced we get, the deeper we can dive. Josh Comeau writes about CSS layout algorithms, and there is a lot to learn from him.
 
 Go beyond…
 
 	David Cain elaborates on why we want problems to be someone else’s fault and why it’s natural for humans to think like that, yet not always the best thing to do. Knowing about it definitely helps reconsidering it and stop doing it all the time.
 
 
 If you like this newsletter, you can contribute here. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>What if... one day everything got better?</title>
         <link href="https://daverupert.com/2022/04/what-if-everything-got-better/"/>
       <updated>2022-04-04T15:04:00.000Z</updated>
       <content type="text">One day I came across this video about Muharrem, a deaf man, and how his whole neighborhood conspired against him for a surprise.
 
 Here I am, crying at my desk, moved to tears by an advertisement for a Samsung product in Turkey. You watch as Muharrem’s expression moves from confused, to disbelief, to overwhelm. Muharrem tears up, I tear up. Imagine what it must feel like to leave your house and the mismatch you feel every day got erased; the world works for you in a way that it didn’t before.
 What if…
 What if we could do this with the Web. What if… one day everything got better?
 The WebAIM Million Project scans the top 1,000,000 homepages and runs automated accessibility tests on them. In the report WebAIM identifies six (6) categories of issues that account for 96.5% of the over 50 million detectable errors.
 
 Low contrast text
 Missing alt text
 Empty links
 Missing form input labels
 Empty buttons
 Missing document language
 
 The most shocking revelation is: All these issues are easy-to-detect and easy-to-fix! I believe if we’re all laser focused on these six issues we can make a dent in the WebAIM Million and maybe the universe.
 I’m confident it’ll make a difference. Holly Tuke documented some of her experiences in a post called “5 most annoying website features I face as a blind person every single day” — her list of issues is nearly the same the WebAIM Million’s list of issues! It’s one data point, but there’s millions out there like Holly who would benefit from even the most basic fixes to your website.
 When?
 Global Accessibility Awareness Day (GAAD) this year is on May 19, 2022. I can’t think of a better day. That gives you or your company about 6+ weeks to land some minor fixes on your homepages.
 If you visit this in the future, GAAD is every third Thursday of May. Roadmap accordingly.
 How?
 The easiest way to find and detect these issues are to use one of the following:
 
 Google Lighthouse (built-in Chrome Web Inspector)
 Microsoft Accessibility Insights (browser extension)
 
 They’re both based on Deque’s axe Accessibility Testing Tool which you can also use. WebAIM’s WAVE evaluation tool is also great. If you haven’t done this before, there will be lots of errors, but stay focused on the six issues above.
 We’re not going to fix the entire web, we’re trying to make a dent. There’s a lot to fix, but I’m a firm believer we need to start somewhere and these six types of issues are –in the nuanced world of accessibility– the easiest to fix. A roadmap of fifty million issues begins with a single step.
 What do I get out of it?
 Yikes. Well… You can write a company blog post saying you took part in GAAD. That goes over well. You get some organic traffic. More users can use your website, they’ll buy your goods. Your site won’t be fully WCAG2 compliant, but at least you checked off some of the lowest hanging (robo-lawyer) fruit.
 What if I finished?
 Awesome work! Next, identify other core templates of your site and repeat. Think about the whole experience. One thing I heard this weekend was how it’s more frustrating to get to the end of a gigantic flow and an accessibilty error (like unlabelled buttons) prevents you from succeeding.
 Like I said above, your site won’t be fully accessible after this, but I applaud you and your team because you’ve taken a major step on your accessibility journey. You can try your hand at guided manual tests. Run user tests. Or hire a professional to give you a full audit.
 Share, translate, commit!
 Let me know if you’re in. I think we can do it. Six issues in six weeks. Knock them out.
 If anyone wants to make a website about this where people can sign, sponsor, or put logos on it, let me know.
 Feel free to translate this post to your native language, all I ask is you provide a link back to the original here in case information changes.</content>
     </entry>
     <entry>
       <title>Why Netflix Should Sell Ads</title>
         <link href="https://stratechery.com/2022/why-netflix-should-sell-ads/"/>
       <updated>2022-04-04T14:26:44.000Z</updated>
       <content type="text">The Information reported over the weekend that Netflix executives have told employees to keep an eye on the bottom line:
 
   In two separate meetings over the past few weeks, Netflix executives cautioned employees to be more mindful about spending and hiring, according to three people familiar with the discussions. The comments, made at an employee town hall on Monday as well as during a management offsite held last month in Anaheim, Calif., come as the streaming giant grapples with sharply slowing subscriber growth…
   Netflix has also been pondering steps that could help offset the revenue impact of the subscriber slowdown, including cracking down on people sharing the passwords to their accounts. While Netflix has long allowed such password sharing, it has become more common in the U.S. and other parts of the world than executives anticipated, the people said. This effort has been underway for about a year, however, well before the slowdown became apparent.
 
 These are presented as two different issues, but there is a connection between them: Netflix should be hiring more people — a lot of them — and those people should be building a product that increases subscriber numbers and revenue. That product is advertising.
 Netflix’s Business Model: Subscriptions
 Netflix is, incredibly enough, 24 years old, and a subscription model has served the company well. Not that Netflix had much choice when it started: the company briefly sold DVDs online, before focusing exclusively on renting them; neither approach offered much surface area for advertising, and besides, the subscription model was revolutionary in its own right.
 DVDs-by-mail was, from a certain perspective, inconvenient: you couldn’t simply drive to your local Blockbuster and peruse the selection; on the other hand, Netflix’s model gave you access to nearly every movie ever released, not just those in stock at your local store. The real innovation, though, was that business model: instead of paying to rent a DVD and being gouged with late fees, you could pay a set amount each month and keep the DVDs Netflix mailed to you as long as you wanted; send one back to get the next one in your queue.
 Consumers loved it, and Netflix has stuck with the model even as the shift to streaming flipped their value proposition on its head: streaming is even more convenient than hopping in your car, but only a subset of content (ever-expanding, to be sure) is on Netflix. That has been more than enough to fuel Netflix’s growth; the service had 222 million subscribers at the end of 2021.
 Still, as The Information noted, that number isn’t increasing as quickly as it used to. Netflix sported over 20% year-over-year subscriber growth for years (usually more than that), but hasn’t broken the 20% mark since Q4 2020; growth for the last three quarters was in the single digits. Some of that is likely due to growth that was pulled forward by the pandemic:
 
 The bigger problem, though, is saturation: Netflix has 75 million subscribers in the US and Canada, where there are around 132 million households. That is nearly as many subscribers as linear TV (84 million), and once you consider shared passwords, penetration may be higher. Other markets like India have more room to grow, but much lower household incomes, and Netflix’s relatively high prices have been an obstacle.
 Netflix has ways to grow other than subscribers, most obviously by raising prices. The company has done just that on a mostly annual basis for eight years: in the U.S. the price of a Standard subscription (HD, 2 screens) has increased from $7.99 to $15.49. Netflix executives argue that customers don’t mind because Netflix keeps increasing the amount of content they find compelling; it’s an argument that is easier to accept when subscriber growth is up-and-to-the-right. Now the task is to keep raising prices while ensuring subscriber numbers don’t start going in the opposite direction.
 Netflix’s New Initiative: Gaming
 To accomplish this Netflix is not only continuing to invest in original programming, but also branching out into new kinds of content, including games. This may seem an odd idea at first: sure, Netflix is generating some new IP, but it would generally be much easier to license that IP than to become proficient at gaming. Netflix, though, believes it has a unique advantage when it comes to gaming: its business model. Chief Product Officer Greg Peters said in the company’s Q2 2021 earnings interview:
 
   Our subscription model yields some opportunities to focus on a set of game experiences that are currently underserved by the sort of dominant monetization models and games. We don’t have to think about ads. We don’t have to think about in-game purchases or other monetization. We don’t have to think about per-title purchases. Really, we can do what we’ve been doing on the movie and series side, which is just hyper laser-focused on delivering the most entertaining game experiences that we can. So we’re finding that many game developers really like that concept and that focus and this idea of being able to put all of their creative energy into just great gameplay and not having to worry about those other considerations that they have typically had to trade off with just making compelling games.
 
 Netflix’s gaming efforts to date have been fairly limited; the company launched with five titles in November, but the fact the company has bought three gaming studios suggests a strong appetite for more — at least amongst Netflix executives.
 But what about consumers?
 Netflix’s Job: TV
 Consumers don’t care so much about business models; they have jobs that they want to get done, and the traditional cable bundle used to do a whole bunch of jobs: information gathering, education, sports, story-telling, escapism, background noise, and more. As I noted in The Great Unbundling, these jobs are increasingly done by completely different services: we get news on the Internet, education from YouTube, story-telling from streaming services, etc.
 Netflix is obviously one of those streaming services, but the company is also investing in movies (escapism), and is increasingly the default choice when it comes to the under-appreciated “background noise” category: the service has oceans of low-brow content ready to be streamed while you are barely paying attention. This is a big reason why for many people their choice of streaming services is a matter of which service do they subscribe to in addition to Netflix.
 Still, all of these jobs are about passively consuming content; from a consumer perspective gaming is something different, in that you are an active participant. To that end, it’s not clear to me why consumers would even think to consider Netflix when it comes to gaming: that’s not what the service’s job is, nor was it the job of the linear TV bundle that Netflix is helping replace.
 Netflix’s Market: Attention
 Then again, as founder and co-CEO Reed Hastings likes to say, Netflix’s competition is much broader than TV; Hastings wrote in the company’s Q4 letter to shareholders:
 
   In the US, we earn around 10% of television screen time and less than that of mobile screen time. In 2 other countries, we earn a lower percentage of screen time due to lower penetration of our service. We earn consumer screen time, both mobile and television, away from a very broad set of competitors. We compete with (and lose to) Fortnite more than HBO. When YouTube went down globally for a few minutes in October, our viewing and signups spiked for that time. Hulu is small compared to YouTube for viewing time, and they are successful in the US, but non-existent in Canada, which creates a comparison point: our penetration in the two countries is pretty similar. There are thousands of competitors in this highly-fragmented market vying to entertain consumers and low barriers to entry for those with great experiences. Our growth is based on how good our experience is, compared to all the other screen time experiences from which consumers choose. Our focus is not on Disney+, Amazon or others, but on how we can improve our experience for our members.
 
 Hastings’ point was that analysts should not be overly focused on the threat posed by other streaming services; Netflix has been fighting for attention for years. This is correct, by the way: thanks to the Internet everything from television to social networking to gaming can be delivered at zero marginal cost; the only scarce resource is time, which means attention is the only thing that needs to be competed for.
 Well, that and money: companies competing for customer money need a way to communicate to customers what they have to sell and why it is compelling; that means advertising, and advertising requires attention. It follows, then, that the most effective business model in the attention economy is advertising: if customers rely on Google or Facebook to navigate the abundance of content that is the result of zero marginal costs, then it is Google and Facebook that are the best-placed to sell effective ads.
 Notice, though, the trouble this Internet reality presents to Netflix: if content is abundant and attention is scarce, it’s easier to sell attention than content; Netflix’s business model, though, is the exact opposite.
 Netflix’s Differentiation: Unique Content
 Netflix, of course, sees this as a differentiator, and for a long time it was: linear TV had commercials, while Netflix had none. Linear TV made you wait for your favorite show, while Netflix gave you entire seasons at once. This was particularly compelling when Netflix had similar content to linear TV: why would you put up with commercials and TV schedules when you could just stream what you wanted to?
 However, as more and more content has moved away from TV and to competing streaming services, differentiation is no longer based on the user experience, but rather uniqueness; on-demand no-commercials is no longer unique, but Stranger Things can only be found on Netflix.
 Here Netflix’s biggest advantage is the sheer size of its subscriber base: Netflix can, on an absolute basis, pay more than its streaming competitors for the content it wants, even as its per-subscriber cost basis is lower. This advantage is only accentuated the larger Netflix’s subscriber base gets, and the more revenue it makes per subscriber; the user experience of getting to that unique content doesn’t really matter.
 
 All of these factors make a compelling case for Netflix to start building an advertising business.
 First, an advertising-supported or subsidized tier would expand Netflix’s subscriber base, which is not only good for the company’s long-term growth prospects, but also competitive position when it comes to acquiring content. This also applies to the company’s recent attempts to crack down on password sharing, and struggles in the developing world: an advertising-based tier is a much more accessible alternative.
 Second, advertising would make it easier for Netflix to continue to raise prices: on one hand, it would provide an alternative for marginal customers who might otherwise churn, and on the other hand, it would create a new benefit for those willing to pay (i.e. no advertising for the highest tiers).
 Third, advertising is a natural fit for the jobs Netflix does. Sure, customers enjoy watching shows without ads — and again, they can continue to pay for that — but filler TV, which Netflix also specializes in, is just as easily filled with ads.
 Above all, though, is the fact that advertising is a great opportunity that aligns with Netflix’s business: while the company once won with a differentiated user experience worth paying for, today Netflix demands scarce attention because of its investment in unique content. That attention can be sold, and should be, particularly as it increases Netflix’s ability to invest in more unique content, and/or charge higher prices to its user base.
 This, I will note, is an about face for me; I’ve long been skeptical that Netflix would ever sell advertising, or that they should. The former may still be warranted, particularly in light of Netflix’s gaming initiative. This feels like solipsism: Netflix’s executives think a lot about their business model, so they are looking for growth opportunities that seem to leverage said business model; I’m not convinced, though, that customers appreciate or care about the differentiation that Netflix claims to be leveraging in gaming, whereas they would appreciate lower prices for streaming, and already have the expectation for ads on TV.
 Meanwhile, subscriber growth has stalled, even as the advertising market has proven to be much larger than even Google or Facebook can cover. Moreover, the post-ATT world is freeing up more money for the sort of top-of-funnel advertising that would probably be the norm on a Netflix advertising service. In short, the opportunity is there, the product is right, and the business need is pressing in a way it wasn’t previously.
 Of course this would be a lot of work, and a big shift in Netflix’s well-defined value proposition; Netflix, though, has made big shifts before: the entire reason why advertising is a possibility is because Netflix is a streamer, not a DVD mailer. In that view a new (additional) business model is just another rung on Netflix’s ladder.
 I wrote a follow-up to this Article in this Daily Update.</content>
     </entry>
     <entry>
       <title>Vibe Check №15</title>
         <link href="https://daverupert.com/2022/04/vibe-check-15/"/>
       <updated>2022-04-02T20:31:00.000Z</updated>
       <content type="text">March came and gone. I chased fitness. We went on Spring Break. Growing a business. Baseball games. Failed on blogging. Here is a retelling of the vibes.
 Closing my rings
 This month I decided I would undergo a challenge to close my rings on my Apple Watch every single day for the whole month and I succeeded!1
 
 
 
 
 It’s safe to say this hasn’t ever happened. My regiment was a mixture of walks, special Apple Fitness+ celebrity-guided walks (Dolly and Malala), and a heap of Apple Fitness+ workouts. As far as workout programs go, I appreciate the body diversity in Apple Fitness+ — it’s not a bunch of yoga thin hard bodies yelling at me to touch my toes to my ears, it’s real people.
 Did it make a difference in my weight? No. Not at all. In fact, I weigh a half pound more. My consumption habits were the same, I did nothing extravagant or different. Not much to glean except that I believe this experiment showed me that my weight is not a function of exercise, even 10x my normal exercise doesn’t make a difference.
 The best thing about the last month is that I’ve carved a bit of a routine into my life. Made some space for me. I have a good sense of what it takes to close my rings each day and it feels sustainable. I feel more fit, even though no one else would know it from the outside.
 The late flight to Phoenix
 For Spring Break we decided to visit grandparents, aunts, uncles, and cousins in Phoenix. Hands down one of the worst flights of my life; a woman too drunk to fly, police to escort her, and her husband (also drunk) deciding to talk back to the police. Per FAA protocol, we all had to deplane to resolve the situation. Another lesson in how much the pandemic has effected our ability to function in society.
 But we got there, sat for 45 minutes on the tarmac for a gate, and arrived at grandma’s house in Phoenix at 3 AM Austin time. Wrecked.
 The kids had a great visit though. Grandparents and cousins work wonders. Cousins self-propel each other. Pools, sunshine, zoo trips, and more. My wife and I could go on long walks, look at campers, and have a couple margaritas.
 The “time off” wasn’t as restful as I needed, due in part to some lingering obligations hanging over my head…
 Work, talks, and workshops
 It’s a busy but important time at Luro. Onboarding new customers but also figuring out next steps in the life of our startup and evaluating a pretty big architectural change (dedicated managed instances → complete multi-tenant application). Lots of discussions and ambient stress. I fried my brain one day doing a cost-benefit analysis on Rails versus Nuxt.
 Adding on to my normal workload, I prepared a talk for An Event Apart and a workshop for Frontend Masters workshop, both on Web Components. It’s been a big year for web components and I’m happy to continue talking about them.
 I enjoy presenting. I feel like it fits within my natural range of extroversion. Talks and workshops are a lot of work though piled on a busy schedule. After years of practice, I feel like I do a decent job estimating the work required and am able to plan accordingly.
 Candidly, the one thing I don’t predict well is the cloud of stress that looms over me a month or so before an event. It’s a looming feeling of excess obligations. As much as I love doing these presentations, they add a lot to a busy schedule and even busier mind. But that impulse to say “no” to future talks conflicts with my impulse to amortize my talks as much as possible.
 One day without caffeine
 I inadvertently attempted a caffeine fast. 10am and I hadn’t had any coffee, so I decided to try it. It did not go well. I was in bed at 4pm with a cursed headache. I went to bed with a headache. I woke up with a headache. I was grumpy, irritable, and almost deleted my whole Twitter account. It took me by surprise how awful my body reacted.
 Apparently 50% of people suffer caffeine withdrawal headaches and it sort of doesn’t matter how much caffeine you have (over 200mg/day). I’m in the unlucky half. I think it’s worthwhile to break chains of addiction when they present themselves, so I may attempt a fast again but may ween myself with half-caf for a bit first before I try again.
 
 💪 Fitness
 
 Closed my rings 31 times.
 Didn’t do much pickleball.
 
 
 🧶 Crafting
 
 I made a Gundam Ground Combat Urban Type and it may be my most favorite one yet. Here’s a picture
 
 
 📖 Reading - 4 books + 2 graphic novels
 
 Run - John Lewis’s retelling of America after the Civil Rights Act and his falling out with SNCC.
 The City of Ember - My son read The City of Ember for a book report so I read the graphic novel and it’s good. I bet the book is way better tho.
 The Four - Scott Galloway’s look at Amazon, Apple , Facebook, and Google. I liked the first half of looking at how each of the companies operate, but lost the thread in the later chapters where the personal anecdotes started coming in.
 Expressive Design Systems - Yesenia Perez-Cruz expertly outlines what makes a good, effective design system, and also how you can add life to a design system by allowing room for customization (or “seasons”). Great read.
 What White People Can Do Next - This one might be controversial, but sort of comes to the conclusion allyship is not the answer 😱  - the crux of the argument is that coalition building is a lot more effective and doesn’t reinforce white savior or victim tropes. I enjoyed the book and I’d be curious to hear what others thought.
 Cubed - This book is so good! It’s the history of the modern workplace. I’m enjoying this book so much it’s taking twice as long to get through because I’m taking notes.
 
 
 📺 TV, Movies, and Anime
 
 Ranking of Kings (Crunchyroll) - The story of young prince Bojji inheriting the throne of his warrior father… but Bojji is deaf, mute, and can’t swing a sword. How will he ever become king?
 
 
 👾 Video games
 
 Olli Olli World
 
 
 🎙 Podcasts and YouTubes
 
 Spacing in Design Systems
 ShopTalkShow has a vanity URL on YouTube now: https://www.youtube.com/shoptalkshow
 
 
 📝 Blogging - 2 posts this month. I still have lots of drafts, but I got sideswiped by other work obligations.
 
 Goodbye, Big Freeda - a eulogy for my old camper
 Different people, different ways - another songcatching post about an old Buffy Sainte Marie song from Sesame Street that we use in parenting our kids.
 I’ve also decided to redesign my blog at some point. Will need to be after the workshops and talks.
 
 
 
 
 
 
 Technically there are two days where I was one tick off on my stand goal (Mar 27) and one minute off on my workout goal (Mar 29). I’m not going to beat myself up about it. ↩
 
 
 </content>
     </entry>
     <entry>
       <title>Very Like a Whale</title>
         <link href="https://logicmag.io/clouds/very-like-a-whale"/>
       <updated>2022-04-01T22:53:49.000Z</updated>
       <content type="text">1/
 What’s in a cloud? 
 Writers have long used clouds as metaphors for metaphor-making. In Hamlet, Hamlet messes with his girlfriend’s dad, the courtier Polonius, by pointing out the different shapes he sees: 
 H: Do you see yonder cloud that’s almost in the shape of a camel?P: By th’ mass, and ’tis like a camel indeed.H: Methinks it is like a weasel.P: It is backed like a weasel.H: Or like a whale?P: Very like a whale.
 As always, the play is playing with perception. Can a person ever know what’s real? Is that a reasonable thing to even care about? The point is also that Polonius cannot be trusted; he is a sycophant. Hamlet will stab him dead two scenes later.
 2/
 Clouds are ambiguous. Liquid solid. Ethereal material. As Shakespeare’s Mark Antony says, their most striking images “mock our eyes with air.”
 The writers in this issue think about clouds of various shapes. Several address the cloud—that is, the global archipelago of warehouses that collectively coordinate the world’s computing power.  
 Today, the press tends to talk about the cloud in imperial terms. It is the Valhalla of Amazon, Microsoft, Google, Alibaba; it is strongman leaders demanding data sovereignty for their country of a billion plus people. But those who have worked in data centers for decades tell a different story. The cloud, they point out, came into many firms from the bottom up, at the behest of engineers, not management. Yet even those who were there on the ground floor, who worked with “bare metal” and knew what it felt like to cut your fingers on the rails that held racks of servers in place (“a badge of honor”), could not anticipate the new forms of power that the cloud would bring. 
 The advent of the cloud did not only create the conditions for the concentration of unprecedented amounts of wealth and information in the hands of a few firms. It also changed how rank-and-file engineers worked. The so-called Agile revolution started before the cloud took off. But it gained speed with it. Like previous developments in the computing industry, Agile combined counterculture and cyberculture; it was ostensibly rebellious, but committed rebels to sprinting toward corporate goals.  
 Other writers in this issue take, literally, to the sky. Aloft, clouds remain difficult to assess. Even as political consensus in favor of trying to reach “net zero” grows, climate scientists will struggle to measure emissions and to compute what it would take to offset them. One trick of the cloud metaphor is to suggest that recording and computation are everywhere, and yet hazy points remain.  
 Obscurities remain on clear days. Taking sprawling aerial surveillance programs in their sights, other contributors argue that the obscurity of individuals and organizations who have amassed the power to see everything must be dispelled. They share strategies for gaining information about government and corporate plans. Transparency is always a struggle.
 Clouds are part of the weather, and another sense of weather is the Romance language one: le temps, el tiempo, il tempo. Time itself. This issue explores other temporalities, and the bodies they are tied to. One writer celebrates “crip time” as an alternative to “flow,” which is not about fulfilling work discipline, but rather about learning to be stuck. Another writer imagines other times altogether. This issue contains the first speculative fiction we have published.
 3/
 Throughout human history, clouds have acted as omens. Aeromancy is the art of divining the future from the sky. This issue considers possible futures, without being too predictive; the sky is a complex canvas, and its patterns shift quickly with the wind.
 
 
 </content>
     </entry>
     <entry>
       <title>Different people, different ways</title>
         <link href="https://daverupert.com/2022/03/different-people-different-ways/"/>
       <updated>2022-03-31T15:32:00.000Z</updated>
       <content type="text">
 There’s a bit of a setup to this song: Big Bird is pouting in his nest, jealous of the new baby Cody which gets a lot of attention. The whole street is concerned about Big Bird. If you ask me, Big Bird is being a bit of a bitch about it all but clearly he’s going through some strong feelings. The ever-graceful Buffy Sainte Marie1 comes in to share a song with him about the different kinds of love and attention you can give people based on their needs, with the refrain “different people, different ways.” Big Bird seems placated.
 “Different people, different ways” has been a useful parenting tool. It helps us resolve unfairness in moments of sibling rivalry, but also helps in explaining differences of the human condition. Why do they use a wheelchair? Why are they short? Why do they talk like that? Why does that kid get to play with toys in the classroom and I don’t? … Different people, different ways.
 A simple enough refrain that if I start it, my kids will finish.
 Ask me in twenty years if it worked, but I think they already get that “fairness” can take different shapes when it’s centered around meeting different people’s different needs.
 I love him &#x27;cause he&#x27;s tiny and small
 I love you because you&#x27;re yellow and tall
 Tiny and small love
 Yellow and tall love
 Everyone loves through all their days
 Different people, different ways
 
 I love him &#x27;cause he has to be fed
 I love you because you make your own bed
 He has to be fed love
 You make your own bed love
 Different people, different ways
 
 I love him &#x27;cause he&#x27;s small and helpless
 He has to be washed and fed each day
 I love you cause you&#x27;re big and helpful
 You can read, sing, and play
 
 I love him &#x27;cause he&#x27;s sugar and spice
 I love you because you&#x27;re awfully nice
 Sugar and spice love
 Awfully nice love
 Tiny and small love
 Yellow and tall love
 Everyone loves through all their days
 Different people, different ways
 
 
 
 
 Buffy Sainte Marie once once breastfed Cody on Sesame Street in a time when breastfeeding, let alone public breastfeeding, was socially unpopular. Pretty cool if you ask me. ↩
 
 
 </content>
     </entry>
     <entry>
       <title>Performance Tool in Firefox DevTools Reloaded</title>
         <link href="https://hacks.mozilla.org/2022/03/performance-tool-in-firefox-devtools-reloaded/"/>
       <updated>2022-03-30T14:59:00.000Z</updated>
       <content type="text">In Firefox 98, we’re shipping a new version of the existing Performance panel. This panel is now based on the Firefox profiler tool that can be used to capture a performance profile for a web page, inspect visualized performance data and analyze it to identify slow areas.
 The icing on the cake of this already extremely powerful tool is that you can upload collected profile data with a single click and share the resulting link with your teammates (or anyone really). This makes it easier to collaborate on performance issues, especially in a distributed work environment.
 The new Performance panel is available in Firefox DevTools Toolbox by default and can be opened by Shift+F5 key shortcut.
 Usage
 The only thing the user needs to do to start profiling is clicking on the big blue button – Start recording. Check out the screenshot below.
 
 As indicated by the onboarding message at the top of the new panel the previous profiler will be available for some time and eventually removed entirely.
 When profiling is started (i.e. the profiler is gathering performance data) the user can see two more buttons:
 
 
 Capture recording – Stop recording, get what’s been collected so far and visualize it
 Cancel recording – Stop recording and throw away all collected data
 
 When the user clicks on Capture recording all collected data are visualized in a new tab. You should see something like the following:
 
 The inspection capabilities of the UI are powerful and let the user inspect every bit of the performance data. You might want to follow this detailed UI Tour presentation created by the Performance team at Mozilla to learn more about all available features.
 Customization
 There are many options that can be used to customize how and what performance data should be collected to optimize specific use cases (see also the Edit Settings… link at the bottom of the panel).
 To make customization easier some presets are available and the Web Developer preset is selected by default. The profiler can be also used for profiling Firefox itself and Mozilla is extensively using it to make Firefox fast for millions of its users. The WebDeveloper preset is intended for profiling standard web pages and the rest is for profiling Firefox.
 The Profiler can be also used directly from the Firefox toolbar without the DevTools Toolbox being opened. The Profiler button isn’t visible in the toolbar by default, but you can enable it by loading https://profiler.firefox.com/ and clicking on the “Enable Firefox Profiler Menu Button” on the page.
 This is what the button looks like in the Firefox toolbar.
 
 As you can see from the screenshot above the UI is almost exactly the same (compared to the DevTools Performance panel).
 Sharing Data
 Collected performance data can be shared publicly. This is one of the most powerful features of the profiler since it allows the user to upload data to the Firefox Profiler online storage. Before uploading a profile, you can select the data that you want to include, and what you don’t want to include to avoid leaking personal data. The profile link can then be shared in online chats, emails, and bug reports so other people can see and investigate a specific case.
 This is great for team collaboration and that’s something Firefox developers have been doing for years to work on performance. The profile can also be saved as a file on a local machine and imported later from https://profiler.firefox.com/
 
 There are many more powerful features available and you can learn more about them in the extensive documentation. And of course, just like Firefox itself, the profiler tool is an open source project and you might want to contribute to it.
 There is also a great case study on using the profiler to identify performance issues.
 More is coming to DevTools, so stay tuned!
 The post Performance Tool in Firefox DevTools Reloaded appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>An Interview with Nvidia CEO Jensen Huang about Manufacturing Intelligence</title>
         <link href="https://stratechery.com/2022/an-interview-with-nvidia-ceo-jensen-huang-about-manufacturing-intelligence-2/"/>
       <updated>2022-03-28T09:09:01.000Z</updated>
       <content type="text">It took a few moments to realize what was striking about the opening video for Nvidia’s GTC conference: the complete absence of humans.
 
 That the video ended with Jensen Huang, the founder and CEO of Nvidia, is the exception that accentuates the takeaway. On the one hand, the theme of Huang’s keynote was the idea of AI creating AI via machine learning; he called the idea “intelligence manufacting”:
 
 
   None of these capabilities were remotely possible a decade ago. Accelerated computing, at data center scale, and combined with machine learning, has sped up computing by a million-x. Accelerated computing has enabled revolutionary AI models like the transformer, and made self-supervised learning possible. AI has fundamentally changed what software can make, and how you make software. Companies are processing and refining their data, making AI software, becoming intelligence manufacturers. Their data centers are becoming AI factories. The first wave of AI learned perception and inference, like recognizing images, understanding speech, recommending a video, or an item to buy. The next wave of AI is robotics: AI planning actions. Digital robots, avatars, and physical robots will perceive, plan, and act, and just as AI frameworks like TensorFlow and PyTorch have become integral to AI software, Omniverse will be essential to making robotics software. Omniverse will enable the next wave of AI.
   We will talk about the next million-x, and other dynamics shaping our industry, this GTC. Over the past decade, Nvidia-accelerated computing delivered a million-x speed-up in AI, and started the modern AI revolution. Now AI will revolutionize all industries. The CUDA libraries, the Nvidia SDKs, are at the heart of accelerated computing. With each new SDK, new science, new applications, and new industries can tap into the power of Nvidia computing. These SDKs tackle the immense complexity at the intersection of computing, algorithms, and science. The compound effect of Nvidia’s full-stack approach resulted in a million-x speed-up. Today, Nvidia accelerates millions of developers, and tens of thousands of companies and startups. GTC is for all of you.
 
 The core idea behind machine learning is that computers, presented with massive amounts of data, can extract insights and ideas from that data that no human ever could; to put it another way, the development of not just insights but, going forward, software itself, is an emergent process. Nvidia’s role is making massively parallel computing platforms that do the calculations necessary for this emergent process far more quickly than was ever possible with general purpose computing platforms like those undergirding the PC or smartphone.
 What is so striking about Nvidia generally and Huang in particular, though, is the extent to which this capability is the result of the precise opposite of an emergent process: Nvidia the company feels like a deliberate design, nearly 29 years in the making. The company started accelerating defined graphical functions, then invented the shader, which made it possible to program the hardware doing that acceleration. This new approach to processing, though, required new tools, so Nvidia invented them, and has been building on their fully integrated stack ever since.
 The deliberateness of Nvidia’s vision is one of the core themes I explored in this interview with Huang recorded shortly after his GTC keynote. We also touch on Huang’s background, including immigrating to the United States as a child, Nvidia’s failed ARM acquisition, and more. One particularly striking takeaway for me came at the end of the interview, where Huang said:
 
   Intelligence is the ability to recognize patterns, recognize relationships, reason about it and make a prediction or plan an action. That’s what intelligence is. It has nothing to do with general intelligence, intelligence is just solving problems. We now have the ability to write software, we now have the ability to partner with computers to write software, that can solve many types of intelligence, make many types of predictions at scales and at levels that no humans can.
   For example, we know that there are a trillion things on the Internet and the number things on the Internet is large and expanding incredibly fast, and yet we have this little tiny personal computer called a phone, how do we possibly figure out of the trillion things in the internet what we want to see on our little tiny phone? Well, there needs to be a filter in between, what people call the personalized internet, but basically an AI, a recommender system. A recommender that figures out based on the nature of the content, the characteristics of the content, the features of the content, based on your implicit and your explicit and implicit preferences, find a way through all of that to predict what you would like to see. I mean, that’s a miracle! That’s really quite a miracle to be able to do that at scale for everything from movies and books and music and news and videos and you name it, products and things like that. To be able to predict what Ben would want to see, predict what you would want to click on, predict what is useful to you. I’m talking about things that are consumer oriented stuff, but in the future it’ll be predict what is the best financial strategy for you, predict what is the best medical therapy for you, predict what is the best health regimen for you, what’s the best vacation plan for you. All of these things are going to be possible with AI.
 
 As I note in the interview, this should ring a bell for Stratechery readers: what Huang is describing is the computing functionality that undergirds Aggregation Theory, wherein value in a world of abundance accrues to those entities geared towards discovery and providing means of navigating this world that is fundamentally disconnected from the constraints of physical goods and geography. Nvidia’s role in this world is to provide the hardware capability for Aggregation, to be the Intel to Aggregators’ Windows. That, needless to say, is an attractive position to be; like many such attractive positions, it is one that was built not in months or years, but decades.
 Read the full interview with Huang here.</content>
     </entry>
     <entry>
       <title>Introducing MDN Plus: Make MDN your own</title>
         <link href="https://hacks.mozilla.org/2022/03/introducing-mdn-plus-make-mdn-your-own/"/>
       <updated>2022-03-24T16:00:29.000Z</updated>
       <content type="text">MDN is one of the most trusted resources for information about web standards, code samples, tools, and everything you need as a developer to create websites. In 2015, we explored how we could expand beyond documentation to provide a structured learning experience. Our first foray was the Learning Area, with the goal of providing a useful addition to the regular MDN reference and guide material. In 2020, we added the first Front-end developer learning pathway. We saw a lot of interest and engagement from users, and the learning area contributed to about 10% of MDN’s monthly web traffic. These two initiatives were the start of our exploration into how we could offer more learning resources to our community. Today, we are launching MDN Plus, our first step to providing a personalized and more powerful experience while continuing to invest in our always free and open webdocs.
 
 
 Build your own MDN Experience with MDN Plus
 In 2020 and 2021 we surveyed over 60,000 MDN users and learned that many of the respondents  wanted a customized MDN experience. They wanted to organize MDN’s vast library in a way that worked for them. For today’s premium subscription service, MDN Plus, we are releasing three new features that begin to address this need: Notifications, Collections and MDN Offline. More details about the features are listed below:
 
 Notifications: Technology is ever changing, and we know how important it is to stay on top of the latest updates and developments. From tutorial pages to API references, you can now get notifications for the latest developments on MDN. When you follow a page, you’ll get notified when the documentation changes, CSS features launch, and APIs ship. Now, you can get a notification for significant events relating to the pages you want to follow. Read more about it here.
 
 
 
 Collections: Find what you need fast with our new collections feature. Not only can you pick the MDN articles you want to save, we also automatically save the pages you visit frequently. Collections help you quickly access the articles that matter the most to you and your work. Read more about it here.
 
 
 
 MDN offline: Sometimes you need to access MDN but don’t have an internet connection. MDN offline leverages a Progressive Web Application (PWA) to give you access to MDN Web Docs even when you lack internet access so you can continue your work without any interruptions. Plus, with MDN offline you can have a faster experience while saving data. Read more about it here.
 
 
 Today, MDN Plus is available in the US and Canada. In the coming months, we will expand to other countries including France, Germany, Italy, Spain, Belgium, Austria, the Netherlands, Ireland, United Kingdom, Switzerland, Malaysia, New Zealand and Singapore. 
 Find the right MDN Plus plan for you
 MDN is part of the daily life of millions of web developers. For many of us MDN helped with getting that first job or helped land a promotion. During our research we found many of these users, users who felt so much value from MDN that they wanted to contribute financially. We were both delighted and humbled by this feedback. To provide folks with a few options, we are launching MDN Plus with three plans including a supporter plan for those that want to spend a little extra. Here are the details of those plans:
 
 MDN Core: For those who want to do a test drive before purchasing a plan, we created an option that lets you try a limited version for free.  
 MDN Plus 5:  Offers unlimited access to notifications, collections, and MDN offline with new features added all the time. $5 a month or an annual subscription of $50.
 MDN Supporter 10:  For MDN’s loyal supporters the supporter plan gives you everything under MDN Plus 5 plus early access to new features and a direct feedback channel to  the MDN team. It’s $10 a month or $100 for an annual subscription.  
 
 Additionally, we will offer a 20% discount if you subscribe to one of the annual subscription plans.
 We invite you to try the free trial version or sign up today for a subscription plan that’s right for you. MDN Plus is only available in selected countries at this time.
  The post Introducing MDN Plus: Make MDN your own appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>Mozilla and Open Web Docs working together on MDN</title>
         <link href="https://hacks.mozilla.org/2022/03/mozilla-and-open-web-docs-working-together-on-mdn/"/>
       <updated>2022-03-17T14:07:34.000Z</updated>
       <content type="text">For both MDN and Open Web Docs (OWD), transparency is paramount to our missions. With the upcoming launch of MDN Plus, we believe it’s a good time to talk about how our two organizations work together and if there is a financial relationship between us. Here is an overview of how our missions overlap, how they differ, and how a premium subscription service fits all this.
 History of our collaboration
 MDN and Open Web Docs began working together after the creation of Open Web Docs in 2021. Our organizations were born out of the same ethos, and we constantly collaborate on MDN content, contributing to different parts of MDN and even teaming up for shared projects like the conversion to Markdown. We meet on a weekly basis to discuss content strategies and maintain an open dialogue on our respective roadmaps.
 MDN and Open Web Docs are different organizations; while our missions and goals frequently overlap, our work is not identical. Open Web Docs is an open collective, with a mission to contribute content to open source projects that are considered important for the future of the Web. MDN is currently the most significant project that Open Web Docs contributes to.
 Separate funding streams, division of labor
 Mozilla and Open Web Docs collaborate closely on sustaining the Web Docs part of MDN. The Web Docs part is and will remain free and accessible to all. Each organization shoulders part of the costs of this labor, from our distinct budgets and revenue sources.
 
 Mozilla covers the cost of infrastructure, development and maintenance of the MDN platform including a team of engineers and its own team of dedicated writers.
 Open Web Docs receives donations from companies like Google, Microsoft, Meta, Coil and others, and from private individuals. These donations pay for Technical Writing staff and help finance Open Web Docs projects. None of the donations that Open Web Docs receive go to MDN or Mozilla; rather they pay for a team of writers to contribute to MDN. 
 
 Transparency and dialogue but independent decision-making
 Mozilla and OWD have an open dialogue on content related to MDN. Mozilla sits on the Open Web Docs’ Steering Committee, sharing expertise and experience but does not currently sit on the Open Web Docs’ Governing Committee. Mozilla does not provide direct financial support to Open Web Docs and does not participate in making decisions about Open Web Docs’ overall direction, objectives, hiring and budgeting.
 MDN Plus: How does it fit into the big picture?
 MDN Plus is a new premium subscription service by Mozilla that allows users to customize their MDN experience. 
 As with so much of our work, our organizations engaged in a transparent dialogue regarding MDN Plus. When requested, Open Web Docs has provided Mozilla with feedback, but it has not been a part of the development of MDN Plus. The resources Open Web Docs has are used only to improve the free offering of MDN. 
 The existence of a new subscription model will not detract from MDN’s current free Web Docs offering in any way. The current experience of accessing web documentation will not change for users who do not wish to sign up for a premium subscription. 
 Mozilla’s goal with MDN Plus is to help ensure that MDN’s open source content continues to be supported into the future. While Mozilla has incorporated its partners’ feedback into their vision for the product, MDN Plus has been built only with Mozilla resources. Any revenue generated by MDN Plus will stay within Mozilla. Mozilla is looking into ways to reinvest some of these additional funds into open source projects contributing to MDN but it is still in early stages.
 A subscription to MDN Plus gives paying subscribers extra MDN features provided by Mozilla while a donation to Open Web Docs goes to funding writers creating content on MDN Web Docs, and potentially elsewhere. Work produced via OWD will always be publicly available and accessible to all. 
 Open Web Docs and Mozilla will continue to work closely together on MDN for the best possible web platform documentation for everyone!
 Thanks for your continuing feedback and support.
  
 
  
 The post Mozilla and Open Web Docs working together on MDN appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 297: Carbon footprint of ads, JS sorting, PHP the right way and using what we have.</title>
         <link href="https://wdrl.info/archive/297"/>
       <updated>2022-03-17T14:00:00.000Z</updated>
       <content type="text">Hey,
 
 from time to time my intro isn’t entirely web-focused. I always struggle a bit when this is the case because usually it’s about sensitive topics which aren’t easy to discuss in public but this is a personal newsletter. And you can always skip the next paragraph. 
 personal intro
 When I sent out the last edition, we were in a pandemic. Now, there’s also a new war between multiple political forces, creating fear and uncertainty around the people of the world. As if the pandemic situation wouldn’t be stressful enough, this adds up to anxiety, feelings of helplessness and »what the hell is happening in the world?« thoughts. Things aren’t normal since two years now and I the only way I currently see is people finding an arrangement with their inner self to deal with this. Which isn’t really a solution, more a compromise. I wish you all out there a lot of strength, passion, empathy and enough energy to find your own path and solution, to build your own opinion on things. If you’re directly affected of the war situation, I send you all the best wishes and my hope is for a peaceful together in near future again. At home, we have new neighbours since two weeks — a mother with her child. We try our best to keep them in a normal life, playing and talking with them while the husband/father is in danger and far away from them.
 web intro
 Speaking about the web we have much nicer topics to talk about, such as new iOS/Safari 15.4 with image lazy loading, Chrome and Firefox version 100, new Roboto Serif font and how to get better at achieving your goals. But if we don’t and make an exception, even then we have a cooler option than being annoyed about ourselves and instead accept the situation. And finally I’ve been surprised this morning to read that CSS-Tricks, one of my all time favorite sites is now part of Digital Ocean and I had to give Chris who built this site a public shoutout and thank you for all what he’s done so far for the web dev community.
 News
 
 	Both Chrome and Firefox will soon reach version 100. First congrats for that. But if you rely on user agent sniffing better check your code as it may break with the first three-digit version out there.
 	iOS 15.4 has finally added icon support in the manifest, Web Push, AR, and VR experiments, being a big improvement for web apps on Apple devices. Here’s what’s new in Safari 15.4, including the &lt;dialog&gt; element :has() selector, Cascade Layers, new viewport units, CSP Level 3 and more.
 	With Safari 15.4 we’ll get accent-color everywhere and so we can style our form inputs in a nice way cross-browser.
 	There is a new design for one of the best – or maybe the best – websites on frontend developer resources. Make sure to check out the MDN Web Docs.
 
 Generic
 
 	PHP: The Right Way is an easy-to-read, quick reference for modern coding standards in PHP, trying to fight all the outdated and partially wrong solutions found on the web.
 	When creating REST APIs we can make so many mistakes. Therefore it is a good idea to learn some basics. Ronald Blüthl provides a good starting point to learn about REST API design.
 
 UI/UX
 
 	You know the Roboto font family? Here’s great news: In addition to the sans-serif there’s now the serif variant of the font family and it’s pretty cool.
 	A well-written headline, a perfectly descriptive label, or the accurate text on a button makes all the difference. Andy Hertzfeld wrote a fun little story about choosing proper text in a user interface.
 
 Tooling
 
 	There are good reasons (e. g. security and maintainability) to keep your programming languages, operating systems, frameworks, or libraries up to date. We can check the end-of-life dates of software on a new well-documented website.
 
 Web Performance
 
 	What is the environmental impact of visiting a website and what part do advertising, and analytics, play? It may be up to 70%. Here’s this interesting research saying using ad blockers could save a lot of CO2.
 
 JavaScript
 
 	Rupert McKay shares common ways to do sorting in JavaScript.
 	Here’s the guide to upgrade your apps to React v18 which is soon going to be released.
 
 CSS
 
 	Did you know you can set up wrapper width with min() and margin-inline much cleaner these days than we’re used to? See the link for the code example.
 
 Work &amp; Life
 
 	The software development industry is growing and growing. For new software developers, it is helpful that experienced people share their learnings. Ivan Stoev teaches us seven lessons he has learned as a software engineer.
 
 Go beyond…
 
 	Adam has some nice ideas on how to manage your personal goal setting with a tiny “framework”.
 	On the topic of personal goal setting I found this interesting thesis that you should »never set a goal that a dead person can do better than you«. It’s about setting realistic goals that we can really achieve, not like “stop eating chocolate” which is unrealistic in so many points.
 	So we’ve learned that we shouldn’t just buy things and we should aim for minimalism in our lives. But well, what if we still bought something in an impulse? Stop being annoyed with yourself and make the best out of it: Use the thing now that you have it and enjoy it.
 
 
 If you like this newsletter, you can contribute here. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Ontology of God and Other Poems</title>
         <link href="https://logicmag.io/beacons/ontology-of-god-and-other-poems"/>
       <updated>2022-03-15T17:41:14.000Z</updated>
       <content type="text">Ontology of God
 Big Mike says I read that dogs
 don’t have a sense of time       a minute 
 is like an hour an hour like a day
              a day like a minute. The continuity
 
  
 
 is skewed &amp; time is placed without
 thought into various boxes. I think 
 what it must be like to be
 a dog because                 yes       I be 
 
  
 
 with my dogs in this massive cage
              trying to exhaust every thread
 of thought surrounding time. Maybe
 that’s why we say things like Oh, [name]? 
 
  
 
 Yeah that’s my dog &amp; use dog
 as a placeholder for when we secret
 the names of those involved
              in the robbery stabbing         extortion. 
 
  
 
               We want to shake off the slough
 of our numbered bodies            hieroglyphs 
               in our skin sluiced onto the floor. 
 We want to live in a space 
 
  
 
 free of calendars &amp; clocks &amp; the minutes
              we must share              but the high
 fruits are not ready to fall from this
 life. Not yet. Ciph &amp; Civ claim God 
 
  
 
 body &amp; who am I to tell them otherwise
                when we all want to claim
 master                            key to lock                    silo
 to grain &amp; again own the con-
 
  
 
 tents of our own dufflebags &amp; spoken
 languages without restraint. Still
 when I call Doc he says What’s good
 God? &amp; tells me about my god-
 
  
 
 daughter           her mews &amp; her small body
 taking hold of the world around
              her. When I buried my faith
 I didn’t dig deep                        no           I didn’t 
 
  
 
 &amp; from the dirt sprung forth a woman
 I asked her                      I said What is your name?
 &amp; she just smiled past me      which left
 me confused. When I woke up
 
  
 
             I went to commune with the poodle
 down the hall who quietly trotted 
 around while her master
 played cards. 
 
 Bodies of Water
 There are no empty vessels when everything has proper weight.
  — James Wood 
 
  
 
 Sitting in the substance abuse class we talk
 about moderation                     the therapist
                 Elissa loosening the clenched jaws each man 
 
  
 
 has labored for years to claim as his own 
                           opening the floor to the stories we 
 claim. The watch-words are criminal thinking 
 
  
 
 &amp; this is commonplace. This is everyday 
 in the joint                       correction. That’s what this is 
              correction                           correction in the depart- 
 
  
 
 ment of closely governed boxes.                  Bodies 
 of water are different                 no longer 
 signified in themselves                     but              these            bursting 
 
  
 
 symbols                       overflowing with money &amp; 
 drugs &amp; the women we see in photo- 
 copied porn. They become our desires 
 
  
 
 transposed amongst the pasts we had assumed 
 to be ours                       stories we lived in real-time 
              yet are read fast by this institution 
 
  
 
 as empty glasses                         vessels to be filled 
 &amp; tossed long into whatever ocean 
 borders the nation with the most bullets 
 
  
 
 &amp; the most mechanisms to keep us 
 from loading those bullets. We are thirsty 
 for any other ocean                 for bodies 
 
  
 
 of water                      not                       weighted with the remnants
 of these floating cages                            of correction
              of anything unseen                               but still policed. 
 
 Intro
 I don’t know how I ended
 up         here       yeah actually I 
 know. I called it              I made myself 
 
  
 
 a dumb prophet &amp; cuffed my own 
 wrists like a God who creates 
 &amp; creates         &amp; creates         too 
 
  
 
 many worlds to wave his hand                     or 
 whatever he believes he’s doing 
              over &amp; grant the prayers 
 
  
 
 of his reckless children. He gets 
 mad because he gets shown up.           He 
 fails at the feet of his 
 
  
 
 creations. I know how 
 I got here.         When I first came 
 down    they tested my criminal- 
 
  
 
 ity by sitting me down 
 in a small room                        an office 
            giving me a battery 
 
  
 
 of statements like If my fam- 
 ily gets hurt I feel the urge 
 to retaliate &amp; some 
 
  
 
 people deserve to be pun- 
 ished (that one I laughed at). I was 
 to answer with agreement                      or 
 
  
 
 strong denial. I must have 
 passed                             my report read Low Prob- 
 ability of Reoffense 
 
  
 
             but a sentence is a sentence 
             &amp; now it’s almost a decade 
 with more to go &amp; all my files 
 
  
 
 in a drawer full of other 
 men’s histories                      so many 
 histories. Do you know 
 
  
 
 the stories       Do you know who 
 I am       Do you understand 
 what I am           Can I tell you 
 
  
 
              I’ll try to sing this broken 
 song    &amp; summon my tribe      ones 
 who will one day carry me 
 
  
 
 home    &amp; damn            damn       I know 
 it’s a moonshot but        maybe 
 you’ll come find me before I lose 
 
  
 
 myself in this jungle. </content>
     </entry>
     <entry>
       <title>Goodbye, Big Freeda</title>
         <link href="https://daverupert.com/2022/03/goodbye-big-freeda/"/>
       <updated>2022-03-08T18:14:00.000Z</updated>
       <content type="text">
 We said goodbye last month to our 2009 Rockwood Freedom popup camper “Big Freeda”. Lots of fond memories in this rustic little camper, but ultimately she cost us about $500 every time we wanted to take it out. Goodbyes are hard, but let us remember the moments we shared:
 
 That time we had a blow out on the way home from Dinosaur Valley.
 That time we were packing up at Pedernales in the rain and the dog escaped off leash but came back covered in coyote shit.
 That time we went to Inks Lake and you wouldn’t pop up.
 That time we went to McKinney falls and you wouldn’t pop up, so we used the backup bolt and sheared it off.
 That time we were coming back from Garner and had two blow outs.
 That time we brought it back from the shop and were trying to show our neighbor the inside of the popup but it only popped up half-way.
 That time we were leaving McKinney and the trailer fell off the hitch.
 That time we missed a trip to Inks Lake because you wouldn’t pop up.
 That time we went to Mustang Island and got eaten alive by mosquitos.
 That time we went to Pedernales and the wheel well fell out.
 That time we hauled you up to Georgetown and you wouldn’t pop up.
 
 Sweet memories of camper ownership, each of them. For real, I cherish every one. Every problem was an “Oh shit” moment, a challenge, that our family had to respond to and figure out. Put outside of our comfort zone. In some ways that’s what camping is: endless fighting against the elements or your equipment. While not “fun”, I wouldn’t trade the experiences.
 I really wanted to yeet Big Freeda into the ocean, or feed it to a Truckasaurus Rex… but alas, a less dignified sale took place. That’s okay. We’re not done camping as a family, some of our best memories are from camping, but we’re not going to jump right away into a new camper. We want to take some time and figure out the best fit for us going forward.</content>
     </entry>
     <entry>
       <title>Disrupting the Garden Walls</title>
         <link href="https://logicmag.io/beacons/dismantling-the-garden"/>
       <updated>2022-03-08T15:17:28.000Z</updated>
       <content type="text">The first time that I borrowed someone else’s voice I was six years old. After surviving two years in care-less foster homes, I had recently moved in with a family I loved and hoped to be a part of permanently. But, I worried, would I really get to stay? At the time, with no other recognizable means to communicate, I used the trailer for the film Angels in the Outfield to ask my most pressing question: “Dad, when are we gonna be a family?” I knew exactly when the kid popped the question, and when I saw him turn to face his dad, I’d hit STOP on the VCR. It took a number of tries before my parents learned to hear me, but eventually they did and began reassuring me that I was there to stay.
 We soon began creating a number of tools to communicate with each other. Cameras became our main translators. My parents took pictures of everything: activities, places, people, foods. They used the photos to make sure I understood them and to teach me how to make choices by picking photos of what I wanted. I could escape a plate of cauliflower by bringing them a logo for KFC. Each photograph was labelled with the printed word, so I could learn sight words and begin to understand what they were saying. Still, not everything could be captured in a photo. I needed other means of communication to say “stop” when my dad tickled me and I needed to catch my breath, and a way to say “bathroom,” when I needed to pee NOW. And so we developed a basic sign language to convey essential messages. Instead of insisting I join their speaking world, my parents learned these new languages with me.
 When it was time to start regular kindergarten at my neighborhood school, I brought my languages with me. Before long, my classmates and I were all using photos and learning to spell with our fingers. But participating in school also required new technologies. I started using a simple voice-output device like the single-switch BIGmack and Cheap Talk 8 that allowed me to play pre-recorded messages in either my mom’s or dad’s voice to answer questions during class. Because I had learned to communicate in these ways, I was taught to read and write, first with laminated sight words and later with a seventeen dollar label maker from Staples.
 By the time I entered middle school in 2003, written English had become my dominant mode of communication, and I began to develop a public voice. As my language got more sophisticated, so did my devices. The Gemini—a large laptop device with a touchscreen that was a quarter of my weight—allowed me to create a countless number of expressions with any degree of sophistication. In ninth grade, I got the Dynavox, a smaller but similarly heavy equivalent to the Gemini, with a clearer mechanical voice. It had a hard drive prepopulated with thousands of phrases, but they didn’t sound like me. With one finger, I laboriously programmed in as many of my own phrases as I could. 
 For more private conversations, I far preferred the silence of written words. I brought my labeler with me everywhere, using it to converse with friends and process trauma with my therapist. It wasn’t until the tenth grade, when I got my first laptop with text-to-speech software, that I had one lightweight device that allowed me to communicate silently or speak with a digital or recorded voice. 
 These are only some of the different technologies and modes of communication that I have used over the past two decades to gain entry—and be heard—in speech-based society. Speech-generating computers and augmentative and alternative communication (AAC) devices, like the Gemini and Dynavox, have allowed me to contribute to discussions about my people, as well as the world around us. Because they are easy for hearing-based communities to comprehend and sophisticated enough for us to convey complicated ideas in an apparently timely and efficient manner, communication technologies have given me and other alternatively communicating people a voice to be heard by large groups of people over space and time. But those technologies have also worked to define—and confine—us through their economics, their software, and the ways in which they reinforce ableist culture and notions of how communication ought to be structured.
 AAC devices have been around for over seventy years, yet most nonspeaking people in the US still experience widespread segregation in school and throughout their lives. I am one of only two alternatively communicating autistics to be fully mainstreamed from kindergarten through college graduation. One of the problems is accessibility. In our society, the Gemini and the Dynavox cost $12,000 and $9,000, respectively. Even at $500, an iPad with text-to-speech software is still unattainable for many disabled adults on social security, who receive $770 per month to cover all of their living expenses. 
 Even for the relatively small number of us who can access these technologies, we are too often left to rely on prerecorded, preordained messages—to speak only the devices’ language. In our speech-centric, hearing-privileged society, speakers are unquestioningly assumed to be able-bodied, self-reliant individuals whose vocal cords effortlessly produce spoken words and whose ears naturally decode spoken language. AAC devices have been designed to mimic this narrow “ideal.” According to research, 90 percent of what a person says in a given day is made up of repetitive, automatic phrases. AAC devices are populated only with these generic messages. 
 This ableist design assumes speakers know best what others should say and limits the kinds of relationships nontraditional communicators can form. The technology also renders invisible how much effort and time it takes to communicate this way, and it requires nothing of the speaking, hearing world. When we choose not to use AAC devices—with their stiff, generic, confining, and inauthentic prerecorded messages—society usually stops offering us other ways to connect and instead declares us “uneducable,” “untrainable,” “asocial,” “unempathetic” and “willingly walled off from the world.”
 I have come to think of ableism as the cultivated garden of a speech-based society. Many assistive technologies assume the disabled are outsiders, striving to inhabit that cultivated garden. These technologies don’t change the world we live in; they just allow a few of us to climb up and over the garden wall, helping us pass or pose as independent, able-bodied, speakers. Once in the garden, we are seen as validating the status quo, further fortifying the very walls that many of us hope to dismantle with other technologies, other modes of communicating, other ways of being.
 Hearing in Red
 We do not have a ready word for the kind of flexible communication that I practice. Instead of calling someone who uses this kind of flexible communication a “multimodal communicator,” as I choose to do, people like me are labelled “nonspeaking.” People use the verbal/nonverbal binary to render nonspeakers unheard and therefore invisible. In a speech-centric, hearing-privileged world, we are always seen as disabled, lacking. “Success stories,” maybe; “inspirations,” perhaps—but always on others’ terms. What would it mean to build technologies that create opportunities for more multimodal communication and the dense interpersonal connections such communication offers?
 For the past five years, I have been working through a combination of art and activism to imagine what these other modes of communication might be. My starting points are the modes of communication that I used as a child. There was an interdependent flourishing that formed around the technologies my classmates and I used in my early school years. These technologies were more multisensory, more communal, and in a sense more democratic; in pictures, sign language, and tangible sight words, my parents, teachers, friends, and I were all learners, all teachers. Using these alternative, communal languages, others in our classroom considered “at-risk for school failure” found their own pathways to literacy: some learned to spell with their fingers, others learning English as a second language used photographs as helpful translators, and visual learners found that pictures grounded in meaning what fleeting, spoken words could not. 
 A decade and a half later, I had another insight into multimodal communication. In 2016, I was late into my undergraduate thesis in Anthropology, before I realized I was writing an autoethnographic study that completely left out the contributions of people who prefer nonalphabetic languages and that remained largely inaccessible to the majority of nontraditional communicators, who are never taught to read. A high achiever in mainstream education from kindergarten through college, I had come to communicate almost entirely in written English. What, I wondered, might I have lost in the process? I began engaging with the visual artwork of various autistics, eventually compelled by the drawings, paintings, and sculptures of seven artists to write a poetic series. By the last poem, modes of communication had begun to blur as I proudly tell five-year-old impressionist artist Iris Grace that “I’m no longer visual exactly; nor am I verbal. When I type, my fingers speak / with an accent.”
 Around the same time I was finishing my thesis, production was also wrapping up on Deej: Inclusion Shouldn’t be a Lottery, a documentary about my life, which I co-produced and narrated. In the first four minutes of the film, I use a myriad of technologies—my laptop and Dynavox, trees, walls, backpack, other people’s bodies and voices, film, literacy, and a combination of spoken poetry and animation—to maneuver my way past communication and physical barriers at my high school. That opening sequence is something of a model for what I imagine communication might be like in a world that doesn’t privilege speech over other, more interdependent, modes of communication.
 The strongest example of this multimodal way of communicating are the parts of the film when my poetry and the oil-paint animations of the British artist Em Cooper converse with each other. In Cooper’s constantly flowing work, no image is static. Figures emerge briefly and then merge into the background, before re-forming into other figures; everything blurs into everything else. In a dynamic that seemed to reproduce the differences between speech-dominant cultures and more multimodal ways of connecting, other animators who were approached to work on the film took my words too literally, pairing the lines “The ear that hears the cardinal hears in red” with a cartoon cardinal and “The eye that spots the salmon sees in wet” with an animated salmon. Cooper’s brushstrokes, by contrast, were full of color, motion, and texture, occasionally offering a fleeting trace of vines, volcanoes, waves, or flags—metaphors I had used elsewhere in my writing to describe myself or challenge the world we lived in. 
 Cooper and I never thought we were taking everyone in the audience to the same destination; instead, we offered people multiple pathways into a world in which everything is interwoven, where motion, rhythm, pattern, color, sound, and texture freely interact, offering endlessly unfolding possibilities. I recognized, however, that this was a rarefied means of communicating; not something that could always be open to me, let alone anyone else. Four years later, at the outset of the pandemic, I began to ask myself how technology might allow us to create new communities in which diverse bodies, voices, and language might come together, as they had in my collaboration with Cooper, and thrive, much like we all had in kindergarten. 
 Cut off and segregated in my own home, I turned again to poetry and technology to create some alternative pathways: co-teaching multigenerational, global, and intersectional poetry writing courses for beginning poets, and collaborating with three fellow poets based on the artwork of the artist Malcolm Corley, who is also autistic. In both the courses and the collaboration, speakers and alternative communicators came together to make work that challenged the supremacy of speech-based culture. Traces of our entanglements live on in a chapbook, Studies in Brotherly Love. In the introduction, poet Claretta Holsey describes our modes of communication this way: “We crafted poems that speak to us and to our causes: awareness of performative utterances, as communication can and does happen outside of written text, outside of simple speech; embrace of Black vernacular, its rhythm and blues; recognition of the Black family as a model of resilience; respect for nature, which awes and overwhelms; respect for the body, made vulnerable by want.”
 Imagining that technology alone can liberate us is a bit shortsighted and, in some ways, disabling. But, if we imagine the cultivated garden of a speech-based society is the only way of being, then the communication technologies we build will continue to keep us stuck in an inclusion/exclusion binary, in which some beings are seen as disposable and others not.</content>
     </entry>
     <entry>
       <title>Vibe Check №14</title>
         <link href="https://daverupert.com/2022/03/vibe-check-14/"/>
       <updated>2022-03-08T01:21:00.000Z</updated>
       <content type="text">February was a month that felt like ten months; cold, hot, beautiful, windy, busy, stressful, and globally catastrophic. But beyond that last part, it’s been a month of growth, reorganizing, and learning about stress.
 Survived the freeze
 Texas had another big winter storm this month. Thankfully the grid didn’t shut down but after talking to a lot of Austinites, I think we all have a bit of trauma from the freeze last year. A state government unable to keep the lights and heat on. HEB, the grocery store, was a better government. Although spared a catastrophe, Texas — in the form of Gov. Greg Abbot — has found out other ways to be shitty by making it illegal to be a caring parent of a trans child. The trauma continues.
 Sold the pop-up
 
 Over Valentine’s weekend, we tried to go camping with our friends, but the pop-up… didn’t… pop-up. We abandoned it at the campsite overnight and drove it to the RV lot the next day where we sold it for next to nothing. We could have put money into it and potentially sold it for more, but we needed out. It cost us ~$500 in maintenance every time we wanted to take it out and that caused me more stress than relaxation. Lots of memories, but a new chapter in camping begins… but not jumping back into the camper ownership game too soon.
 Gunpla and mechanical keyboard crafting
 I made two crafts this month that I’m pleased with, but also some mixed reviews.
 XXXG-01H Gundam Heavyarms
 
 A Gundam but with extra guns, the XXXG-01H Gundam Heavyarms. This was my first High Grade (HG) 1/144th scale gunpla. It’s a cool model with lots of expression but I was initially a bit disappointed by a wobbly waist and a leg that kept falling off. Between meetings one day I decided to rebuild the legs, found the place where I messed up, and now it’s sturdy-gurdy and much more enjoyable.
 I bought another HG model I plan on doing but am feeling the itch to go up to a Master Grade (MG) 1/100th scale model. In the meantime, I’m going to try my hand at panel lining (I bought ~$30 worth of markers!) to add some depth to my current models.
 KBDfans Mountain Ergo
 
 On an impulse I bought the KBDfans Mountain Ergo keyboard kit. It’s an Alice-like layout (split, but staggard layout) but the tented chassis makes it a bit more ergonomic. ~$500 is a lot for a keyboard, but I regretted missing the group buy, so I impulsively bought it when restock units went on sale.
 Such a satisfying build. The sandwiched base-plate layers, the snap and click as switches lock in to the hotswappable circuit board, and thocky keypresses of lubed stabilizers. Finished off with a (cheap) set of all-white Japanese hiragana keycaps. It’s an absolute unit and weighs more than a newborn child.
 I may post a more in-depth review, but as good and beautiful as this keyboard is… I like my Moonlander better? Or I had the Moonlander tuned a bit more ergonomically for me? It’s nice to type on though, but I think my problems are more related to 60% keyboard layouts being suboptimal for development.
 Frontend Masters Web Components Workshop
 Most February evenings went to putting together a Web Components course for Frontend Masters. I planned the workshop, made a website, and got feedback from friends in the ShopTalk Discord. Workshops aren’t my forté, but it’s coming together and feels good to be ahead of schedule rather than scrambling as the deadline approaches.
 I’m also updating my web component talk for An Event Apart: Spring Summit, so all the web component research parlays together nicely. There’s a sense of accomplishment in having turned a talk into a workshop and a website. Who knows what’s next, a book? A graphic novel? A movie in the Marvel Cinematic Universe? Time will tell.
 Luro 📈
 Another great month of progress on Luro. We began onboarding our first customer which brought up a lot of major and minor issues with the application, nearly tripling the number of issues in the backlog. Expected, of course, but it triggered a need for better issue management (projects, labels, etc) and taking time to triage incoming issues rather than letting them remain in memory.
 We went with GitHub’s new beta Projects feature. With Projects, I can view the board in a handful of dimensions instead of only a reverse chronological list. It’s been helpful for prioritizing issues and grouping work. The issue queue is nearly back down to pre-flood levels, but we may need some dedicated help soon.
 Ukraine
 The Russian invasion of Ukraine has occupied a lot of my heart and worried brain. There’s a lot to say on this but to watch a tiny tyrant amass hundreds of thousands of troops, lie so brazenly, threaten nuclear war, bomb innocent civilians, all in an attempt to seize a country by force… and there’s so little we can do to stop it… it’s heart-wrenching. As I heard someone say, the ultimate outcome of this war — the great costs paid — depends entirely on a single autocrat coming to his senses.
 My heart goes out to all those who are victims of an unjust war.
 High-level recap of some stat-based metrics
 
 💪 Fitness
 
 Closed my rings 7 times.
 Skill cap problems at Pickleball, a lot of 3.5/4.0+ tennis players have showed up and losing every point makes Pickleball less fun. “Git gud”, I guess or… no.
 
 
 📖 Reading - I read two books this month that touch on similar topics, but I liked one and hated the other. The juxtaposition helped me realize what kind of non-fiction books I like; those that tend to be more academic and/or written by people with PhDs.
 
 Everything is Figureoutable - This book comes highly recommended but I did not enjoy it. I agree with the premise, that you can figure out most problems, but the narrative frustrated me.
 The Upside of Stress - This was my kind of book. Correcting old science with new science, adjacent scientific studies. Written by a PhD. I welcome the challenge to the societal mindset that stress kills.
 
 
 📺 TV, Movies, and Anime
 
 Gundam II, Gundam III (Netflix)
 
 
 👾 Video games
 
 Olli Olli World
 
 
 🎙 Podcasts and YouTubes - We moved ShopTalk over to its own YouTube channel. It’s a minor but impactful change that will allow all ShopTalk content to exist in one home.
 
 New ShopTalk YouTube Channel - Like and subscribe
 CSS Quiz Show - Chris and I did a couple of CSS quizzes.
 
 
 📝 Blogging - 6 posts, 3 new formats. I started doing end-of-week link roundups, a blogging series where I pretend I’m dying or about to quit tech, and a “songcatching” series where I share meaningful songs.
 
 Five links for a Friday afternoon
 Before I go: When it comes to complaining about web browsers - Highest traffic post.
 Six end of week links
 Been in this war for too long
 Τ’ακορντεόν
 原爆を許すまじ
 
 
 
 Also… the ShopTalk Discord braintrust and I have determined that I need to redesign my website. Let it be recorded in the annals of time that process has begun.</content>
     </entry>
     <entry>
       <title>Announcing Interop 2022</title>
         <link href="https://hacks.mozilla.org/2022/03/interop-2022/"/>
       <updated>2022-03-03T17:00:02.000Z</updated>
       <content type="text">A key benefit of the web platform is that it’s defined by standards, rather than by the code of a single implementation. This creates a shared platform that isn’t tied to specific hardware, a company, or a business model.
 Writing high quality standards is a necessary first step to an interoperable web platform, but ensuring that browsers are consistent in their behavior requires an ongoing process. Browsers must work to ensure that they have a shared understanding of web standards, and that their implementation matches that understanding.
 Interop 2022
 Interop 2022 is a cross-browser initiative to find and address the most important interoperability pain points on the web platform. The end result is a public metric that will assess progress toward fixing these interoperability issues.
 
 In order to identify the areas to include, we looked at two primary sources of data:
 
 Web developer feedback (e.g., through developer facing surveys including MDN’s Web DNA Report) on the most common pain points they experience.
 End user bug reports (e.g., via webcompat.com) that could be traced back to implementation differences between browsers.
 
 During the process of collecting this data, it became clear there are two principal kinds of interoperability problems which affect end users and developers:
 
 Problems where there’s a relatively clear and widely accepted standard, but where implementations are incomplete or buggy.
 Problems where the standard is missing, unclear, or doesn’t match the behavior sites depend on.
 
 Problems of the first kind have been termed “focus areas”. For these we use web-platform-tests: a large, shared testsuite that aims to ensure web standards are implemented consistently across browsers. It accepts contributions from anyone, and browsers, including Firefox, contribute tests as part of their process for fixing bugs and shipping new features.
 The path to improvement for these areas is clear: identify or write tests in web-platform-tests that measure conformance to the relevant standard, and update implementations so that they pass those tests.
 Problems of the second kind have been termed “investigate areas”. For these it’s not possible to simply write tests as we’re not really sure what’s necessary to reach interoperability. Such unknown unknowns turn out to be extremely common sources of developer and user frustration!
 We’ll make progress here through investigation. And we’ll measure progress with more qualitative goals, e.g., working out what exact behavior sites depend on, and what can be implemented in practice without breaking the web.
 In all cases, the hope is that we can move toward a future in which we know how to make these areas interoperable, update the relevant web standards for them, and measure them with tests as we do with focus areas.
 Focus areas
 Interop 2022 has ten new focus areas:
 
 Cascade Layers
 Color Spaces and Functions
 Containment
 Dialog Element
 Forms
 Scrolling
 Subgrid
 Typography and Encodings
 Viewport Units
 Web Compat
 
 Unlike the others the Web Compat area doesn’t represent a specific technology, but is a group of specific known problems with already shipped features, where we see bugs and deviations from standards cause frequent site breakage for end users.
 There are also five additional areas that have been adopted from Google and Microsoft’s “Compat 2021” effort:
 
 Aspect Ratio
 Flexbox
 Grid
 Sticky Positioning
 Transforms
 
 A browser’s test pass rate in each area contributes 6% — totaling at 90% for fifteen areas — of their score of Interop 2022.
 We believe these are areas where the standards are in good shape for implementation, and where improving interoperability will directly improve the lives of developers and end users.
 Investigate areas
 Interop 2022 has three investigate areas:
 
 Editing, contentEditable, and execCommand
 Pointer and Mouse Events
 Viewport Measurement
 
 These are areas in which we often see complaints from end users, or reports of site breakage, but where the path toward solving the issues isn’t clear. Collaboration between vendors is essential to working out how to fix these problem areas, and we believe that Interop 2022 is a unique opportunity to make progress on historically neglected areas of the web platform.
 The overall progress in this area will contribute 10% to the overall score of Interop 2022. This score will be the same across all browsers. This reflects the fact that progress on the web platform requires browsers to collaborate on new or updated web standards and accompanying tests, to achieve the best outcomes for end users and developers.
 Contributions welcome!
 Whilst the focus and investigate areas for 2022 are now set, there is still much to do. For the investigate areas, the detailed targets need to be set, and the complex work of understanding the current state of the art, and assessing the options to advance it, are just starting. Additional tests for the focus areas might be needed as well to address particular edge cases.
 If this sounds like something you’d like to get involved with, follow the instructions on the Interop 2022 Dashboard.
 Finally, it’s also possible that Interop 2022 is missing an area you consider to be a significant pain point. It won’t be possible to add areas this year, but, if the effort is a success we may end up running further iterations. Feedback on browser differences that are making your life hard as developer or end user are always welcome and will be helpful for identifying the correct focus and investigate areas for any future edition.
 Partner announcements
 Bringing Interop 2022 to fruition was a collaborative effort and you might be interested in the other announcements:
 
 Apple’s Working together on Interop 2022
 Bocoup and Interop 2022
 Google’s Interop 2022: browsers working together to improve the web for developers
 Igalia and Interop 2022
 Microsoft and Interop 2022
 
 The post Announcing Interop 2022 appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>The 15 minute fix vs the 30 day fix</title>
         <link href="https://daverupert.com/2022/03/15-minute-fix-vs-30-day-fix/"/>
       <updated>2022-03-02T19:27:00.000Z</updated>
       <content type="text">Here’s something I’ve been mulling around. Often in programming you hit a situation where you have to choose two paths:
 
 The 15 minute fix
 The 30 day fix (+30 days of repercussions)
 
 I know, I know, that’s a gross generalization but it’s related to the tension between speed and sustainability in software development. I’ll explain a bit further…
 The 15 minute fix
 The fifteen minute fix is the most direct way to solve the problem. A hard-coded value, installing a third-party library, adding another prop, or introducing a manual process to get the job done. You don’t have time to find the right abstraction, to write it yourself, think through all the edge cases, or automate the process. But deep in the back of your mind, you know that you made a compromise…
 Sometimes the compromise is worth it. Meeting a deadline, not over-thinking code, avoiding a maintenance burden, a lot of “unknown unknowns”, or simply finding the relief of getting something off your plate. Life is full of compromises and codebases full of tradeoffs.
 The 30 day fix
 The 30 day fix is a more thoughtful abstraction. It takes a much larger chunk of time but —with the right amount of consideration and discussion— you stand a better chance at architecting and building a scalable solution in one shot. More robust, less glue code, less manual switch-flipping or knowledge transfer, a healthier baseline for the codebase going forward.
 There’s a feeling of relief when the 30 day fix lands. Technical debt paid down, an organized kitchen pantry, and the codebase now reflects your current understanding of the problem. Adding a line of code doesn’t feel like the whole project will topple over.
 But… the 30 day fix also has a hidden cost in the form of an extra 30 days of fixing the repercussions introduced by the fix itself. Breaking a test suite, an unintended side-effect, a broken layout, or a new API that leads to combing through the entire codebase to upgrade all the props to the new schema. There’s always a need to refactor a critical component to make it all work error-free.
 30 days is a long time for a codebase. 60 days even more. Surely it’s cheaper to keep applying 15 minute fixes, right? Well… can I tell you a secret? Unless a small miracle happens (or your product never grows), you almost always end up needing the 30 day fix. The shortcut works for a short while.
 Example: The case of the evolving API needs
 I built an API to retrieve posts from a database. A basic CRUD sort of thing.
 GET    /posts/    # index
 GET    /posts/:id # read
 POST   /posts/:id # create
 PATCH  /posts/:id # update
 DELETE /posts/:id # destroy
 
 But when I started using it, across the application I needed a speedier list view of post.title and post.id fields (for a dropdown). I cracked my knuckles and wrote a 15 minute fix by creating another endpoint to solve that problem.
 GET    /posts/     # index
 GET    /posts/list # index, but with less fields
 GET    /posts/:id  # read
 POST   /posts/:id  # create
 PATCH  /posts/:id  # update
 DELETE /posts/:id  # destroy
 
 Then we needed to have a category filter on the /posts/lists endpoint. Another 15 minute fix added to the mix.
 GET    /posts/     # index
 GET    /posts/list/:category? # index, but with less fields (optional: filter by category)
 GET    /posts/:id  # read
 POST   /posts/:id  # create
 PATCH  /posts/:id  # update
 DELETE /posts/:id  # destroy
 
 This worked well for a couple months until I saw four of these /posts/list/:category queries on a dashboard page, but we showed ~5 records in the UI, but were fetching potentially thousands. I added another 15 minute fix to allow a limit as a query param.
 GET    /posts/     # index
 GET    /posts/list/:category?limit&#x3D;num # index, but with less fields (optional: filter by category, limit number records returned)
 GET    /posts/:id  # read
 POST   /posts/:id  # create
 PATCH  /posts/:id  # update
 DELETE /posts/:id  # destroy
 
 Ahh.. Are we done yet? No. Absolutely not. We’re doing a lot of data-massaging on the client that would be better (and faster) on the server. Now we need orderBy and groupBy params for sorting, include params for an ad-hoc JOIN of related records, ability to sort JOIN records; a little less rigidity in the design.
 We — as predicted — are heading towards a 30 day fix situation. With all the added options, the /posts/list endpoint is a mess — it may not even need to exist! — and we could solve a lot of problems by sitting down and thinking about the solution a little bit harder. But I actually don’t know if we made a mistake. We were running towards getting feedback. We don’t need a big infrastructure project when sandbags would fix the issue.
 To build or over-build, that is the question
 It’s a tough situation that comes down to estimating. Not estimating in a “choose some fake fibonacci numbers” sense, but in an overcoming personal biases sense. Are you over-estimating your need for the 30 day fix? Are you under-estimating how well the 15 minute fix is going to work? Are you operating emotionally out of a bad experience from a previous situation? Have you overcome your personal optimism bias? What’s the pressure from management like?
 Often you don’t know the scope of the problem until you have users, so over-building at the beginning of the process is probably a mistake. The 15 minute fix gets you closer to actually learning what your codebase needs. But months down the line, you have a codebase full of 15 minute fixes that now need 30 day upgrades. Time-bombs. Could you have saved time by architecting the 30 day fix from the beginning? Or could you have saved a lot of money by chipping away at the problem with more and more 15 minute fixes?
 And that’s what it comes down to: economics. A 15 minute fix and a 30 day fix have colossally different costs and impacts. Can your business afford that time? Can your users afford the delay? Is it a Monty Hall Rewrite problem where the fix could actually be worse once it’s landed? I struggle with this because I hate rework, but if you think of software as evolving, then rework and maintenance is part of the job.
 How do you decide? I don’t know. The best advice I have is: Never go to the grocery store when you’re hungry… but y’know, for software.</content>
     </entry>
     <entry>
       <title>A new year, a new MDN</title>
         <link href="https://hacks.mozilla.org/2022/03/a-new-year-a-new-mdn/"/>
       <updated>2022-03-01T14:00:24.000Z</updated>
       <content type="text">If you’ve accessed the MDN website today, you probably noticed that it looks quite different. We hope it’s a good different. Let us explain!
 MDN has undergone many changes in its sixteen-year history from its early beginning as a wiki to the recent migration of a static site backed by GitHub. During that time MDN grew organically, with over 45,000 contributors and numerous developers and designers. It’s no surprise that the user experience became somewhat inconsistent throughout the website. 
 In mid-2021 we started to think about modernizing MDN’s design, to create a clean and inviting website that makes navigating our 44,000 articles as easy as possible. We wanted to create a more holistic experience for our users, with an emphasis on improved navigability and a universal look and feel across all our pages. 
 A new Homepage, focused on community
 The MDN community is the reason our content can be counted on to be both high quality and trustworthy. MDN content is scrutinized, discussed, and yes, in some cases argued about. Anyone can contribute to MDN, either by writing content, suggesting changes or fixing bugs.
 We wanted to acknowledge and celebrate our awesome community and our homepage is the perfect place to do so.
 The new homepage was built with a focus on the core concepts of community and simplicity. We made an improved search a central element on the page, while also showing users a selection of the newest and most-read articles. 
 We will also show the most recent contributions to our GitHub content repo and added a contributor spotlight where we will highlight MDN contributors.
 
 Redesigned article pages for improved navigation
 It’s been years—five of them, in fact—since MDN’s core content presentation has received a comprehensive design review. In those years, MDN’s content has evolved and changed, with new ways of structuring content, new ways to build and write docs, and new contributors. Over time, the documentation’s look and feel had become increasingly disconnected from the way it’s read and written.
 While you won’t see a dizzying reinvention of what documentation is, you’ll find that most visual elements on MDN did get love and attention, creating a more coherent view of our docs. This redesign gives MDN content its due, featuring:
 
 More consistent colors and theming
 Better signposting of major sections, such as HTML, CSS, and JavaScript
 Improved accessibility, such as increased contrast
 Added dark mode toggle for easy switching between modes
 
 
  
 We’re especially proud of some subtle improvements and conveniences. For example, in-page navigation is always in view to show you where you are in the page as you scroll:
 
 We’re also revisiting the way browser compatibility data appears, with better at-a-glance browser support. So you don’t have to keep version numbers in your head, we’ve put more emphasis on yes and no iconography for browser capabilities, with the option to view the detailed information you’ve come to expect from our browser compatibility data. We think you should check it out. 
 And we’re not stopping there. The work we’ve done is far-reaching and there are still many opportunities to polish and improve on the design we’re shipping.
 A new logo, chosen by our community
 As we began working on both the redesign and expanding MDN beyond WebDocs we realized it was also time for a new logo. We wanted a modern and easily customizable logo that would represent what MDN is today while also strengthening its identity and making it consistent with Mozilla’s current brand.
 We worked closely with branding specialist Luc Doucedame, narrowed down our options to eight potential logos and put out a call to our community of users to help us choose and invited folks to vote on their favorite. We received over 10,000 votes in just three days and are happy to share with you “the MDN people’s choice.”
 
 The winner was Option 4, an M monogram using underscore to convey the process of writing code. Many thanks to everyone who voted!
 
 What you can expect next with MDN
 
 Bringing content to the places where you need it most
 In recent years, MDN content has grown more sophisticated for authors, such as moving from a wiki to Git and converting from HTML to Markdown. This has been a boon to contributors, who can use more powerful and familiar tools to create more structured and consistent content.
 With better tools in place, we’re finally in a position to build more visible and systematic benefits to readers. For example, many of you probably navigate MDN via your favorite search engine, rather than MDN’s own site navigation. We get it. Historically, a wiki made large content architecture efforts impractical. But we’re now closer than ever to making site-wide improvements to structure and navigation.
 Looking forward, we have ambitious plans to take advantage of our new tools to explore improved navigation, generated standardization and support summarizes, and embedding MDN documentation in the places where developers need it most: in their IDE, browser tools, and more.
 Coming soon: MDN Plus
 MDN has built a reputation as a trusted and central resource for information about standards, codes, tools, and everything you need as a developer to create websites. In 2015, we explored ways to be more than a central resource through creating a Learning Area, with the aim of providing a useful counterpart to the regular MDN reference and guide material. 
 In 2020, we added the first Front-end developer learning pathway to it.  We saw a lot of interest and engagement from users, the learning area currently being responsible for 10% of MDN’s monthly web traffic. This started us on a path to see what more we can do in this area for our community.
 Last year we surveyed users and asked them what they wanted out of their MDN experience. The top requested features included notifications, article collections and an offline experience on MDN. The overall theme we saw was that users wanted to be able to organize MDN’s vast library in a way that worked for them. 
 We are always looking for ways to meet our users’ needs whether it’s through MDN’s free web documentation or personalized features. In the coming months, we’ll be expanding MDN to include a premium subscription service based on the feedback we received from web developers who want to customize their MDN experience. Stay tuned for more information on MDN Plus.
 
 Thank you, MDN community
 We appreciate the thousands of people who voted for the new logo as well as everyone who participated in the early beta testing phase since we started this journey. Also, many thanks to our partners from the Open Web Docs, who gave us valuable feedback on the redesign and continue to make daily contributions to MDN content. Thanks to you all we could make this a reality and we will continue to invest in improving even further the experience on MDN.
 The post A new year, a new MDN appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>La Patria</title>
         <link href="https://www.logodesignlove.com/la-patria"/>
       <updated>2022-02-24T17:23:11.000Z</updated>
       <content type="text">
 
 
 
 
 
 
 There are more than 1,000 items in the collection. As well as logos you’ll find historical posters, stamps, book and record covers.
 “There have been few attempts to narrate or document Uruguayan design tradition; because of this, the country appears to lack a design history. A commitment to my profession, my discipline, and my nation led me to want to fill those gaps, so we founded an archive of Uruguayan graphic design on July 10, 2018.” — Amijai Benderski
 I was surprised to see Edward Johnston’s 1918 London Underground logo in the mix. Turns out he was born in Uruguay before his Scottish parents moved back across the ocean to England when he was just three.
 The Underground roundel is one of Amijai’s favourites. In his words, “What I idolise about the accomplishment of Edward Johnston is that it is an everlasting design that has transformed into a symbol of British culture.”
 
 
 
 
 
 
 
 
 Matt Lamont interviewed Amijai for Design Reviewed and asked if he felt Uruguayan design is under-represented in design history.
 “I don’t feel that Uruguayan design is under-represented, I know it is. In my opinion, this happens because there lacks a narration of our history where a tale is constructed.
 “For example, I learned about our graphic design background when I was in my thirties. This shows that there is a problem where me and my colleagues didn’t create a report of the history where shared.
 “The main objective of the archive is to enable design students to be aware of our heritage as soon as they start studying our vocation. La Patria will help to raise awareness of what was designed in our territory.”
 
 
 
 
 
 
 
 
 
 
 
 
 
 For more on the worthy endeavour visit The Daily Heller, and the site itself La Patria.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 296: 15 years a freelancer, work-life productivity and CSS</title>
         <link href="https://wdrl.info/archive/296"/>
       <updated>2022-02-21T14:00:00.000Z</updated>
       <content type="text">Hey,
 
 This year marks fifteen years of freelancing in my life. That’s 100% of my work life and nearly half the time I live on this world. So I wrote something down on it on my blog. WDRL itself will turn 10 next year already which amazes me when I think about it. I want to say thank you to all who subscribe to my newsletter, who support me with recommendations to others, with money contributions or other things. You’re keeping this project alive and make it a usually joyful thing to send out a new edition.
 To close the intro and dive into the articles, I want to share this mini documentary about my work-life in which I share how I combine web development and market gardening today as well as how to protect yourself from burn-out situations:  
 
 News
 
 	This month a lot changed from a legal perspective for website owners: Two court rulings now decided that both using Google Webfonts as well as Google Analytics isn’t a legitimate interest and therefore can’t be active by default. You may still use these services if you ask for permission via Cookie Banner explicitly but it fails to serve the reason to use these services. Webfonts shouldn’t be replaced only by user action and analytics that are only collecting a small portion of user data are barely useful. So in future, self-host webfonts (from a performance perspective it’s also not worse than using the service), and use an analytics provider that’s matching GDPR rules.
 	What’s new in PHP in 2022? A lot, according to this article by Brent from Stitcher.
 
 Generic
 
 	Alan Dávalos writes about the new baseline for web development in 2022: Now that Internet Explorer seems to die really in June this year, so now we should focus on low-spec Android devices, older Safari versions or slow networks.
 	I’m a big fan of the paradigm “choose based on the current needs”, so when I had to prototype a recent client project on a tight budget, I made a choice that I found risky but was the best for this project: Using TailwindCSS.
 
 UI/UX
 
 	Digital products increasingly often depend on design systems. How can we evolve those design systems without breaking the products themselves? Brad Frost lays out the pros and cons about whether version the whole library or individual components.
 
 Web Performance
 
 	Simon Hearne looks at some key cache scenarios and recommend the ideal headers to set. Understanding caching is still one of the harder parts of the web and often disregarded.
 
 HTML &amp; SVG
 
 	Madza shares a couple of lesser known HTML attributes that nevertheless are very helpful and may reduce JavaScript overhead.
 
 Accessibility
 
 	We should keep learning about accessibility and make it one of our top priorities. Melanie Sumner writes about why accessibility is still seen as a side issue and how we can change it.
 	Do you know about the CSS pseudo-class :focus-visible? Pawel Grzybek explains the difference between :focus-visible and :focus.
 
 JavaScript
 
 	Chris Ferdinandi shares an easier way to write if … or checks with vanilla JavaScript by using Array.includes() when we need to check multiple conditions.
 	Zach Leatherman shares a Web Component that enhances &lt;details&gt; elements with very useful behaviour that I also need nearly all the time using these HTML elements. For example keyboard actions, default closed on mobile but opened on big screens.
 
 CSS
 
 	This is a nice collection of Tailwind CSS components to use for free. But even if you don’t use Tailwind it’s a nice inspiration for building components.
 	One thing that often gets neglected when discussing about Tailwind’s utility class approach is that it isn’t limited to that. You can use Tailwind’s directives and functions to make use of Tailwind’s presets inside your own CSS. That could go as far as writing completely your own class references and only using the styling inside your CSS file.
 	Wonder what fit-content is suitable for? Here’s a quick guide leading you to the use cases.
 
 Work &amp; Life
 
 	James Clear, the author of the bestseller book Atomic Habits, reminds us of how important it is to say no if you want to stay productive, healthy, and focused.
 	Joel Spolsky writes about difficulties with getting started at work and why it is okay to move on one step at a time.
 	If you work in web design or web development, this article might be an interesting read for you. Dan Mall reflects on the business of his design systems agency in 2021. He comes up with some inspiring conclusions.
 	In a workplace that isn’t psychologically sound, you’ll find a culture where people fear being embarrassed or blamed or publicly shamed. Adam Blanchard on psychological safety.
 
 
 If you like this newsletter, you can contribute to financing the project or just forward it to your colleagues and friends or share on social media. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Version 100 in Chrome and Firefox</title>
         <link href="https://hacks.mozilla.org/2022/02/version-100-in-chrome-and-firefox/"/>
       <updated>2022-02-15T18:05:20.000Z</updated>
       <content type="text">Chrome and Firefox will reach version 100 in a couple of months. This has the potential to cause breakage on sites that rely on identifying the browser version to perform business logic.  This post covers the timeline of events, the strategies that Chrome and Firefox are taking to mitigate the impact, and how you can help.
 User-Agent string
 User-Agent (UA) is a string that browsers send in HTTP headers, so servers can identify the browser.  The string is also accessible through JavaScript with navigator.userAgent. It’s usually formatted as follows:
 browserName/majorVersion.minorVersion
 For example, the latest release versions of browsers at the time of publishing this post are:
 
 Chrome: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36
 Firefox: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0
 Safari: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15
 
 Major version 100—three-digit version number
 Major version 100 is a big milestone for both Chrome and Firefox. It also has the potential to cause breakage on websites as we move from a two-digit to a three-digit version number.  Web developers use all kinds of techniques for parsing these strings, from custom code to using User-Agent parsing libraries, which can then be used to determine the corresponding processing logic. The User-Agent and any other version reporting mechanisms will soon report a three-digit version number.
 Version 100 timelines
 Version 100 browsers will be first released in experimental versions (Chrome Canary, Firefox Nightly), then beta versions, and then finally on the stable channel.
 
 
 
 Chrome (Release Schedule)
 March 29, 2022
 
 
 Firefox (Release Schedule)
 May 3, 2022
 
 
 
 Why can a three-digit version number be problematic?
 When browsers first reached version 10 a little over 12 years ago, many issues were discovered with User-Agent parsing libraries as the major version number went from one digit to two.
 Without a single specification to follow, different browsers have different formats for the User-Agent string, and site-specific User-Agent parsing. It’s possible that some parsing libraries may have hard-coded assumptions or bugs that don’t take into account three-digit major version numbers.  Many libraries improved the parsing logic when browsers moved to two-digit version numbers, so hitting the three-digit milestone is expected to cause fewer problems. Mike Taylor, an engineer on the Chrome team, has done a survey of common UA parsing libraries which didn’t uncover any issues. Running Chrome experiments in the field has surfaced some issues, which are being worked on.
 What are browsers doing about it?
 Both Firefox and Chrome have been running experiments where current versions of the browser report being at major version 100 in order to detect possible website breakage. This has led to a few reported issues, some of which have already been fixed. These experiments will continue to run until the release of version 100.
 There are also backup mitigation strategies in place, in case version 100 release to stable channels causes more damage to websites than anticipated.
 Firefox mitigation
 In Firefox, the strategy will depend on how important the breakage is. Firefox has a site interventions mechanism. Mozilla webcompat team can hot fix broken websites in Firefox using this mechanism. If you type about:compat in the Firefox URL bar, you can see what is currently being fixed. If a site breaks with the major version being 100 on a specific domain, it is possible to fix it by sending version 99 instead.
 If the breakage is widespread and individual site interventions become unmanageable, Mozilla can temporarily freeze Firefox’s major version at 99 and then test other options.
 Chrome mitigation
 In Chrome, the backup plan is to use a flag to freeze the major version at 99 and report the real major version number in the minor version part of the User-Agent string (the code has already landed).
 The Chrome version as reported in the User-Agent string follows the pattern &lt;major_version&gt;.&lt;minor_version&gt;.&lt;build_number&gt;.&lt;patch_number&gt;.
 If the backup plan is employed, then the User-Agent string would look like this:
 99.101.4988.0
 Chrome is also running experiments to ensure that reporting a three-digit value in the minor version part of the string does not result in breakage, since the minor version in the Chrome User-Agent string has reported 0 for a very long time. The Chrome team will decide on whether to resort to the backup option based on the number and severity of the issues reported.
 What can you do to help?
 Every strategy that adds complexity to the User-Agent string has a strong impact on the ecosystem. Let’s work together to avoid yet another quirky behavior. In Chrome and Firefox Nightly, you can configure the browser to report the version as 100 right now and report any issues you come across.
 Configure Firefox Nightly to report the major version as 100
 
 Open Firefox Nightly’s Settings menu.
 Search for “Firefox 100” and then check the “Firefox 100 User-Agent String” option.
 
 Configure Chrome to report the major version as 100
 
 Go to chrome://flags/#force-major-version-to-100
 Set the option to &#x60;Enabled&#x60;.
 
 Test and file reports
 
 If you are a website maintainer, test your website with Chrome and Firefox 100. Review your User-Agent parsing code and libraries, and ensure they are able to handle three-digit version numbers. We have compiled some of the patterns that are currently breaking.
 If you develop a User-Agent parsing library, add tests to parse versions greater than and equal to 100. Our early tests show that recent versions of libraries can handle it correctly. But the Web is a legacy machine, so if you have old versions of parsing libraries, it’s probably time to check and eventually upgrade.
 If you are browsing the web and notice any issues with the major version 100, file a report on webcompat.com.
 
 The post Version 100 in Chrome and Firefox appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>Improving the Storage Access API in Firefox</title>
         <link href="https://hacks.mozilla.org/2022/02/improving-the-storage-access-api-in-firefox/"/>
       <updated>2022-02-08T16:59:21.000Z</updated>
       <content type="text">Before we roll out State Partitioning for all Firefox users, we intend to make a few privacy and ergonomic improvements to the Storage Access API. In this blog post, we’ll detail a few of the new changes we made.
 With State Partitioning, third parties can’t access the same cookie jar when they’re embedded in different sites. Instead, they get a fresh cookie jar for each site they’re embedded in. This isn’t just limited to cookies either—all storage is partitioned in this way.
 In an ideal world, this would stop trackers from keeping tabs on you wherever they’re embedded because they can’t keep a unique identifier for you across all of these sites. Unfortunately, the world isn’t so simple—trackers aren’t the only third parties that use storage. If you’ve ever used an authentication provider that requires an embedded resource, you know how important third-party storage can be.
 Enter the Storage Access API. This API lets third parties request storage access as if they were a first party. This is called “unpartitioning” and it gives browsers and users control over which third parties can maintain state across first-party origins as well as determine which origins they can access that state from. This is the preferred way for third parties to keep sharing storage across sites.
 The Storage Access API leaves a lot of room for the browser to decide when to allow a third party unrestricted storage access. This is a feature that gives the browser freedom to make decisions it feels are best for the user and decide when to present choices about storage permissions to users directly. 
 On the other hand, this means the Storage Access API can vary from browser to browser and version to version. As a result, the developer experience will suffer unless we do two things: 1) Design with the developer experience in mind; and 2) communicate what we’re doing. 
 So let’s dive in! Here are four changes we’re making to the Storage Access API that will improve user privacy and maintain a strong developer experience…
 Requiring User Consent for Third-Parties the User Never Interacted With
 With Storage API, the browser determines whether to involve the user in the decision to grant storage access to a third party. Previously, Firefox didn’t involve users until a third party already had access to its storage on five different sites. At that point, the third party’s storage access requests were presented to users to make a decision. 
 We’re allowing third parties some leeway to unpartition their storage on a few sites because we’re worried about overwhelming users with popup permission requests. We feel that allowing only a few permission grants per third party would keep the permission frequency down while still preventing any one party from tracking the user on many sites.
 We also wanted to improve user privacy in our Storage Access API implementation by reducing the number of times third parties can automatically unpartition themselves without overwhelming the user with storage access requests. The improvement we settled on was requiring the user to have interacted with the third party recently to give them storage access without explicitly asking the user whether or not to allow it. We believe that removing automatic storage access grants for sites the user has never seen before captures the spirit of State Partitioning without having to bother the user too much more.
 Careful readers may now be concerned that any embed-only pages, like some authentication services, will be heavily impacted by this. To tip the scales even further toward low user touch, we expanded the definition of “interacting with a site” to support embed-only contexts. Now, whenever a user grants storage access via permission popups or interacts with an iframe with storage access, these both count as user interactions. This change is the result of a lot of careful balancing between preserving legitimate use cases, protecting user privacy, and not annoying users with endless permission prompts. We think we found the sweet spot.
 Changing the Scope of First-Party Storage Access to Site
 While rolling out State Partitioning, we’ve seen the emergence of a fair number of use cases for the Storage Access API. One common use is to enable authentication using a third party.
 We found on occasion the login portal that gave first-party storage access to the authentication service was a subdomain, like https://login.example.com. This caused problems when the user navigated to https://example.com after logging in… they were no longer logged in! This is because the storage access permission was only granted to the login subdomain and not the rest of the site. The authentication provider had access to its cookies on https://login.example.com, but not on https://example.com. 
 We fixed this by moving the storage access permission to the Site-scope. This means that when a third party gets storage access on a page, it has access to unpartitioned storage on all pages on that same Site. So in the example above, the authenticating third party would have access to the user’s login cookie on https://login.example.com, https://example.com, and https://any.different.subdomain.example.com! Yet they still wouldn’t have access to that login cookie on http://example.com or https://different-example.com.
 Cleaning Up User Interaction Requirements
 Requiring user interaction when requesting storage access was one rough edge of the Storage Access API definition. Let’s talk about that requirement.
 If a third party calls requestStorageAccess as soon as a page loads, it should not get that storage access. It needs to wait until the user interacts with their iframe. Scrolling or clicking are good ways to get this user interaction and it will expire a few seconds after it’s granted. Unfortunately, there were some corner cases in this requirement that we needed to clean up. 
 One corner case concerns what to do with the user’s interaction state when they click Accept or Deny on a permission prompt. We decided that when a user clicks Deny on a storage access permission prompt, the third party should lose their user interaction. This prevents the third party from immediately requesting storage access again, bothering the user until they accept. 
 Conversely, we decided to reset the timer for user interaction if the user clicks Accept to reflect that the user did interact with the third party. This will allow the third party to use APIs that require both storage access and user interaction with only one user interaction in their iframe.
 Another corner case concerned how strict to be when requiring user interaction for storage access requests. As we’ve iterated on the Storage Access API, minor changes have been introduced. One of the changes has to do with the case of giving a third party storage access on a page, but then the page is reloaded. Does the third party have to get a user interaction before requesting storage access again? Initially, the answer was no, but now it is yes. We updated our implementation to reflect that change and align with other browsers. 
 Integrating User Cookie Preferences
 In the settings for Firefox Enhanced Tracking Protection, users can specify how they want the browser to handle cookies. By default, Firefox blocks cookies from known trackers. But we have a few other possible selections, such as allowing all cookies or blocking all third-party cookies. Users can alter this preference to their liking.
 We have always respected this user choice when implementing the Storage Access API. However, this wasn’t clear to developers. For example, users that set Firefox to block all third-party cookies will be relieved to know the Storage Access API in no way weakens their protection; even a storage access permission doesn’t give a third party any access to storage. But this wasn’t clear to the third party’s developers. 
 The returned promise from requestStorageAccess would resolve, indicating that the third party had access to its unpartitioned storage. We endeavored to fix this. In Firefox 98, when the user has disabled third-party cookies via the preferences, the function requestStorageAccess will always return a rejecting promise and hasStorageAccess will always return false.
  
  
 The post Improving the Storage Access API in Firefox appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>Retrospective and Technical Details on the recent Firefox Outage</title>
         <link href="https://hacks.mozilla.org/2022/02/retrospective-and-technical-details-on-the-recent-firefox-outage/"/>
       <updated>2022-02-02T09:00:50.000Z</updated>
       <content type="text">On January 13th 2022, Firefox became unusable for close to two hours for users worldwide. This incident interrupted many people’s workflow. This post highlights the complex series of events and circumstances that, together, triggered a bug deep in the networking code of Firefox.
 What Happened?
 Firefox has a number of servers and related infrastructure that handle several internal services. These include updates, telemetry, certificate management, crash reporting and other similar functionality. This infrastructure is hosted by different cloud service providers that use load balancers to distribute the load evenly across servers. For those services hosted on Google Cloud Platform (GCP) these load balancers have settings related to the HTTP protocol they should advertise and one of these settings is HTTP/3 support with three states: “Enabled”, “Disabled” or “Automatic (default)”. Our load balancers were set to the “Automatic (default)” setting and on January 13, 2022 at 07:28 UTC, GCP deployed an unannounced change to make HTTP/3 the default. As Firefox uses HTTP/3 when supported, from that point forward, some connections that Firefox makes to the services infrastructure would use HTTP/3 instead of the previously used HTTP/2 protocol.¹
 Shortly after, we noticed a spike in crashes being reported through our crash reporter and also received several reports from inside and outside of Mozilla describing a hang of the browser.
 Backlog of pending crash reports building up and reaching close to 300K unprocessed reports.
 As part of the incident response process, we quickly discovered that the client was hanging inside a network request to one of the Firefox internal services. However, at this point we neither had an explanation for why this would trigger just now, nor what the scope of the problem was. We continued to look for the “trigger” — some change that must have occurred to start the problem. We found that we had not shipped updates or configuration changes that could have caused this problem. At the same time, we were keeping in mind that HTTP/3 had been enabled since Firefox 88 and was actively used by some popular websites.
 Although we couldn’t see it, we suspected that there had been some kind of “invisible” change rolled out by one of our cloud providers that somehow modified load balancer behavior. On closer inspection, none of our settings were changed. We then discovered through logs that for some reason, the load balancers for our Telemetry service were serving HTTP/3 connections while they hadn’t done that before. We disabled HTTP/3 explicitly on GCP at 09:12 UTC. This unblocked our users, but we were not yet certain about the root cause and without knowing that, it was impossible for us to tell if this would affect additional HTTP/3 connections.
 ¹ Some highly critical services such as updates use a special beConservative flag that prevents the use of any experimental technology for their connections (e.g. HTTP/3).
 A Special Mix of Ingredients
 It quickly became clear to us that there must be some combination of special circumstances for the hang to occur. We performed a number of tests with various tools and remote services and were not able to reproduce the problem, not even with a regular connection to the Telemetry staging server (a server only used for testing deployments, which we had left in its original configuration for testing purposes). With Firefox itself, however, we were able to reproduce the issue with the staging server.
 After further debugging, we found the “special ingredient” required for this bug to happen. All HTTP/3 connections go through Necko, our networking stack. However, Rust components that need direct network access are not using Necko directly, but are calling into it through an intermediate library called viaduct.
 In order to understand why this mattered, we first need to understand some things about the internals of Necko, in particular about HTTP/3 upload requests. For such requests, the higher-level Necko APIs² check if the Content-Length header is present and if it isn’t, it will automatically be added. The lower-level HTTP/3 code later relies on this header to determine the request size. This works fine for web content and other requests in our code.
 When requests pass through viaduct first, however, viaduct will lower-case each header and pass it on to Necko. And here is the problem: the API checks in Necko are case-insensitive while the lower-level HTTP/3 code is case-sensitive. So if any code was to add a Content-Length header and pass the request through viaduct, it would pass the Necko API checks but the HTTP/3 code would not find the header.
 It just so happens that Telemetry is currently the only Rust-based component in Firefox Desktop that uses the network stack and adds a Content-Length header. This is why users who disabled Telemetry would see this problem resolved even though the problem is not related to Telemetry functionality itself and could have been triggered otherwise.
 A specific code path was required to trigger the problem in the HTTP/3 protocol implementation.
 ² These are internal APIs, not accessible to web content.
 The Infinite Loop
 With the load balancer change in place, and a special code path in a new Rust service now active, the necessary final ingredient to trigger the problem for users was deep in Necko HTTP/3 code.
 When handling a request, the code looked up the field in a case-sensitive way and failed to find the header as it had been lower-cased by viaduct. Without the header, the request was determined by the Necko code to be complete, leaving the real request body unsent. However, this code would only terminate when there was no additional content to send. This unexpected state caused the code to loop indefinitely rather than returning an error. Because all network requests go through one socket thread, this loop blocked any further network communication and made Firefox unresponsive, unable to load web content.
 Lessons Learned
 As so often is the case, the issue was a lot more complex than it appeared at first glance and there were many contributing factors working together. Some of the key factors we have identified include:
 
 
 GCP’s deployment of HTTP/3 as default was unannounced. We are actively working with them to improve the situation. We realize that an announcement (as is usually sent) might not have entirely mitigated the risk of an incident, but it would likely have triggered more controlled experiments (e.g. in a staging environment) and deployment.
 
 
 Our setting of “Automatic (default)” on the load balancers instead of a more explicit choice allowed the deployment to take place automatically. We are reviewing all service configurations to avoid similar mistakes in the future.
 
 
 The particular combination of HTTP/3 and viaduct on Firefox Desktop was not covered in our continuous integration system. While we cannot test every possible combination of configurations and components, the choice of HTTP version is a fairly major change that should have been tested, as well as the use of an additional networking layer like viaduct. Current HTTP/3 tests cover the low-level protocol behavior and the Necko layer as it is used by web content. We should run more system tests with different HTTP versions and doing so could have revealed this problem.
 
 
 We are also investigating action points both to make the browser more resilient towards such problems and to make incident response even faster. Learning as much as possible from this incident will help us improve the quality of our products. We’re grateful to all the users who have sent crash reports, worked with us in Bugzilla or helped others to work around the problem.
 The post Retrospective and Technical Details on the recent Firefox Outage appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 295: CSS Parent Selectors, Interoperability of the web, web3 reflections and sanitizing.</title>
         <link href="https://wdrl.info/archive/295"/>
       <updated>2022-02-01T16:00:00.000Z</updated>
       <content type="text">Hey,
 
 It is January February already, and it feels like time flies. In the world of technology, there are always so many news, trends, and possible distractions. On top of that, we already have so much work to do, projects to finish, and people to please. It is easy to feel empty or get burned out. Plus, we still live in the middle of an uncomfortable situation for everyone, a so far unseen pandemic situation; This causes a lot of mental problems on its own and drains a lot of energy from us.
 What we can do in this case — although it’s hard to accept — is slowing down.
 When I feel overwhelmed, I try to go outside as much as possible. I will get some fresh air, have a long walk and reflect on the things I do. I try to answer the question: »What brings me joy in web development?«. After that, I find myself working more calmly and happily.
 Now, if you have the time and energy, you can read through some interesting articles we curated for you. This week, Malte wrote the intro.
 Generic
 
 	Cory Doctorow by the EFF shares some of the issues of today’s web and how that may change over the next years as both U.S. and the EU initialized law proposals for better interoperability of services and software. There’s quite some interesting stuff in the article.
 	Arnold Galovics shares his first-hand experiences with microservices. He offers some strong arguments on why microservices might not be a good fit for your next project.
 	Doeke Norg shares how to use PHP generators instead of iterating through arrays with traditional methods. Same approach works in JavaScript as well.
 
 UI/UX
 
 	Josh W. Cameau blogged about his custom CSS reset which includes a nice idea of more variable, better line-height via calc()
 	Oliver Schöndorfer in the ideal line length and line heigh in web design. He shares examples of what doesn’t work and how to improve legibility of a website with some easy and generic rules.
 	Icones.js is a huge collection of iconsets with a preview.
 
 Tooling
 
 	esbuild is a new JavaScript compiler that’s super fast and could shake up the current tooling world.
 	ParcelCSS is a super fast parser, transpiler, and minified for CSS. It’s so fast because it’s not a node.js tool but written in Rust.
 
 Security
 
 	This is something I love to see coming to our browsers: The HTML Sanitizer API. If that becomes reality, we finally get a safer frontend web and our written JavaScript code is slimmer and less bloated up with sanitising libraries. Until then (and maybe afterwards as we still need it for some cases), we can use DOMPurify.
 
 Web Performance
 
 	It’s not been long ago that we’ve seen HTTP/2 making a huge impact to website performance. Now HTTP/3 is here and brings another set of performance benefits.
 
 HTML &amp; SVG
 
 	Did you know you can control autocapitalization in user input with the autocapitalize property?
 
 Accessibility
 
 	Stephanie Eckles has a short code snippet how to standardise focus styles on a web project easily with CSS custom properties.
 
 JavaScript
 
 	This cool JavaScript snippet offers &quot;add event to calendar&quot; buttons for websites.
 	Stephanie Eckles explains how to use the Intersection Observer Web API as a performant way to track where elements are in the viewport and other scrollable regions.
 	What’s the state of ES6 native modules in our browsers? Right now, it’s still a bit complicated but things get better and once a few more things land in browsers, we may finally reduce the complexity of frontend code and workspaces again.
 
 CSS
 
 	Bramus Van Damme shares what’s to expect in 2022 from CSS — which could be a lot. Things like Container Queries, Cascade Layers, Color functions, better viewport units, Parent (:has()) selector, Subgrid, Accent color, Media Query ranges.
 	Temani Afif shares an approach how to build reponsive layouts with less media queries based on the way we note Flex and Grid patterns. By using min, max, clamp or autofit functions, we reduce the complexity of media queries.
 	The CSS :has() selector is way more than a “Parent Selector”.
 	This CSS snippet collection by 30 seconds of code contains great utilities and interactive CSS modules, such as custom checkbox styling, scroll snap, and lots more.
 	Stefan Judis explains how the hwb() color syntax works and why it’s probably the easiest to work and understand as human. Safari and Firefox both recently implemented it with others to follow soon hopefully.
 	CSS has come a long way since the early days of web development, when tables and various other hacks were used for layout and positioning. Today&#x27;s developers can enjoy writing CSS that works in all major browsers, without having to bend over backwards to implement tricky layout requirements. Not only does this make it easier to create dynamic layouts, but it also allows you to ship smaller (and simpler) stylesheets by removing unnecessary cruft. In this article, we&#x27;ll look at various scenarios where modern techniques can reduce the complexity of your code and allow you to write better CSS.
 
 Work &amp; Life
 
 	
  I was busy and productive, which made me feel important and accomplished. Yet my heart told me a different story.
 
 Andrew Rocha on how to prioritise important things and why everything else is extra. Apps aren’t solving these matters, we need to.
 
 Go beyond…
 
 	When we buy something, we get an immediate boost. But what happens once we have the item for a few days? Most of the time, it doesn’t match our expectations of becoming a happier person by buying it. Leo shares his findings on buying less and focus on the inner self to solve these issues. With that in mind, we tend to buy only really useful things while being happier overall. Win-win.
 	There is currently a lot of hype around crypto and web3 around. Moxie Marlinspike shares his impressions and down-to-earth thoughts on those topics. But even if you’re not very interested in web3, this article reveals a couple of interesting problems with the current system. For example the fact that “Even nerds do not want to run their own servers at this point.”
 	As service platform provider, should you clean up your inactive users regularly or collect them for your numbers instead? Here’s the Flare app team sharing why and how they remove inactive users and teams.
 
 
 I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you like this newsletter, you can contribute to financing the project. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Hacks Decoded: Adewale Adetona</title>
         <link href="https://hacks.mozilla.org/2022/01/hacks-decoded-adewale-adetona/"/>
       <updated>2022-01-31T17:44:10.000Z</updated>
       <content type="text">Welcome to our Hacks: Decoded Interview series!
 Once a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.
 Meet Adetona Adewale Akeem!
 
 Adetona Adewale Akeem, more popularly known as iSlimfit, is a Nigeria-born revered digital technologist and marketing expert. He is the co-founder of Menopays, a fintech startup offering another Buy Now Pay Later (BNPL) option across Africa. 
 So, I’ve got to ask — where does the name iSlimfit come from?
 “Slimfit” is a nickname from my University days. But when I wanted to join social media, Twitter, in particular, I figured out the username Slimfit was already taken. All efforts to reach and plead with the user — who even up until now has never posted anything on the account — to release the username for me proved abortive. Then I came up with another username by adding “i” (which signifies referring to myself) to the front of Slimfit. 
 
 How did you get started in the tech industry, iSlimfit?
 My journey into tech started as far back as 2014, when I made the switch from working at a Media &amp; Advertising Agency in Lagos Nigeria to working as a Digital Marketing Executive in a Fintech Company called SystemSpecs in Nigeria. Being someone that loved combining data with tech, I have always had a knack for growth marketing. So the opportunity to work in a fintech company in that capacity wasn’t something I could let slide.
 Where are you based currently? And where are you from originally? How does where you’re from affect how you move through the tech industry?
 I am currently based in Leeds, United Kingdom after recently getting a Tech Nation Global Talent endorsement by the UK government. I am from Ogun State, Nigeria. 
 There is actually no negative impact from my background or where I am from as regards my work in tech. The Nigerian tech space is huge and the opportunities are enormous. Strategic positioning and working with a goal in mind has helped me in navigating my career in tech so far.
 What brought about the idea of your new vlog Tech Chat with iSlimfit?
 My desire to make an impact and contribute to the growth of upcoming tech professionals birthed the vlog. Also, I wanted to replicate what I do offline with Lagos Digital Summit, in an online manner. The vlog is basically a series of YouTube chat series where I bring various people in tech — growth marketers, UI/UX designers, product managers, startup founders, mobile app developers, etc. — to share their career journey, background, transitioning, their career journey, learnings, and general questions about their day-to-day job so that Tech enthusiasts can learn from their expertise.
 I have to bring up the fact that in 2021, you were endorsed by Tech Nation as an Exceptional agent in Digital Tech. What’s it feel like to achieve something like that?
 
 The Tech Nation endorsement by the UK government is one of my biggest achievements. It made me realize how important my impact on the Nigerian tech industry over the years has been. The endorsement was granted based on my significant contribution to the Nigerian Digital Tech sector, my mentorship &amp; leadership capabilities, and also the potential contribution my talent &amp; expertise would add to the UK digital economy. I am particularly grateful for the opportunity to positively make an impact to the digital economy of the United Kingdom.
 What’s something folks may not immediately realize about the tech sector in Nigeria if they’re not from there?
 Easy: the fact that the tech sector in Nigeria is the biggest in Africa, and the impact of tech solutions developed in Nigeria is felt all over Africa. Also, as we can see from a recent report, Nigerian startups lead the list of African Startups that received funding in 2021.
 What digital policy or policies do you think Nigeria (your home country) should pursue in order to accelerate digital development in the country?
 The Nigerian government need to come to terms with the fact that digital technology is the bedrock for the development of the Nation. They need to develop policies that will shape the Nation’s digital economy and design a roadmap for grassroots digital Tech empowerment of Nigeria’s agile population. 
 We also need more people to champion and improve on our quest for digital entrepreneurship development through various platforms.
 You helped co-found a company called Menopays. What were some of the hurdles when it comes to getting a tech company off the ground over there? What about the opposite? What are the ways those in tech benefit from founding and working in Nigeria?
 Some hurdles in starting a tech company is putting together the right team for the job. This cuts across legal, product, marketing, and the tech itself. The idea could be great but without the right team, execution is challenging. 
 A great benefit is that the continent of Africa is gaining in popularity and the world is watching, so a genuine team founding a business will get the benefits of foreign investments which is great in terms of dollar value.
 Some take issue with Buy Now Pay Later apps and services like Menopays in how they may profit off of buyers who may have less. How is Menopays different? How does the company make money? What measures are in place to make sure you aren’t taking advantage of people?
 Menopays is different because our focus goes beyond the profitability of the industry. We tailored a minimum spendable amount with a decent repayment period for the minimum wage in Nigeria. Our vision stands in the middle of every decision we make both business-wise and/or product development-wise. 
 The measure in place is that decisions are guided by why we started Menopays, which is “to fight poverty”. We don’t charge customers exorbitant interest as it goes against what we are preaching as a brand. So our Vision is imprinted in the heart of all the team members working towards making Menopays a family brand.
 You’ve mentioned Menopays is fighting poverty in Nigeria and eventually all of Africa, how so?
 
 Thinking about one of the incidents that happened to one of our co-founders, Reuben Olawale Odumosu, about eight years back. He lost his best friend because of a substandard malaria medication. His best friend in high school died because his parents couldn’t afford NGN2,500 malaria medication at the time and point of need which led to them going for a cheaper drug that eventually led to his death. Menopays exists to prevent such situations by making basic needs like healthcare, groceries and clothing available to our customers even when they don’t have the money to pay at that moment.
 So in light of this, at Menopays, we believe that if some particular things are taken care of, individuals stand a lot more chances of survival. Take for instance, someone earns NGN18,000, spends NGN5,000 on transport, NGN7,000 on food and rent and some other miscellaneous of NGN6,000; with Menopays, we take out the cost of transportation and food (by providing you access to our merchants) and we give them more time to pay over the next three months. Which means each month the customer is positive cash flow of NGN6,000. We turn a negative cash flow into a positive cash flow and savings, thereby fighting poverty.
 If you didn’t help found Menopays, what would you be doing now instead?
 I would probably be working on founding another tech startup doing something for the greater good of the world and helping brands achieve their desired marketing objectives.
 How can the African tech diaspora help startups similar to Menopays?
 One way African tech diaspora can help startups similar to Menopays is by promoting their services, sharing with potential users, and also by investing in it.
 How did you come up with the idea for Lagos Digital Summit?
 Lagos Digital Summit started in 2017 with just an idea in my small shared apartment back then in Lagos with my friend who is now in Canada. The goal back then was simply to facilitate a platform for the convergence of 50 to 60 digital marketing professionals and business thought leaders for the advancement of SMEs and Digital Media enthusiasts within our network.
 
 Five years down the line, despite being faced with plenty of challenges, it’s been a big success story. We have had the privilege of empowering over 5,000 businesses and individuals with diverse digital marketing skills. 
 What’s it been like arranging that sort of summit in the midst of a pandemic?
 Lagos Digital Summit 2020 has been the only edition that we’ve had to do full virtual because it was in the peak of the COVID-19 pandemic. Every other edition before then had been physical with fully packed attendees of an average of 1,000. For the 2021 edition, it was hybrid because Covid-19 restrictions were relaxed, where we had just 300 people attend physically and every other people watched online.
 What’s something you see everywhere in tech that you wish more people would talk about?
 I wish more people would talk about the struggle, the disappointments, the challenges and the numerous sacrifices that comes with building a tech startup. A lot of times, the media only portray the success stories, especially when a startup raises funds; the headlines are always very inspiring and rosy. 
 What’s been the most impactful thing you’ve done since working in tech? What’s been the most memorable?
 
 That should be founding Lagos Digital Summit; the kind of sponsors, corporate organisations, high-profiled speakers, volunteers and attendees that the Summit has been able to attract has been a memorable and proud feeling.
 What sort of lasting impact do you want to have on the industry and the world? What keeps you going?
 Waking up every day, knowing that a lot of people would have a smile on their faces because I have chosen to impact lives and make the world a better place through relevant tech solutions and platforms is the best feeling for me. The fact that I can read through reports and data and see the number of people using Menopays as a Buy Now Pay Later (BNPL) payment option to ease their lifestyle is a big motivation for me. 
 What’s some advice you’d give to others hoping to enter the tech world or hoping to start up a company?
 Venturing into Tech or building a Startup takes a whole lot of concerted effort and determination. Getting the right set of partner(s) would however make the journey easier for you. Just have partners or cofounders with similar vision and complementing skills.
 —
 You can keep up with Adewale’s work by following him here. Stay tuned for more Hacks Decoded Q&amp;A’s!
 The post Hacks Decoded: Adewale Adetona appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>Creating a Schema-Based Form System</title>
         <link href="https://www.taniarascia.com/schema-based-form-system/"/>
       <updated>2022-01-31T00:00:00.000Z</updated>
       <content type="text">
 View the Source or Demo for the schema-based form system described in this article.
 
 Working with forms on the front end is tedious and repetitive. If you don&#x27;t have a good system set up, it can involve a lot of copy and pasting. If you have a bad abstraction, it can be much worse.
 I&#x27;ve worked with some nightmare systems that were significantly worse than just manually writing all the form logic, error handling, validation, dirty state, etc. But nonetheless, taking care of all that can really start to add up and take a lot of time.
 Wouldn&#x27;t it be nice if we could do something like this: define the schema of a form and pass it into a component that takes care of all the common form needs...
 Schema-based form example
 const ExampleForm &#x3D; () &#x3D;&gt; {
   const schema &#x3D; [
     { name: &#x27;name&#x27;, label: &#x27;Name&#x27;, componentType: &#x27;text&#x27;, required: true },
     {
       name: &#x27;class&#x27;,
       label: &#x27;Class&#x27;,
       componentType: &#x27;select&#x27;,
       options: [
         { value: &#x27;ranger&#x27;, label: &#x27;Ranger&#x27; },
         { value: &#x27;wizard&#x27;, label: &#x27;Wizard&#x27; },
       ],
     },
   ]
 
   return &lt;AdvancedForm schema&#x3D;{schema} onSubmit&#x3D;{handleSubmit} /&gt;
 }
 Instead of writing all this: handling the values, errors, validation, and components manually?
 Manual form example
 const ExampleForm &#x3D; () &#x3D;&gt; {
   const [formValues, setFormValues] &#x3D; useState({})
   const [errors, setErrors] &#x3D; useState({})
   const [touched, setTouched] &#x3D; useState({})
 
   const handleSubmit &#x3D; () &#x3D;&gt; {
     /* ... */
   }
 
   return (
     &lt;form onSubmit&#x3D;{handleSubmit}&gt;
       &lt;label htmlFor&#x3D;&quot;name&quot;&gt;Name (required)&lt;/label&gt;
       &lt;input
         type&#x3D;&quot;text&quot;
         id&#x3D;&quot;name&quot;
         name&#x3D;&quot;name&quot;
         value&#x3D;{value}
         onChange&#x3D;{() &#x3D;&gt; {
           /* ... */
         }}
       /&gt;
 
       &lt;label htmlFor&#x3D;&quot;class&quot;&gt;Class&lt;/label&gt;
       &lt;select
         id&#x3D;&quot;class&quot;
         name&#x3D;&quot;class&quot;
         value&#x3D;{value}
         onChange&#x3D;{() &#x3D;&gt; {
           /* ... */
         }}
       &gt;
         &lt;option key&#x3D;&quot;ranger&quot; value&#x3D;&quot;ranger&quot;&gt;
           Ranger
         &lt;/option&gt;
         &lt;option key&#x3D;&quot;wizard&quot; value&#x3D;&quot;wizard&quot;&gt;
           Wizard
         &lt;/option&gt;
       &lt;/select&gt;
 
       &lt;button type&#x3D;&quot;submit&quot;&gt;Submit&lt;/button&gt;
     &lt;/form&gt;
   )
 }
 I made an example GitHub repo and demo of such a system. This follows the rule from the Tao of React - Do not hardcode markup.
 Of course, disclaimer time, this isn&#x27;t a production-ready repository with tests and edge cases accounted for and every type of form component includes and bindings for different frameworks - it&#x27;s just an example that you can use to learn from or build from.
 The simple example I made does include a text field, select, checkbox, radio group, text area, as well as conditional fields. To make it useful for the real world, you could integrate it with your UI framework of choice (such as Material UI or Semantic UI) if you&#x27;re using one, or you can add support for multi-selects, checkbox groups, asyncronous responses, and much more!
 
       
     
   
   
     
 Technology
 Although I&#x27;m not using a UI framework in the example to handle form component styles, I am using a library to handle values and form submission - Formik. It&#x27;s an extremely widely-used tool for working with forms that takes care of much of the annoying stuff, while still being simple under the hood and not bringing in theb complexity of Redux, MobX, observables, or anything else - just simple React state.
 Additionally, Yup can be used for validation, in order to avoid writing all the same common regex over and over again.
 Using Formik in the project makes it easy to abstract it out and allow us to pass some simple schema in.
 Form System
 Based on the example I showed above, you can see that a schema property will get passed in, as well as the onSubmit handler. This is basically enough for any &quot;create&quot; form, and for an &quot;edit&quot; form, I&#x27;ve also added an initialValues prop that can pre-populate the form with any existing values.
 I&#x27;m using the &lt;Formik&gt; component (reference) from Formik to build this system. It contains render props that contain all the values in the entire form at all times, as well as some helpers like isValid or isSubmitting, which let you know the current state of the form.
 AdvancedForm.js
 import { Formik, Field } from &#x27;formik&#x27;
 
 import { getInitialValues, getDefaultValues, getValidationSchema } from &#x27;./helpers&#x27;
 
 export const AdvancedForm &#x3D; ({ schema, onSubmit, initialValues, ...props }) &#x3D;&gt; {
   const defaultValues &#x3D; getDefaultValues(schema)
   const validationSchema &#x3D; getValidationSchema(schema)
 
   return (
     &lt;Formik
       initialValues&#x3D;{getInitialValues(defaultValues, initialValues)}
       validationSchema&#x3D;{validationSchema}
       onSubmit&#x3D;{onSubmit}
       validateOnMount
       {...props}
     &gt;
       {({ handleSubmit, isSubmitting, isValid, values }) &#x3D;&gt; {
         return (
           &lt;form onSubmit&#x3D;{handleSubmit}&gt;
             {/* Form schema components will go here */}
             &lt;button type&#x3D;&quot;submit&quot; disabled&#x3D;{!isValid || isSubmitting}&gt;
               Submit
             &lt;/button&gt;
           &lt;/form&gt;
         )
       }}
     &lt;/Formik&gt;
   )
 }
 Get default values for the schema
 Before I go into creating and rendering the components, we want to make sure we get default values for all the items in the schema.
 This means if the schema looks like this:
 const schema &#x3D; [
   { name: &#x27;name&#x27;, label: &#x27;Name&#x27;, type: &#x27;text&#x27; },
   { name: &#x27;is_manager&#x27;, label: &#x27;Is Manager&#x27;, type: &#x27;checkbox&#x27; },
 ]
 We want a default values object that looks like this:
 const defaultValues &#x3D; { name: &#x27;&#x27;, is_manager: false }
 We can accomplish that by running through the schema and reducing it based on component type:
 helpers/getDefaultValues
 export const getDefaultValues &#x3D; (schema) &#x3D;&gt; {
   return schema.reduce((acc, val) &#x3D;&gt; {
     let defaultValue
 
     switch (val.componentType) {
       case &#x27;text&#x27;:
       case &#x27;textarea&#x27;:
       case &#x27;select&#x27;:
       case &#x27;radioGroup&#x27;:
         defaultValue &#x3D; &#x27;&#x27;
         break
       case &#x27;checkbox&#x27;:
         defaultValue &#x3D; false
         break
       default:
         defaultValue &#x3D; &#x27;&#x27;
     }
 
     return { ...acc, [val.name]: val.defaultValue || defaultValue }
   }, {})
 }
 This way, we&#x27;re never passing in null or undefined values into the form, and it&#x27;s always receiving the type it expects.
 Get validation object for the schema
 Just like the default values, we&#x27;ll want a schema object. Using the Yup library, we can just pass in values, like Yup.string() for text fields, radio values, etc. and Yup.array() for something like a multi-select or checkbox group.
 helpers/getValidationSchema
 export const getValidationSchema &#x3D; (schema) &#x3D;&gt; {
   const validationObject &#x3D; schema.reduce((acc, val) &#x3D;&gt; {
     let validationType
 
     switch (val.componentType) {
       case &#x27;text&#x27;:
       case &#x27;textarea&#x27;:
       case &#x27;select&#x27;:
       case &#x27;radioGroup&#x27;:
         validationType &#x3D; Yup.string()
         break
       case &#x27;checkbox&#x27;:
       default:
         validationType &#x3D; null
     }
 
     if (val.required &amp;&amp; validationType) {
       validationType &#x3D; validationType.required(&#x60;${val.label} is required&#x60;)
     }
 
     return { ...acc, ...(validationType &amp;&amp; { [val.name]: validationType }) }
   }, {})
 
   return Yup.object().shape(validationObject)
 }
 Now that I think about it, there&#x27;s probably some way to use the Yup schema for both default values and validation, but I did not look further into it.
 Set initial values
 Now we can set the initial values - either the defaultValues by default, or a passed in initialValues if you&#x27;re editing an existing form.
 helpers/getInitialValues
 export const getInitialValues &#x3D; (defaultValues, initialValues) &#x3D;&gt; {
   if (!initialValues) return defaultValues
 
   return { ...defaultValues, ...initialValues }
 }
 All the setup for the form is there now, and now you can start creating bindings for whatever form components you want in the system.
 Form Components
 In this article, won&#x27;t go into how to create all the individual form components (you can just view the source), I&#x27;ll just focus on one, but for every type of form component that you want to include in the form system, make a file for it.
 forms/index.js
 import { Checkbox } from &#x27;./Checkbox.js&#x27;
 import { Select } from &#x27;./Select.js&#x27;
 import { TextField } from &#x27;./TextField.js&#x27;
 import { TextArea } from &#x27;./TextArea.js&#x27;
 import { RadioGroup } from &#x27;./RadioGroup.js&#x27;
 
 export { Checkbox, Select, TextField, TextArea, RadioGroup }
 Now back in the main AdvancedForm component, you can import all those components and put them in an array. When looping through the schema, you can now find the correct component to render. The component will be rendered using the Formik &lt;Field&gt; component (reference), which gives you access to the onChange events, touched, errors, values, etc. for each form field.
 AdvancedForm.js
 import { Formik, Field } from &#x27;formik&#x27;
 
 import { getInitialValues, getDefaultValues, getValidationSchema } from &#x27;./helpers&#x27;
 
 // Import all the form components and map them to their respective schema componentTypeimport { Checkbox, Select, TextArea, TextField, RadioGroup } from &#x27;.&#x27;const components &#x3D; [  { componentType: &#x27;text&#x27;, component: TextField },  { componentType: &#x27;textarea&#x27;, component: TextArea },  { componentType: &#x27;select&#x27;, component: Select },  { componentType: &#x27;checkbox&#x27;, component: Checkbox },  { componentType: &#x27;radioGroup&#x27;, component: RadioGroup },]
 export const AdvancedForm &#x3D; ({ schema, onSubmit, initialValues, ...props }) &#x3D;&gt; {
   const defaultValues &#x3D; getDefaultValues(schema)
   const validationSchema &#x3D; getValidationSchema(schema)
 
   return (
     &lt;Formik
       initialValues&#x3D;{getInitialValues(defaultValues, initialValues)}
       validationSchema&#x3D;{validationSchema}
       onSubmit&#x3D;{onSubmit}
       validateOnMount
       {...props}
     &gt;
       {({ handleSubmit, isSubmitting, isValid, values }) &#x3D;&gt; {
         return (
           &lt;form onSubmit&#x3D;{handleSubmit}&gt;
             {schema.map(({ componentType, condition, ...formSchema }) &#x3D;&gt; {              // Find the correct component from the schema based on componentType              const Component &#x3D; components.find(                (component) &#x3D;&gt; component.componentType &#x3D;&#x3D;&#x3D; componentType              ).component              // Pass the formSchema data into the Field component              return &lt;Field key&#x3D;{formSchema.name} component&#x3D;{Component} {...formSchema} /&gt;            })}            &lt;button type&#x3D;&quot;submit&quot; disabled&#x3D;{!isValid || isSubmitting}&gt;
               Submit
             &lt;/button&gt;
           &lt;/form&gt;
         )
       }}
     &lt;/Formik&gt;
   )
 }
 Now you can actually make the bindings for each form type.
 Text field component
 All the data gets passed down to the &lt;Field&gt;, such as the onChange, onBlur, whether or not it has been touched or has errors, and then anything special you want to add to it. For example, the Select component would have an options prop so you can pass down a list of all the key/values for the select options.
 Here is an example of a simple text field input. This could also be extended and modified to have an email type, a password type, or anything else you might want a regular input to be able to handle.
 forms/TextField.js
 export const TextField &#x3D; ({
   label,
   field: { name, value, ...fieldProps },
   form: { touched, errors },
   required,
   ...props
 }) &#x3D;&gt; {
   const hasError &#x3D; errors[name] &amp;&amp; touched[name]
 
   return (
     &lt;&gt;
       &lt;label htmlFor&#x3D;{name}&gt;
         {label}
         {required &amp;&amp; &lt;sup className&#x3D;&quot;required&quot;&gt;*&lt;/sup&gt;}
       &lt;/label&gt;
       &lt;input type&#x3D;&quot;text&quot; id&#x3D;{name} name&#x3D;{name} value&#x3D;{value} {...fieldProps} {...props} /&gt;
       {hasError &amp;&amp; &lt;small className&#x3D;&quot;error&quot;&gt;{errors[name]}&lt;/small&gt;}
     &lt;/&gt;
   )
 }
 The same code can be extended for checkboxes, radios, selects, multi-selects, radio groups, sliders, and any other form type you need.
 Required
 In the getValidationSchema helper, we set up default types for each field in the schema. If one of them has required: true in the schema, and nothing is entered, an error will appear that says &quot;[label name] is required&quot;.
 if (val.required &amp;&amp; validationType) {
   validationType &#x3D; validationType.required(&#x60;${val.label} is required&#x60;)
 }
 The way the form is set up, empty values won&#x27;t start off in an error state, but if they&#x27;re touched and not filled out, then the error state will appear.
 Errors
 You can check if an error exists by seeings if the related error exists and the field has been touched.
 const hasError &#x3D; errors[name] &amp;&amp; touched[name]
 Conditional Fields
 I added a little bonus where you can make certain fields only appear if certain conditions are met. This example schema is set up with key, value, and operator of the condition.
 There&#x27;s a &quot;Class&quot; select, that has Ranger, Wizard, and Healer as options. If you select Wizard, then another field pops up with the Spell select.
 condition: { key: &#x27;class&#x27;, value: &#x27;wizard&#x27;, operator: &#x27;&#x3D;&#x27; },
 Here&#x27;s the whole schema:
 const schema &#x3D; [
   {
     name: &#x27;class&#x27;,
     label: &#x27;Class&#x27;,
     componentType: &#x27;select&#x27;,
     options: [
       { label: &#x27;Ranger&#x27;, value: &#x27;ranger&#x27; },
       { label: &#x27;Wizard&#x27;, value: &#x27;wizard&#x27; },
       { label: &#x27;Healer&#x27;, value: &#x27;healer&#x27; },
     ],
   },
   {
     name: &#x27;spell&#x27;,
     label: &#x27;Spell&#x27;,
     componentType: &#x27;select&#x27;,
     options: [
       { label: &#x27;Fire&#x27;, value: &#x27;fire&#x27; },
       { label: &#x27;Ice&#x27;, value: &#x27;ice&#x27; },
     ],
     condition: { key: &#x27;class&#x27;, value: &#x27;wizard&#x27;, operator: &#x27;&#x3D;&#x27; },
   },
 ]
 In the advanced form, you can check for condition and render a ConditionalField wrapper around the field that will hide, show, and add default values as needed.
 AdvancedForm.js
 // ...
 
 if (condition) {
   return (
     &lt;ConditionalField
       key&#x3D;{formSchema.name}
       show&#x3D;{
         condition.operator &#x3D;&#x3D;&#x3D; &#x27;&#x3D;&#x27;
           ? values[condition.key] &#x3D;&#x3D;&#x3D; condition.value
           : values[condition.key] !&#x3D;&#x3D; condition.value
       }
       onCollapse&#x3D;{() &#x3D;&gt; {
         setFieldValue(formSchema.name, defaultValues[formSchema.name])
         setFieldTouched(formSchema.name, false)
       }}
       onShow&#x3D;{() &#x3D;&gt; {
         setFieldValue(formSchema.name, defaultValues[formSchema.name])
       }}
     &gt;
       &lt;Field component&#x3D;{Component} {...formSchema} /&gt;
     &lt;/ConditionalField&gt;
   )
 }
 
 // ...
 The component is a simple conditonal gate that renders children if the conditon is met.
 forms/ConditionalField.js
 import { useEffect } from &#x27;react&#x27;
 
 export const ConditionalField &#x3D; ({ show, onCollapse, onShow, children }) &#x3D;&gt; {
   useEffect(() &#x3D;&gt; {
     if (show) {
       onShow()
     } else {
       onCollapse()
     }
     // eslint-disable-next-line react-hooks/exhaustive-deps
   }, [show])
 
   return show ? children : null
 }
 Conclusion
 So there you have it, with the combined power of Formik and Yup, you can build an abstraction to drastically reduce the amount of code you have to write overall to work with forms. The concepts outlined in this article can be extended to any component library...Material UI, Ant Design, Blueprint, Semantic UI, or just plain HTML and CSS as seen here.
 The conditional code here is very simple, relying on whether certain items were selected or not in the form, but you could use something like JSON schema to extend it further. A lot more component types can be created, such as an email type that has a default email regex added to the validationObject, and a multi-select dropdown component type. You might also want to factor in asynchronous conditionals.
 Hopefully this article helped you think more about defining data upfront and passing it into components as opposed to hard-coding markup and manually handling form state. There will always be some situations an abstraction doesn&#x27;t handle well or at all, and in those cases you might need to manually work with your form, but a system like AdvancedForm can help in many common situations.
 View the Source or Demo for the schema-based form system described in this article. Thanks for reading!</content>
     </entry>
     <entry>
       <title>Logo Wave Awards</title>
         <link href="https://www.logodesignlove.com/logo-wave-awards"/>
       <updated>2022-01-20T13:14:02.000Z</updated>
       <content type="text">
 
 I’ve been judging the Logo Wave awards for a number of years now. It’s an independent award scheme by designer Kyle Courtright. Kyle and I have yet to meet in person, but we’ve come to know each other online over the years and I have a lot of respect for him as a family man and as a designer. Kyle kindly answered some questions to give us an insight into the awards.
 What made you want to start a logo competition?
 Like many ideas, Logo Wave was born out of necessity. I wanted to enter my own work into a logo awards competition and felt frustrated with the antiquated and drawn-out submission process. At that point, I knew I wanted to start a design awards to help support my fellow creatives, and I had three criteria out of the gate:
 
 Must be centered around logos
 Must encourage and give value to creatives
 Must fill a gap in the design awards space
 
 After feeling like each of those boxes had the potential to be checked (and getting the green light from my wife), I decided to move forward. We launched in June of 2016.
 
 
 
 
 
 You describe Logo Wave as a “reimagined” awards platform. What sets it apart?
 1. The “Wave” structure
 The wave model sets us apart. There are usually 3–4 waves per year (each lasting 3–4 months). Other design awards tend to run one annual competition.
 Freelancers and studios upload their best logo work from their design portfolios and when the wave’s complete our jury carefully reviews the entries and determines the winners. When one wave ends a new one begins, like waves in the ocean.
 2. Massive prize bundle
 Our prize pack includes $1,000 USD, custom-crafted bamboo trophies, a one year membership to LogoLounge, the Logo Package Express extension, a winner certificate, recognition on the Logo Wave website, additional network exposure, and more.
 3. The investment
 We’re one of the most affordable design awards and we’ve intentionally maintained a wallet-friendly, prize-heavy platform.
 4. Internationally acclaimed jury
 We have been fortunate enough to bring on a jury of design authors who genuinely care about moving our profession forward. People who love being a part of recognising and celebrating design. Virtuosos like Louise Fili, David Airey, Gail Anderson, Chris Do, Alina Wheeler, Jacob Cass, Debbie Millman, and Bill Gardner have graced our panel over the years.
 
 
 
 
 
 5. Level playing field
 We’ve intentionally created a competition where freelancers, studios, and agencies alike have the chance to heighten their credibility and recognition. The platform was built by designers for designers, with the heart behind the brand being to give creatives a credible, exciting boost of social proof.
 6. User-friendly and fun
 Part of reimagining the awards space meant creating a user-friendly competition. As I mentioned, this was a big pain point from experiences with other design awards. We’re mindful that busy designers don’t have time to fill out long, drawn-out forms, so a simple, straightforward entry process was important.
 While we have an expectation of excellence, we don’t take ourselves too seriously. We want designers to have fun as we help to celebrate and shine a spotlight on their work, offering great prizes along the way.
 7. Value-add
 Logo Wave exists to help designers and studios secure a greater percentage of bids, land more consistently qualified clients, and raise their rates as a result of added credibility and value.
 Who do you see as your competition (other design/logo awards)?
 It’s interesting to see A’ Design Award and the platform they’ve built. Core77 is doing some nice things as well!
 Did you set any goals at the beginning, or have you reached any milestones that have helped keep you going?
 Initial entries were mostly from freelancers. After a brand refresh, increased cash prize, additional trophies with an updated design, and increased brand recognition, there has been a seismic evolution in the audiences we’ve attracted. The number of studios and agencies deciding to take part has greatly increased in line with the value and credibility of the brand.
 
 
 
 
 
 I’m grateful to every designer who’s reached out over the years to say Logo Wave has helped level-up their business. That encouragement is part of why I love building the brand. Here’s just a couple of testimonials:
 “Logo Wave has been the catalyst in helping me win more large, qualified branding and identity gigs. With their top-tier jury, $1,000/trophy prize, and more than reasonable entry fee, this one is a no-brainer.”
 – Costa Mamangakis, Orfik Design
 “Winning Logo Wave helped my studio generate new business leads and secure new clients as a result.”
 – Filippos Pente, Mistershot
 Those are the kind of stories that keep me trucking along.
 Has anything proved more difficult than expected? Any plans to change the awards in future?
 In early 2020, I was on the cusp of rolling out a membership/subscription option. Feedback from designers was overwhelmingly positive and I was excited for the big launch. Unfortunately, I realised that I didn’t have the time or infrastructure in place for the roll-out, so needed to pump the brakes. That was pretty tough on me. I put a lot of time and effort into its creation, all while running an independent graphic design business. With a commitment to my wife and three kiddos that I’d limit work to 40–45 hours per week I needed to put things on hold until I had right people in place.
 My vision is to pivot Logo Wave into more of a designer ecosystem. Members will have access to our job board with qualified and primarily remote logo/branding/identity gigs. We’ll also have a community forum where like-minded branding designers can support each other and access exclusive discounts on design-centric products such as Adobe CC, Dribbble, etc. and other perks/resources. We’ll still have the logo awards as the cornerstone, with the prizes and recognition that offers.
 Fast forward to the time of this writing, I’ve slowly added new team members and I’m hopeful for a 2023 beta launch of the new membership experience. Designers can reach out here to be a part of our beta.
 I’ve also learned that I have a heart for students and less-experienced creatives. I want to give the next generation a chance to stand out in a crowded marketplace. And this is how Logo Wave Students came to be.
 It’s structured differently. In short, we’ve partnered with universities to bring the competition to the classroom. Whether it’s helping final year students land their dream studio job, or helping attract qualified clients to their independent design businesses, the mission is to make a positive impact on the careers of design students across the globe.
 Have you had any issues with the making or supply of your trophies?
 We had difficult experiences with our first two trophy providers. Items not delivered on time, suppliers forgetting to engrave the winners names, poor customer experience. The list goes on.
 In 2021, Katie Totheroh and the team at Awarding You came along as a breath of fresh air. They’ve been outstanding, taking the custom trophy design I created and producing a perfect replica. Their turnaround times, quality of work, and communication have been superb.
 
 
 
 How can designers enter the awards?
 Upload your logos here, 900 x 900px on a white background, and you’re all set. We get an average of three logos per designer/studio, per wave, and the maximum upload amount is ten. Here are the entry details and other relevant info.
 —
 It’s a pleasure to be part of the esteemed jury, Kyle, and here’s to much success with the membership launch.
 Logo credit:
 Calvary Church, by Logan Brazeau
 Crib Genie, by Adolfo Teixeira
 Mujahid, by Shyam Agarwal
 Penguin Brand, by Francesco Vittorioso
 Fox Stevenson, by Samadara Ginige
 Kieran Hawes, self-designed
 Art as Therapy, by Nothing
 Baby Zebra, by Mr Simc
 Tiki Tonga Coffee, by John Vingoe</content>
     </entry>
     <entry>
       <title>Contributing to MDN: Meet the Contributors</title>
         <link href="https://hacks.mozilla.org/2022/01/contributing-to-mdn-meet-the-contributors/"/>
       <updated>2022-01-18T16:07:59.000Z</updated>
       <content type="text">If you’ve ever built anything with web technologies, you’re probably familiar with MDN Web Docs. With about 13,000 pages documenting how to use programming languages such as HTML, CSS and JavaScript, the site has about 8,000 people using it at any given moment.
 MDN relies on contributors to help maintain its ever-expanding and up to date documentation. Supported by companies such as Open Web Docs, Google, w3c, Microsoft, Samsung and Igalia (to name a few), contributions also come from community members. These contributions take many different forms, from fixing issues to contributing code to helping newcomers and localizing content.
 We reached out to 4 long-time community contributors to talk about how and why they started contributing, why they kept going, and ask what advice they have for new contributors.
 Meet the contributors
 MDN contributors come from all over the world, have different backgrounds, and contribute in different ways. 
 Irvin and Julien’s main area of contribution is localizations. They are part of a diverse team of volunteers that ensure that MDN is translated in seven different languages (Discover here how translations of MDN content happens. 
 Since the end of 2020, the translation of MDN articles happen on the new GitHub based platform.
 
 Irvin, @irvinfly, volunteer from Mozilla Taiwan Community
 I had been a front-end engineer for more than a decade. I had been a leisure contributor on MDN for a long time. I check MDN all the time when writing websites, but only made some simple contributions, like fixing typos.
 In early 2020, the MDN team asked us if zh (Chinese) locale would like to join the early stage of the localization system on Yari, the new Github-based platform. We accepted the invitation and formed the zh-review-team. Since then, I have begun to contribute to MDN every week.
 My primary work is collaboration with other zh reviewers to check and review the open pull requests on both Traditional Chinese and Simplified Chinese locales. Our goal is to ensure that all the changes to the zh docs are well done, both regarding the file format and translations. 
 
 Sphinx  (Julien) (he / him), @Sphinx_Twitt 
 Most of my contributions revolve around localizing MDN content in French (translating new articles and also maintaining existing pages). Since MDN moved to GitHub, contributing also encompasses reviewing other’s contributions. 
 I started to contribute when, having time as a student, I joined a collaborative translation project led by Framasoft. After a few discussions, I joined a mailing list and IRC. One of the first contribution proposals I saw was about improving the translation of the MDN Glossary in French to help newcomers. 
 I started helping and was welcomed by the team and community at that time. One thing led to another, and I started helping to translate other areas of MDN in French.
 Tanner and Kenrick are also longtime volunteers. Their main areas of activity are contributing code, solving issues in MDN repositories, as well as reviewing and assisting the submissions of other contributors.
 In MDN, all users can add issues to the issue tracker, as well as contributing fixes, and reviewing other people fixes. 
 
 Tanner Dolby, @tannerdolby 
  I contribute to MDN by being active in the issue tracker of MDN repositories. 
 I tend to look through the issues and search for one I understand, then I read the conversation in the issue thread for context. If I have any questions or notice that the conversation wasn’t resolved, I comment in the thread to get clarification before moving forward. 
 From there, I test my proposed changes locally and then submit a pull request to fix the issue on GitHub. The changes I submit are then reviewed by project maintainers. After the review, I implement recommended changes. 
 Outside of this, I contribute to MDN by spotting bugs and creating new issues, fixing existing issues, making feature requests for things I’d like to see on the site, assisting in the completion of a feature request, participating in code review and interacting with other contributors on existing issues.
 I started contributing to MDN by creating an issue in the mdn/yari repository. I was referencing documentation and wanted to clarify a bit of information that could be a typo. 
 The MDN Web Docs team was welcoming of me resolving the issue, so I opened and reviewed/merged a PR I submitted, which fixed things. The Yari project maintainers explained things in detail, helping me to understand that the content for MDN Web Docs lived in mdn/content and not directly in mdn/yari source. The issue I originally opened was transferred to mdn/content and the corresponding fix was merged. 
 My first OSS experience with MDN was really fun. It helped me to branch out and explore other issues/pull requests in MDN repositories to better understand how MDN Web Docs worked, so I could contribute again in the future.
 
 Kenrick, @kenrick95
 I’ve edited content and contributed codes to MDN repositories: browser-compat-data, interactive-examples, and yari.
 My first contribution to content was a long time ago, when we could directly edit on MDN. I can no longer recall what it was, probably fixing a typo. 
 My first code contribution was to the “interactive-examples” repo. I noticed that the editor had some bugs, and I found the GitHub issue. After I read the codes, it seemed to me that the bug could be easily fixed, so I went ahead and sent a pull request
 Why contribute?
 Contributions are essential to the MDN project. When talking about why they deem contribution to MDN a critical task, contributors underlined different facets, stressing its importance as an open, reliable and easily accessible resource to programmers, web developers and learners. 
 Contributions to MDN documentation and infrastructure help insure the constant improvement of this resource. 
 Contributions to MDN are important because it helps to provide a reliable and accessible source of information on the Web for developers. MDN Web Docs being open source allows for bugs to quickly be spotted by contributors and for feature requests to be readily prototyped. 
 Building in the open creates an environment that allows for contributors from all over the world to help make MDN a better resource for everyone and that is incredible. (Tanner)
 Contributions to the platform and tools that powers MDN are important to enhance users experience (Kenrick)
 Small and big contributions are all significant and have a real impact. A common misconception about contributing to MDN is that you can only contribute code, but that is not the case! 
 MDN is the primary place for people to check any references on web-dev tech. As small as fixing one typo, any contribution to MDN can always help thousands of programmers and learners. (Irvin)
 Contribution to localization allows learners and developers to access this resource in languages other than English, making it more accessible. 
 Especially for those who are struggling with reading English docs, localization can enable them to access the latest and solid knowledge (Irvin)
 Contributing to localization help beginners on the Web finding quality documentation and explanations so that they can build sites, apps and so on without having to know English. MDN is a technical reference, but also a fantastic learning ground to educate newcomers. From basic concepts to complex techniques, language should not be a barrier to build something on the Web. (Julien)
 
 Contributing is a rewarding experience
 We asked contributors why they find contributing to MDN a rewarding experience. They told us that contribution is a way to help others, but also to learn new things. They spoke about the relationship that volunteers build with other people while contributing, and the possibility to learn from and help others. 
 The part of contributing that I enjoy most is providing a fix for something that positively impacts the experience for users browsing MDN Web Docs. This could be an update to documentation to help provide developers with accurate docs, or helping to land a new feature on the site that will provide users new or improved functionality. Before I started contributing to MDN, I referenced MDN Web Docs very often and really appreciated the hard work that was put into the site. To this day, I’m motivated to continue help making MDN Web Docs the best resource it can be through open source contributions. (Tanner)
 I enjoy finding different points of view on how to achieve the same things. This is natural, since the people I interact comes from different part of the world and we all are influenced by our local cultures (Kenrick)
 The part of contributing I most enjoy is definitely the part when I’m learning and discovering from what I’m translating (…). My best memory to contribute to MDN is that I had the great privilege of spending an evening watching a sunset of lava and sea with people related to MDN for whom I have the deepest esteem. (Julien)
 The journey of contribution itself is important. The support of MDN maintainers and the exchange of ideas is essential. Contribution does not happen in a silo but is a collaborative effort between volunteers and the MDN team.
 My best memory of contributing to MDN would have to be the journey of creating the copy-to-clipboard functionality for code snippets on MDN Web Docs. I remember prototyping the feature in mdn/yari locally and then beginning to see it come to life really quickly, which was wonderful to see. 
 The code review process for this feature was such a joy and incredibly motivating. Each step of the feature was tested thoroughly and every win was celebrated. 
 Each morning, I would wake up and eagerly check my email and see if any “Re: [mdn/yari]” labelled emails were there because it meant I could get back to collaborating with the MDN Web Docs team. This contribution really opened my eyes to how incredibly fun and rewarding open source software can be. (Tanner)
 My best memory of contributing to MDN was working on https://github.com/mdn/yari/pull/172. The change in itself wasn’t big, but the solution changed several times after lengthy discussion. I’m amazed on how open the maintainers are in accepting different point of views for achieving the end goal (Kenrick)
 Contributions to be proud of
 All contributions are important, but some hold a special place with each volunteer.
 The contribution that I’m most proud of is adding copy-to-clipboard functionality to all code snippets for documentation pages on MDN Web Docs. I use this utility very often while browsing pages on MDN Web Docs and seeing a feature I helped build live on the site for other people to use is an amazing feeling. 
 This contribution was something I wanted to see on the site and after discussing the feature with the Yari team, I began prototyping and participating in code review until the feature was merged into the live site. This utility was one of the first “large” feature requests that I contributed to mdn/yari and is something I’m very proud of. (Tanner)
 The contribution I am most proud of is having the HTML, CSS, and JavaScript section complete and up-to-date in French in 2017 after being told this would be impossible :) . More recently, helping rebuilding tools for localizers on the new MDN platform with a tracking dashboard (Julien)
 Kenrick was most proud of adding a feature that marks the page you are looking at in the sidebar. This change makes a significant difference for visual learners. 
 It was a simple change, but I felt that this UX improvement is important because it serves as a guide to the reader to check what are the documents related to the one they are reading. 
 
 Getting started 
 There are many ways to contribute to MDN! Our seasoned contributors suggest starting with reporting issues and trying to fix them, follow the issue trackers and getting familiarized with GitHub. Don’t be afraid to ask questions, and to make mistakes, there are people that will help you and review your work.
  Go at your own pace, don’t hesitate to ask questions. If you can, try to hack things to fix the issues you encounter on a project. If you are eager to learn things about the Web, check MDN as a way to contribute to open source (Julien)
 Suppose you become aware of a bug in any MDN doc (such as a typo), you are welcome to fix them directly by clicking the “Edit on Github” button. The review team will ensure it’s good, so you don’t need to worry about making any mistakes. (Irvin)
 From taking the first steps, contributors can then progress to more difficult issues and contributions. 
 Don’t be afraid of reading code. Pick up any issue from GitHub, and you can easily start contributing code! (Kenrick)
 My advice for new contributors or those getting started with open source is to get familiarized with the project that they wish to contribute in and then begin staying up-to-date with the issue tracker. 
 Start being active in the project by looking through issues and reading through the comments, this is a sure-fire way to learn about the project. If there is something that you aren’t ready to contribute but want to have a conversation about, drop a comment in the issue thread or create a discussion in the repository for a great way to inspire conversation about a topic. 
 Lastly, understanding a version control software like Git is recommended for those that are considering starting to contribute to open source software. Be open to help in any way you can when first getting started in open source, I started small with documentation fixes on MDN Web Docs and then gradually worked my way into more complex contributions as I became more familiar with the project. (Tanner)
 If you want to start contributing, please check out these resources:
 
 Contributing to MDN
 MDN activity
 
 If you have any questions, join the matrix chat room for MDN.
 The post Contributing to MDN: Meet the Contributors appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>2021 into 2022</title>
         <link href="https://www.taniarascia.com/2021-into-2022/"/>
       <updated>2022-01-01T00:00:00.000Z</updated>
       <content type="text">Wow, here we are, the sixth installment in the New Year series of posts.
 
       
     
   
   
     
 This year feels almost like a lost weekend. Aside from a few small cabin and camping trips throughout the year, I hardly left my city or even my home. I spent much of the time relaxing, reading books, being with friends and family, playing games, and I even made a few paintings.
 It&#x27;s been good to focus on myself and not feel guilty about not writing enough or creating enough. It&#x27;s hard to believe it, but I&#x27;ve been working as a developer and creating all my side projects for nearly eight years now. I&#x27;ve experienced major burnout in the past and that has manifested in different ways, such as quitting my job and taking a three-month long solo stint in Europe, as well as long periods where I didn&#x27;t look at any code outside of work. Burnout is a hard thing to get over so I&#x27;m very careful about not over doing it.
 Right now, I&#x27;m still working on trying to balance my life, my job, and my creative endeavors both in coding and otherwise.
 Overall, I can say I&#x27;m content. Every day, I&#x27;m happy to be alive. I marvel at the simple fact that I can move my hands and create with them, and the fact that I can connect with other human beings. I&#x27;ve worked hard and it has paid off in my professional life, and I&#x27;m proud of that.
 Here&#x27;s a couple of cool things I discovered this year:
 
 
 Inside - Bo Burnham - Being a fan of Bo Burnham&#x27;s previous comedy specials, I clicked on Inside the instant I saw it pop up on Netflix. I felt completely captivated from the first to the last instant, totally glued to the screen. There&#x27;s nothing more to say about Inside that hasn&#x27;t already been said, and it&#x27;s extremely polarizing - everyone either loves it or hates it. For me, it&#x27;s the only pandemic-related piece of pop culture I want or need.
 
 
 Get Back - The Beatles Documentary - Like many, I discovered The Beatles at some point in my youth and became obsessed with them. Once this has happened, no matter how long you&#x27;ve gone without listening to The Beatles or thinking about them, anything can set you off and start the obsession all over again. Having recently watched this documentary directed by Peter Jackson about the Let It Be sessions, Abbey Road is back to turning on my record player, and I&#x27;m listening to many songs with a new appreciation. I&#x27;ve also been exploring and enjoying Paul McCartney&#x27;s solo albums. Temporary Secretary might be the most interesting one...
 
 
 A few more - Miracle Musical, Michael Kiwanuka, and Lord Huron.
 
 Children of Time - Adrian Tchaikovsky - I&#x27;ve had mixed luck with just picking up a random book and seeing how it goes. With The Expanse series, which I picked up off the shelf randomly a few years and didn&#x27;t know that it was popular and had a TV show, it was an amazing choice and just what I was looking for. The last book in the series came out recently and brought it to a close. I picked up a lot of books that I was less than satisfied with, but Children of Time really surprised me. It didn&#x27;t have the amazing character development I crave from reading The Expanse or A Song of Ice and Fire, but it has a very interesting, unique, and compelling story.
 
 A few more - Demon Haunted World (Carl Sagan), Sapiens.
 As for resolutions? I wouldn&#x27;t say I have any resolution technically, but first and foremost, I want to focus on working on myself, overcoming my own issues and understanding myself better. My priority is still spending time with my friends and family, appreciating the people close to me and enjoying the time we have together. I would like to write more, particularly more non-technical writing. I&#x27;d like to record more songs, a favorite hobby of mine that I haven&#x27;t done for several years. I&#x27;d like to continue focusing on my health, working out and paying attention to what I eat. Mostly, I want to keep doing what I&#x27;m doing.
 I wrote 7 technical articles
 I haven&#x27;t written as much this year as in previous years. However, I&#x27;m proud of the quality of the articles I&#x27;ve written.
 I ended up writing about one technical article every two months. The articles I&#x27;m writing are more focused and specific, so I think they reach a smaller audience than before. I hoped that I might write more non-technical articles by adding a new section, but I only wrote one this year.
 
 
 Integration Tests with Jest, Supertest, Knex, and Objection in TypeScript - this article is a little misleading, in that it&#x27;s a lot more useful and interesting than the title suggests, at least to me. Testing is kind of a boring topic, but I was proud of getting the full integration test suite set up for a TypeScript API. The more interesting part is the TypeScript API itself, using Express, Knex, and Objection ORM.
 
 
 How and When to Use Context in React with Hooks - I often write articles for my own reference, and the original article I wrote on React Context was out of date, since it relied on class components. I wrote this one to give a real world example of why you might use context, when Redux might be better, and how to use it.
 
 
 Using OAuth with PKCE Authorization Flow (Proof Key for Code Exchange) - the most up-to-date secure way to handle OAuth authentication for a web or mobile app is using the PKCE flow. This article eplains the different flows, and how to set up the code challenge/code verifier in JavaScript for PKCE.
 
 
 React Architecture: How to Structure and Organize a React Application - React architecture is a topic that&#x27;s surprisingly hard to find good resources on. In the past I set it up the way I had seen other people do it, but when I discovered a domain-based approach, I decided to write about it to help anyone in the future looking for a better way.
 
 
 Writing a Sokoban Puzzle Game in JavaScript - I had fun using what I learned on the Chip-8 project to make another little puzzle game in JavaScript. I messed it up by allowing you to push multiple blocks at once (which was actually the hardest part to program) but I left it in.
 
 
 Front End Tables: Sorting, Filtering, and Pagination - every job I&#x27;ve had in development has involved tables, usually with sorting, filtering, and pagination on them. Previously I wrote an article about adding all those features to a back end API, and this article goes into doing it on the front end. There are so many (usually outdated) libraries for dealing with tables, but depending on your needs, it&#x27;s often much more effective to build it yourself.
 
 
 An Introduction to GraphQL - my only DigitalOcean article of the year, this is the first article about GraphQL. GraphQL is another one of those things that&#x27;s scary until you delve into the core of it and strip away all the libraries and implementations (such as Apollo). I&#x27;m planning on writing more on the topic.
 
 
 I started a new job
 When 2020 began, I was working at Yum. I had started in February/March of 2020, right when the lockdowns began. I had never worked remotely before and never really wanted to, but I learned that being remote really works for me. I started a new job as a Staff Software Engineer in mid 2021, and it&#x27;s been going great. When not everything is certain and life is confusing, having an everyday routine, obbligation, and a job that I enjoy and am good at adds stability to my life.
 I redesigned my site
 Once again, I redesigned my website, a particular hobby of mine. This was version 5.0, or so I&#x27;ve decided to call it. Realistically, there have been hundreds of &quot;versions&quot; of the website, from all the tweaks I&#x27;ve done over the years.
 It was fun to do and make my site look like a programming IDE as well as having interactive elements like the theme color, but personally I think it&#x27;s too busy and will probably redesign it once again in 2022 to try to keep it minimalist while still having a personal touch. I do enjoy the pixel art I made for it the most.
 I got a kitten
 Not much to say here, but definitely the biggest life change for me of the year. I got a little 10-week-old kitten in August as a family friend&#x27;s cat had a litter, and named him Dimo. He&#x27;s the most adorable little thing and keeps me company every day as I work.
 It seems like not really a big deal, but it was for me. Making the conscious decision to take care of a life for the next 20 years was a big responsibility to me. Having a little animal around has brought a lot of joy into my life!
 
       
     
   
   
     
 I made a lot of commits
 Through it all, I&#x27;ve been churning out code. A smattering of updates here and there on my public repo, and a very consistent output on my work repo.
 
       
     
   
   
     
 
       
     
   
   
     
 Well, that&#x27;s it for now! Happy New Year!</content>
     </entry>
     <entry>
       <title>Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author</title>
         <link href="https://hacks.mozilla.org/2021/12/hacks-decoded-sara-soueidan-award-winning-ui-design-engineer-and-author/"/>
       <updated>2021-12-30T15:13:51.000Z</updated>
       <content type="text">Welcome to our Hacks: Decoded Interview series! 
 Once a month, Mozilla Foundation’s Xavier Harding speaks with people in the tech industry about where they’re from, the work they do and what drives them to keep going forward. Make sure you follow Mozilla’s Hacks blog to find more articles in this series and make sure to visit the Mozilla Foundation site to see more of our org’s work.
  
 Meet Sara Soueidan!
 
 
 
 
 Sara Soueidan is an independent Web UI and design engineer, author, speaker, and trainer from Lebanon.
 Sara has worked with companies around the world, building web user interfaces, designing systems, and creating digital products that focus on responsive design and accessibility. She’s worked with companies like SuperFriendly, Herman Miller, Khan Academy, and has given workshops within companies like Netflix and Telus that focus on building scalable, resilient design.
 When Sara isn’t offering keynote speeches at conferences (she’s done so a dozen times) she’s writing books like “Codrops CSS Reference” and “Smashing Book 5.” Currently, she’s working on a new course, “Practical Accessibility,” meant to teach devs and designers ways to make their products accessible.
 In 2015, Sara was voted Developer of the Year in the net awards, and shortlisted for the Outstanding Contribution of the Year award. She also won an O’Reilly Web Platform Award for “exceptional leadership, creativity, and collaboration in the development of JavaScript, HTML, CSS, and the supporting Web ecosystem.”
 We chatted with Sara about front-end web development, the importance of design and her appreciation of birds.
 
 
 
 Where did you get your start? How did you end up working in tech?
 I took my first HTML class in eighth grade. I instantly fell in love with it. It just made sense; and it felt like a second language that I found myself speaking fluently. But back then, it was just another class. As I continued my journey through high school, I considered architecture as a major. I never thought I’d major in anything even remotely related to tech. I always thought I’d choose a career that had nothing to do with computers. In fact, before choosing computer science as a major, I was preparing to study architecture in the Faculty of Arts.
 Then, life happened. A series of events had me choosing CS as a major. And even after I did, I didn’t really think I’d make a career in tech. I spent 18 months after college pondering what I could do for a living with a CS major in Lebanon, but I didn’t find my calling anywhere.
 My love for the web was rekindled when someone suggested I learn web development and try making websites for a living. The appeal of that was two-fold: I’d get to work remotely from the comfort of my home, and I’d get to be my own boss, and have full control over my time and the work that I choose.
 After a few weeks of learning modern HTML and CSS, and dipping my feet into JavaScript, I was hooked. I found myself spending more time learning and practicing. Codepen was new back then, and it was a great place to do quick code exercises and experiments. I also created a one-page Web site — because if you’re going to work freelance and accept work requests, you gotta have that!
 As I continued learning and experimenting for a few months, I started sharing what I learned as articles on a blog that I started in 2013. A few weeks after I published my first article, I got my first client request to create the UI for a Facebook-like Web application. And over the course of the first year, I got one small client project after another.
 My career really kicked off though in 2014. By then, I was writing more, getting more client work, and writing a CSS reference for Codrops. Conference speaking invitations started flooding in after I delivered my first talk at CSSConf in 2014. I gave my first workshop in LA in 2015. And I have been doing what I do now since.
 I am grateful things didn’t work out the way I wanted them to after high school.
 You’ve been programming for a while now, you’ve co-authored a book about the craft, you’ve created guides like the Codrops CSS Reference — what drives you?
 A thirst for knowledge and a craving for variety in work. I don’t think I’d be inspired enough to do any kind of work that doesn’t satisfy both. I also need to feel like I’m doing something meaningful, like helping others. And I’ve been able to fulfill all of these needs in this field. That’s why I fell in love with it.
 Being independent, I have full control over my time and the type of work I spend it on. While building websites is my main work and source of income, I do spend a large portion of my time switching between writing, editing, giving talks, running workshops (in-house and at events), making courses (this one’s new!) and working on personal projects.
 Everything I do complements one another: I learn, to write, to teach; I code, to write, to speak; I code, to learn, to share. It’s a wonderful circle of creative work! This variety helps keep the spark alive, and helps me rekindle my passion for the web even after frequent burnouts.
 I like that I must keep learning for a living! And that I get to also teach (another passion and — dare I say — talent of mine) as part of my job. I teach through writing, through speaking, through running workshops, and even through direct collaboration with designers and engineers on client projects.
 I always think that even if I end up changing careers, I would still make some time to fiddle with code and make web projects on the side of whatever else I’d be doing for a living.
 When it comes to front-end versus back-end versus full stack, you seem to be #TeamFrontEnd. What is it about front-end web and app development that calls your name (more so than back-end)?
 I love working at the intersection of design and engineering! This is the area of the front end typically referred to as “the front of the front end.” It is the perfect sweet spot between design and engineering. It stimulates both parts of my brain, and keeps me inspired and challenged — a combination my brain needs to stay creative.
 I find building interfaces fascinating. I love the fact that the interfaces I build are the bridge between people and the information they access online.
 That comes with great responsibility, of course. Building for people is not easy because people are so diverse and so are the ways they access the Web. And it’s the interfaces they use that determine whether they can!
 It is our responsibility as front-end developers and designers to ensure that what we create is inclusive of as many people as possible.
 While this may sound intimidating and maybe even scary, I find it inspiring. It is what gives more meaning to what I do, and what pushes me to keep learning and trying to do better. The front of the front end is where I found my sweet spot: a place where I can be challenged and inspired.
 A couple of years ago, I was feeling this so much that I shared that moment on Twitter. Among the many replies I got, this quote by Douglas Adams stuck with me:
 “We all like to congregate, at boundary conditions. Where land meets water. Where earth meets air. Where body meets mind. Where space meets time.”
 What do you love about coding? What’s your least favorite part?
 My favorite part is the satisfaction of seeing my code “come to life”. The idea that I can write a few lines of code that computers understand, and that so many people can consume and interact with it using various technologies — present and in the future.
 I also appreciate the short feedback loop in modern code environments: you write code or make changes to existing one, and see the results immediately in the browser. It is almost magical. And who doesn’t like a little bit of magic in their lives?
 My least favorite part, however, is that it requires so little movement. There is life in movement! One of my favorite yoga teachers once said: “Once you stop moving, you start dying.” And I felt that. Spending so much time in front of a screen is very taxing.
 Regular exercise is crucial for my ability to continue doing what I do. But I still sometimes feel like I need more movement during my work sessions. So I got a standing desk a couple of years ago.
 Switching between standing and sitting gives my body short “breathers” throughout the day and allows for better blood flow. A balanced lifestyle is crucial to maintaining a good health when you spend as much time in front of a screen. Try to move, drink lots of water, and go outside more.
 You’re based out of Lebanon. What’s something many folks may not realize about the tech scene there?
 I know this isn’t the answer you’re expecting, but I think what many people don’t realize about the tech scene here is how challenging it is! In Lebanon, we live in a country that has a massive, serious, and ongoing power crisis.
 This crisis, as you can imagine, affects almost every facet of our lives, including the digital. You need power to do work. And you need an internet connection to do work. We’ve always had problems with internet speed. And with the fuel shortage, full power outages, and reception problems, having a reliable connection is less likely than before.
 But there are some incredibly talented designers and developers still making it work through this all. Living in Lebanon brings daily challenges, but being challenged in life is inevitable.
 I try to look on the bright side of everything. Working on a slow connection has its upsides, you know. You learn to appreciate performance more and strive to make better, faster Web sites. You appreciate tech like Service Worker more, and learn to use it to make content available offline. If anything, living here has made many of us more resilient to change, and more creative with our solutions in the face of crisis.
 How do you find (tech) supporting communities in Lebanon, if not where does your community live?
 I don’t. But that’s mainly because I live in an area with no active tech community. And I live far from where any tech meetups happen. I also don’t know any front-end focused developers in Lebanon. I’m sure they exist; it’s just that, being the introvert that I am, I don’t happen to know any. So my community is mainly online — on Twitter, and in a couple of not-very-busy Slack channels.
 Ok, random question. We’ve gotta know about the birds. You’ve raised at least a dozen. What’s the story there?
 It all started back in 2009, I think. A close friend had, for whatever reason, decided that I might enjoy taking care of baby birds. So, he got me a baby White-spectacled Bulbul (my favorite bird species currently), with all the bird food I needed to start. He taught me what I needed to know to take care of it. And he told me that, when it grows up, it won’t need to live in a cage because I would be its home. I had no idea back then how much I’d fall in love with that bird.
 I’ve raised 10+ birds since. Not a single one of them was kept in a cage. I would raise them and train them so that, when they grew up, they would fly out in the morning — making friends, living like they were meant to, and return home before the end of the day.
 They would drink from my tea cup, share my sandwiches, eat out of my plate (mainly rice) and spend most of the day either sitting on my shoulder and head, or napping on my arm. Friends have always told me that I was like a Disney princess with my birds. I’m not sure about that, but it did sometimes feel that way. x)
 Here’s a photo of my last two baby birds from a couple of years ago. I took them out in a car drive to “explore the outside world” for the first time.
 They just sat there chilling on my arm, as they watched the world (cars, mainly) pass by.
 
 Years after my friend got me my first bird, I asked him why he did, and whether he knew about the connection that was going to happen. His answer was short. He said: “You have the heart of a bird. I knew you’d love creatures that are like you.”
 Another random question: In an interview, you mentioned mainly working in the morning (6am-10am), and slowing down after lunch. You’re like me! How important is a flexible work day to your workflow? (And how do we convince more people that 9-to-5 work isn’t realistic for everyone? How do we normalize hard work in the morning, meetings and calls in the afternoon?) 
 I can’t imagine myself working on a 9-to-5 schedule! That’s actually one of the few reasons I never took a full-time job. As I mentioned earlier, flexibility was a key factor in choosing a freelance career.
 I am an early bird. On a typical day, I wake up no later than 5:30 in the morning. So my day starts very early. My brain’s information retention powers are at their highest early in the morning. So I get my best work done during that time. With my brain firing on all cylinders, I make quite a bit of headway with the day’s tasks. What makes this time even more productive is the fact that there are no expectations, nor interruptions: no emails, no client communication, not even any IRL interruptions.
 The earlier you start in the day, and knowing that most people are only really productive for about 4.5 hours a day, I believe it makes a lot of sense to slow down after lunch.
 I realize this is easier said than done, though. Being freelance gives me this flexibility but I realize others may not have that working full time. But with more companies going fully or partially remote now, I think more people will hopefully get to choose when they work during the day.
 You’re working on an accessibility course, can you talk a bit about why you decided to develop this course and the importance of creating more accessible web interfaces?
 Before COVID-19 hit, I traveled to run workshops at conferences and in-house at companies. The lockdown had us all, well, locked down, so that was put on temporary hold.
 Over the years, I collected some amazing feedback to my accessibility workshop from former attendees. I knew I had useful content that many others would find helpful.
 As many events went online, running the workshop online was the sensible plan B. But the fact that my Internet was unreliable made that a little risky — I wouldn’t want my internet connection to fail in the middle of an online workshop! So that plan was put on hold too.
 On the other hand, working with designers and engineers on client projects made me realize that there was a big accessibility knowledge gap in most companies I’ve worked with. I love to teach teams I work with about accessibility at every chance I get, but there’s only so much you can share in Zoom meetings and Slack channels. In-house workshops were not always an option, and online training was not feasible at the time.
 And last but not least, I noticed that there is quite a bit of misinformation and bad advice circulating the web community around accessibility. You can cover a good amount of information in articles, but I already had a good bunch of content I could start with from the accessibility workshop that I can use as a foundation for a more comprehensive series of teaching materials — sort of like a mini curriculum.
 By developing this course I am scratching my own itch. All the reasons mentioned above had me wishing I had created a course that I could share around, especially with client teams, and then with members of the community. So with the time I have in between client projects and speaking, I started working on it!
 The course is called Practical Accessibility, and is still under active development, coming in 2022. The content of the course is going to be much more comprehensive than that of the workshop, and it will cover much more ground, and hopefully be a great foundation for anyone wanting to learn how to create more accessible websites.
 Of everything you worked on, what’s your favorite?
 Out of all the projects I’ve worked on, probably the one that stood out for me is a project for Herman Miller that I collaborated with SuperFriendly on. The project was under NDA, and was discontinued a few weeks after COVID-19 hit and the world realized it was going to change moving forward; so I, unfortunately, don’t have any details to share about the project itself.
 But what made this opportunity so special is that this was the first and only project that I was involved in from the very start— from early kick-off meetings and ideation, through research and user testing, UX and UI design, and development. I learned so much working with an amazing group of SuperFriends. The trip to the Herman Miller showroom in Atlanta, where we ran a workshop with the team at Herman Miller, was the last trip most of us took before the big lockdown.
 Herman Miller is a furniture company. And what many people don’t know about me is how much I love interior design. I even took an interior design course last year! So, on this project, I got to (1) work with an amazing team (who I get to call my friends now ), (2) on a creative project, (3) for a company specializing in making modern furniture, (4) in the field of interior design! How could I not love that?!
 The cherry on top of the cake was that I got a generous discount which I used to upgrade my office chair and desk to an ergonomic Herman Miller chair and standing desk. So even my body and health were thankful for this opportunity!
 
 Final question: What would you tell folks learning a programming language or aspiring to be a front end developer, or any sort of developer. What advice would you give them?
 Learn the fundamentals — HTML, accessibility, CSS, and just enough vanilla JavaScript to get started. Build upon those skills with tools and frameworks as your work needs.
 Don‘t get intimidated or overwhelmed by what everybody else is doing. Learn what you need when you need it. And practice as much as you can. Practice won’t make you perfect because there is no Perfect in this field, but it will make you better!
 This probably should have been the first piece of advice though: Put the user first. User experience should trump developer convenience. Once you let that guide your work, you’re already halfway through to being a better developer than many others.
 Oh and last but certainly not least: Create a personal website! Own your content. And share your work with the world!
 —
 You can keep up with Sara’s work by following her blog on her personal site here. Stay tuned for more Hacks Decoded Q&amp;A’s!
 The post Hacks Decoded: Sara Soueidan, Award-Winning UI Design Engineer and Author appeared first on Mozilla Hacks - the Web developer blog.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 294: Floating UI, Aria not allowed, Open Props, Mercy and Opinion overload reduction</title>
         <link href="https://wdrl.info/archive/294"/>
       <updated>2021-12-27T16:30:00.000Z</updated>
       <content type="text">Hey,
 
 every year towards the end of it I want to remind you that if you like my work, you can help financing this newsletter by giving a contribution. I want to say thank you to everyone here who reads or supports this project.
 This year has been challenging for many. For me, it definitely was. Thankfully, I made a very good decision three years ago that enables me to be outside in nature most of the days. It helps me a lot dealing with the many challenges that are in everyone’s heads right now. Especially now as a father of a young child, it’s not always easy to stay positive, to not be stressed out or discuss, talk about certain topics all the time. But nature has helped me, my child has helped me, meditation and focusing on my own opinion, on my own body, on my own health helps me staying sane. A big reveal was during my Covid infection: Once I realised how “pre-informed” I was about the disease, I stripped out this bias and listened to my body instead, my health improved next day and recovery was very quick from then on while it stalled over some days before. I share this because I think it’s very important to listen to yourself, to keep track on your own health instead of being drawn down by everyday TV shows that don’t even offer any solution but only share how bad everything is.
 My wish for the future is to empower you to be your own you. You already have the ability, now to find out how to enable it. It may not be the easiest but probably worth it.
 There’s a new year coming and I’d love to see that more people will trust their own mind and body again and work towards this goal. Read you next year!
 News
 
 	Do you like or do you not like Tailwind CSS? No matter which camp we are in, we should give love and respect to Adam and the team. The output, communication, and quality are impressive. Tailwind CSS v3.0 was released.
 
 Generic
 
 	It feels like there is an unstoppable unhealthy business around web analytics. Jeremy Keith points out why he thinks tracking users is wrong.
 	Some small tips to make it easy for newcomers to join your team and participate in your codebase.
 	David Heinemeier Hansson is known for his absolute statements. This time he writes about not celebrating incompetence. We are competent professionals, and we should act like it.
 
 UI/UX
 
 	There is so much to learn about writing and formatting text. Also, there is so much to achieve by using just words and HTML. Slava Shestopalov teaches us all about setting better links for websites and emails.
 	Floating UI is a small and modern library that offers you elements floating to another element (e.g. tooltips).
 
 Accessibility
 
 	Martin Underhill shows us a real-world example of the difficulties between user experience, technical complexity, and valid ARIA.
 
 CSS
 
 	Open Props is a set of open source CSS Custom Properties that are providing a consistent set of defaults working with dark and light and various other theme options.
 
 Go beyond…
 
 	I know this comes too late for Christmas but it doesn’t come too late to put these tips into action. Giving gifts that matter shares some nice examples of gifting your beloved ones with something that really comes from your heart and isn’t just something that we buy to have a gift because we’re used to this habit.
 	How to feel gratitude as perfectionist is important to so many people and helped me better understand why it’s so hard for me to accept things, to feel real gratitude in so many situations. But training helps a lot once you try to understand and change this.
 	A heart of Mercy describes how we can transform other people’s lives and our own by being open-minded and trying to understand other people even if we disagree, by showing and telling mercy and empathy.
 	
 We&#x27;re blasted with views from others everywhere we look. It seems everyone has an opinion on how best we should conduct our lives, spend our money, and other things.
 
 Dr. Egypt Iredia gives some great tips on how we can reduce opinion overload.
 
 
 I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you like this newsletter, you can contribute to financing the project. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Marked with thought #3</title>
         <link href="https://www.logodesignlove.com/marked-with-thought-3"/>
       <updated>2021-12-14T16:06:13.000Z</updated>
       <content type="text">
 
 The original Vancouver Canucks logo was designed in 1970 by freelance graphic designer and lifelong hockey fan Joe Borovich. Joe heard rumours that the Canucks would be entering the NHL, so he crafted a new team logo and took it to the Canucks’ stadium office for consideration. About a month later Joe got the approval call and was invited back to design tickets and various promotional items. The stick and rink design has proven timeless, combining the shape of a hockey rink with a hockey stick to form a C for Canucks.
 
 Medivet was founded in 1986 with to raise standards of veterinary care. With plans for expansion across the UK, Medivet hired Turner Duckworth who redesigned the visual identity with an unmistakable M icon. It’s the simplicity that makes this a memorable veterinary mark.
 
 Swirl is a vegan ice cream shop in Montréal. The tasty, flowing wordmark, designed by Montréal-based Wedge, was inspired by the product’s smooth creamy texture and trail left by a spoon.
 
 The three founding partners of Cubic Metre Furniture all had names beginning with M. The wonderfully fitting idea was conjured by Minale Tattersfield in 1981.
 
 Dutchscot’s fitting logo for Feed — a new content creation agency — is “a living, breathing representation of the word, constantly in a state of transition, scrolling as a user would on their phone, tablet or computer.” Excellent animated context on the Dutchscot website.
 
 The Guild of Food Writers is an established organisation dedicated to excellence in food writing and culinary education. This spoon inside a pen nib is timeless, crafted in 2005 by the gone-but-not-forgotten 300million. “What you take away is just as important as what you keep,” said Katie Morgan, senior designer at 300million.
 
 The CultureBus monogram was designed by Kit Hinrichs while at Pentagram (in 2008 as far as I can tell). “The logo brings together the letters C and B in a form that resembles a route.” While the San Francisco bus service only lasted a year or two before being parked, the clever thinking behind the mark lives on.
 
 Positioning a woofer and tweeter to form a stylised letter g helped Goodmans Loudspeakers leave their mark. Designed in 1992 by Associated Design Consultants.
 
 In 2005 Malcolm Grear Designers came up with this intelligent use of negative space for the New Bedford Whaling Museum. The logo reflects the idea of “whaling in the age of sail.”
 
 Mr Cooper is an an ice-cream business specialising in alcoholic and gourmet flavours. The challenge (in 2015) for Johnson Banks was to use positive and negative space to spell out the brand name and the words “ice cream” within their idea of a typographic lipstick mark. Type specialist Rob Clarke was brought in to add some finesse. Kath Tudball (then Johnson Banks, now creative director of Superunion) narrated an intriguing design walkthrough.
 More in the series:
 Marked with thought #1
 Marked with thought #2</content>
     </entry>
     <entry>
       <title>Made by James</title>
         <link href="https://www.logodesignlove.com/made-by-james"/>
       <updated>2021-12-06T15:36:39.000Z</updated>
       <content type="text">
 
 James Martin, also known as Made by James, is a UK-based graphic designer who specialises in logos. Working from his studio in Shedfield (south England), James recently completed his hardback book, Made by James. He shares his journey of how he became a full-time logo specialist, and discusses the tools and techniques he uses on a daily basis working with clients on a wide variety of logo projects.
 
 
 
 
 
 The book’s aimed at those just getting started, and covers topics such as finding work, design pricing, coming up with ideas, red flags. It’s similar in many ways to Logo Design Love (it’s a logo process book after all) but this is all James — no other designers or studios involved — and his personality shines throughout.
 
 
 
 
 
 In his own words, as a designer James is eighty percent drawing, and that’s what I like most about the book. James doesn’t hold back on sharing his sketches. I’m positive that’s what contributed to his huge Instagram appeal, too, because there’s an insight you get from sketches that can’t be seen in any other part of a design project.
 
 
 
 
 
 
 
 
 
 
 One man’s raw, honest look at how he’s made a career creating logos, Made by James is published by Rockport, and is available for pre-order through Amazon.com (.co.uk), shipping in January/February 2022.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 293: AI translations, the golden ratio, line length challenged and sticky solutions</title>
         <link href="https://wdrl.info/archive/293"/>
       <updated>2021-12-02T07:20:00.000Z</updated>
       <content type="text">Hey,
 
 every year towards the end of it I want to remind you that if you like my work, you can help financing this newsletter by giving a contribution. At this point I want to say thank you to every single reader, to everyone who supports me, sends me feedback or suggestions. I love this project so much because my readership (you!) is so amazing.
 
 When I looked up something these days in my archive, I realised how true the introduction from last year is still today.
 On the topic of creating for the web: I personally think micro-typography, good translations and good content writing matters a lot to the success of a product. So I wrote an article on well crafted translations and why automation isn’t good enough yet, showing some prominent examples that just aren’t good enough.
 News
 
 	PHP loses one of its main contributors but the good part is that this triggered a process of building the PHP foundation. This is cool because it’s a new level of stability for the product that most of the internet builds on.
 	Each year, a couple of big companies publish the news color trends for the upcoming year. This time, I found Shutterstock’s Color Trends 2022 and have to say I like their way of defining it, showing it and just delivering one hex color code for it. With Pantone Color Trends it’s more tricky and I just realised they now sell an entire book on trends and only publish some fashion color trends for the next year without providing any useful color codes anymore.
 
 Generic
 
 	Arjuna Sky Kok wrote a pretty complete guide on how to create, retrieve and format date and time in PHP.
 	Brent Roose &amp; Freek Van der Herten published a free modern PHP cheat sheet, serving also as summary of all awesome PHP features up to PHP8.1.
 
 UI/UX
 
 	Line length revisited: following the research is a in-depth article on typography and reading experience challenging the status quo of how wide a text should flow. By Mary Dyson.
 	Kelley Gordon shares how we can use the golden ratio for UI design. A pretty interesting read I used a lot in past projects already since I heard of the concept in school. Back then I learned what it does, how important it is for photography and so I applied it naturally for designs as well. Nowadays I think it’s not applicable everywhere or for all projects but often helps to design better interfaces.
 
 Tooling
 
 	ssshape is a SVG blob/shape generator that is pretty handy for nice elements when you don’t want to open one of the big vector applications.
 
 Web Performance
 
 	Harsh people say Safari is the new Internet Explorer, but we should remind ourselves that real people are working on the Safari web browser. So let us treat them with respect and be friendly. On November 15th, there was an amazing Safari release. Safari Tech Preview now has lazy image loading, accent-color for browser UI controls enabled by default and supports dynamic viewport units, flex-basis: content, and rel&#x3D;&quot;noopener/noreferrer&quot; for form elements. Pretty cool for just one preview release, isn’t it?
 
 JavaScript
 
 	Ivaylo Gerchev shares the Vue 3 Composition API and its advantages over previous methods. If you’re using Vue, this is a cool intro and comparison what to use for which solution.
 	Until near future, we need to reverse an array to find values or its index in an JavaScript array which causes an array mutation and is a memory-increasing action. Luckily, there&#x27;s an ECMAscript proposal for findLast and findLastIndex.
 	Rematch.js is using the best practices from Redux but without the boilerplate and it’s super tiny (2kb). It has TypeScript support, native async/await, a simple plugin API, and needs no configuration. Usable with React, Vue, Angular or others.
 
 CSS
 
 	A sticky footer solution by Sílvio Rosa that only needs two lines of CSS. Sticky footer means the element is on the bottom of the page even if not enough content is there while not being in fixed position on the site as well.
 	Ahmad Shadeed often writes about little findings in CSS that are super useful. Such as this one where he explains why position: sticky does not work automatically with CSS Grid and how to use them in combination.
 	It looks like there is finally a way to animate a dynamic height using only CSS. Nelson Menezes shows us how he achieved this in a funny and interesting read.
 	This is a very interesting case of why it’s best to avoid !important: While you can use !important in CSS Custom Properties, it doesn’t work as we’re used to. Instead, it’s stripped from the definition by the parser but influences the CSS cascade. See the article to understand this better.
 
 Go beyond…
 
 	Habits and streaks are cool but what happens usually when we miss a day and break our streak? We tend to let the habit fall completely. When things aren’t going according to plan, Practicing with Zero is simply pausing, and deciding how we want to proceed from here. With intention.
 
 
 I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you like this newsletter, you can contribute to financing the project. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>On Yak Shaving and , a new HTML element for Markdown</title>
         <link href="https://lea.verou.me/2021/11/on-yak-shaving-and-md-block-an-html-element-for-markdown/"/>
       <updated>2021-11-26T16:08:17.000Z</updated>
       <content type="text">This week has been Yak Shaving Galore. It went a bit like this:
 
 
 
 I’ve been working on a web component that I need for the project I’m working on. More on that later, but let’s call it &lt;x-foo&gt; for now.Of course that needs to be developed as a separate reusable library and released as a separate open source project. No, this is not the titular component, this was only level 1 of my multi-level yak shaving… I wanted to showcase various usage examples of that component in its page, so I made another component for these demos: &lt;x-foo-live&gt;. This demo component would have markup with editable parts on one side and the live rendering on the other side.I wanted the editable parts to autosize as you type. Hey, I’ve written a library for that in the past, it’s called Stretchy! But Stretchy was not written in ESM, nor did it support Shadow DOM. I must rewrite Stretchy in ESM and support Shadow DOM first! Surely it won’t take more than a half hour, it’s a tiny library.(It took more than a half hour)Ok, now I have a nice lil’ module, but I also need to export IIFE as well, so that it’s compatible with Stretchy v1. Let’s switch to Rollup and npm scripts and ditch Gulp.Oh look, Stretchy’s CSS is still written in Sass, even though it doesn’t really need it now. Let’s rewrite it to use CSS variables, use PostCSS for nesting, and use conic-gradient() instead of inline SVG data URIs.Ok, Stretchy v2 is ready, now I need to update its docs. Oooh, it doesn’t have a README? I should add one. But I don’t want to duplicate content between the page and the README. Hmmm, if only…I know! I’ll make a web component for rendering both inline and remote Markdown! I have an unfinished one lying around somewhere, surely it won’t take more than a couple hours to finish it?(It took almost a day, two with docs, demos etc)Done! Here it is! https://md-block.verou.meGreat! Now I can update Stretchy’s docs and release its v2Great! Now I can use Stretchy in my &lt;x-foo-live&gt; component demoing my &lt;x-foo&gt; component and be back to only one level of yak shaving!Wow, it’s already Friday afternoon?! 
 
 
 
 Hopefully you find &lt;md-block&gt; useful! Enjoy!</content>
     </entry>
     <entry>
       <title>Aphex Twin</title>
         <link href="https://www.logodesignlove.com/aphex-twin"/>
       <updated>2021-11-26T14:03:55.000Z</updated>
       <content type="text">
 
 There are few more memorable logos in the music profession than the alien-like ‘A’ of Aphex Twin. It was designed in 1991 in time for the 92 release of Irish-born music producer Richard David James’ seminal album Selected Ambient Works (the first track, Xtal, brings back some fine, fine memories).
 
 
 
 Paul Nicholson is the graphic designer behind the mark (site under construction at time of post), and a few years ago he shared these logo sketches and construction images on his Instagram profile.
 
 
 
 “The original Aphex Twin logo was drawn by hand using circle templates and rulers in late 1991. With there being many incorrect versions of this logo floating about, I thought it about time to release the definitive logo construction. Enjoy.”
 
 
 
 
 “A little piece of trivia about the logo: at the time I met Richard, I had being doing artwork for a San Francisco-based skatewear label called Anarachic Adjustment. Their ‘thing’ at the time had been the whole “alien” vibe (remember, we are talking 1991). So, I had been creating loads of designs based around the letter ‘A’ which got knocked back. Richard, having seen the work in progress, liked where I was going with the amorphic shape and from these I developed what is now the Aphex ‘A.’ The logo was finished early 1992, in time to appear on the ‘Xylem Tube’ sleeve.”
 Quote source.
 
 
 
 The monogram was lifted from a larger wordmark in the same style, but it was always the symbol that was star of the show.
 
 
 
 
 Nostalgia.</content>
     </entry>
     <entry>
       <title>An Introduction to GraphQL</title>
         <link href="https://www.taniarascia.com/introduction-to-graphql/"/>
       <updated>2021-11-17T00:00:00.000Z</updated>
       <content type="text">Introduction
 As web and mobile applications become more mature and complex, software engineers invent clever new ways of improving the interaction between client and server within an application. One of the biggest paradigm shifts over the last few years in this regard has been GraphQL, an open-source query language and runtime for manipulating APIs. GraphQL was designed by Facebook in 2012 (and released publicly in 2015) to solve various weaknesses with traditional REST architecture by making a new system that is declarative, client-driven, and performant.
 In this article, you will learn what GraphQL is, familiarize yourself with important terminology and concepts of GraphQL, and discover how the GraphQL specification compares with the REST architectural style.
 What is GraphQL?
 GraphQL stands for Graph Query Language, but unlike other query languages such as SQL (Structured Query Language), it is not a language for communicating directly with a database, but rather a language that defines a contract through which a client communicates with a API server. The GraphQL specification is an open standard that describes the rules and characteristics of the language. It also provides instructions for executing a GraphQL query.
 Due to the fact that GraphQL is defined by an open-standard, there is no official implementation of GraphQL. A GraphQL implementation can be written with any programming language, integrate with any type of database, and support any client (such as mobile or web applications), as long as it follows the rules outlined in the spec. One of the most popular commercial GraphQL implementations is Apollo GraphQL, a which touts several GraphQL client and server implementations, but it is not necessary to use Apollo to use or understand GraphQL.
 GraphQL Characteristics
 There are several key characteristics of GraphQL design. GraphQL queries are declarative and hierarchical, and a GraphQL schema is strongly-typed and introspective.
 Declarative
 GraphQL queries are declarative, meaning the client will declare exactly which fields it is interested in, and the response will only include those properties.
 This example GraphQL query for a hypothetical fantasy game API requests a wizard with an ID of &quot;1&quot;, and requests the name and race fields on that object.
 {
   wizard(id: &quot;1&quot;) {
     name
     race
   }
 }
 The response, which is returned in JSON format, will return a data object that contains the found wizard object, with the two fields the query requested.
 {
   &quot;data&quot;: {
     &quot;wizard&quot;: {
       &quot;name&quot;: &quot;Merlin&quot;,
       &quot;race&quot;: &quot;HUMAN&quot;
     }
   }
 }
 Since a GraphQL response only gives you the exact information you want, it results in a more efficient and performant network request than alternatives that always provide a complete set of data.
 Hierarchical
 GraphQL queries are also hierarchical. The data returned follows the shape of the query. In this example, the query has been extended to include spells, and is requesting the name and attack fields of every spell.
 {
   wizard(id: &quot;1&quot;) {
     name
     spells {
       name
       attack
     }
   }
 }
 The response will now include an array of all the spell objects associated with this particular wizard. Although wizards and spells might be stored in separate database tables, they can be fetched with a single GraphQL request. (However, GraphQL is not opinionated about how the data itself is stored, so that is a presumption.)
 {
   &quot;data&quot;: {
     &quot;wizard&quot;: {
       &quot;name&quot;: &quot;Merlin&quot;,
       &quot;spells&quot;: [
         {
           &quot;name&quot;: &quot;Lightning Bolt&quot;,
           &quot;attack&quot;: 2
         },
         {
           &quot;name&quot;: &quot;Ice Storm&quot;,
           &quot;attack&quot;: 2
         },
         {
           &quot;name&quot;: &quot;Fireball&quot;,
           &quot;attack&quot;: 3
         }
       ]
     }
   }
 }
 Strongly-typed
 GraphQL is strongly-typed, as described by the GraphQL Type system. Types describe the capabilities of the values within a GraphQL server. The GraphQL types will be familiar to most programmers, with scalars (primitive values) like strings, booleans, and numeric integers, as well as more advanced values like objects.
 This example creates a Spell Object type with fields that correspond to String and Int scalar types.
 type Spell {
   name: String!
   attack: Int
   range: Int
 }
 A GraphQL schema is defined using the type system, which allows the server to determine whether or not a query is valid before attempting to query the data. GraphQL Validation ensures the request is syntactically correct, unambiguous, and mistake-free.
 Self-documenting
 The Introspection feature allows GraphQL clients and tools to query the GraphQL server for the underlying schema&#x27;s shape and data. This allows for the creation of tools like GraphiQL, an in-browser IDE and playground for working with GraphQL queries, and other tools for automatically generating documentation.
 For example, you can find out more about the Spell type through this introspection feature via the __schema.
 {
   __schema {
     types {
       name
       kind
       description
     }
   }
 }
 The response will also be JSON like any other GraphQL response
 {
   &quot;data&quot;: {
     &quot;__schema&quot;: {
       &quot;types&quot;: [
         {
           &quot;name&quot;: &quot;Spell&quot;,
           &quot;kind&quot;: &quot;OBJECT&quot;,
           &quot;description&quot;: &quot;A powerful spell that a wizard can read from a scroll.&quot;
         }
       ]
     }
   }
 }
 Client-driven
 The work of developing a GraphQL API happens on the backend, where the schema is defined and implemented. However, since all of the power of the GraphQL API is encompassed a single endpoint on the server, it is up to the client via declarative queries to decide exactly what data it needs. This empowers developers to iterate quickly, as the front end developer can continue to query the data the GraphQL API exposes without doing any additional backend work.
 Architecture
 GraphQL exists in the application layer between client and data. The GraphQL server describes the capabilities exposed in the API, and the client describes the requirements of the request.
 Server
 A GraphQL API is defined with a single endpoint, usually the /graphql endpoint, which can access the full capabilities of the GraphQL server. Since GraphQL is an application layer technology and is transport agnostic, it can be served over any protocol, but it is most commonly served over HTTP.
 A GraphQL server implementation can be written with any programming language, such as the express-graphql middleware which allows you to create a GraphQL API on a Node/Express HTTP server. GraphQL is also database agnostic, and the data for the application can be stored in MySQL, PostgreSQL, MongoDB, or any other database. The data can even be supplied by an aggregation of several traditional REST API endpoints. All that matters is that the data is defined in a GraphQL schema, which defines the API by describing the data available to be queried.
 Client
 Requests made to a GraphQL server are called documents and consist of operations such as queries (for read requests) and mutations (for write requests).
 Although there are advanced GraphQL clients, such as Apollo Client or Facebook&#x27;s Relay which provide mechanisms for caching as well as additional tools, no special client is required to make a request to a GraphQL server. A simple XMLHttpRequest or fetch from a web browser is sufficient for making requests by sending a GraphQL document to a GraphQL server.
 Below is an example of a fetch request to a /graphql endpoint, which passes the GraphQL document as a string in the body of the POST request.
 async function fetchWizards() {
   const response &#x3D; await fetch(&#x27;/graphql&#x27;, {
     method: &#x27;POST&#x27;,
     headers: {
       &#x27;Content-Type&#x27;: &#x27;application/json&#x27;,
     },
     body: JSON.stringify({
       query: &#x60;{
     wizards {
       id
       name
     },
   }&#x60;,
     }),
   })
   const wizards &#x3D; await response.json()
 
   return wizards
 }
 
 fetchWizards()
 This will return a JSON response for the request.
 {
   &quot;data&quot;: {
     &quot;wizards&quot;: [
       { &quot;id&quot;: &quot;1&quot;, &quot;name&quot;: &quot;Merlin&quot; },
       { &quot;id&quot;: &quot;2&quot;, &quot;name&quot;: &quot;Gandalf&quot; }
     ]
   }
 }
 GraphQL vs. REST
 GraphQL and REST are not interchangeable concepts, but they solve similar problems for applications. REST stands for Representational State Transfer, and is a software architectural style for sharing data between different systems. A RESTful API is an API that adheres to the principles and constraints of REST, which include being stateless, cacheable, enforcing a separation of concerns between the client and server, and having a uniform interface, such as through URIs. GraphQL, as covered previously, is a specification for a query language and runtime for executing queries.
 There are advantages and disadvantages to both systems, and both have their use in modern API development. However, GraphQL was developed to combat some perceived weaknesses with the REST system, and to create a more efficient, client-driven API.
 
 
 Architecture - A REST API is typically defined by multiple endpoints on a server, but GraphQL exchanges data over a single endpoint. A GraphQL endpoint can return a complex graph of data that might require multiple REST queries, reducing the number of requests over the network for a single view.
 
 
 Data fetching - A REST API returns the set of data that was determined on the server. This might be far too much data, such as if the view only requires one property from a response, or it might not be enough, such as a list endpoint that doesn&#x27;t return every property that a table requires in the view. GraphQL prevents this over and under fetching of data via declarative queries.
 
 
 Error Handling - Since it is not necessary for GraphQL to be served over HTTP, there is no specification about using HTTP response codes for errors. Typically all GraphQL endpoints will resolve with a 200 HTTP code response, and failed results will include an errors property alongside the data property in the response. RESTful APIs, on the other hand, utilize different 400 level HTTP codes for client errors and 200 level HTTP codes for successful responses.
 
 
 Versioning - GraphQL APIs strive to be backwards compatible and avoid breaking changes, contrasting with the common REST pattern of versioning endpoints, often with a /v1 or /v2 in the URL itself to determine the version. However, it is possible to implement your own versioning with GraphQL, or version via evolution with REST, it&#x27;s just less conventional.
 
 
 Caching - Cacheability is an integral part of the REST guiding constraints. Since HTTP-based REST APIs consist of multiple endpoints using different HTTP methods, it can take advantage of existing HTTP conventions for caching and avoiding refetching resource. And since essentially every GraphQL request will be different but use the single endpoint, it cannot take advantage of any of the built-in HTTP caching mechanisms. GraphQL clients can take advantage of Global Object Identification to enable simple caching.
 
 
 This list does not cover all the similarities and differences between REST and GraphQL, but summarizes many of the most critical points. Additionally, GraphQL can be used as a gateway that aggregates multiple REST endpoints or services, in which case both technologies can be used in harmony side-by-side.
 
 
 
 Feature
 GraphQL
 REST
 
 
 
 
 Description
 GraphQL is a query language for APIs, and a server-side runtime
 An architectural style for designing web services
 
 
 Data Fetching
 A single HTTP endpoint that responds to deterministic queries
 A set of HTTP endpoints that typically return a predetermined dataset
 
 
 Versioning
 Versioning discouraged
 Versioning common
 
 
 HTTP Status Codes
 All responses, including errors, are typically 200
 Implements HTTP Status codes
 
 
 Validation
 Built-in metadata validation
 Validation must be manually implemented
 
 
 Documentation
 Built-in via type system and introspection
 Not self-documenting, tools like OpenAPI available
 
 
 Caching
 No
 Yes
 
 
 Request Methods
 Queries, mutations, and subscriptions (over POST for HTTP)
 All HTTP methods utilized (GET, POST, PATCH, PUT, DELETE, etc)
 
 
 Response Content-Type
 JSON
 Any (JSON, XML, HTML, etc.)
 
 
 
 Conclusion
 GraphQL is an open-source query language and runtime for APIs. GraphQL was invented by developers at Facebook to solve various issues encountered with traditional REST APIs, such as over/under fetching data and inefficient network requests, by making a client-driven, declarative query language for APIs.
 While GraphQL is not an interchangeable concept with REST, they both describe different ways to manage communication between a client and a server. In this article, you learned what GraphQL is, key differences and similarities between GraphQL and REST, and how a GraphQL server exposes data to a client.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 292: Alt emotions, holding together in tough times, sticky definitions, and in-page filter</title>
         <link href="https://wdrl.info/archive/292"/>
       <updated>2021-11-11T10:40:00.000Z</updated>
       <content type="text">Hey,
 
 thanks for all the feedback that reached me last time, it was awesome to read that so many people support me even when my schedule of writing isn’t regular. Now that I had a thought on how to continue with WDRL I realised that this project won’t be sold, won’t be transferred to someone else or such thing. It’s just too personal to do this. But Malte Riechmann will help me collect and write some links from this edition on — it’s all in the early stages but I’m glad because it will help me and save me time.
 In the past two weeks I had the chance to think about a lot of things that are influencing our lives at the moment. It made me realise that it’s upon us how we see our worlds, how we see the problems of our world. It’s our choice how we deal with other people, whether we confront people, cut them short, or whether we’re calm, interested and open minded. I know the latter is not possible all the time, but trying is really worth it. If we understand each other better, we create emotions, we can hold together, we will get stronger and happier. We can support each other in unstable situations, when we’re not sure what to do or what to believe. It’s the community that works for humans, and it’s hate and envy that boosts the bad things in life. We choose our path every day.
 To make this easier for myself, I blocked all major news sites via pi-hole DNS for my devices again for an unlimited time. You can also set 1.1.1.2 for families as baseline and fallback.
 News
 
 	Safari 15 now has WebExtensions on iOS and iPadOS, supports theme-color meta tag, supports CSS aspect-ratio, new color syntaxes. The web inspector features a CSS Grid helper, ES6 modules are now supported in  Workers and ServiceWorkers. Top-level await, Error.cause, private class methods and accessors are supported, too. And more…
 
 Generic
 
 	Pawel Grzybek explains the history of the @. It is always interesting to know where icons and signs come from. We usually cannot remember their original meaning. E. g. think about the disc icon, which often stands for save in user interfaces.
 	Web development is only as complex as the belonging requirements. Therefore we should thrive for simpler solutions and fewer options. Jason Fried writes about this in one of his recent blog posts.
 
 UI/UX
 
 	Arco is an intelligent design system with code and design files available. Pretty cool!
 
 Tooling
 
 	We can now go to vscode.dev with our web browsers and a lightweight version of VS Code will start. It is pretty amazing how far web development has come. We can directly begin coding without installing anything. With Chrome and Edge we can even open entire directories into the web application using the File System Access API.
 
 Web Performance
 
 	Johan Isaksson tells us how he improved a Google app’s scroll performance 10x by simply adding an old CSS table property. What does this tell us? That we as developers should always think about whether there are native HTML or CSS solutions out there instead of JavaScript.
 
 HTML &amp; SVG
 
 	Within the last few years, all of us created hamburger menus. And probably all of us made some kind of mistake while doing so. Manuel Matuzović summarizes and explains these mistakes very well. He also shows a valid, semantic, and well-structured hamburger menu.
 	Writing and understanding semantically correct HTML is still one of the important basics in web development. Ben Myers brings to us some details on the &lt;dl&gt; HTML element.
 
 Accessibility
 
 	Jake Archibald shares why emotions matter when writing alt text for images.
 
 JavaScript
 
 	I love simple solutions. That’s why I love this simple in-page filter search written in a few lines of plain JavaScript. A few years ago, I’ve built something similar but back then it required a few more lines and wasn’t that neat due to missing JavaScript functions.
 	The JavaScript console object can do so much more than console.log(). Marko Denic shows us all the different features so we can improve our JavaScript debugging and become better web developers.
 	These unstyled, accessible React components are nice because we just need to style them. No need to reinvent the functionality all the time.
 
 CSS
 
 	This article is a good reminder of how object-fit and background-size work. Ahmad Shadeed goes into detail and explains to us when and when not to use these CSS properties. He also shows us how to fix the padding hack with the CSS property aspect-ratio.
 	Stephanie Eckles explains how to style radio buttons the modern way with CSS Grid layout, currentColor, appearance and em units. No images, no weird absolute positioning or similar.
 	With plain CSS we can now achieve sticky section indicators in definition lists. Pretty neat and something we had to do for years with comparably complex JavaScript code.
 	Do you know @supports can not only check on CSS properties but on CSS selectors, too? In one of his latest articles, Chris Coyier tells us all about this helpful CSS feature.
 	We all have used CSS hacks. Especially in the early days, those were sometimes the only solution to problems. Today we rarely see them anymore. But have a look at this interesting CSS hack from the developers at Facebook.
 
 
 I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Reframing tech debt</title>
         <link href="https://increment.com/planning/reframing-tech-debt/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">If we bake addressing tech debt into our plans, could it become an opportunity to build abundance into our systems?</content>
     </entry>
     <entry>
       <title>Planning for momentum</title>
         <link href="https://increment.com/planning/planning-for-momentum/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">On reimagining planning as a dynamic and generative process.</content>
     </entry>
     <entry>
       <title>Planning in the dark</title>
         <link href="https://increment.com/planning/planning-for-product-market-fit/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">When planning product development before you have product-market fit, continual iteration and getting comfortable with discomfort can help light the way.</content>
     </entry>
     <entry>
       <title>An IC’s guide to roadmap planning</title>
         <link href="https://increment.com/planning/individual-contributors-guide-to-roadmap-planning/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">How individual contributors can leverage their unique perspective to advance alignment and clarity of vision.</content>
     </entry>
     <entry>
       <title>A primer on product management for engineers</title>
         <link href="https://increment.com/planning/product-management-for-engineers/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">Breaking down the basics and benefits for engineers looking to manage product development with precision and verve.</content>
     </entry>
     <entry>
       <title>The great tightrope act</title>
         <link href="https://increment.com/planning/planning-for-engineering-managers/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">By thoughtfully balancing ideas and inputs during the planning process, engineering managers can enrich team impacts and business outcomes.</content>
     </entry>
     <entry>
       <title>Planning for change with RFCs</title>
         <link href="https://increment.com/planning/planning-with-requests-for-comments/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">How an enduring framework for knowledge-sharing and decision-making helped one team make more informed, egalitarian decisions—and how you can too.</content>
     </entry>
     <entry>
       <title>Road to somewhere</title>
         <link href="https://increment.com/planning/product-planning-as-road-trip/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">By sharing both the route and the destination to our strategic objectives, we can set ourselves up for a smooth planning journey.</content>
     </entry>
     <entry>
       <title>Software development as a wicked problem</title>
         <link href="https://increment.com/planning/software-development-as-a-wicked-problem/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">An exploration of the foundational complexities of building software at scale—and why they often distill into human, rather than technical, challenges.</content>
     </entry>
     <entry>
       <title>Tools for people</title>
         <link href="https://increment.com/planning/planning-and-tooling/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">Considering the complementary—but not all-encompassing—role tooling can play in the planning process.</content>
     </entry>
     <entry>
       <title>Open-source excursions: The poetry of planning</title>
         <link href="https://increment.com/planning/open-source-planning/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">Reflections on the value of a planning process that emphasizes transparency, empathy, and the people behind the products.</content>
     </entry>
     <entry>
       <title>Planning for pause</title>
         <link href="https://increment.com/planning/milestones-and-agile-development/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">As a companion to agile development practices, milestones offer a meditative—and productive—opportunity to decelerate.</content>
     </entry>
     <entry>
       <title>How to make pathfinder soup</title>
         <link href="https://increment.com/planning/software-development-with-pathfinders/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">Like the parable of stone soup, pathfinders can help development teams deliver complex and ambiguous projects, one ingredient at a time.</content>
     </entry>
     <entry>
       <title>Just hire great people?</title>
         <link href="https://increment.com/planning/how-to-build-effective-hiring-systems/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">Why enabling growing teams to flourish begins with an examination of our systems—and a well-considered hiring plan.</content>
     </entry>
     <entry>
       <title>Planning for privacy</title>
         <link href="https://increment.com/planning/planning-for-privacy/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">A look at the proactive practices and perspectives that can help companies center privacy in product development.</content>
     </entry>
     <entry>
       <title>On planning in public</title>
         <link href="https://increment.com/planning/planning-in-public-at-youtrack/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">YouTrack’s Elena Pishkova shares how the team plans products in public view and in collaboration with customers.</content>
     </entry>
     <entry>
       <title>What planning is like at…</title>
         <link href="https://increment.com/planning/what-planning-is-like-at-netflix-mailchimp-and-more/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">From sprint cadence to success metrics, here’s a snapshot of the planning process at Netflix, Mailchimp, Asana, LaunchDarkly, and more.</content>
     </entry>
     <entry>
       <title>Planning with flare</title>
         <link href="https://increment.com/planning/formal-specifications-and-planning/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">On thorny software projects, formal specifications can serve as beacons that illuminate the terrain ahead.</content>
     </entry>
     <entry>
       <title>The best-laid plans</title>
         <link href="https://increment.com/planning/the-best-laid-plans-tech-careers/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text">Wherein a career in machine learning begins with a virtual turtle, an appetite for literature, and an economics degree…</content>
     </entry>
     <entry>
       <title>Letter from the editor</title>
         <link href="https://increment.com/planning/letter-from-the-editor/"/>
       <updated>2021-11-09T10:00:00.000Z</updated>
       <content type="text"></content>
     </entry>
     <entry>
       <title>Front End Tables: Sorting, Filtering, and Pagination</title>
         <link href="https://www.taniarascia.com/front-end-tables-sort-filter-paginate/"/>
       <updated>2021-10-22T00:00:00.000Z</updated>
       <content type="text">One thing I&#x27;ve had to do at every job I&#x27;ve had is implement a table on the front end of an application that has sorting, filtering, and pagination.
 Sometimes, all that will be implemented on the back end, and I&#x27;ve previously documented how to structure those APIs in REST API: Sorting, Filtering, and Pagination. Other times, the data coming back is guaranteed to be small enough that implementing it all in the back end isn&#x27;t necessary, but still a good idea on the front end to reduce the amount of DOM nodes rendered to the page at a time.
 Initially, I would look up libraries like react-table or Ant Design table and try to ensure they had everything I needed. And that&#x27;s certainly a viable option, but often the libraries don&#x27;t match the design and needs of your particular case, and have a lot of features you don&#x27;t need. Sometimes it&#x27;s a better option to implement it yourself to have complete flexibility over functionality and design.
 So I&#x27;m going to demonstrate how to do it using React (but conceptually it can apply to any framework or non-framework).
 
       
     
   
   
     
 Prerequisites
 
 Knowledge of JavaScript, React
 
 Goals
 Make a table in React that implements:
 
 Pagination
 Sorting for strings, Booleans, numbers, and dates (case-insensitive)
 Filtering for strings, Booleans, numbers, and dates (case-insensitive)
 
 We&#x27;re also not going to implement any styles or use any frameworks to reduce complexity.
 And here&#x27;s a CodeSandbox demo: Click me! I&#x27;m the demo!
 Getting Started
 I&#x27;m going to set up some data that includes a string, a number, a Boolean, and a date in the dataset, and enough rows that pagination can be implemented and tested. I&#x27;ll stick some null data in there as well.
 const rows &#x3D; [
   { id: 1, name: &#x27;Liz Lemon&#x27;, age: 36, is_manager: true, start_date: &#x27;02-28-1999&#x27; },
   { id: 2, name: &#x27;Jack Donaghy&#x27;, age: 40, is_manager: true, start_date: &#x27;03-05-1997&#x27; },
   { id: 3, name: &#x27;Tracy Morgan&#x27;, age: 39, is_manager: false, start_date: &#x27;07-12-2002&#x27; },
   { id: 4, name: &#x27;Jenna Maroney&#x27;, age: 40, is_manager: false, start_date: &#x27;02-28-1999&#x27; },
   { id: 5, name: &#x27;Kenneth Parcell&#x27;, age: Infinity, is_manager: false, start_date: &#x27;01-01-1970&#x27; },
   { id: 6, name: &#x27;Pete Hornberger&#x27;, age: null, is_manager: true, start_date: &#x27;04-01-2000&#x27; },
   { id: 7, name: &#x27;Frank Rossitano&#x27;, age: 36, is_manager: false, start_date: null },
   { id: 8, name: null, age: null, is_manager: null, start_date: null },
 ]
 We&#x27;ll also want to define the columns on the table.
 const columns &#x3D; [
   { accessor: &#x27;name&#x27;, label: &#x27;Name&#x27; },
   { accessor: &#x27;age&#x27;, label: &#x27;Age&#x27; },
   { accessor: &#x27;is_manager&#x27;, label: &#x27;Manager&#x27;, format: value &#x3D;&gt; (value ? &#x27;✔️&#x27; : &#x27;✖️&#x27;) },
   { accessor: &#x27;start_date&#x27;, label: &#x27;Start Date&#x27; },
 ]
 So now can begin making a table abstraction that loops through the columns for the headers, and accesses the proper data for each row. I also added an optional format option if you want to display the data differently. It would be a good idea to use it on the date field.
 
 I prefer to always use brackets and the return statement when mapping in React. It makes debugging and editing a lot easier than with implicit returns.
 
 Table.js
 const Table &#x3D; ({ columns, rows }) &#x3D;&gt; {
   return (
     &lt;table&gt;
       &lt;thead&gt;
         &lt;tr&gt;
           {columns.map(column &#x3D;&gt; {
             return &lt;th key&#x3D;{column.accessor}&gt;{column.label}&lt;/th&gt;
           })}
         &lt;/tr&gt;
       &lt;/thead&gt;
       &lt;tbody&gt;
         {rows.map(row &#x3D;&gt; {
           return (
             &lt;tr key&#x3D;{row.id}&gt;
               {columns.map(column &#x3D;&gt; {
                 if (column.format) {
                   return &lt;td key&#x3D;{column.accessor}&gt;{column.format(row[column.accessor])}&lt;/td&gt;
                 }
                 return &lt;td key&#x3D;{column.accessor}&gt;{row[column.accessor]}&lt;/td&gt;
               })}
             &lt;/tr&gt;
           )
         })}
       &lt;/tbody&gt;
     &lt;/table&gt;
   )
 }
 
 Note: Including the proper keys in this table is essential. If the keys are not the correct unique values, the table will go crazy.
 
 And I&#x27;ll pass data into my abstract table component.
 &lt;Table rows&#x3D;{rows} columns&#x3D;{columns} /&gt;
 
       
     
   
   
     
 Now there&#x27;s a basic table set up, and we can move on to pagination.
 Pagination
 There are a lot of ways to set up pagination on the front end.
 For example, you have Google, which will show you a next button, a previous button if you&#x27;re past page 1, and a few additional responses to the left and right of the page currently selected.
 
       
     
   
   
     
 Personally, I prefer to have &quot;first ️️️⏮️&quot;, &quot;previous ⬅️&quot;, &quot;next ➡️&quot;, and &quot;last ⏭️&quot; options, so that&#x27;s the way I&#x27;ll set it up here.
 You should be able to click &quot;first&quot; or &quot;last&quot; to go to the beginning and end, and &quot;previous&quot; and &quot;next&quot; to go back and forth by a single page. If you can&#x27;t go back or forward anymore, the options should not appear, or should be disabled.
 
 A lot of libraries seem to handle pagination differently, with page 1 being either 1 or 0. Keep it simple. Just use 1 for page 1. No need for extra calculations.
 
 In the table, I want to calculate:
 
 The active page, which is what you&#x27;ll be updating as you paginate, so it&#x27;ll go in state
 The count, which is the total number of rows in a front end only table pre-filtering
 The rows per page, which I&#x27;m setting to a low number so I can test it all with a small data set, but you can also hold this in state if the user should be able to change it
 The total pages, which will be the total rows divided by rows per page, rounded up
 
 Table.js
 const Table &#x3D; ({ columns, rows }) &#x3D;&gt; {
   const [activePage, setActivePage] &#x3D; useState(1)  const rowsPerPage &#x3D; 3  const count &#x3D; rows.length  const totalPages &#x3D; Math.ceil(count / rowsPerPage)  const calculatedRows &#x3D; rows.slice((activePage - 1) * rowsPerPage, activePage * rowsPerPage)
   /* ... */
 
   return (
     &lt;&gt;
       &lt;table&gt;{/* ... */}&lt;/table&gt;
       &lt;Pagination        activePage&#x3D;{activePage}        count&#x3D;{count}        rowsPerPage&#x3D;{rowsPerPage}        totalPages&#x3D;{totalPages}        setActivePage&#x3D;{setActivePage}      /&gt;    &lt;/&gt;
   )
 }
 Finally, the calculated rows are the rows that will be displayed in the front end, which ultimately will be affected by filtering, sorting, and paginating. This is the one spot where the page number vs. index needs to be calculate. slice takes a start and end index, so for example on page 3, it would be slice(4, 6), showing the 5th and 6th item in the array.
 
 It would be a good idea to memoize the calculated rows.
 
 I&#x27;ll start getting the pagination set up.
 Pagination.js
 const Pagination &#x3D; ({ activePage, count, rowsPerPage, totalPages, setActivePage }) &#x3D;&gt; {
   return (
     &lt;div className&#x3D;&quot;pagination&quot;&gt;
       &lt;button&gt;⏮️ First&lt;/button&gt;
       &lt;button&gt;⬅️ Previous&lt;/button&gt;
       &lt;button&gt;Next ➡️&lt;/button&gt;
       &lt;button&gt;Last ⏭️&lt;/button&gt;
     &lt;/div&gt;
   )
 }
 
       
     
   
   
     
 Now we just have to do a few calculations. If you&#x27;re on the first page, there&#x27;s no &quot;first&quot; or &quot;previous&quot; options, and if you&#x27;re on the last page, there&#x27;s no &quot;next&quot; or &quot;last&quot; options.
 First and last will always take you to 1 or totalPages, and previous and next just need to add or remove a page.
 Meanwhile, you can show the beginning and the end of the rows displayed (such as &quot;showing rows 20-29&quot;)
 Pagination.js
 const Pagination &#x3D; ({ activePage, count, rowsPerPage, totalPages, setActivePage }) &#x3D;&gt; {
   const beginning &#x3D; activePage &#x3D;&#x3D;&#x3D; 1 ? 1 : rowsPerPage * (activePage - 1) + 1
   const end &#x3D; activePage &#x3D;&#x3D;&#x3D; totalPages ? count : beginning + rowsPerPage - 1
 
   return (
     &lt;&gt;
       &lt;div className&#x3D;&quot;pagination&quot;&gt;
         &lt;button disabled&#x3D;{activePage &#x3D;&#x3D;&#x3D; 1} onClick&#x3D;{() &#x3D;&gt; setActivePage(1)}&gt;
           ⏮️ First
         &lt;/button&gt;
         &lt;button disabled&#x3D;{activePage &#x3D;&#x3D;&#x3D; 1} onClick&#x3D;{() &#x3D;&gt; setActivePage(activePage - 1)}&gt;
           ⬅️ Previous
         &lt;/button&gt;
         &lt;button disabled&#x3D;{activePage &#x3D;&#x3D;&#x3D; totalPages} onClick&#x3D;{() &#x3D;&gt; setActivePage(activePage + 1)}&gt;
           Next ➡️
         &lt;/button&gt;
         &lt;button disabled&#x3D;{activePage &#x3D;&#x3D;&#x3D; totalPages} onClick&#x3D;{() &#x3D;&gt; setActivePage(totalPages)}&gt;
           Last ⏭️
         &lt;/button&gt;
       &lt;/div&gt;
       &lt;p&gt;
         Page {activePage} of {totalPages}
       &lt;/p&gt;
       &lt;p&gt;
         Rows: {beginning &#x3D;&#x3D;&#x3D; end ? end : &#x60;${beginning} - ${end}&#x60;} of {count}
       &lt;/p&gt;
     &lt;/&gt;
   )
 }
 That pretty much covers pagination, it should be easy to modify that for any additional design.
 Filtering
 Next up is filtering. We want to be able to do case insensitive matching of partial strings and numbers, so enn will match Jenna Maroney and Kenneth Parcell.
 I&#x27;m going to make a search object, so keys and values can be stored for each value being searched on, and therefore multiple searches can be combined.
 Every search should reset the pagination back to page one, because pagination will no longer make sense in the middle when the number of entries have changed.
 The count will now be determined by the filteredRows, since there will be less total items after filtering.
 Table.js
 const Table &#x3D; ({ columns, rows }) &#x3D;&gt; {
   const [activePage, setActivePage] &#x3D; useState(1)
   const [filters, setFilters] &#x3D; useState({})  const rowsPerPage &#x3D; 3
 
   const filteredRows &#x3D; filterRows(rows, filters)  const calculatedRows &#x3D; filteredRows.slice(    (activePage - 1) * rowsPerPage,    activePage * rowsPerPage  )  const count &#x3D; filteredRows.length  const totalPages &#x3D; Math.ceil(count / rowsPerPage)
 }
 For the filterRows function, we&#x27;ll just return the original array if no filters are present, otherwise check if it&#x27;s a string, Boolean, or number, and to the desired check - I have it checking for partial strings, true or false for Booleans, and exact number match (so no 3 for 33...it even allows for searching on Infinity, however pointless that may be).
 function filterRows(rows, filters) {
   if (isEmpty(filters)) return rows
 
   return rows.filter(row &#x3D;&gt; {
     return Object.keys(filters).every(accessor &#x3D;&gt; {
       const value &#x3D; row[accessor]
       const searchValue &#x3D; filters[accessor]
 
       if (isString(value)) {
         return toLower(value).includes(toLower(searchValue))
       }
 
       if (isBoolean(value)) {
         return (searchValue &#x3D;&#x3D;&#x3D; &#x27;true&#x27; &amp;&amp; value) || (searchValue &#x3D;&#x3D;&#x3D; &#x27;false&#x27; &amp;&amp; !value)
       }
 
       if (isNumber(value)) {
         return value &#x3D;&#x3D; searchValue
       }
 
       return false
     })
   })
 }
 
 I made little helper functions for isString, isBoolean, etc - you could use Lodash or whatever else.
 
 Now another row can be added in the headers with a search bar. In a design, this might be populated by clicking on a filter icon, or it can just be displayed in the row as in this example. Although I&#x27;m just doing a search bar that requires manual typing for simplicity here, you would probably want to handle each datatype differently - for example, a Boolean could be handled by a dropdown with options for true, false, and clear. You might also have a dataset that includes an enum where the values are known, like role where the options are Writer, Manager, Producer. That could be a dropdown as well instead of making the user type in the values. You could also require only numbers in the number field, and use a date picker for the date field.
 Here, if a user types into any search bar, it will add to the list of filters. If a filter is cleared or deleted, it should delete the key. (Leaving the key with a string value of &quot;&quot; can cause problems with the filtering for numbers, Booleans, etc. if you don&#x27;t handle that case).
 The search function:
 const handleSearch &#x3D; (value, accessor) &#x3D;&gt; {
   setActivePage(1)
 
   if (value) {
     setFilters(prevFilters &#x3D;&gt; ({
       ...prevFilters,
       [accessor]: value,
     }))
   } else {
     setFilters(prevFilters &#x3D;&gt; {
       const updatedFilters &#x3D; { ...prevFilters }
       delete updatedFilters[accessor]
 
       return updatedFilters
     })
   }
 }
 In the column header:
 &lt;thead&gt;
   &lt;tr&gt;{/* ... */}&lt;/tr&gt;
   &lt;tr&gt;
     {columns.map(column &#x3D;&gt; {
       return (
         &lt;th&gt;
           &lt;input
             key&#x3D;{&#x60;${column.accessor}-search&#x60;}
             type&#x3D;&quot;search&quot;
             placeholder&#x3D;{&#x60;Search ${column.label}&#x60;}
             value&#x3D;{filters[column.accessor]}
             onChange&#x3D;{event &#x3D;&gt; handleSearch(event.target.value, column.accessor)}
           /&gt;
         &lt;/th&gt;
       )
     })}
   &lt;/tr&gt;
 &lt;/thead&gt;
 
       
     
   
   
     
 Now filters are set up, and should handle adding and removing values for all the data types, as well as ignoring null and undefined values.
 Sorting
 With sorting, we want to be able to do three things for each column:
 
 Sort ascending (⬆️)
 Sort descending (⬇️)
 Reset sort/no sort (↕️)
 
 Here&#x27;s a little table, because I often forget how ascending and descending apply to different types of data.
 Ascending vs. Descending
 
 
 
 Type
 Order
 Example
 Description
 
 
 
 
 Alphabetical
 Ascending
 A - Z
 First to last
 
 
 Alphabetical
 Descending
 Z - A
 Last to first
 
 
 Numerical
 Ascending
 1 - 9
 Lowest to highest
 
 
 Numerical
 Descending
 9 - 1
 Highest to lowest
 
 
 Date
 Ascending
 01-01-1970 - Today
 Oldest to newest
 
 
 Date
 Descending
 Today - 01-01-1970
 Newest to oldest
 
 
 
 Unlike filtering, I&#x27;m only going to set up the sort to sort one column at a time. Multi sort might be an option on some tables, but it would be a challenge determining which ones take precedence and how to display it, so I&#x27;m going to stick with single sort, which will reset the sort with each new column.
 We need to hold two pieces of data in state for sorting:
 
 orderBy - which column is being sorted
 order - whether it&#x27;s ascending or descending
 
 const Table &#x3D; ({ columns, rows }) &#x3D;&gt; {
   const [activePage, setActivePage] &#x3D; useState(1)
   const [filters, setFilters] &#x3D; useState({})
   const [sort, setSort] &#x3D; useState({ order: &#x27;asc&#x27;, orderBy: &#x27;id&#x27; })  // ...
 }
 I&#x27;m setting the default accessor to id because I know that&#x27;s the main key here, but realistically you&#x27;d want to pass in a variable into the table to determine the column index.
 Once again, we need to check for the types and sort accordingly on the rows - we can do this after filtering them. I&#x27;m using the built in localeCompare function, which can handle strings, numbers, and date strings.
 function sortRows(rows, sort) {
   return rows.sort((a, b) &#x3D;&gt; {
     const { order, orderBy } &#x3D; sort
 
     if (isNil(a[orderBy])) return 1
     if (isNil(b[orderBy])) return -1
 
     const aLocale &#x3D; convertType(a[orderBy])
     const bLocale &#x3D; convertType(b[orderBy])
 
     if (order &#x3D;&#x3D;&#x3D; &#x27;asc&#x27;) {
       return aLocale.localeCompare(bLocale, &#x27;en&#x27;, { numeric: isNumber(b[orderBy]) })
     } else {
       return bLocale.localeCompare(aLocale, &#x27;en&#x27;, { numeric: isNumber(a[orderBy]) })
     }
   })
 }
 The sort function will again restart the pagination, and set the accessor and sort order. If it&#x27;s already descending, we want to set it to ascending, and so on.
 const handleSort &#x3D; accessor &#x3D;&gt; {
   setActivePage(1)
   setSort(prevSort &#x3D;&gt; ({
     order: prevSort.order &#x3D;&#x3D;&#x3D; &#x27;asc&#x27; &amp;&amp; prevSort.orderBy &#x3D;&#x3D;&#x3D; accessor ? &#x27;desc&#x27; : &#x27;asc&#x27;,
     orderBy: accessor,
   }))
 }
 In the main table header, I&#x27;m adding the sort button next to the column label. Some designs will choose to display the current state of ascending vs. descending, and some will choose to show what it will be after you press the button. I went with showing the current state.
 &lt;thead&gt;
   &lt;tr&gt;
     {columns.map(column &#x3D;&gt; {
       const sortIcon &#x3D; () &#x3D;&gt; {
         if (column.accessor &#x3D;&#x3D;&#x3D; sort.orderBy) {
           if (sort.order &#x3D;&#x3D;&#x3D; &#x27;asc&#x27;) {
             return &#x27;⬆️&#x27;
           }
           return &#x27;⬇️&#x27;
         } else {
           return &#x27;️↕️&#x27;
         }
       }
 
       return (
         &lt;th key&#x3D;{column.accessor}&gt;
           &lt;span&gt;{column.label}&lt;/span&gt;
           &lt;button onClick&#x3D;{() &#x3D;&gt; handleSort(column.accessor)}&gt;{sortIcon()}&lt;/button&gt;
         &lt;/th&gt;
       )
     })}
   &lt;/tr&gt;
   &lt;tr&gt;{/* ... */}&lt;/tr&gt;
 &lt;/thead&gt;
 Now in the table, you&#x27;ll filter, then sort, then paginate.
 export const Table &#x3D; ({ columns, rows }) &#x3D;&gt; {
   const [activePage, setActivePage] &#x3D; useState(1)
   const [filters, setFilters] &#x3D; useState({})
   const [sort, setSort] &#x3D; useState({ order: &#x27;asc&#x27;, orderBy: &#x27;id&#x27; })
   const rowsPerPage &#x3D; 3
 
   const filteredRows &#x3D; useMemo(() &#x3D;&gt; filterRows(rows, filters), [rows, filters])  const sortedRows &#x3D; useMemo(() &#x3D;&gt; sortRows(filteredRows, sort), [filteredRows, sort])  const calculatedRows &#x3D; paginateRows(sortedRows, activePage, rowsPerPage)
   // ...
 }
 I added in useMemo here to make it more performant and so nobody yells at me. (Would need to add a deeper comparison for pagination to be memoized on a sorted row, though.)
 
       
     
   
   
     
 And now, the table is ready! You can sort, you can paginate, you can filter, oh my!
 Conclusion
 Success! We have a table in React implementing sorting, filtering, and pagination without using any libraries. It&#x27;s ugly as sin but since we know how it all works, we know how to improve it, make it harder, better, faster, stronger.
 A few possible improvements:
 
 Update the search bar for Boolean types to be a dropdown or checkbox/switch
 Update the search bar for date types to be a datepicker
 Implement range search for numbers and dates
 Implement sorting and filtering for arrays (for example, a list of tags)
 Find whatever edge my lazy ass didn&#x27;t test for and fix it
 
 And don&#x27;t forget to play around with it and try to break it 👇
 View the demo! Break me!
 Please don&#x27;t @ me about having to manually type in &quot;true&quot; for Booleans, or manually type in dates instead of using a date picker, or having exact number search instead of date range, etc. That&#x27;s your homework! Fix it!
 And remember, if you&#x27;re doing all this work on the back end because the data sets are large, you can look at REST API: Sorting, Filtering, and Pagination. Then the code in the table will make API calls instead of handling the arrays and you&#x27;ll just show a loading state in between each search.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 291: Back again with a mostly CSS edition</title>
         <link href="https://wdrl.info/archive/291"/>
       <updated>2021-10-20T16:00:00.000Z</updated>
       <content type="text">Hey,
 
 no, you didn’t miss an edition, I just failed to write again. The struggles of writing WDRL lately made me think and write down something but so far I have no conclusion on how it’ll go on. A few ideas are in my mind already. But autumn in full progress here in Germany, I found some time to read through articles and technologies and here is an edition mostly showing nice CSS solutions but also a couple of things to think about.
 Security
 
 	Jake Archibald teaches us how we win at CORS (Cross-Origin Resource Sharing). This is great because it’s one of the trickier things to test, debug and grasp CORS as a developer.
 
 HTML &amp; SVG
 
 	Christian Kozalla shows the proper approach to implement and style a dialog element to show a dialog/overlay/modal to a user.
 
 Accessibility
 
 	Eric Eggert explains the differences between buttons and links and why we shouldn’t just use what we want to for our web apps but the proper types instead.
 
 JavaScript
 
 	Livewire is a nice technology to use in dynamic Laravel apps. Caleb Porzio shares how it works and how to use it in a deep dive article.
 	If you’re using Laravel already, here’s a nice alternative to the hosted services for adding a support bubble to your website.
 	Vizzu is a library for animated data visualizations and data stories. Here’s an overview of what it can do.
 
 CSS
 
 	Stefan Judis shares how we can do conditional border-radius today but future holds a much more readable approach for us.
 	Chris Coyier shares future additions currently in development for CSS, and they’re super cool. It would allow for native nesting, container queries and units, cascade layers, @when (native if/else), and scoping. Alle we ever wished for in the past decade.
 	David Hall explains how much of a difference a well crafted drop shadow makes for interaction. I love these detailed explanations of something that looks very simple.
 	Chris Coyier on the different degrees of custom property usage and why the decision how to do it fully depends on the context of the team and project we’re working in.
 	First of all I’m convinced that absolute positioning in CSS has its value and its use-cases. But in many cases it also creates new problems, especially in responsive and modular contexts. So this documentation of use-cases where we don’t need absolute positioning in CSS anymore is great.
 	Mikael Ainalem shares an approach to create pop-out effects in CSS using clip-path().
 	We know a lot of the techniques highlighted in this article from our work as CSS developers. But gladly with new techniques we can switch away from pseudo-element hacks to more stable solutions today.
 	There’s a new CSS property called accent-color which makes it quick and easy to roll out global styles.
 
 Work &amp; Life
 
 	Long time ago I wrote this piece and had the intend to let it publish somewhere else. But it didn’t happen so here it is on my personal blog: It’s time for a healthy tech approach.
 
 Go beyond…
 
 	Berwyn Powell considers the impact of a web application or website on carbon emissions and how we can reduce them as developers.
 
 
 I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Custom properties with defaults: 3+1 strategies</title>
         <link href="https://lea.verou.me/2021/10/custom-properties-with-defaults/"/>
       <updated>2021-10-15T11:22:40.000Z</updated>
       <content type="text">When developing customizable components, one often wants to expose various parameters of the styling as custom properties, and form a sort of CSS API. This is still underutlized, but there are libraries, e.g. Shoelace, that already list custom properties alongside other parts of each component’s API (even CSS parts!).
 
 
 
 Note: I’m using “component” here broadly, as any reusable chunk of HTML/CSS/JS, not necessarily a web component or framework component. What we are going to discuss applies to reusable chunks of HTML just as much as it does to “proper” web components.
 
 
 
 Let’s suppose we are designing a certain button styling, that looks like this:
 
 
 
 
 
 
 
 
 
 
 
 We want to support a --color custom property for creating color variations by setting multiple things internally:
 
 
 
 .fancy-button {
 	border: .1em solid var(--color);
 	background: transparent;
 	color: var(--color);
 }
 
 .fancy-button:hover {
 	background: var(--color);
 	color: white;
 }
 
 
 
 Note that with the code above, if no --color is set, the three declarations using it will be IACVT and thus we’ll get a nearly unstyled text-only button with no background on hover (transparent), no border on hover, and the default black text color (canvastext to be precise).
 
 
 
 
 
 
 
 That’s no good! IT’s important that we set defaults. However, using the fallback parameter for this gets tedious, and WET:
 
 
 
 .fancy-button {
 	border: .1em solid var(--color, black);
 	background: transparent;
 	color: var(--color, black);
 }
 
 .fancy-button:hover {
 	background: var(--color, black);
 	color: white;
 }
 
 
 
 To avoid the repetition and still ensure --color always has a value, many people do this:
 
 
 
 .fancy-button {
 	--color: black;
 	border: .1em solid var(--color);
 	background: transparent;
 	color: var(--color);
 }
 
 .fancy-button:hover {
 	background: var(--color);
 	color: white;
 }
 
 
 
 However, this is not ideal for a number of reasons:
 
 
 
 It means that people cannot take advantage of inheritance to set --color on an ancestor.It means that people need to use specificity that overrides your own rules to set these properties. In this case this may only be 0,1,0, but if your selectors are complex, it could end up being quite annoying (and introduce tight couplings, because developers should not need to know what your selectors are).
 
 
 
 If you insist going that route, :where() can be a useful tool to reduce specificity of your selectors while having as fine grained selection criteria as you want. It’s also one of the features I proposed for CSS, so I’m very proud that it’s now supported everywhere. :where() won’t solve the inheritance problem, but at least it will solve the specificity problem.
 
 
 
 What if we still use the fallback parameter and use a variable for the fallback? 
 
 
 
 .fancy-button {
 	--color-initial: black;
 	border: .1em solid var(--color, var(--color-initial));
 	background: transparent;
 	color: var(--color, var(--color-initial));
 }
 
 .fancy-button:hover {
 	background: var(--color, var(--color-initial));
 	color: white;
 }
 
 
 
 This works, and it has the advantage that people could even customize your default if they want to (though I cannot think of any use cases for that). But isn’t it so horribly verbose? What else could we do?
 
 
 
 My preferred solution is what I call pseudo-private custom properties. You use a different property internally than the one you expose, which is set to the one you expose plus the fallback:
 
 
 
 .fancy-button {
 	--_color: var(--color, black);
 	border: .1em solid var(--_color);
 	background: transparent;
 	color: var(--_color);
 }
 
 .fancy-button:hover {
 	background: var(--_color);
 	color: white;
 }
 
 
 
 I tend to use the same name prepended with an underscore. Some people may flinch at the idea of private properties that aren’t really private, but I will remind you that we’ve done this in JS for over 20 years (we only got real private properties fairly recently).
 
 
 
 Bonus: Defaults via @property registration
 
 
 
 If @property is fair game (it’s only supported in Chromium, but these days that still makes it supported in 70% of users’ browsers — which is a bit sad, but that’s another discussion), you could also set defaults that way:
 
 
 
 @property --color {
 	syntax: &quot;&lt;color&gt;&quot;;
 	inherits: true;
 	initial-value: black;
 }
 
 .fancy-button {
 	border: .1em solid var(--color);
 	background: transparent;
 	color: var(--color);
 }
 
 .fancy-button:hover {
 	background: var(--color);
 	color: white;
 }
 
 
 
 Registering your property has several benefits (e.g. it makes it animatable), but if you’re only registering it for the purposes of setting a default, this way has several drawbacks:
 
 
 
 Property registration is global. Your component’s custom properties may clash with the host page’s custom properties, which is not great. The consequences of this can be quite dire, because @property fails silently, and the last one wins so you may just get the initial value of the host page’s property. In this case, that could very likely be transparent, with terrible results. And if your declaration is last and you get your own registered property, that means the rest of the page will also get yours, with equally potentially terrible results.With this method you cannot set different initial values per declaration (although you usually don’t want to).Not all custom property syntaxes can be described via @property yet.
 
 
 
 Bonus: Customizable single-checkbox pure CSS switch
 
 
 
 Just for the lulz, I made a switch (styling loosely inspired from Shoelace switch) that is just a regular &lt;input type&#x3D;checkbox&gt; with a pretty extensive custom property API:
 
 
 
 CodePen Embed Fallback
 
 
 
 It is using the pseudo-private properties approach. Note how another bonus of this method is that there’s a little self-documentation right there about the component’s custom property API, even before any actual documentation is written.
 
 
 
 As an aside, things like this switch make me wish it was possible to create web components that subclass existing elements. There is an existing — somewhat awkward — solution with the is attribute, but Apple is blocking it. The alternative is to use a web component with ElementInternals to make it form-associated and accessible and mirror all checkbox methods and properties, but that is way too heavyweight, and prone to breakage in the future, as native checkboxes add more methods. There is also a polyfill, but for a simple switch it may be a bit overkill. We really shouldn’t need to be painstakingly mirroring native elements to subclass them…
 
 
 
 Enjoyed this article and want to learn more? I do teach courses on unlocking the full potential of CSS custom properties. You can watch my Frontend Masters Dynamic CSS course (currently in production), or attend my upcoming Smashing workshop.</content>
     </entry>
     <entry>
       <title>Marken und Signete</title>
         <link href="https://www.logodesignlove.com/marken-und-signete"/>
       <updated>2021-10-04T15:53:26.000Z</updated>
       <content type="text">
 
 The 1957 book Marken und Signete, published by Julius Hoffmann, features logos by designers such as Helmut Salden, Roger Excoffon, Anton Stankowski, Philip Grushkin, and George Salter. Images have been archived on Flickr thanks to the Herb Lubalin Study Center and Stephen Coles.
 
 Stephen’s copy (above) was missing its dust jacket (below).
 
 A whimsical mark caught my eye — this one for Basel-based stationer Papyrus AG.
 
 
 
 Designed by Swiss graphic artist Hermann Eidenbenz (1902–1993), who also designed Swiss and German banknotes. The only logo context I could find was this poster via the Poster Musem.
 Papyrus AG poster
 The designers in the book are listed beside each logo. This monogram for KHR (below) was designed by Nuremberg-born Alfred Finsterer (1908–1996). Alfred returned to his hometown from Leipzig after the Second World War to find that all his work had been destroyed.
 
 
 
 The Aspen Festival logo was by Austrian-born artist, sculptor, photographer, architect, and designer Herbert Bayer (1900–1985).
 
 
 
 And this for Machinenfabrik Molins was crafted by the renowned FHK Henrion (1914–1990).
 
 
 
 Used copies are available from AbeBooks. Also, Stephen Coles’ @typographica profile on Twitter is worth a follow.
 
 
 
 Related, you might like this 1976 book, Signet Signal Symbol, by Walter Diethelm.</content>
     </entry>
     <entry>
       <title>Sait Maden</title>
         <link href="https://www.logodesignlove.com/sait-maden"/>
       <updated>2021-09-27T14:35:53.000Z</updated>
       <content type="text">
 
 Born on May 3rd, 1931, in Çorum, Turkey, Sait Maden started writing poetry at the age of 13 and began learning French and Ottoman at the age of 16. He learned Spanish to be able to translate Lorca from his mother tongue. From 1949–55 he studied at IDGSA Painting Department in Istanbul, graduating from Bedri Rahmi Eyüboğlu Workshop.
 As a side job between 1955 and 1960 Maden designed logos, billboard displays, theatre decors, and cinema posters. It wasn’t until after 1960 when he focused on the graphic design discipline, concentrating mainly on publication design. He designed around 8,000 book and magazine covers for many publishing houses, as well as periodicals, brochures, packaging, labels, and around 500 logos. He crafted some of the fonts used in his books, and was the designer of many political election posters. In 1964 he established his own private workshop.
 
 
 Trailer for the Sait Maden documentary by Miraç Güldoğan.
 He was among the founders of the Graphic Artists Association in 1969, spending a period as chairman of the association. In 1979, in an effort to document the history of Turkish graphic design, Sait started the book project “Turkish Graphic Art from the Beginning to the Present.” This would be the first book of its kind for his country. Although he couldn’t complete the book due to a lack of time and support, parts of the project were published in the “Graphic Art” magazine, and a section was published in the journal “Cevre.” Sait’s attention to the lack of design awareness in Turkey and his comments on the absence of a graphic designer in the museum are thought to explain the lack of support for his work.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Sait collated his logo designs for his book titled “Simgeler” (Icons) published in 1990. Sadly out of print, as far as I can tell. In the preface he wrote, “Since my first award-winning icon was drawn in 1955, I have been an observer and practitioner for fifty years. In these fifty years, I have drawn hundreds of symbols.” Sadık Karamustafa said the following about Sait, “It would be fair to say that he is the designer who brought the contemporary typographic approach to Turkish graphic design.”
 In 1996, he founded Çekirdek Publishing in order to publish poetry and translation books. He reflected the different moods that literature brought him into his various illustration styles.
 
 
 
 Various retrospective exhibitions were publicly shown between 2009 and 2019, and the Turkish design great passed away on June 19th, 2013.
 
 
 
 
 
 
 Sait Maden’s page on the Ankara Library website (in Turkish).
 The logos in this post are archived with more details in the Sait Maden section of the excellent Logo Book.
 If you have any stories about Sait, or resources for his work, please comment or get in touch.</content>
     </entry>
     <entry>
       <title>Redesign: Version 5.0</title>
         <link href="https://www.taniarascia.com/redesign-version-5/"/>
       <updated>2021-09-02T00:00:00.000Z</updated>
       <content type="text">Over the last week or two I&#x27;ve had a lot of fun building out a new site from scratch. I thought it would be fun to make my site look like a VS Code application. I didn&#x27;t want to go full gimmick with it, but it&#x27;s highly inspired by spending all day working in an IDE.
 I&#x27;ve always tried to keep my site very clean and minimal and distraction free, but this means that the average person who lands on my site most likely does not explore any further because it doesn&#x27;t seem like there&#x27;s anything else to see. So I&#x27;ve decided to change it up this time around and categorize all my posts and make a category sidebar.
 
       
     
   
   
     
 
       
     
   
   
     
 What do you think? I had fun making little pixel icons, of a floppy disk and a Twitter bird and GitHub OctoCat and so on. I wanted to add a little 8-bit retro vibe to the site and make it more unique than just using emojis or icons. I also implemented a little theme switcher at the top, where not only can you switch between light and dark, but also choose the primary color of the site.
 For example, here it is in red:
 
       
     
   
   
     
 I wrote the CSS from scratch in a single file, because I always find that the easiest and fastest. I&#x27;ve tried the whole CSS-in-JS thing multiple times with multiple libraries, and I still see way more pros to just using plain CSS or SCSS. I did my best to make sure all the focus states look good, and the whole site is tabbable by keyboard.
 Implementing the light theme was simply a matter of overwriting some variables in a light theme class.
 :root {
   --font-color: white;
 }
 
 .light.theme {
   --font-color: black;
 }
 This website has gone through a lot of iterations. I mean, a lot. Take a look at the very first iteration of the site, from way back in 2014. (Yes, that&#x27;s an accordion favicon.)
 
       
     
   
   
     
 The blog has come a long way since then! I wrote about Version 2, which was really semantically somewhere around version 1.863.0, and then I wrote about Version 4, because apparently version 3 didn&#x27;t happen. Really, I haven&#x27;t really cared about versioning this site, I just get excited about some change and perpetually tweak it. Which includes making my site look like an MS-DOS prompt as some point.
 
       
     
   
   
     
 Here&#x27;s what the site looked like until this release:
 
       
     
   
   
     
 I&#x27;m excited about this design, because the site is more interactive, more personalized, and yet familiar. And I just had fun doing it. Hopefully you like it, too!</content>
     </entry>
     <entry>
       <title>The Spirit of Ecstasy</title>
         <link href="https://www.logodesignlove.com/rolls-royce-spirit-of-ecstasy"/>
       <updated>2021-08-30T15:31:33.000Z</updated>
       <content type="text">
 
 Lord Montagu of Beaulieu was one of Britain’s motoring pioneers. As founder and editor of The Car Illustrated magazine, he employed an illustrator, Charles Sykes, and a private secretary, Eleanor Velasco Thornton.
 In 1909, Lord Montagu commissioned Sykes (who was also a sculptor) to make a mascot for his Rolls-Royce Silver Ghost. Sykes produced a statue of a young woman in fluttering robes, which he named “The Whisper.” The figure is holding a finger to her lips, which some claim is a reference to Lord Montagu’s close relationship with Eleanor. Others suggest, more prosaically, that it relates to the engine’s quietness.
 
 
 
 Whatever the truth, The Whisper went on to adorn every Montagu Rolls-Royce. Soon, other owners were having their own ornaments made, much to the displeasure of Rolls-Royce general managing director Claude Johnson. In 1910, Johnson commissioned Sykes to make an official mascot to protect the company’s products from these unsightly additions. Sykes subtly reinterpreted The Whisper, and created what become known as the Spirit of Ecstasy.
 
 
 
 Challenging the social conventions of the time, her appearance became instantly iconic, leaning into the wind, arms outstretched, her dress billowing as if in flight.
 
 Charles Sykes made the mascots from February 1911, until his daughter, Jo, took over in 1928. Jo, the only child of Charles and Jessica Sykes, made the mascots from then until 1939, and has written that her mascot production was about seven per week. This suggests that only about forty percent of the Rolls-Royce cars manufactured between 1911 and 1939 were ordered with mascots (an optional extra). We know that the Sykes’ ceased mascot manufacture after 1939 but most pre-WWII Rolls-Royce cars seen today wear mascots. That would mean that perhaps half of the mascots now on these cars are copies, not made by Charles or his daughter.
 
 
 
 The most common question asked by Rolls-Royce owners is whether their mascot is original. The answer depends on what’s defined as “original.” Charles Sykes’ master models are obviously original. From these he made copies, usually four. He then used these as masters to make agar jelly moulds from which in turn he cast wax patterns to be used to make castings. He then polished these castings and sold them to Rolls-Royce to be put on cars. Almost everyone would call these last original, even though they are copies of copies of Sykes’ original sculpture.
 In recent years the emblem was carefully redrawn by Marina Willer’s team at Pentagram in London.
 
 
 
 
 
 
 
 
 
 The static interpretation was combined with Pentagram’s “expression generator,” adding some flexibility, freshness, and distinction to the Rolls-Royce identity.
 
 
 
 
 
 
 
 Today, each Spirit of Ecstasy ornament is cast in Southampton, England, and can be produced in 24-carat gold, sterling silver, glass, stainless steel, and even illuminated. The Black Badge cars feature a black Spirit of Ecstasy, and for security the figurine retracts when the ignition is turned off.
 
 
 
 It’s still unclear whether The Whisper or the Spirit of Ecstasy were truly based on Eleanor Thornton. Either way, Eleanor would never see her likeness achieve global fame. In 1915, she was killed when the P&amp;O liner SS Persia, on which she and Lord Montagu were travelling to India, was torpedoed off the coast of Crete by a German U-boat.
 Eleanor Thornton, copyright Motoring Picture Gallery
 Charles Sykes never spoke publicly about Eleanor, while his daughter Jo confined herself to saying, “Eleanor was a lovely person. It is an interesting story, and if it makes you happy, let the myth prevail.”
 
 
 
 Sources:
 rolls-roycemotorcars.com
 rroc.org.au (PDF)
 bmwgroup.com
 beaulieu.co.uk
 wikipedia.org</content>
     </entry>
     <entry>
       <title>Writing a Sokoban Puzzle Game in JavaScript</title>
         <link href="https://www.taniarascia.com/sokoban-game/"/>
       <updated>2021-07-26T00:00:00.000Z</updated>
       <content type="text">So the other day, I made an implementation of a Sokoban puzzle game in JavaScript.
 Here&#x27;s the source code and here&#x27;s the demo.
 The game consists of a wall, a playable character, blocks, and spots on the ground that are storage locations. The aim of the game is to push all the blocks into all the storage locations. It can be challenging because it&#x27;s easy to end up in a state where a block can no longer be moved and now you have to restart the game.
 Here&#x27;s the one I made:
 
 The original game has slightly better graphics:
 
 In my version, the big blue dot is the character, the pink dots are the storage locations, and the orange blocks are the crates.
 I wrote it up on the fly over the course of a few hours. Making little games is a lot different than what I usually do at work, so I found it to be a fun, achievable challenge. Fortunately with some previous projects (Snek and Chip8) I had some experience with the concept of plotting out coordinates.
 Map and entities
 The first thing I did was build out the map, which is a two-dimensional array where each row corresponds to a y coordinate and each column corresponds to an x coordinate.
 const map &#x3D; [
   [&#x27;y0 x0&#x27;, &#x27;y0 x1&#x27;, &#x27;y0 x2&#x27;, &#x27;y0 x3&#x27;],
   [&#x27;y1 x0&#x27;, &#x27;y1 x1&#x27;, &#x27;y1 x2&#x27;, &#x27;y1 x3&#x27;],
   // ...etc
 ]
 So accessing map[0][0] would be y0 x0 and map[1][3] would be y1 x3.
 From there, it&#x27;s easy to make a map based on an existing Sokoban level where each coordinate is an entity in the game - terrain, player, etc.
 Entities
 const EMPTY &#x3D; &#x27;empty&#x27;
 const WALL &#x3D; &#x27;wall&#x27;
 const BLOCK &#x3D; &#x27;block&#x27;
 const SUCCESS_BLOCK &#x3D; &#x27;success_block&#x27;
 const VOID &#x3D; &#x27;void&#x27;
 const PLAYER &#x3D; &#x27;player&#x27;
 Map
 const map &#x3D; [
   [EMPTY, EMPTY, WALL, WALL, WALL, WALL, WALL, EMPTY],
   [WALL, WALL, WALL, EMPTY, EMPTY, EMPTY, WALL, EMPTY],
   [WALL, VOID, PLAYER, BLOCK, EMPTY, EMPTY, WALL, EMPTY],
   // ...etc
 With that data, I can map each entity to a color and render it to the screen on an HTML5 canvas. So now I have a map that looks right, but it doesn&#x27;t do anything yet.
 Game logic
 There aren&#x27;t too many actions to worry about. The player can move orthogonally - up, down, left, and right - and there are a few things to consider:
 
 The PLAYER and BLOCK cannot move through a WALL
 The PLAYER and BLOCK can move through an EMPTY space or a VOID space (storage location)
 The player can push a BLOCK
 A BLOCK becomes a SUCCESS_BLOCK when it&#x27;s on top of a VOID.
 
 And that&#x27;s literally it. I also coded one more thing in that&#x27;s not part of the original game, but it made sense to me:
 
 A BLOCK can push all other BLOCK pieces
 
 When the player pushes a block that&#x27;s next to other blocks, all the blocks will move until it collides with a wall.
 In order to do this I just need to know the entities adjacent to the player, and the entities adjacent to a block if a player is pushing a block. If a player is pushing multiple blocks, I&#x27;ll have to recursively count how many there are.
 Moving
 Therefore, the first thing we need to do any time a change happens is find the player&#x27;s current coordinates, and what type of entity is above, below, to the left, and to the right of them.
 function findPlayerCoords() {
   const y &#x3D; map.findIndex(row &#x3D;&gt; row.includes(PLAYER))
   const x &#x3D; map[y].indexOf(PLAYER)
 
   return {
     x,
     y,
     above: map[y - 1][x],
     below: map[y + 1][x],
     sideLeft: map[y][x - 1],
     sideRight: map[y][x + 1],
   }
 }
 Now that you have the player and adjacent coordinates, every action will be a move action. If the player is trying to move through a traversible cell (empty or void), just move the player. If the player is trying to push a block, move the player and block. If the adjacent unit is a wall, do nothing.
 function move(playerCoords, direction) {
   if (isTraversible(adjacentCell[direction])) {
     movePlayer(playerCoords, direction)
   }
 
   if (isBlock(adjacentCell[direction])) {
     movePlayerAndBlocks(playerCoords, direction)
   }
 }
 Using the initial game state, you can figure out what should be there. As long as I pass the direction to the function, I can set the new coordinates - adding or removing a y will be up and down, adding or removing an x will be left or right.
 function movePlayer(playerCoords, direction) {
   // Replace previous spot with initial board state (void or empty)
   map[playerCoords.y][playerCoords.x] &#x3D; isVoid(levelOneMap[playerCoords.y][playerCoords.x])
     ? VOID
     : EMPTY
 
   // Move player
   map[getY(playerCoords.y, direction, 1)][getX(playerCoords.x, direction, 1)] &#x3D; PLAYER
 }
 If the player is moving a block, I wrote a little recursive function to check how many blocks are in a row, and once it has that count, it will check what the adjacent entity is, move the block if possible, and move the player if the block moved.
 function countBlocks(blockCount, y, x, direction, board) {
   if (isBlock(board[y][x])) {
     blockCount++
     return countBlocks(blockCount, getY(y, direction), getX(x, direction), direction, board)
   } else {
     return blockCount
   }
 }
 
 const blocksInARow &#x3D; countBlocks(1, newBlockY, newBlockX, direction, map)
 Then, if the block can be moved, it will just either move it or move it and transform it into a success block, if it&#x27;s over a storage location, followed by moving the player.
 map[newBoxY][newBoxX] &#x3D; isVoid(levelOneMap[newBoxY][newBoxX]) ? SUCCESS_BLOCK : BLOCK
 movePlayer(playerCoords, direction)
 Rendering
 It&#x27;s easy to keep track of the entire game in a 2D array and render the update game to the screen with each movement. The game tick is incredibly simple - any time a keydown event happens for up, down, left, right (or w, a, s, d for intense gamers) the move() function will be called, which uses the player index and adjacent cell types to determine what the new, updated state of the game should be. After the change, the render() function is called, which just paints the entire board with the updated state.
 const sokoban &#x3D; new Sokoban()
 sokoban.render()
 
 // re-render
 document.addEventListener(&#x27;keydown&#x27;, event &#x3D;&gt; {
   const playerCoords &#x3D; sokoban.findPlayerCoords()
 
   switch (event.key) {
     case keys.up:
     case keys.w:
       sokoban.move(playerCoords, directions.up)
       break
     case keys.down:
     case keys.s:
       sokoban.move(playerCoords, directions.down)
       break
     case keys.left:
     case keys.a:
       sokoban.move(playerCoords, directions.left)
       break
     case keys.right:
     case keys.d:
       sokoban.move(playerCoords, directions.right)
       break
     default:
   }
 
   sokoban.render()
 })
 The render function just maps through each coordinate and creates a rectangle or circle with the right color.
 function render() {
   map.forEach((row, y) &#x3D;&gt; {
     row.forEach((cell, x) &#x3D;&gt; {
       paintCell(context, cell, x, y)
     })
   })
 }
 Basically all rendering in the HTML canvas made a path for the outline (stroke), and a path for the inside (fill). Since one pixel per coordinate would be a pretty tiny game, I multiplied each value by a multipler, which was 75 pixels in this case.
 function paintCell(context, cell, x, y) {
   // Create the fill
   context.beginPath()
   context.rect(x * multiplier + 5, y * multiplier + 5, multiplier - 10, multiplier - 10)
   context.fillStyle &#x3D; colors[cell].fill
   context.fill()
 
   // Create the outline
   context.beginPath()
   context.rect(x * multiplier + 5, y * multiplier + 5, multiplier - 10, multiplier - 10)
   context.lineWidth &#x3D; 10
   context.strokeStyle &#x3D; colors[cell].stroke
   context.stroke()
 }
 The render function also checks for a win condition (all storage locations are now success blocks) and shows &quot;A winner is you!&quot; if you win.
 Conclusion
 This was a fun little game to make. I organized the files like this:
 
 Constants for entity data, map data, mapping colors to entities, and key data.
 Utility functions for checking what type of entity exists at a particular coordinate, and determining what the new coordinates should be for the player.
 Sokoban class for maintaining game state, logic, and rendering.
 Script for initializing the instance of the app and handling key events.
 
 I found it easier to code than to solve. 😆
 Hope you enjoyed reading about this and feel inspired to make your own little games and projects.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 290: CSS Flexgrids, image performance and selectors</title>
         <link href="https://wdrl.info/archive/290"/>
       <updated>2021-07-01T03:45:00.000Z</updated>
       <content type="text">Hey,
 
 this time we’re going to look deeply into a lot of CSS articles together with optimising images. In CSS, we get nice tools that help us create better layouts, faster. We also look at new technologies like parent selectors, like responsive images in CSS, reverting CSS settings and container queries.
 One thing that I recognize now that I have my gardening job is that I finally am able to look at web things from a more distant perspective. Which is nice because I now don’t spend any thoughts and nerves on discussions about »native HTML« and similar things that drive Twitter’s platform and make it so negative. When reading my Twitter feed, often a little bit later than the tweets were written, so many things seem unnecessary to me. Previously, I struggled with it a lot and spent hours wasting time on topics that in reality aren’t relevant to me as a person or for our community. I learnt that what matters really is doing something for the community instead of arguing against other people’s opinions. It’s one of the reasons why I started writing the newsletter again, it’s why I organise events here in my neighbourhood nowadays.
 Enjoy your week with a smile now, no matter what your brain says. Worth a try, isn’t it?
 —Anselm
 Generic
 
 	We all love regular expressions, don’t we? Here’s a guide how to write better RegExp in PHP.
 
 Web Performance
 
 	Jake Archibald shares another summary of how to serve sharp images to high density screens while saving data efficiently.
 
 CSS
 
 	Chris Coyier shares the differences and use cases for inherit, initial, unset and revert properties in CSS.
 	Ahmad Shadeed shares how to make custom scrollbars in CSS the modern way. While this is very useful, don’t style them unless you really need this. Also, keep in mind the different light/dark modes and high contrast and other media queries and helpful accessibility functionality.
 	This layout generator lets you create and adjust CSS Grid layouts very easily with a UI.
 	Let’s talk about Flexbox: “How can we create equal columns with Flexbox](https://css-tricks.com/equal-columns-with-flexbox-its-more-complicated-than-you-might-think/)? This is the best example to show why we now have Flexbox and CSS Grid but none could ever replace the other, they’re for different purposes.
 	LayoutIt! is another layout generator for CSS Grids, a bit different in its UI than the other one mentioned in the newsletter. Both great, try them!
 	Max Böck on media queries in times of container queries. While we’re inclined to think container queries would replace media queries in all cases, that’s not entirely true. For page layouts itself, media queries will stay the easiest and most suitable solution. For small projects where we don’t necessarily need a huge component system, they will be the go to solution. For big projects, they will play a smaller role in future.
 	Adrian Bece explores an early draft of :has(), a CSS parent selector that may come into our browsers soon.
 	Someone remembers image-set() in CSS? Safari supports it since a long time, actually before we had picture support. Now Firefox added support, so we can have another look into it and check the use-cases: Responsive images in CSS.
 
 
 I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Inherit ancestor font-size, for fun and profit</title>
         <link href="https://lea.verou.me/2021/06/inherit-ancestor-font-size-for-fun-and-profit/"/>
       <updated>2021-06-24T17:08:50.000Z</updated>
       <content type="text">If you’ve been writing CSS for any length of time, you’re probably familiar with the em unit, and possibly the other type-relative units. We are going to refer to em for the rest of this post, but anything described works for all type-relative units.
 
 
 
 As you well know, em resolves to the current font size on all properties except font-size, where it resolves to the parent font size. It can be quite useful for making scalable components that adapt to their context size.
 
 
 
 However, I have often come across cases where you actually need to “circumvent” one level of this. Either you need to set font-size to the grandparent font size instead of the parent one, or you need to set other properties to the parent font size, not the current one. 
 
 
 
 
 
 
 
 If you’re already familiar with the problem and just want the solution, skip ahead. The next few paragraphs are for those thinking “but when would you ever need this?”
 
 
 
 Sometimes, there are workarounds, and it’s just a matter of keeping DRY. For example, take a look at this speech bubble:
 
 
 
 CodePen Embed Fallback
 
 
 
 Note this in the CSS:
 
 
 
 /* This needs to change every time the font-size changes: */
 top: calc(100% + 1em / 2.5); 
 font-size: 250%;
 
 
 
 Note that every time we change the font size we also need to adjust top. And ok, when they’re both defined in the same rule we can just delegate this to a variable:
 
 
 
 --m: 2.5;
 top: calc(100% + 1em / var(--m)); 
 font-size: calc(var(--m) * 100%);
 
 
 
 However, in the general case the font size may be defined elsewhere. For example, a third party author may want to override the emoji size, they shouldn’t also need to override anything else, our CSS should just adapt.
 
 
 
 In other cases, it is simply not possible to multiply and divide by a factor and restore the ancestor font size. Most notably, when the current (or parent) font-size is set to 0 and we need to recover what it was one level up.
 
 
 
 I’ve come across many instances of this in the 16 years I’ve been writing CSS. Admittedly, there were way more use cases pre-Flexbox and friends, but it’s still useful, as we will see. In fact, it was the latest one that prompted this post.
 
 
 
 I needed to wrap &lt;option&gt; elements by a generic container for a library I’m working on. Let me stop you there, no, I could not just set classes on the options, I needed an actual container in the DOM.
 
 
 
 As you can see in this pen, neither &lt;div&gt; nor custom elements work here: when included in the markup they are just discarded by the parser, and when inserted via script they are in the DOM, but the options they contain are not visible. The only elements that work inside a &lt;select&gt; are: &lt;option&gt;, &lt;optgroup&gt;, and script-supporting elements (currently &lt;template&gt; and &lt;script&gt;). Except &lt;optgroup&gt;, none of the rest renders any contents and thus, is not fit for my use case. It had to be &lt;optgroup&gt;, sadly.
 
 
 
 However, using &lt;optgroup&gt;, even without a label attribute inserts an ugly gap in the select menu, where the label would have gone (pen):
 
 
 
 (There were also gaps on the left of the labels, but we applied some CSS to remove them)
 
 
 
 There appears to be no way to remove said gap.
 
 
 
 Ideally, this should be fixed on the user agent level: Browsers should not generate a label box when there is no label attribute. However, I needed a solution now, not in the far future. There was no pseudo-element for targeting the generated label. The only solution that worked was along these lines (optgroup:not([label]) {
 	font-size: 0;
 }
 
 optgroup:not([label]) &gt; * {
 	font-size: 13.333px;
 }
 
 
 
 The weird 13.333px value was taken directly from the Chrome UA stylesheet (as inspected). However, it is obviously flimsy, and will break any additional author styling. It would be far better if we could say “give me whatever 1em is on the grandparent”. Can we?
 
 
 
 The solution
 
 
 
 What if we could use custom properties to solve this? Our first attempt might look something like this:
 
 
 
 select {
 	--em: 1em;
 }
 
 optgroup:not([label]) {
 	font-size: 0;
 }
 
 optgroup:not([label]) &gt; * {
 	font-size: var(--em);
 }
 
 
 
 However this is horribly broken:
 
 
 
 All the options have disappeared!!
 
 
 
 What on Earth happened here?!
 
 
 
 By default, custom properties are just containers for CSS tokens.When they inherit, they inherit as specified, with only any var() references substituted and no other processing. This means that the 1em we specified inherits as the 1em token, not as whatever absolute length it happens to resolve to on select. It only becomes an absolute length at the point of usage, and this is whatever 1em would be there, i.e. 0. So all our options disappeared because we set their font size to 0!
 
 
 
 If only we could make 1em resolve to an actual absolute length at the point of declaration and inherit as that, just like native properties that accept lengths?
 
 
 
 Well, you’re in luck, because today we can!
 
 
 
 You may be familiar with the @property rule as “the thing that allows us to animate custom properties”. However, it is useful for so much more than that. 
 
 
 
 If we register our custom property as a &lt;length&gt;, this makes the 1em resolve on the element we specified it on, and inherit as an absolute length! Let’s try this:
 
 
 
 @property --em {
 	syntax: &quot;&lt;length&gt;&quot;;
 	initial-value: 0;
 	inherits: true;
 }
 
 select {
 	--em: 1em;
 }
 
 optgroup:not([label]) {
 	display: contents;
 	font-size: 0;
 }
 
 optgroup:not([label]) &gt; * {
 	font-size: var(--em);
 }
 
 /* Remove Chrome gap */
 :where(optgroup:not([label]) &gt; option)::before {
 	content: &quot;&quot;;
 }
 
 
 
 CodePen Embed Fallback
 
 
 
 And here is the same technique used for the speech bubble:
 
 
 
 CodePen Embed Fallback
 
 
 
 Fallback
 
 
 
 This is all fine and dandy for the 68%  (as of June 2021) of users that are using a browser that supports @property, but what happens in the remaining 32%? It’s not pretty:
 
 
 
 
 
 
 
 We get the default behavior of an unregistered property, and thus none of our options show up! This is bad.
 
 
 
 We should clearly either provide a fallback or conditionally apply these rules only in browsers that support @property.
 
 
 
 We can easily detect @property support in JS and add a class to our root element:
 
 
 
 if (window.CSSPropertyRule) {
 	let root &#x3D; document.documentElement;
 	root.classList.add(&quot;supports-atproperty&quot;);
 }
 
 
 
 Then we can just use the descendant combinator:
 
 
 
 :root.supports-atproperty optgroup:not([label]) {
 	font-size: 0;
 }
 
 
 
 CSS-only fallback for @property
 
 
 
 While the JS fallback works great, I couldn’t help but wonder if there’s a CSS only way.
 
 
 
 My first thought was to use @supports:
 
 
 
 @supports (--em: flugelhorn) {
 	/* Does not support @property */
 }
 
 
 
 The theory was, if a browser supported any value to be assigned on a property registered as a &lt;length&gt;, surely it does not support property registration.
 
 
 
 It turns out, registered properties do not validate their syntax at parse time, and thus are always valid for @supports. This is explained in the spec:
 
 
 
 When parsing a page’s CSS, UAs commonly make a number of optimizations to help with both speed and memory.One of those optimizations is that they only store the properties that will actually have an effect; they throw away invalid properties, and if you write the same property multiple times in a single declaration block, all but the last valid one will be thrown away. (This is an important part of CSS’s error-recovery and forward-compatibility behavior.)This works fine if the syntax of a property never changes over the lifetime of a page. If a custom property is registered, however, it can change its syntax, so that a property that was previously invalid suddenly becomes valid.The only ways to handle this are to either store every declaration, even those that were initially invalid (increasing the memory cost of pages), or to re-parse the entire page’s CSS with the new syntax rules (increasing the processing cost of registering a custom property). Neither of these are very desirable.Further, UA-defined properties have their syntax determined by the version of the UA the user is viewing the page with; this is out of the page author’s control, which is the entire reason for CSS’s error-recovery behavior and the practice of writing multiple declarations for varying levels of support. A custom property, on the other hand, has its syntax controlled by the page author, according to whatever stylesheet or script they’ve included in the page; there’s no unpredictability to be managed. Throwing away syntax-violating custom properties would thus only be, at best, a convenience for the page author, not a necessity like for UA-defined properties.
 
 
 
 Ok this is great, and totally makes sense, but what can we do? How can we provide a fallback?
 
 
 
 It turns out that there is a way, but brace yourself, as it’s quite hacky. I’m only going to describe it for entertainment purposes, but I think for real usage, the JS way is far more straightforward, and it’s the one I’ll be using myself.
 
 
 
 The main idea is to take advantage of the var() fallback argument of a second registered variable, that is registered as non-inheriting. We set it to the fallback value on an ancestor. If @property is supported, then this property will not be defined on the element of interest, since it does not inherit. Any other properties referencing it will be invalid at computed value time, and thus any var() fallbacks will apply. If @property is not supported, the property will inherit as normal and thus using it becomes our fallback. 
 
 
 
 Here is an example with a simple green/red test to illustrate this concept:
 
 
 
 @property --test {
 	syntax: &quot;*&quot;;
 	inherits: false;
 }
 
 html {
 	--test: red;
 }
 
 body {
 	background: var(--test, green);
 }
 
 
 
 CodePen Embed Fallback
 
 
 
 And here is how we can use the same concept to provide a fallback for the &lt;select&gt; example:
 
 
 
 @property --test {
 	syntax: &quot;*&quot;;
 	inherits: false;
 }
 
 select {
 	--test: 1em; /* fallback */
 	--em: 1em;
 }
 
 optgroup:not([label]) {
 	font-size: var(--test, 0);
 }
 
 
 
 Here is the finished demo.</content>
     </entry>
     <entry>
       <title>React Architecture: How to Structure and Organize a React Application</title>
         <link href="https://www.taniarascia.com/react-architecture-directory-structure/"/>
       <updated>2021-06-23T00:00:00.000Z</updated>
       <content type="text">There is no consensus on the right way to organize a React application. React gives you a lot of freedom, but with that freedom comes the responsibility of deciding on your own architecture. Often the case is that whoever sets up the application in the beginning throws almost everything in a components folder, or maybe components and containers if they used Redux, but I propose there&#x27;s a better way. I like to be deliberate about how I organize my applications so they&#x27;re easy to use, understand, and extend.
 I&#x27;m going to show you what I consider to be an intuitive and scalable system for large-scale production React applications. The main concept I think is important is to make the architecture focused on feature as opposed to type, organizing only shared components on a global level and modularized all the other related entities together in the localized view.
 Tech assumptions
 Since this article will be opinionated, I&#x27;ll make some assumptions about what technology the project will be using:
 
 Application - React (Hooks)
 Global state management - Redux, Redux Toolkit
 Routing - React Router
 Styles - Styled Components
 Testing - Jest, React Testing Library
 
 I don&#x27;t have a very strong opinion about the styling, whether Styled Components or CSS modules or a custom Sass setup is ideal, but I think Styled Components is probably one of the best options for keeping your styles modular.
 I&#x27;m also going to assume the tests are alongside the code, as opposed to in a top-level tests folder. I can go either way with this one, but in order for an example to work, and in the real world, decisions need to be made.
 Everything here can still apply if you&#x27;re using vanilla Redux instead of Redux Toolkit. I would recommend setting up your Redux as feature slices either way.
 I&#x27;m also ambivalent about Storybook, but I&#x27;ll include what it would look like with those files if you choose to use it in your project.
 For the sake of the example, I&#x27;ll use a &quot;Library App&quot; example, that has a page for listing books, a page for listing authors, and has an authentication system.
 Directory Structure
 The top level directory structure will be as follows:
 
 assets - global static assets such as images, svgs, company logo, etc.
 components - global shared/reusable components, such as layout (wrappers, navigation), form components, buttons
 services - JavaScript modules
 store - Global Redux store
 utils - Utilities, helpers, constants, and the like
 views - Can also be called &quot;pages&quot;, the majority of the app would be contained here
 
 I like keeping familiar conventions wherever possible, so src contains everything, index.js is the entry point, and App.js sets up the auth and routing.
 .
 └── /src
     ├── /assets
     ├── /components
     ├── /services
     ├── /store
     ├── /utils
     ├── /views
     ├── index.js
     └── App.js
 I can see some additional folders you might have, such as types if it&#x27;s a TypeScript project, middleware if necessary, maybe context for Context, etc.
 Aliases
 I would set up the system to use aliases, so anything within the components folder could be imported as @components, assets as @assets, etc. If you have a custom Webpack, this is done through the resolve configuration.
 module.exports &#x3D; {
   resolve: {
     extensions: [&#x27;js&#x27;, &#x27;ts&#x27;],
     alias: {
       &#x27;@&#x27;: path.resolve(__dirname, &#x27;src&#x27;),
       &#x27;@assets&#x27;: path.resolve(__dirname, &#x27;src/assets&#x27;),
       &#x27;@components&#x27;: path.resolve(__dirname, &#x27;src/components&#x27;),
       // ...etc
     },
   },
 }
 It just makes it a lot easier to import from anywhere within the project and move files around without changing imports, and you never end up with something like ../../../../../components/.
 Components
 Within the components folder, I would group by type - forms, tables, buttons, layout, etc. The specifics will vary by your specific app.
 In this example, I&#x27;m assuming you&#x27;re either creating your own form system, or creating your own bindings to an existing form system (for example, combining Formik and Material UI). In this case, you&#x27;d create a folder for each component (TextField, Select, Radio, Dropdown, etc.), and inside would be a file for the component itself, the styles, the tests, and the Storybook if it&#x27;s being used.
 
 Component.js - The actual React component
 Component.styles.js - The Styled Components file for the component
 Component.test.js - The tests
 Component.stories.js - The Storybook file
 
 To me, this makes a lot more sense than having one folder that contains the files for ALL components, one folder that contains all the tests, and one folder that contains all the Storybook files, etc. Everything related is grouped together and easy to find.
 .
 └── /src
     └── /components
         ├── /forms
         │   ├── /TextField
         │   │   ├── TextField.js
         │   │   ├── TextField.styles.js
         │   │   ├── TextField.test.js
         │   │   └── TextField.stories.js
         │   ├── /Select
         │   │   ├── Select.js
         │   │   ├── Select.styles.js
         │   │   ├── Select.test.js
         │   │   └── Select.stories.js
         │   └── index.js
         ├── /routing
         │   └── /PrivateRoute
         │       ├── /PrivateRoute.js
         │       └── /PrivateRoute.test.js
         └── /layout
             └── /navigation
                 └── /NavBar
                     ├── NavBar.js
                     ├── NavBar.styles.js
                     ├── NavBar.test.js
                     └── NavBar.stories.js
 You&#x27;ll notice there&#x27;s an index.js file in the components/forms directory. It is often rightfully suggested to avoid using index.js files as they&#x27;re not explicit, but in this case it makes sense - it will end up being an index of all the forms and look something like this:
 src/components/forms/index.js
 import { TextField } from &#x27;./TextField/TextField&#x27;
 import { Select } from &#x27;./Select/Select&#x27;
 import { Radio } from &#x27;./Radio/Radio&#x27;
 
 export { TextField, Select, Radio }
 Then when you need to use one or more of the components, you can easily import them all at once.
 import { TextField, Select, Radio } from &#x27;@components/forms&#x27;
 I would recommend this approach more than making an index.js inside of every folder within forms, so now you just have one index.js that actually indexes the entire directory, as opposed to ten index.js files just to make imports easier for each individual file.
 Services
 The services directory is less essential than components, but if you&#x27;re making a plain JavaScript module that the rest of the application is using, it can be handy. A common contrived example is a LocalStorage module, which might look like this:
 .
 └── /src
     └── /services
         ├── /LocalStorage
         │   ├── LocalStorage.service.js
         │   └── LocalStorage.test.js
         └── index.js
 An example of the service:
 src/services/LocalStorage/LocalStorage.service.js
 export const LocalStorage &#x3D; {
   get(key) {},
   set(key, value) {},
   remove(key) {},
   clear() {},
 }
 import { LocalStorage } from &#x27;@services&#x27;
 
 LocalStorage.get(&#x27;foo&#x27;)
 Store
 The global data store will be contained in the store directory - in this case, Redux. Each feature will have a folder, which will contain the Redux Toolkit slice, as well as actions and tests. This setup can also be used with regular Redux, you would just create a .reducers.js file and .actions.js file instead of a slice. If you&#x27;re using sagas, it could be .saga.js instead of .actions.js for Redux Thunk actions.
 .
 └── /src
     ├── /store
     │   ├── /authentication
     │   │   ├── /authentication.slice.js
     │   │   ├── /authentication.actions.js
     │   │   └── /authentication.test.js
     │   ├── /authors
     │   │   ├── /authors.slice.js
     │   │   ├── /authors.actions.js
     │   │   └── /authors.test.js
     │   └── /books
     │       ├── /books.slice.js
     │       ├── /books.actions.js
     │       └── /books.test.js
     ├── rootReducer.js
     └── index.js
 You can also add something like a ui section of the store to handle modals, toasts, sidebar toggling, and other global UI state, which I find better than having const [isOpen, setIsOpen] &#x3D; useState(false) all over the place.
 In the rootReducer you would import all your slices and combine them with combineReducers, and in index.js you would configure the store.
 Utils
 Whether or not your project needs a utils folder is up to you, but I think there are usually some global utility functions, like validation and conversion, that could easily be used across multiple sections of the app. If you keep it organized - not just having one helpers.js file that contains thousands of functions - it could be a helpful addition to the organization of your project.
 .
 └── src
     └── /utils
         ├── /constants
         │   └── countries.constants.js
         └── /helpers
             ├── validation.helpers.js
             ├── currency.helpers.js
             └── array.helpers.js
 Again, the utils folder can contain anything you want that you think makes sense to keep on a global level. If you don&#x27;t prefer the &quot;multi-tier&quot; filenames, you could just call it validation.js, but the way I see it, being explicit does not take anything away from the project, and makes it easier to navigate filenames when searching in your IDE.
 Views
 Here&#x27;s where the main part of your app will live: in the views directory. Any page in your app is a &quot;view&quot;. In this small example, the views line up pretty well with the Redux store, but it won&#x27;t necessarily be the case that the store and views are exactly the same, which is why they&#x27;re separate. Also, books might pull from authors, and so on.
 Anything within a view is an item that will likely only be used within that specific view - a BookForm that will only be used at the /books route, and an AuthorBlurb that will only be used on the /authors route. It might include specific forms, modals, buttons, any component that won&#x27;t be global.
 The advantage of keeping everything domain-focused instead of putting all your pages together in components/pages is that it makes it really easy to look at the structure of the application and know how many top level views there are, and know where everything that&#x27;s only used by that view is. If there are nested routes, you can always add a nested views folder within the main route.
 .
 └── /src
     └── /views
         ├── /Authors
         │   ├── /AuthorsPage
         │   │   ├── AuthorsPage.js
         │   │   └── AuthorsPage.test.js
         │   └── /AuthorBlurb
         │       ├── /AuthorBlurb.js
         │       └── /AuthorBlurb.test.js
         ├── /Books
         │   ├── /BooksPage
         │   │   ├── BooksPage.js
         │   │   └── BooksPage.test.js
         │   └── /BookForm
         │       ├── /BookForm.js
         │       └── /BookForm.test.js
         └── /Login
             ├── LoginPage
             │   ├── LoginPage.styles.js
             │   ├── LoginPage.js
             │   └── LoginPage.test.js
             └── LoginForm
                 ├── LoginForm.js
                 └── LoginForm.test.js
 
 Keeping everything within folders might seem annoying if you&#x27;ve never set up your project that way - you can always keep it more flat, or move tests to its own directory that mimics the rest of the app.
 
 Conclusion
 This is my proposal for a sytem for React organization that scales well for a large production app, and handles testing and styling as well as keeping everything together in a feature focused way. It&#x27;s more nested than the traditional structure of everything being in components and containers, but that system is a bit more dated due to Redux being much easier to implement with Hooks, and &quot;smart&quot; containers and &quot;dumb&quot; components no longer being necessary.
 It&#x27;s easy to look at this system and understand everything that is needed for your app and where to go to work on a specific section, or a component that affects the app globally. This system may not make sense for every type of app, but it has worked for me. I&#x27;d love to hear any comments about ways this system can be improved, or other systems that have merit.</content>
     </entry>
     <entry>
       <title>Using OAuth with PKCE Authorization Flow (Proof Key for Code Exchange)</title>
         <link href="https://www.taniarascia.com/oauth-pkce-authorization/"/>
       <updated>2021-06-20T00:00:00.000Z</updated>
       <content type="text">If you&#x27;ve ever created a login page or auth system, you might be familiar with OAuth 2.0, the industry standard protocol for authorization. It allows an app to access resources hosted on another app securely. Access is granted using different flows, or grants, at the level of a scope.
 For example, if I make an application (Client) that allows a user (Resource Owner) to make notes and save them as a repo in their GitHub account (Resource Server), then my application will need to access their GitHub data. It&#x27;s not secure for the user to directly supply their GitHub username and password to my application and grant full access to the entire account. Instead, using OAuth 2.0, they can go through an authorization flow that will grant limited access to some resources based on a scope, and I will never have access to any other data or their password.
 Using OAuth, a flow will ultimately request a token from the Authorization Server, and that token can be used to make all future requests in the agreed upon scope.
 
 Note: OAuth 2.0 is used for authorization, (authZ) which gives users permission to access a resource. OpenID Connect, or OIDC, is often used for authentication, (authN) which verifies the identity of the end user.
 
 Grant Types
 The type of application you have will determine the grant type that will apply.
 
 
 
 Grant Type
 Application type
 Example
 
 
 
 
 Client Credentials
 Machine
 A server accesses 3rd-party data via cron job
 
 
 Authorization Code
 Server-side web app
 A Node or Python server handles the front and back end
 
 
 Authorization Code with PKCE
 Single-page web app/mobile app
 A client-side only application that is decoupled from the back end
 
 
 
 For machine-to-machine communication, like something that cron job on a server would perform, you would use the Client Credentials grant type, which uses a client id and client secret. This is acceptable because the client id and resource owner are the same, so only one is needed. This is performed using the /token endpoint.
 For a server-side web app, like a Python Django app, Ruby on Rails app, PHP Laravel, or Node/Express serving React, the Authorization Code flow is used, which still uses a client id and client secret on the server side, but the user needs to authorize via the third-party first. This is performed using both an /authorize and /token endpoints.
 However, for a client-side only web app or a mobile app, the Authorization Code flow is not acceptable because the client secret cannot be exposed, and there&#x27;s no way to protect it. For this purpose, the Proof Key for Code Exchange (PKCE) version of the authorization code flow is used. In this version, the client creates a secret from scratch and supplies it after the authorization request to retrieve the token.
 Since PKCE is a relatively new addition to OAuth, a lot of authentication servers do not support it yet, in which case either a less secure legacy flow like Implicit Grant is used, where the token would return in the callback of the request, but using Implicit Grant flow is discouraged. AWS Cognito is one popular authorization server that supports PKCE.
 PKCE Flow
 The flow for a PKCE authentication system involves a user, a client-side app, and an authorization server, and will look something like this:
 
 The user arrives at the app&#x27;s entry page
 The app generates a PKCE code challenge and redirects to the authorization server login page via /authorize
 The user logs in to the authorization server and is redirected back to the app with the authorization code
 The app requests the token from the authorization server using the code verifier/challenge via /token
 The authorization server responds with the token, which can be used by the app to access resources on behalf of the user
 
 So all we need to know is what our /authorize and /token endpoints should look like. I&#x27;ll go through an example of setting up PKCE for a front end web app.
 GET /authorize endpoint
 The flow begins by making a GET request to the /authorize endpoint. We need to pass some parameters along in the URL, which includes generating a code challenge and code verifier.
 
 
 
 Parameter
 Description
 
 
 
 
 response_type
 code
 
 
 client_id
 Your client ID
 
 
 redirect_uri
 Your redirect URI
 
 
 code_challenge
 Your code challenge
 
 
 code_challenge_method
 S256
 
 
 scope
 Your scope
 
 
 state
 Your state (optional)
 
 
 
 We&#x27;ll be building the URL and redirecting the user to it, but first we need to make the verifier and challenge.
 Verifier
 The first step is generating a code verifier, which the PKCE spec defines as:
 
 Verifier - A high-entropy cryptographic random STRING using the unreserved characters [A-Z] / [a-z] / [0-9] / &quot;-&quot; / &quot;.&quot; / &quot;*&quot; / &quot;~&quot; from Section 2.3 of [RFC3986], with a minimum length of 43 characters and a maximum length of 128 characters.
 
 I&#x27;m using a random string generator that Aaron Parecki of oauth.net wrote:
 function generateVerifier() {
   const array &#x3D; new Uint32Array(28)
   window.crypto.getRandomValues(array)
 
   return Array.from(array, (item) &#x3D;&gt; &#x60;0${item.toString(16)}&#x60;.substr(-2)).join(
     &#x27;&#x27;
   )
 }
 Challenge
 The code challenge performs the following transformation on the code verifier:
 
 Challenge - BASE64URL-ENCODE(SHA256(ASCII(code_verifier)))
 
 So the verifier gets passed into the challenge function as an argument and transformed. This is the function that will hash and encode the random verifier string:
 async function generateChallenge(verifier) {
   function sha256(plain) {
     const encoder &#x3D; new TextEncoder()
     const data &#x3D; encoder.encode(plain)
 
     return window.crypto.subtle.digest(&#x27;SHA-256&#x27;, data)
   }
 
   function base64URLEncode(string) {
     return btoa(String.fromCharCode.apply(null, new Uint8Array(string)))
       .replace(/\+/g, &#x27;-&#x27;)
       .replace(/\//g, &#x27;_&#x27;)
       .replace(/&#x3D;+\$/, &#x27;&#x27;)
   }
 
   const hashed &#x3D; await sha256(verifier)
 
   return base64URLEncode(hashed)
 }
 Build endpoint
 Now you can take all the needed parameters, generate the verifier and challenge, set the verifier to local storage, and redirect the user to the authentication server&#x27;s login page.
 async function buildAuthorizeEndpointAndRedirect() {
   const host &#x3D; &#x27;https://auth-server.example.com/oauth/authorize&#x27;
   const clientId &#x3D; &#x27;abc123&#x27;
   const redirectUri &#x3D; &#x27;https://my-app-host.example.com/callback&#x27;
   const scope &#x3D; &#x27;specific,scopes,for,app&#x27;
   const verifier &#x3D; generateVerifier()
   const challenge &#x3D; await generateChallenge(verifier)
 
   // Build endpoint
   const endpoint &#x3D; &#x60;${host}?
     response_type&#x3D;code&amp;
     client_id&#x3D;${clientId}&amp;
     scope&#x3D;${scope}&amp;
     redirect_uri&#x3D;${redirectUri}&amp;
     code_challenge&#x3D;${challenge}&amp;
     code_challenge_method&#x3D;S256&#x60;
 
   // Set verifier to local storage
   localStorage.setItem(&#x27;verifier&#x27;, verifier)
 
   // Redirect to authentication server&#x27;s login page
   window.location &#x3D; endpoint
 }
 At what point you call this function is up to you - it might happen at the click of a button, or automatically if a user is deemed to not be authenticated when they land on the app. In a React app it would probably be in the useEffect().
 useEffect(() &#x3D;&gt; {
   buildAuthorizeEndpointAndRedirect()
 }, [])
 Now the user will be on the authentication server&#x27;s login page, and after successful login via username and password they&#x27;ll be redirected to the redirect_uri from step one.
 POST /token endpoint
 The second step is retrieving the token. This is the part that is usually accomplished server side in a traditional Authorization Code flow, but for PKCE it&#x27;s also through the front end. When the authorization server redirects back to your callback URI, it will come along with a code in the query string, which you can exchange along with the verifier string for the final token.
 The POST request for a token must be made as a x-www-form-urlencoded request.
 
 
 
 Header
 Description
 
 
 
 
 Content-Type
 application/x-www-form-urlencoded
 
 
 
 
 
 
 Parameter
 Description
 
 
 
 
 grant_type
 authorization_code
 
 
 client_id
 Your client ID
 
 
 code_verifier
 Your code verifier
 
 
 redirect_uri
 The same redirect URI from step 1
 
 
 code
 Code query parameter
 
 
 
 async function getToken(verifier) {
   const host &#x3D; &#x27;https://auth-server.example.com/oauth/token&#x27;
   const clientId &#x3D; &#x27;abc123&#x27;
   const redirectUri &#x3D; &#x60;https://my-app-server.example.com/callback&#x60;
 
   // Get code from query params
   const urlParams &#x3D; new URLSearchParams(window.location.search)
   const code &#x3D; urlParams.get(&#x27;code&#x27;)
 
   // Build params to send to token endpoint
   const params &#x3D; &#x60;client_id&#x3D;${clientId}&amp;
     grant_type&#x3D;${grantType}&amp;
     code_verifier&#x3D;${verifier}&amp;
     redirect_uri&#x3D;${redirectUri}&amp;
     code&#x3D;${code}&#x60;
 
   // Make a POST request
   try {
     const response &#x3D; await fetch(host, {
       method: &#x27;POST&#x27;,
       headers: {
         &#x27;Content-Type&#x27;: &#x27;application/x-www-form-urlencoded&#x27;,
       },
       body: params,
     })
     const data &#x3D; await response.json()
 
     // Token
     console.log(data)
   } catch (e) {
     console.log(e)
   }
 }
 Once you obtain the token, you should immediately delete the verifier from localStorage.
 const response &#x3D; await getToken(localStorage.getItem(&#x27;verifier&#x27;))
 localStorage.removeItem(&#x27;verifier&#x27;)
 When it comes to storing the token, if your app is truly front end only, the option is to use localStorage. If the option of having a server is available, you can use a Backend for Frontend (BFF) to handle authentication. I recommend reading A Critical Analysis of Refresh Token Rotation in Single-page Applications.
 Conclusion
 And there you have it - the two steps to authenticate using PKCE. First, build a URL for /authorize on the authorization server and redirect the user to it, then POST to the /token endpoint on the redirect. PKCE is currently the most secure authentication system that I know of for a front-end only web or mobile app. Hopefully this helps you understand and implement PKCE in your app!</content>
     </entry>
     <entry>
       <title>How and When to Use Context in React with Hooks</title>
         <link href="https://www.taniarascia.com/react-context-api-hooks/"/>
       <updated>2021-06-15T00:00:00.000Z</updated>
       <content type="text">A while ago, I wrote an article about Using Context API in React. However, most of my examples on that page used Class components, static contextType, and Consumer, which is a legacy way of dealing with Context and in TYOOL 2021 we want nice, clean, functional components. I needed to use Context for something recently after quite a while, and I wanted a more succinct explanation using only modern syntax. I decided I&#x27;d write a little follow up here for a realistic use of Context.
 Context allows you to pass data across any number of React components, regardless of nesting.
 Redux or Context?
 In a very small application, you might be able to get away with just using Context for most of your global data storage needs, but in a large-scale production environment, you&#x27;re likely using Redux for global state management. Redux still provides improved performance, improved debugging capabilities, architectural consistency, the ability to use middleware, and more. Therefore, Context is not a replacement for a proper global state management system.
 Often, examples for Context will show something like a dark mode toggle, which is fine for a quick example. However, a real-life example of dark theme usage outside of a small blog or website would probably involve a user with settings they can save and persist across any session, not just temporary state in localStorage that gets toggled via Context. In that case, your dark mode state would be saved into Redux, since it would probably be saved as the whole currently logged-in user object, and require an API call to make changes.
 So I&#x27;m going to provide a summary of just how to set up Context with modern React syntax, then go into an example of using Context and how it might work.
 Summary
 If you just want some code to copy to create, provide, and consume context, here it is:
 You&#x27;ll usually have one file that uses createContext and exports a Provider wrapper:
 Creating
 import React, { createContext } from &#x27;react&#x27;
 
 export const Context &#x3D; createContext()
 
 export const Provider &#x3D; ({ children }) &#x3D;&gt; {
   const [state, setState] &#x3D; useState({})
 
   const value &#x3D; {
     state,
     setState,
   }
 
   return &lt;Context.Provider value&#x3D;{value}&gt;{children}&lt;/Context.Provider&gt;
 }
 Then you&#x27;ll wrap whatever component needs access to the Context state with the Provider:
 Providing
 import React from &#x27;react&#x27;
 
 import { Provider } from &#x27;./Context&#x27;
 import { ConsumingComponent } from &#x27;./ConsumingComponent&#x27;
 
 export const Page &#x3D; () &#x3D;&gt; {
   return (
     &lt;div&gt;
       &lt;Provider&gt;
         &lt;ConsumingComponent /&gt;
       &lt;/Provider&gt;
     &lt;/div&gt;
   )
 }
 And the consuming component can now use the useContext hook to access the data:
 Consuming
 import React, { useContext } from &#x27;react&#x27;
 
 import { Context } from &#x27;./Context&#x27;
 
 export const ConsumingComponent &#x3D; () &#x3D;&gt; {
   const { state } &#x3D; useContext(Context)
 
   return null
 }
 Example
 So when should you use Context, if it&#x27;s not used for the same purposes as Redux? Well, in my experience, Context makes sense for something a little bit more localized and reusable. For example, you have a Dashboard widget that has controls that are common across many types of widgets. Let&#x27;s say every widget receives data but can change the view between bar graph, line graph, or table view. In that case, you can create a Context Provider that sets the state of the controls and updates them, and pass them to any consumer.
 You use createContext() to create a Context, which also creates a Provider and a Consumer, but you only need the Provider, which will allow any React element below it in the tree to use the Context.
 Creating Context
 DashboardWidget.context.js
 import React, { useState, createContext } from &#x27;react&#x27;
 
 export const DashboardWidgetContext &#x3D; createContext()
 
 export const DashboardWidgetProvider &#x3D; ({ children }) &#x3D;&gt; {
   const [dataView, setDataView] &#x3D; useState(&#x27;table&#x27;)
 
   const handleChangeView &#x3D; value &#x3D;&gt; {
     setDataViewView(value)
   }
 
   const value &#x3D; {
     dataView,
     handleChangeView,
   }
 
   return &lt;DashboardWidgetContext.Provider value&#x3D;{value}&gt;{children}&lt;/DashboardWidgetContext.Provider&gt;
 }
 Consuming Context
 Then you might have a component that handles the actions. This is a contrived example, but it would contain a select that lets you switch between a bar graph, line chart, or table view. Maybe it also has an &quot;export as CSV&quot; button, or some other actions that can apply to all the data in the widget. Now you don&#x27;t have to handle the controls for each widget individually, but one time for all widgets.
 Here you can see the useContext hook allows you to access the data from Context.
 DashboardWidgetControls.js
 import React, { useContext } from &#x27;react&#x27;
 
 import { DashboardWidgetContext } from &#x27;./DashboardWidget.context&#x27;
 
 export const DashboardWidgetControls &#x3D; ({ label }) &#x3D;&gt; {
   const { dataView, handleChangeView } &#x3D; useContext(DashboardWidgetContext)
 
   return (
     &lt;div&gt;
       &lt;select value&#x3D;{dataView} onChange&#x3D;{handleChangeView}&gt;
         &lt;option value&#x3D;&quot;bar_graph&quot;&gt;Bar Graph&lt;/option&gt;
         &lt;option value&#x3D;&quot;line_chart&quot;&gt;Line Chart&lt;/option&gt;
         &lt;option value&#x3D;&quot;table&quot;&gt;Table&lt;/option&gt;
       &lt;/select&gt;
     &lt;/div&gt;
   )
 }
 Whatever unique data you need to do on a localized level, you can do in the individual component while still having access to the outer control data. This part might be handled individually, because it might be a grouped or a stacked bar chart, or a nested table, and maybe there are a lot of tweaks that have to happen on that level.
 SomeDataComponent.js
 import React, { useContext } from &#x27;react&#x27;
 
 import { DashboardWidgetContext } from &#x27;./DashboardWidget.context&#x27;
 
 export const SomeDataComponent &#x3D; () &#x3D;&gt; {
   const { dataView } &#x3D; useContext(DashboardWidgetContext)
 
   switch (dataView) {
     case &#x27;table&#x27;:
       return &lt;Table /&gt;
     case &#x27;line_chart&#x27;:
       return &lt;LineChart /&gt;
     case &#x27;bar_chart&#x27;:
       return &lt;BarChart /&gt;
   }
 }
 Providing Context
 Now wherever you need the widget, you can bring in the Provider and the controls. I&#x27;ll just put it in to a wrapper component:
 import React from &#x27;react&#x27;
 
 import { DashboardWidgetProvider } from &#x27;./DashboardWidget.context&#x27;
 import { DashboardWidgetControls } from &#x27;./WidgetControls&#x27;
 
 export const DashboardWidget &#x3D; ({ title, children }) &#x3D;&gt; {
   return (
     &lt;WidgetProvider&gt;
       &lt;section&gt;
         &lt;h2&gt;{title}&lt;/h2&gt;
         &lt;WidgetControls /&gt;
         {children}
       &lt;/section&gt;
     &lt;/WidgetProvider&gt;
   )
 }
 DashboardPage.js
 import React from &#x27;react&#x27;;
 
 import { DashboardWidget } from &#x27;./DashboardWidget&#x27;;
 
 export const DashboardPage &#x3D; () &#x3D;&gt; {
   return (
     &lt;div&gt;
       &lt;h1&gt;Dashboard&lt;/h1&gt;
 
       &lt;DashboardWidget title&#x3D;&quot;Distance of Planets to the Sun&quot;&gt;
         &lt;PlanetDistance /&gt;
       &lt;/DashboardWidgetProvider&gt;
 
       &lt;DashboardWidget title&#x3D;&quot;Time Dilation and the Speed of Light&quot;&gt;
         &lt;SpeedOfLight /&gt;
       &lt;/DashboardWidget&gt;
     &lt;/div&gt;
   );
 };
 Perhaps in this case the actual data is stored in Redux because it might be used elsewhere aside from just this dashboard component, and only the controls need to be handled on a localized level. This is one example where I can see Context making a lot of sense, because passing that data around manually can start to become unintutive or there would be a lot of repetition to handle the same kind of state. I feel like it would be messy to try to handle something like this in Redux, because if you wanted multiple widgets to all be visible at once you&#x27;d need it to look like widgets: { widget1: &#x27;bar&#x27;, widget2: &#x27;table&#x27; } or have a separate store for each individual widget.
 Conclusion
 I hope that was a relatively clear example of a situation in which you might use Context and the modern syntax with which to use it.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 289: Simplify your life and work and new stuff on the web</title>
         <link href="https://wdrl.info/archive/289"/>
       <updated>2021-06-09T03:30:00.000Z</updated>
       <content type="text">Hey,
 
 this week I want to share thoughts from Jens Oliver Meiert on »Listening« with you. I wholeheartedly agree with the point raised in the text that we are at a point where we don’t listen to others anymore, where we disagree or dismiss other people’s ideas straight without reconsidering them, without trying to understand the standpoint. It’s either 100% or 0%. But there are so many goodies if we change this habit that we should exercise the practise of listening and understanding other opinions again. When reading, I realise that I’m trying my best already, I realise that I wish more people would be more open but I also realise that I struggle with it quite regularly, too. So I’ll promise to myself now again to do my best to get better at not dismissing other opinions straight away.
 News
 
 	Chrome now supports the import of JSON modules.
 	Apple just announced Safari 15 that’s coming in autumn and here are the highlights: Support for HTML attribute autocomplete&#x3D;one-time-code (2FA), CSS aspect-ratio, lab(), lch(), hwb() color syntaxes, theme-color meta tag supoport, and in JavaScript top level await, ES6 Modules in Service Workers and Workers, Error.cause. Apart from that a lot of privacy and security enhancements are coming as well as WebGL2, and Web Share level 2.
 
 Generic
 
 	A little bit of a different view that shows the benefits of auto deleting inactive users from your platform. When you do, your database will get faster, more secure, and easier to handle and administrate. In the end, inactive users make a great number but the platform itself doesn’t benefit from them. Care about those on your platform.
 
 UI/UX
 
 	Flagpack is a set of flags we can use for internationalization and languages choosers on our apps and websites. It’s open source.
 
 Tooling
 
 	HeadlessUI is a set of completely unstyled, fully accessible UI components, designed to integrate beautifully with Tailwind CSS. Designed for React and Vue.
 
 Web Performance
 
 	Fork is a lightweight solution for running PHP code concurrently.
 
 Accessibility
 
 	Sandrina Pereira explains us how to make disabled buttons more inclusive.
 	Kitty Giraudel shares how to implement an accessible toggle component in your app.
 	Vitaly Friedman wrote up a complete guide to accessible front-end components. This is worth reading it again from time to time.
 
 CSS
 
 	David Bushell argues for using start and end as syntax instead of left and right for a more universal layout language.
 	Adam Argyle shares how to create split-text animations and word animations the proper way.
 	Jim Nielsen reveals the system colors we can use in CSS and how useful this can be for light and dark mode themes.
 	Barry Pollard shares how we can control font loading with less impact to the rendering with the cool new CSS Font Descriptors feature.
 
 Work &amp; Life
 
 	This is a great piece of content on how less is often more. Kent C. Dodds shares with the example of electric cars why sometimes we need to think entirely different to what we’re used to to simplify things. Electric cars need less maintenance and are easier to build because they need less parts. If we apply that to our life, or to building websites we can save ourselves from a lot of trouble.
 	Julia Evans shares helpful advice on how to improve the relationship to your manager by telling them thing they might not know, such as why the team might be slow, what the technical debt is, what goals you may have, the extra work people in the team are doing, and more.
 
 
 I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>WDRL — Edition 288: Back again with Kirby 3 and some fresh content for you</title>
         <link href="https://wdrl.info/archive/288"/>
       <updated>2021-05-28T03:45:00.000Z</updated>
       <content type="text">Hey,
 
 it’s been a long while and I’ll start with a big sorry to all my loyal readers over the past years. I’ve neglected this project since January this year and just now found enough time to bring it back to life. Why to life? Because I broke my setup in January and then went the bigger upgrade path and had to update and partially refactor my entire custom software for the newsletter. I’m now using Kirby 3 (previously v2) for the project together with my browser web extension to save links into my draft editions. I also switched to use TailwindCSS to understand the concept a bit better and have to say it definitely is worth it. It’s probably not the most suitable solution for this kind of project but for bigger projects in a team with many devs or if a website has to be built on a budget it’s a great option.
 I’m busy with my market garden business growing lots of food which works fine despite weather surprises us here again with colder than usual temperatures and lots of rain over the past weeks. But we now serve over 40 customers, which means we give food to around 80 – 100 people every week during growing period. This is an awesome feeling!
 In parallel I spend a lot of time seeing my son grow up, playing with him. When there’s enough time I work on small website projects and try to organize an event about soil and climate action in summer here in Germany. Let’s see if it’s possible to make this event real.
 The political and social situation we’re currently in continues to baffle me and surprise me. Mostly not in a positive way though which means I’m spending more time reflecting on my thoughts and news, meditating for my mind and to stay sane and calm. I stuggle to accept that so many people have hardline-thoughts in a specific direction, I don’t even care so much in which one. But I think we’re still individuals and every single person should be able to think on their own and decide on their own (thinking as a social, empathic being).
 News
 
 	Safari 14.1 is an interesting one because it’s a minor update but brings great joy for us developers: Flexbox gap support, Date &amp; Time Inputs on macOS (woohoo!), WebM support to name the big ones.
 
 Generic
 
 	Una Kravets recaps the new complexity of building modern responsive designs for the web. But there’s a new player called Container Queries in the house which makes many things for us developers easier to work with a component driven system.
 
 Web Performance
 
 	Silvestar Bistrović shares useful tools for auditing CSS.
 	Barry Pollard explains how Core Web Vitals work and what we should do as developers to get a better score by Google.
 
 HTML &amp; SVG
 
 	Amber Wilson shares the love story of HTML input fields and labels. A good reminder or starter to understand how these two play well together.
 	Adrian Roselli shows us how to easily configure &lt;select&gt; elements to look custom instead of choosing a library that does get accessibility or usability wrong.
 	Kitty Giraudel shares how to build a navigation. Sounds boring? Well, we probably forget a lot of the things that went into this one in our daily work.
 
 JavaScript
 
 	Ever struggled to understand DOM Events and their options? Here’s DOMEvents.dev to help you understand it better with an interactive playground.
 	Parvus is an accessible, open-source image lightbox with no dependencies.
 	Adrian Roselli shows how to build sortable table columns in an inclusive and simple way.
 
 CSS
 
 	Max Böck had a chance to try out container queries and shares his findings, his struggles and combines them with a good real world example.
 	This tutorial by Ahmad Shadeed provides nice solutions for how to put text over images so it’s still readable.
 	The folks from Sentry built a dark mode for their app. Now they share their path of cleaning up, refactoring variables, then building a new design system and then reimplementing the styles into their components.
 	Lea Verou has an easy solution to implement dark mode by using inverted lightness variables. This is especially useful if you’re building new projects or one that already makes use of the hsl() color mode.
 	Callum Hart shares how to name and define variables that adapt to different contexts like Chameleons do.
 	Stephanie Eckles shares the updates to CSS that improve accessibility. She explains them so we understand better when and how to use them.
 	Matsuko from craft cms shares how to create custom focus indicators that look great in CSS.
 	Scott Kellum on why he thinks intrinsic typography is the future of styling text on the web. With detailed comparison-view examples and code suggestions he wrote a great article that’s useful for us developers.
 	Stephanie Eckles explains all the modern CSS pseudo-class selectors like :any-link, :where() and others.
 	Chris Coyier tried out the new CSS color-contrast() function that’s in the Safari preview. A pretty cool function when we’re working with variables and in different contexts.
 
 Work &amp; Life
 
 	Leo Babauta is great at pointing out things that we all know if we think about it but we constantly forget to think about. Here’s advice for the feeling of guilt to not working more when we’re done for the day already. A habit worth integrating into our lifestyles.
 
 
 I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Is the current tab active?</title>
         <link href="https://lea.verou.me/2021/05/is-the-current-tab-active/"/>
       <updated>2021-05-24T11:49:17.000Z</updated>
       <content type="text">Today I ran into an interesting problem. Interesting because it’s one of those very straightforward, deceptively simple questions, that after a fair amount of digging, does not appear to have a definite answer (though I would love to be wrong!). 
 
 
 
 The problem was to determine if the current tab is active. Yes, as simple as that. 
 
 
 
 
 
 
 
 Why? (i.e. my use case)
 
 
 
 I was working on my slide deck framework, Inspire.js. There is a presenter mode plugin, which spawns a new window with your slides (“projector view”), whereas your current window becomes a “presenter view”, with open notes, preview of the next slide, optional progress indicator for time etc.
 
 
 
 However, this plugin was not very good. The two windows are synced, but only if you use presenter view to navigate slides. If you use the projector view to advance slides, the syncing breaks. Why would you use the projector mode? Many reasons, e.g. to interact with a live demo, or even play a video. If you have a live demo heavy presentation, you may even want to mirror your screen and only ever interact with the projector mode, while having the presenter mode on a secondary screen, just to look at.
 
 
 
 The way the plugin worked was that every time the slide changed in the presenter view, it propagated the change in the projector view. To make the syncing bidirectional, it would be good to know if the current window is the active tab, and if so, propagate all slide navigation to the other one, regardless of which one is the projector view and which one is the presenter view.
 
 
 
 And this, my friends, is how I ended up in this rabbit hole.
 
 
 
 (Yes, there are other solutions to this particular problem. I could just always propagate regardless and have checks in place to avoid infinite loops. But that’s beside the point.)
 
 
 
 What about the Visibility API?
 
 
 
 In most resources around the Web, people were rejoicing about how the Visibility API makes this problem trivial. “Just use document.hidden!” people would gleefully recommend to others. 
 
 
 
 Yes, the Visibility API is great, when you want to determine whether the current tab is visible. That is not the same as whether it is active.
 
 
 
 You may have two windows side by side, both visible, but only one of them is active. You may even have a window entirely obscuring another window, but you can still tab through to it and make it active. Active and visible are entirely orthogonal states, which are only loosely correlated. 
 
 
 
 In my use case, given that both the projector view and presenter view would be visible at all times, this is a no-go that doesn’t even solve a subset of use cases.
 
 
 
 What about focus and blur events on window?
 
 
 
 The other solution that was heavily recommended was using the focus and blur events on window. This does get us partway there. Indeed, when the current tab becomes active, the focus event fires. When another tab becomes active, the blur event fires.
 
 
 
 Notice the emphasis on “becomes”. Events notify us about a state change, but they are no help for determining the current state. If we get a focus or blur event, we know whether our tab is active or not, but if we don’t get any, we simply don’t know. A tab can start off as active or not, and there is no way to tell. 
 
 
 
 How can a tab possibly start off as inactive? One easy way to reproduce this is to hit Return on the address bar and immediately switch to another window. The tab you just loaded just starts off as inactive and no blur event is ever fired.
 
 
 
 What about document.activeElement?
 
 
 
 The document.activeElement property will always return the currently focused element in a page. Can we use it to determine if a window currently has focus? Nope, cause that would be too easy.
 
 
 
 Run setTimeout(() &#x3D;&gt; console.log(document.activeElement), 2000) in the console and quickly switch windows. Return &gt;2 seconds later and see what was logged. It’s the &lt;body&gt; element! 
 
 
 
 Wait, maybe we can assume that if the currently focused element is a &lt;body&gt; element then the current window is inactive? Nope, you get the same result in an active tab, if you simply haven’t focused anywhere.
 
 
 
 What about document.hasFocus()?
 
 
 
 When I discovered document.hasFocus() I thought that was the end of it. Surely, this is exactly what I need?!? The spec made it sound so promising. I quickly switched to my about:blank tab that I use for trying things out, and ran it in the console.
 
 
 
 &gt; document.hasFocus()
 &lt; false
 
 
 
 
 
 
 
 Neeeext!
 
 
 
 Edit: document.hasFocus() may be the solution after all! As pointed out to me on Twitter, the problem above was that unlike I did with document.activeElement, I ran this synchronously in the console and it returned false because the console as the active window. An asynchronous log while I make sure the actual window is focused would do the trick.
 
 
 
 The anti-climactic conclusion
 
 
 
 Edit: I left this section in because the moral is still valid for other cases, but it looks like document.hasFocus() was the solution after all.
 
 
 
 If you’re expecting this to end with a revelation of an amazing API that I had originally missed and addresses this, you will be disappointed. If there is such a silver bullet, I did not find it. Maybe someone will point it out to me after publishing this blog post, in which case I will update it so that you don’t struggle like I did.
 
 
 
 But in my case, I simply gave up trying to find a general solution. Instead, I took advantage of the knowledge my code had in this specific situation: I knew what the other window was, and I primarily cared which one of the two (if any) had focus. 
 
 
 
 // Track whether presenter or projector is the active window
 addEventListener(&quot;focus&quot;, _ &#x3D;&gt; {
 	Inspire.isActive &#x3D; true;
 
 	// If this window is focused, no other can be
 	if (Inspire.projector) {
 		Inspire.projector.Inspire.isActive &#x3D; false;
 	}
 	else if (Inspire.presenter) {
 		Inspire.presenter.Inspire.isActive &#x3D; false;
 	}
 });
 
 addEventListener(&quot;blur&quot;, _ &#x3D;&gt; {
 	Inspire.isActive &#x3D; false;
 
 	// If this window is not focused,
 	// we cannot make assumptions about which one is.
 });
 
 
 
 Given that the presenter view calls window.focus() after opening the projector view, in practice this was pretty bulletproof.
 
 
 
 What’s the moral of this story?
 
 
 
 Sometimes simple questions do not have a good answer when it comes to the Web PlatformIf your code cannot answer the general question correctly in all cases, maybe it can answer a specific one that solves your particular problem, even if that leads to a less elegant solution.
 
 
 
 That’s it folks. </content>
     </entry>
     <entry>
       <title>82% of developers get this 3 line CSS quiz wrong</title>
         <link href="https://lea.verou.me/2021/05/82-of-developers-get-this-3-line-css-quiz-wrong/"/>
       <updated>2021-05-21T15:13:16.000Z</updated>
       <content type="text">(I always wanted to do a clickbait title like this and when this chance came along I could not pass it up.  Sorry!)
 
 
 
 While putting my ideas into slides for my Dynamic CSS workshop for next week, I was working on a slide explaining how the CSS wide keywords work with custom properties. inherit, initial, unset I had used numerous times and knew well. But what about revert? How did that work? I had an idea, but quickly coded up a demo to try it out. 
 
 
 
 The code was:
 
 
 
 :root {
     --accent-color: skyblue;
 }
 
 div {
     --accent-color: revert; 
     background: var(--accent-color, orange);
 }
 
 
 
 Phew, I was correct, but the amount of uncertainty I had before seeing the result tipped me that I might be on to something.
 
 
 
 Before you read on, take a moment to think about what you would vote. Warning: Spoilers ahead!
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 So I posted a quiz on Twitter:
 
 
 
 
 Thursday CSS quiz! Without trying this out, what background do you expect &lt;div&gt; to have?:root {–accent-color: skyblue;}div { –accent-color: revert; background: var(–accent-color, orange);}— Lea Verou (@LeaVerou) May 20, 2021
 
 
 
 
 These were the results after the 24 hours it ran for:
 
 
 
 
 
 
 
 orange was the clear winner, and the actual correct answer, skyblue only got 18.1%, nearly the same as transparent!
 
 
 
 If you got it wrong, you’re in very good company: not only did 82% of poll respondents get it wrong as well, but even the editor of the CSS Variables spec and co-editor of CSS Cascading and Inheritance (which defines revert), Tab Atkins, told me privately that he got it wrong too: he voted for orange! (Yes, I did get his permission to mention this)
 
 
 
 So what actually happens? Why do we get skyblue? I will try to explain as best as I can.
 
 
 
 Let’s start by what revert does: It reverts the cascaded value of the property from its current value to the value the property would have had if no changes had been made by the current style origin to the current element.
 
 
 
 This means it cancels out any author styles, and resets back to whatever value the property would have from the user stylesheet and UA stylesheet. Assuming there is no --accent-color declaration in the user stylesheet, and of course UA stylesheets don’t set custom properties, then that means the property doesn’t have a value.
 
 
 
 Since custom properties are inherited properties (unless they are registered with inherits: false, but this one is not), this means the inherited value trickles in, which is — you guessed it — skyblue. You can see for yourself in this codepen.
 
 
 
 CodePen Embed Fallback
 
 
 
 What if our property were registered as non-inheriting? Would it then be orange? Nice try, but no. When we register a custom property, it is mandatory to provide an initial value. This means that the property always resolves to a value, even --accent-color: initial does not trigger the fallback anymore. You can see this for yourself in this codepen (Chrome only as of May 2021).
 
 
 
 CodePen Embed Fallback
 
 
 
 Liked this? Then you will love the workshop! There are still a few tickets left!</content>
     </entry>
     <entry>
       <title>Integration Tests with Jest, Supertest, Knex, and Objection in TypeScript</title>
         <link href="https://www.taniarascia.com/integration-testing-with-jest-typescript-objection/"/>
       <updated>2021-04-06T00:00:00.000Z</updated>
       <content type="text">Recently, I set up unit and integration tests for a Node API in TypeScript, and I couldn&#x27;t find a lot of resources for setting up and tearing down, database seeding, and hooking everything up in TypeScript, so I&#x27;ll share the approach I went with.
 Prerequisites
 This article will help if:
 
 You&#x27;re using TypeScript as the language for an API in Node/Express.
 You&#x27;re using Objection.js as an ORM for your API, which runs on Knex behind the scenes.
 You&#x27;re using Jest for testing.
 
 Goals
 
 You want to be able to spin up a test database, make real API calls with responses and errors, and tear down the database at the end of the tests.
 
 This is not meant to be a complete tutorial that gives step-by-step instructions for every detail, but will give you the big picture of setting up the TypeScript API with Objection and making a test suite for it.
 Installation
 This app involves objection, knex, pg, express, and typescript, with jest and supertest for testing.
 npm i objection knex pg express
 npm i -D typescript jest jest-extended supertest ts-jest ts-node
 Setup
 Assume you have an API with an endpoint at GET /books/:id that returns a Book object. Your Objection model for the Book would look like this, assuming there&#x27;s a book table in the database:
 book.model.ts
 import { Model } from &#x27;objection&#x27;
 
 export class Book extends Model {
   id!: string
   name!: string
   author!: string
 
   static tableName &#x3D; &#x27;book&#x27; // database table name
   static idColumn &#x3D; &#x27;id&#x27; // id column name
 }
 
 export type BookShape &#x3D; ModelObject&lt;Book&gt;
 Here&#x27;s an Express app with a single endpoint. It&#x27;s essential to export app and NOT run app.listen() here so tests won&#x27;t start the app and cause issues.
 app.ts
 import express, { Application, Request, Response, NextFunction } from &#x27;express&#x27;
 import { Book } from &#x27;./book.model&#x27;
 
 // Export the app
 export const app: Application &#x3D; express()
 
 app.use(express.json())
 app.use(express.urlencoded({ extended: true }))
 
 // GET endpoint for the book
 app.get(
   &#x27;/books/:id&#x27;,
   async (request: Request, response: Response, next: NextFunction) &#x3D;&gt; {
     try {
       const { id } &#x3D; request.params
 
       const book: BookShape &#x3D; await Book.query().findById(id)
 
       if (!book) {
         throw new Error(&#x27;Book not found&#x27;)
       }
 
       return response.status(200).send(book)
     } catch (error) {
       return response.status(404).send({ message: error.message })
     }
   }
 )
 The index.ts is where you would set up your database connection and start the app.
 index.ts
 import Knex from &#x27;knex&#x27;
 import { Model } from &#x27;objection&#x27;
 
 // Import the app
 import { app } from &#x27;./app&#x27;
 
 // Set up the database (assuming Postgres)
 const port &#x3D; 5000
 const knex &#x3D; Knex({
   client: &#x27;pg&#x27;,
   connection: {
     host: &#x27;localhost&#x27;,
     database: &#x27;books_database&#x27;,
     port: 5432,
     password: &#x27;your_password&#x27;,
     user: &#x27;your_username&#x27;,
   },
 })
 
 // Connect database to Objection
 Model.knex(knex)
 
 // Start the app
 app.listen(port, () &#x3D;&gt; console.log(&#x60;*:${port} - Listening on port ${port}&#x60;))
 So now you have a complete API for the /books/:id endpoint. This API would start with:
 tsc &amp;&amp; npm start
 Or you could use nodemon to get a dev server going.
 Migration
 In Knex, you can use a migration to seed the schema/data instead of just using raw SQL. To make a migration, you&#x27;d just use the Knex CLI to create a migration file:
 knex migrate:make initial-schema
 And set up the data - in this case, making book table with a few columns:
 db/migrations/initial-chema.js
 exports.up &#x3D; async function (knex) {
   await knex.schema.createTable(&#x27;book&#x27;, function (table) {
     table.increments(&#x27;id&#x27;).primary().unique()
     table.string(&#x27;name&#x27;).notNullable()
     table.string(&#x27;author&#x27;).notNullable()
   })
 }
 
 exports.down &#x3D; async function (knex) {
   await knex.schema.dropTable(&#x27;book&#x27;)
 }
 Similar instructions are available for seed.
 Test Configuration
 Your basic jest.config.js would look something like this:
 jest-config.js
 module.exports &#x3D; {
   clearMocks: true,
   moduleFileExtensions: [&#x27;ts&#x27;],
   roots: [&#x27;&lt;rootDir&gt;&#x27;],
   testEnvironment: &#x27;node&#x27;,
   transform: {
     &#x27;^.+\\.ts?$&#x27;: &#x27;ts-jest&#x27;,
   },
   setupFilesAfterEnv: [&#x27;jest-extended&#x27;],
   globals: {
     &#x27;ts-jest&#x27;: {
       diagnostics: false,
     },
   },
   globalSetup: &#x27;&lt;rootDir&gt;/tests/global-setup.ts&#x27;,
   globalTeardown: &#x27;&lt;rootDir&gt;/tests/global-teardown.ts&#x27;,
 }
 Note the globalSetup and globalTeardown properties and their corresponding files. In those files, you can seed and migrate the database, and tear it down when you&#x27;re done.
 Global setup
 In the global setup, I made a two step process - first connect without the database to create it, then migrate and seed the database. (Migration instructions are in the Knex documentation.)
 tests/global-setup.ts
 import Knex from &#x27;knex&#x27;
 
 const database &#x3D; &#x27;test_book_database&#x27;
 
 // Create the database
 async function createTestDatabase() {
   const knex &#x3D; Knex({
     client: &#x27;pg&#x27;,
     connection: {
       /* connection info without database */
     },
   })
 
   try {
     await knex.raw(&#x60;DROP DATABASE IF EXISTS ${database}&#x60;)
     await knex.raw(&#x60;CREATE DATABASE ${database}&#x60;)
   } catch (error) {
     throw new Error(error)
   } finally {
     await knex.destroy()
   }
 }
 
 // Seed the database with schema and data
 async function seedTestDatabase() {
   const knex &#x3D; Knex({
     client: &#x27;pg&#x27;,
     connection: {
       /* connection info with database */
     },
   })
 
   try {
     await knex.migrate.latest()
     await knex.seed.run()
   } catch (error) {
     throw new Error(error)
   } finally {
     await knex.destroy()
   }
 }
 Then just export the function that does both.
 tests/global-setup.ts
 module.exports &#x3D; async () &#x3D;&gt; {
   try {
     await createTestDatabase()
     await seedTestDatabase()
     console.log(&#x27;Test database created successfully&#x27;)
   } catch (error) {
     console.log(error)
     process.exit(1)
   }
 }
 Global teardown
 For teardown, just delete the database.
 tests/global-teardown.ts
 module.exports &#x3D; async () &#x3D;&gt; {
   try {
     await knex.raw(&#x60;DROP DATABASE IF EXISTS ${database}&#x60;)
   } catch (error) {
     console.log(error)
     process.exit(1)
   }
 }
 Integration Tests
 With an integration test, you want to be able to seed some data in the individual test, and be able to test all the successful responses as well as error responses.
 In the test setup, you can add any additional seed data to the database that you want, creating a new Knex instance and connecting it to the Objection model.
 These tests will utilize Supertest, a popular library for HTTP assertions.
 Import supertest, knex, objection, and the app, seed whatever data you need, and begin writing tests.
 books.test.ts
 import request from &#x27;supertest&#x27;
 import Knex from &#x27;knex&#x27;
 import { Model } from &#x27;objection&#x27;
 
 import { app } from &#x27;../app&#x27;
 
 describe(&#x27;books&#x27;, () &#x3D;&gt; {
   let knex: any
   let seededBooks
 
   beforeAll(async () &#x3D;&gt; {
     knex &#x3D; Knex({
       /* configuration information with test_book_database */
     })
     Model.knex(knex)
 
     // Seed anything
     seededBooks &#x3D; await knex(&#x27;book&#x27;)
       .insert([{ name: &#x27;A Game of Thrones&#x27;, author: &#x27;George R. R. Martin&#x27; }])
       .returning(&#x27;*&#x27;)
   })
 
   afterAll(() &#x3D;&gt; {
     knex.destroy()
   })
 
   decribe(&#x27;GET /books/:id&#x27;, () &#x3D;&gt; {
     // Tests will go here
   })
 })
 Successful response test
 At this point, all the setup is ready and you can test a successful seed and GET on the endpoint.
 tests/books.test.ts
 it(&#x27;should return a book&#x27;, async () &#x3D;&gt; {
   const id &#x3D; seededBooks[0].id
 
   const { body: book } &#x3D; await request(app).get(&#x60;/books/${id}&#x60;).expect(200)
 
   expect(book).toBeObject()
   expect(book.id).toBe(id)
   expect(book.name).toBe(&#x27;A Game of Thrones&#x27;)
 })
 Error response test
 It&#x27;s also important to make sure all expected errors are working properly.
 tests/books.test.ts
 it(&#x27;should return 404 error &#x27;, async () &#x3D;&gt; {
   const badId &#x3D; 7500
   const { body: errorResult } &#x3D; await request(app)
     .get(&#x60;/books/${badId}&#x60;)
     .expect(404)
 
   expect(errorResult).toStrictEqual({
     message: &#x27;Book not found&#x27;,
   })
 })
 Conclusion
 Now once you run npm run test, or jest, in the command line, it will create the test_book_database database, seed it with any migrations you had (to set up the schema and any necessary data), and you can access the database in each integration test.
 This ensures the entire process from database seeding to the API controllers are working properly. This type of code will give you full coverage on the models, routes, and handlers within the app.</content>
     </entry>
     <entry>
       <title>Dark mode in 5 minutes, with inverted lightness variables</title>
         <link href="https://lea.verou.me/2021/03/inverted-lightness-variables/"/>
       <updated>2021-03-30T17:40:36.000Z</updated>
       <content type="text">By now, you probably know that you can use custom properties for individual color components, to avoid repeating the same color coordinates multiple times throughout your theme. You may even know that you can use the same variable for multiple components, e.g. HSL hue and lightness:
 
 
 
 :root {
 	--primary-hs: 250 30%;
 }
 
 h1 {
 	color: hsl(var(--primary-hs) 30%);
 }
 
 article {
 	background: hsl(var(--primary-hs) 90%);
 }
 
 article h2 {
 	background: hsl(var(--primary-hs) 40%);
 	color: white;
 }
 
 
 
 Here is a very simple page designed with this technque:
 
 
 
 CodePen Embed Fallback
 
 
 
 Unlike preprocessor variables, you could even locally override the variable, to have blocks with a different accent color:
 
 
 
 :root {
 	--primary-hs: 250 30%;
 	--secondary-hs: 190 40%;
 }
 
 article {
 	background: hsl(var(--primary-hs) 90%);
 }
 
 article.alt {
 	--primary-hs: var(--secondary-hs);
 }
 
 
 
 CodePen Embed Fallback
 
 
 
 This is all fine and dandy, until dark mode comes into play. The idea of using custom properties to make it easier to adapt a theme to dark mode is not new. However, in every article I have seen, the strategy suggested is to create a bunch of custom properties, one for each color, and override them in a media query.
 
 
 
 This is a fine approach, and you’ll likely want to do that for at least part of your colors eventually. However, even in the most disciplined of designs, not every color is a CSS variable. You often have colors declared inline, especially grays (e.g. the footer color in our example). This means that adding a dark mode is taxing enough that you may put it off for later, especially on side projects.
 
 
 
 The trick I’m going to show you will make anyone who knows enough about color cringe (sorry Chris!) but it does help you create a dark mode that works in minutes. It won’t be great, and you should eventually tweak it to create a proper dark mode (also dark mode is not just about swapping colors) but it’s better than nothing and can serve as a base.
 
 
 
 
 
 
 
 The basic idea is to use custom properties for the lightness of colors instead of the entire color. Then, in dark mode, you override these variables with 100% - lightness. This generally produces light colors for dark colors, medium colors for medium colors, and dark colors for light colors, and still allows you to define colors inline, instead of forcing you to use a variable for every single color. This is what the code would look like for our example:
 
 
 
 root {
 	--primary-hs: 250 30%;
 	--secondary-hs: 190 40%;
 
 	--l-0: 0%;
 	--l-30: 30%;
 	--l-40: 40%;
 	--l-50: 50%;
 	--l-90: 90%;
 	--l-100: 100%;
 }
 
 @media (prefers-color-scheme: dark) {
 	:root {
 		--l-0: 100%;
 		--l-30: 70%;
 		--l-40: 60%;
 		--l-90: 10%;
 		--l-100: 0%;
 	}
 }
 
 
 body {
 	background: hsl(0 0% var(--l-100));
 	color: hsl(0 0% var(--l-0));
 }
 
 h1 {
 	color: hsl(var(--primary-hs) var(--l-30));
 }
 
 article {
 	background: hsl(var(--primary-hs) var(--l-90));
 }
 
 article h2 {
 	background: hsl(var(--primary-hs) 40%);
 	color: white;
 }
 
 footer {
 	color: hsl(0 0% var(--l-40));
 }
 
 
 
 CodePen Embed Fallback
 
 
 
 The result looks like this in light &amp; dark mode:
 
 
 
 
 
 The light mode we designed and the auto-generated dark mode, side by side
 
 
 
 
 
 Note that here we indiscriminately replaced all lightnesses with lightness variables. In reality, we don’t need to be quite as sweeping. For example, the article titles would actually look better and would have better contrast if we just kept them the same:
 
 
 
 Comparison of dark mode with every lightness becoming a variable versus a more refined approach, where we make exceptions as needed (in this case the background and text colors for article &gt; h2).
 
 
 
 These are decisions that are easy to make while you go through your CSS replacing lightness percentages with variables and previewing the result.
 
 
 
 The problem with HSL
 
 
 
 But why were the article headers easier to read with their original colors than with inverted lightness? The root cause is that HSL lightness does not actually correspond to what humans perceive as lightness, and the same lightness difference can produce vastly different perceptual differences.
 
 
 
 That is the big problem with this approach: it assumes that HSL lightness actually means something, but as we’ve discussed before, it does not. Yellow and blue have the same HSL lightness (50%) for crying out loud! Also, you will notice that your dark colors have smaller differences between them than your light colors, because HSL is not perceptually uniform. 
 
 
 
 Does that mean the technique is not useful for anything other than a placeholder while we develop our real dark mode, if that? 
 
 
 
 Well, things are not quite as grim.
 
 
 
 Soon enough, we will get LCH colors in the browser. The first browser implementation just recently shipped in Safari and there is activity in that space among the other browser vendors too.
 
 
 
 LCH is a much better color space for this technique, because its lightness actually means something, not just across different lightnesses of the same color, but across different hues and chromas. 
 
 
 
 This next example needs Safari TP 120+ . Compare these two gradients, the top one showing various HSL colors all with lightness 50%, and the bottom various LCH colors, all with lightness 50%. You can even adjust the slider and try different lightnesses:
 
 
 
 CodePen Embed Fallback
 
 
 
 Here is a screenshot for those of you who don’t have access to Safari TP 120+:
 
 
 
 
 
 
 
 Notice that in HSL, some colors (like yellow and cyan) are much lighter than others. In LCH, all colors at the same lightness are, well, the same lightness.
 
 
 
 Keep in mind that LCH chroma doesn’t really correspond to HSL lightness, so even though we’ve set it to the same number, it doesn’t correspond to the same thing.
 
 
 
 So, how would this technique work with LCH colors? Let’s try it out!
 
 
 
 I used this tool to convert the existing HSL colors to LCH, then tweaked the values manually a bit as the initially converted colors didn’t look nice across all LCH lightnesses (note that HSL colors with the same hue and saturation may have different hue and chromas in LCH. The opposite would defeat the point!). This is what this technique looks like with LCH colors instead (you will need Safari TP 120 or later to view this):
 
 
 
 CodePen Embed Fallback
 
 
 
 And here is a screenshot:
 
 
 
 Light mode and auto-generated dark mode via inverted lightness variables in LCH. 
 
 
 
 Not only does dark mode look a lot better, but even in light mode, our two alternate colors actually look more uniform since they have the same LCH lightness.
 
 
 
 Here is a comparison of the two dark modes:
 
 
 
 Comparison of the two auto-generated dark modes, via HSL lightness on the left and LCH lightness on the right.
 
 
 
 Here you can see an animated comparison of them over each other:
 
 
 
 
 
 
 
 
 
 
 Note that in reality, until LCH colors are reliably supported everywhere you’d need to provide a fallback via @supports, but for brevity, I did not include one in this demo.
 
 
 
 Automating generation of lightness variables
 
 
 
 If you are using a preprocessor that supports loops, such as Sass, you can automate the generation of these variables, and make them even more granular, e.g. every 5%:
 
 
 
 :root {
     @for $i from 0 through 20 {
         --l-#{$i * 5}: #{$i * 5}%;
     }
 }
 
 @media (prefers-color-scheme: dark) {
     :root {
         @for $i from 0 through 20 {
             --l-#{$i * 5}: #{100 - $i * 5}%;
         }
     }
 }
 
 
 
 Can we make lightness variables more DRY?
 
 
 
 Some of you may have disliked the repetition of values: we need to declare e.g. --l-40 as 40%, then set it to 60% in dark mode. Can’t we derive it somehow, by subtracting the value we already have from 100%?
 
 
 
 Those with experience in programming may try something like this:
 
 
 
 --l-40: calc(100% - var(--l-40));
 
 
 
 However, this will not work. CSS is not an imperative language. It does not have steps of calculation, where variables have different values before and after each step. There is no such concept of time, all declarations that are currently applied, need to be true at once. It’s more similar to the reactive evaluation of spreadsheet formulas than to computation in JS and other popular programming languages (there are general purpose reactive programming languages, but they are less well known). Therefore, declarations like the one above are considered cycles: since --l-40 cannot refer to itself, this is an error, and --l-40 would be set to its initial value as an error recovery mechanism (since CSS cannot throw errors).
 
 
 
 So, is there a way to avoid declaring lightness variables twice, once for light mode and once for dark mode?
 
 
 
 There is, but I wouldn’t recommend it. It makes the code more convoluted to read and comprehend, for little benefit. But for the sake of intellectual amusement, I’m going to describe it here.
 
 
 
 Instead of setting --l-40 to 40%, we are going to set it in terms of its difference from 50%, i.e. -10%. Then,  calc(50% + var(--l-40)) gives us 40% and calc(50% - var(--l-40)) gives us 60%, the two values we need. We can therefore declare one variable that is -1 in dark mode and 1 in light mode, and just multiply with that.
 
 
 
 Here is a subset of what our code would be like with this:
 
 
 
 :root {
 	--dm: 1;
 
 	/* Example declaration: */
 	--l-40: -10%;
 }
 
 @media (prefers-color-scheme: dark) {
 	:root {
 		--dm: -1;
 	}
 }
 
 /* Example usage: */
 
 footer {
 	color: hsl(0 0% calc(50% + var(--dm) * var(--l-40));
 	/* Ewww! */
 }
 
 
 
 And hopefully now you can see why I wouldn’t recommend this: it makes usage much more complicated, to DRY up a few declarations that would only be specified once. It’s this kind of obsessive adherence to DRY that programmers eventually realize is counterproductive.
 
 
 
 
 
 
 
 Liked this article? Sign up for my Smashing Workshop on Dynamic CSS for more content like this!</content>
     </entry>
     <entry>
       <title>Mass function overloading: why and how?</title>
         <link href="https://lea.verou.me/2021/02/mass-function-overloading-why-and-how/"/>
       <updated>2021-02-10T18:28:26.000Z</updated>
       <content type="text">One of the things I’ve been doing for the past few months (on and off—more off than on TBH) is rewriting Bliss to use ESM 1. Since Bliss v1 was not using a modular architecture at all, this introduced some interesting challenges.
 
 
 
 
 
 
 
 Bliss is essentially a collection of helper functions. Most of these functions have a number of different signatures, to allow for more compact, readable code. The functions can be used for single things (one element, one set of arguments) or they can operate en masse (arrays of elements, object literals with multiple key-value pairs). As you might guess, this practice has been strongly inspired by the heavy use of overloading in jQuery, which was one of the driving factors behind its huge success.
 
 
 
 For example, let’s take $.style(). It can be used to set a single CSS property, on a single element, being a rather thin abstraction over element.style:
 
 
 
 $.style(element, &quot;top&quot;, rect.top);
 
 
 
 It can also be used to set a single CSS property on multiple elements:
 
 
 
 $.style($$(&quot;.popup&quot;), &quot;top&quot;, rect.top);
 
 
 
 It can also be used to set multiple properties on a single element:
 
 
 
 $.style(element, {
 	top: rect.top,
 	right: rect.right,
 	bottom: rect.bottom,
 	left: rect.left
 );
 
 
 
 Or to set multiple properties on multiple elements:
 
 
 
 $.style($$(&quot;.popup&quot;), {
 	top: rect.top,
 	right: rect.right,
 	bottom: rect.bottom,
 	left: rect.left
 });
 
 
 
 I’m a strong believer in overloading for handling both aggregate operations, as well as singular data. Supporting only aggregate operations would mean that developers have to pointlessly wrap single values in object literals or arrays. E.g. if $.style() only accepted arrays and object literals, our first example would be:
 
 
 
 $.style([element], {top: rect.top});
 
 
 
 Not the end of the world, but certainly annoying and error-prone. Developers would often try setting the pair as separate arguments because it’s more natural, remember it doesn’t work, then adjust their code. 
 
 
 
 The opposite situation is much worse. If $.style() only supported singular operations, our last example would be:
 
 
 
 let values &#x3D; {
 	top: rect.top,
 	right: rect.right,
 	bottom: rect.bottom,
 	left: rect.left
 };
 for (let element of $$(&quot;.popup&quot;)) {
 	for (let property in values) {
 		$.style(element, property, values[property]);
 	}
 }
 
 
 
 Yikes! You don’t need a library for that! Just using element.style and Object.assign() would have actually fared better here:
 
 
 
 for (let element of $$(&quot;.popup&quot;)) {
 	Object.assign(element.style, {
 		top: rect.top,
 		right: rect.right,
 		bottom: rect.bottom,
 		left: rect.left
 	});
 }
 
 
 
 $.style() is not unique here: any Bliss function that accepts a main target element (the function’s subject as it’s called in the Bliss docs) also accepts arrays of elements. Similarly, any Bliss function that accepts key-value pairs as separate arguments, also accepts object literals with multiple of them.
 
 
 
 In talks about API Design, I have presented this pattern (and overloading in general) as an instance of the Robustness principle in action: “Be liberal in what you accept” is good practice for designing any user interface, and APIs are no exception. An analog in GUI design would be bulk operations: imagine if e.g. you could only delete emails one by one?
 
 
 
 In JS, overloading is typically implemented by inspecting the types and number of a function’s arguments in the function, and branching accordingly. However, doing this individually on every function would get quite repetitive. Consider the following, very simplified implementation of $.style() with the overloading logic inlined:
 
 
 
 style(subject, ...args) {
 	if (Array.isArray(subject)) {
 		subject.forEach(e &#x3D;&gt; style(e, ...args));
 	}
 	else if ($.type(args[0]) &#x3D;&#x3D;&#x3D; &quot;object&quot; &amp;&amp; args.length &#x3D; 1) {
 		for (let p in args[0]) {
 			style(subject, p, args[0][p]);
 		}
 	}
 	else {
 		subject.style[args[0]] &#x3D; args[1];
 	}
 
 	return subject;
 }
 
 
 
 Note that the actual code of this function is only 1 line out of the 13 lines of code it contains. The other 12 are just boilerplate for overloading. What a nightmare for maintainability and readability!
 
 
 
 In Bliss v1, all functions were contained a single file, so they could be defined in their most singular version (one element, a single key-value pair as separate arguments etc), and the aggregate signatures could be automatically generated by looping over all defined functions and wrapping them accordingly.
 
 
 
 However, in Bliss v2, each function is defined in its own module, as a default export. There is also a module pulling them all together and adding them on $, but people should be able to do things like:
 
 
 
 import style from &quot;https://v2.blissfuljs.com/src/dom/style.js&quot;;
 
 
 
 And style() would need to support its full functionality, not be some cut down version allowing only single elements and one property-value pair. What use would that be?
 
 
 
 This means that the overloading needs to happen in the module defining each function. It cannot happen via a loop in the index.js module. How can we do this and still keep our code maintainable, short, and easy to change? I explored several alternatives.
 
 
 
 (We are not going to discuss the implementation of overload() in each case below, but if you’re interested in the current one, it’s on Github. Do note that just like everything in Bliss v2, it’s subject to heavy change before release)
 
 
 
 Option 1: Inside each function
 
 
 
 export default function style(subject, ...args) {
 	return overload(subject, args, (element, property, value) &#x3D;&gt; {
 		element.style[property] &#x3D; value;
 	})
 }
 
 
 
 While this at first seems like the most natural way to abstract the inlined code we previously had, it’s the most verbose and hard to read. Furthermore, it adds extra code that needs to be executed every time the function is called and needs us to pass the current execution context through. It’s far better to go with a solution that takes the singular function as input, and gives you a modified function that just works. That’s what the next two options use.
 
 
 
 Option 2: Wrap with overload()
 
 
 
 export default overload(function style(element, property, value) {
 	element.style[property] &#x3D; value;
 });
 
 
 
 Option 3: Overload at export
 
 
 
 function style(element, property, value) {
 	element.style[property] &#x3D; value;
 }
 
 export default overload(style);
 
 
 
 Options 2 and 3 are very similar. I was originally inclined to go with 2 to avoid typing the function name twice, but I eventually concluded that it made the code harder to read, so I went with option 3: Declaring the function, then overloading it &amp; exporting it.
 
 
 
 I wasn’t super happy with any of these options. Something inside me protested the idea of having to include even a line of boilerplate in every single module, and almost every Bliss function depending on another module. However, in the large scheme of things, I think this boilerplate is as minimal as it gets, and certainly beats the alternatives.
 
 
 
 Have you had to perform a transform on a number of different modules in your code? How did you abstract it away? 
 
 
 
 1 You can see the rewrite progress in the v2 branch on Github, and even use v2.blissfuljs.com to import modules from and experiment. Note that at the time of writing, all of the progress is in the code, the docs and tests are still all about v1.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 287: A new year, a new start and nothing ground changing.</title>
         <link href="https://wdrl.info/archive/287"/>
       <updated>2021-01-22T07:00:00.000Z</updated>
       <content type="text">Hey,
 
 Welcome to another new year, 2021. WDRL is now seven and a half year old already and the first edition had about twenty email subscribers. What was in there? We had webfont loading behaviour, Speedcurve, a HTML validator, GrumpIcon which created PNG fallbacks for SVG icons via Grunt (remember?), and we moved away from jQuery and started using native JS functions instead. This throwback was quite interesting for me this week. Some topics are still relevant today while others have completely disappeared or solved.
 Coming 2021 we now get the ability to slowly replace old hacky code for aspect-ratio with Chrome supporting it now in the latest stable (88), Firefox as well behind flags and in Safari Tech Preview. But we still heavily need CanIUse as project to lookup browser compatibility — thanks so much for building it, Fyrd!
 Not every new year needs new habits, new spectacular projects. It’s enough to realign our minds, our goals, our perspectives. It’s a good time to reconsider active or stale projects, clean up old stuff, declutter or start one new thing.
 I’m personally dedicating a lot of time currently into my Market Gardening business as well as playing with my son in spare time. Besides that, I started organizing my web development business and declutter it. There’s no time to hold on to projects that aren’t in active development or I don’t like anymore but also don’t bring me joy or money.  
 I’m also actively looking for small website projects currently so if you know something or need someone building a small company/project website, email me.
 If you know about an environmental, social or humanity project that needs my help and could hire me (reduced rate possible) for a limited time, connect me.
 I wish you all the best, health and joy.
 News
 
 	Chrome 88 is out and brings CSS aspect-ratio to all users, it defaults target&#x3D;&quot;_blank&quot; to treat as rel&#x3D;&quot;no-opener&quot; always by default (which is now the web standard).
 
 Generic
 
 	The Sustainable Web Manifesto is a statement we can all sign to make the web less of a CO2 polluter and more sustainable, in all regards.
 
 UI/UX
 
 	Oliver Schöndorfer shares how to choose a typeface for apps and UIs. It’s a great read that shows the difference between normal, good and excellent app design. He also has a new YouTube channel for Better Type.
 
 Tooling
 
 	Integrating Stripe can be quite a complex task and switching between documentation and code editor can be annoying. So Stripe now has a VS Code extension that brings the Stripe environment into your code editor and makes this task so much more comfortable.
 
 JavaScript
 
 	Addy Osmani wrote another insightful guide how to import code on interaction to save traffic and improve performance on web apps. It’s definitely the gold standard of blog articles, and probably gold standard for web apps, if you implement this.
 	Mathias Schäfer with a new article on maintaining javascript applications in the long term, describing the approach for an app built in 2014. Describing why they still use Backbone.js instead of their agency standard React, how they moved from CoffeeScript to TypeScript, handle documentation and maintain a test suite, Mathias concludes why abstractions may bite you in future.
 
 CSS
 
 	Bramus Van Damme shares that we can nest Media Queries. Interesting that most of us have never heard about it.
 	Melanie Richards writes how we can integrate sticky elements into CSS Grid layouts without many hacks or side-effects.
 
 Go beyond…
 
 	Exxon, the oil company, knows its carbon future and keeps the data from view. They know that they’re one of the biggest polluters in the world, yet they’re not ready to change something. But given our business and economical system, can we judge them? At least not fully, simply stepping out of the business would make them bankrupt. So it remains an interesting, challenging issue of our generation to solve the economical and ecological problems of this industry so save our society.
 
 
 I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>2020 into 2021</title>
         <link href="https://www.taniarascia.com/2020-into-2021/"/>
       <updated>2021-01-01T00:00:00.000Z</updated>
       <content type="text">Well, 2020, it&#x27;s been a slice. I&#x27;ve been pretty AWOL lately on all things internet, and I can&#x27;t decide if I have a lot to say or if I&#x27;d rather say nothing at all, but I&#x27;ve always made an end of the year post since I started this blog - going into 2017, 2018, 2019, 2020, so I&#x27;ll keep up the tradition and write something today for 2021.
 It&#x27;s fun for me to look over those posts - I can see a zoomed out view of what I did and what I was focused on throughout each year. I wrote hundreds of articles, recorded the occasional song, made a lot of commits, built a few open-source projects, had a break up or three, made a bit of art, traveled all over Europe, and learned a ton.
 This year is different though, right? A lot of things happened that we&#x27;re all aware of, but one thing that happened was that life slowed down a bit. No more commuting to work for me, no more conferences, a lot more spare time. A lot more time on the internet for everyone.
 I&#x27;ve spent so long focusing on being productive, feeling like I have to be productive all the time. I made it a point to never make promises or obligations about my creative output, but I&#x27;ve always tried to put out a consistent stream of quality material - at least a good tutorial once a month for the past five years or so. I felt good and productive if I produced something for the web and learned something new and wrote about it.
 But that&#x27;s five years of working 8 hours every day on code during my day job, and often spending the rest of my evening working on articles for DigitalOcean, for this website, for other publications, making my own open-source projects, talking about code on Twitter, and feeling like I should accept or consider all the speaking engagements and podcast requests and anything else that comes my way, because if I start turning them all down, the momentum I&#x27;m creating will disappear. (Fortunately I&#x27;ve usually kept “HELL YEAH!” or “no.” by Derek Sivers in mind when responding to things.)
 It started getting to the point where I was dreading code, dreading speaking engagements, and the last thing I wanted to do was scroll through the Twitter or Reddit feed and read little arguments about this or that framework intermixed with political outrage, or maintain dozens of open-source GitHub repositories for tutorials that are slowly getting out of date. Not to mention all the emails and comments that would come in, which could be anything from a nice email from a thoughtful person who would become a new friend, to people emailing me their coding questions as if I&#x27;m Google, people attempting to shut down or critique every aspect of an article, and of course plenty of &quot;are you single?&quot; and &quot;you&#x27;re such a good coder for a girl&quot; type emails, or worse things I&#x27;d rather not mention.
 Of course, the vast majority is positive, but it&#x27;s always the negative stuff that lingers and bothers you throughout the day, and regardless of good or bad, it all requires mental energy. Even the good is odd to me, because often people will have an idea on me based what little I&#x27;ve put out into the world, and if you came to know me as the impatient, flawed and complex human being I am, it likely would not align and reality might possibly disappoint you. If anything, I&#x27;m more scared and hesitant of praise than critiques.
 So, I&#x27;ve been burnt out on all code and community after several years of my life revolving around it completely. I&#x27;m far from famous, but with nearly 10,000 followers on GitHub and 15,000 on Twitter, I can&#x27;t just say anything or put anything out there without lots of eyes on it, and it puts more pressure on me than if nobody was paying attention to anything I did.
 Interestingly, when I opt out of all of this and only pay attention to the real world, it completely disappears. None of my friends or family really have any idea of anything I&#x27;ve done online. They know I do some coding stuff, but that&#x27;s where it ends, and that&#x27;s reality.
 There are amazing people all around the world and the internet still gives us so many opportunities to connect and create and help others and do wonderful things, but there is so much I don&#x27;t like about the way things are right now: being constantly bombarded by ads and distractions, infinite scroll, algorithmic manipulation, addiction, corporate interests, dopamine hits from increasing follows and shares and influence, impatience and a lack of focus, negativity, fear and paranoia from always hearing about all the worst things that are happening in the world, conspiracies and a lack of critical thinking. I just don&#x27;t want to be part of this and affected by it as much as I can, I want to reject it all and go against the grain and live a slower, simpler life.
 One small thing I can do is encourage a few other people to also go against the grain and read a book instead of scrolling a feed, and focus on a few deeper relationships as opposed to collecting and quantifying strangers. I&#x27;m learning to be my own best friend. I deleted all my tweets and stopped scrolling through Twitter, I removed my email from any easy-to-find spot, and I make sure not to log into anything like Reddit. Without being tailored to my interests, you&#x27;ll find that most of these sites are pretty boring and it&#x27;s easy to just check something out for a few minutes and move on.
 I found the latest three articles on The Raptitude interesting and helpful, so I&#x27;ll share them with you.
 
 How to Handle the Beast - it was helpful for me to read this and know that other people struggle with a lack of productivity and drive, and that it&#x27;s okay and normal.
 News is the Last Thing We Need Right Now - real news, fake world.
 Own the Tools - why it might be a better idea to - for example - open a physical dictionary to find a word as opposed to looking it up online.
 
 Meanwhile, I&#x27;ve been making the best of this weird time. I&#x27;m really glad that I still enjoy my job quite a bit and the puzzle-solving aspect of coding that makes it so fun and challenging, but I&#x27;ve wanted to explore things outside of coding in my spare time that makes more use of my hands. I built a PC from scratch. I learned some basic woodworking skills to make my own desk. I learned how to knit. I painted some Bob Ross paintings. I climbed one of the tallest mountains in the U.S. and slept among the stars above the tree line. I took a solo road trip along the coast from L.A. to Seattle in an oversized Jeep Wrangler. I started a new job. I&#x27;ve just been livin&#x27;.
 Here&#x27;s me on top of Mount Langley.
 
       
     
   
   
     
 Here&#x27;s my first finished scarf, that I just made last week.
 
       
     
   
   
     
 Here&#x27;s the desk and PC I built.
 
       
     
   
   
     
 Aside from my thoughts, here are some of the stats I usually keep track of.
 I finished TakeNote
 TakeNote is my biggest project yet, and I wrote all about it here. It&#x27;s a web-based note-taking app for developers that looks like an IDE and syncs to GitHub. It uses TypeScript, Node, Express, React, Redux, Codemirror, and several other awesome open-source projects. I ultimately decided not to ship it, but I did finish it, and I&#x27;m proud of it.
 I wrote 21 articles
 I did two project write-ups - TakeNote and Chip8.js, several articles on JavaScript for DigitalOcean&#x27;s fundamentals series, and covered few big concepts like Redux, Docker, and webpack. Honestly, I got a lot more done than I remembered, which is one of those reasons I like to make these posts. It&#x27;s easy to forget all that you&#x27;ve done over an entire year.
 
 Docker Tutorial: Create a CI/CD Pipeline
 Understanding Map and Set in JavaScript
 macOS Catalina: Setting up a Mac for Development
 Understanding Generators in JavaScript
 Redux Tutorial: An Overview and Walkthrough
 Understanding Default Parameters in JavaScript
 Writing an Emulator in JavaScript (Chip-8)
 Understanding Destructuring, Rest Parameters, and Spread Syntax
 Another Website Redesign - and Making the Internet Better
 Understanding Template Literals in JavaScript
 Using Git Submodules for Private Content
 Wildnerness Backpacking Gear List
 Understanding Arrow Functions in JavaScript
 Adding Comments to My Blog (via Utterances)
 REST API: Sorting, Filtering, and Pagination
 Understanding the Event Loop, Callbacks, Promises, and Async/Await in JavaScript
 webpack Tutorial: How to Set Up webpack 5 From Scratch
 Understanding Modules, Import and Export in JavaScript
 Everyday Systems That Help Me
 Building My First PC
 Building TakeNote, a Notes App for Developers With GitHub Sync
 
 Newsletter
 The newsletter is up to 11,283 subscribers. I don&#x27;t post very often, but it is really the only way I&#x27;m communicating with the world since I&#x27;m not active on Twitter or Reddit. I&#x27;m glad you all are interested in what I&#x27;m creating and I still hope to create interesting things in 2021.
 Learning
 I didn&#x27;t learn most of the things I wanted to this year. Data structures and algorithms have been on the back burner for a few years now, and I never seem to be able to focus on it. I think the best article I wrote this year is Understanding the Event Loop, Callbacks, Promises, and Async/Await in JavaScript, which is a deep dive on how JavaScript and the event loop work under the hood. This article was something I wanted to write and understand for a long time, so I&#x27;m glad I finally did. I still want to learn computer science fundamentals, so next time I decide to sit down and try to learn something coding related that&#x27;s probably what I&#x27;ll do.
 So, thank you for reading and I hope this answers your questions if you were wondering why I haven&#x27;t been around or active much. I appreciate all of your support and I&#x27;m looking forward to seeing what 2021 brings.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 286: A few words, a few articles towards the end of 2020.</title>
         <link href="https://wdrl.info/archive/286"/>
       <updated>2020-12-24T07:00:00.000Z</updated>
       <content type="text">Hey,
 
 It’s nearly the end of the year and the world is still different than we could imagine before 2020. We still don’t know how to deal with the new kinds of diseases short and long-term. We still have no real clue and conclusion how to save our planet (means us humans, the planet will survive). Many of us now are in a state of uncertainty, of frustration, of stress. Working remotely isn’t easy for all of us, the split between childcare and work is hard,  and our social contacts have been reduced to the minimum. That’s definitely not a combination that makes our lives easy, calm and ourselves happy. That’s why I found this writing so useful: News is the last thing we need right now but it’s addictive like a drug. Reading this made me finally block all the major news sites. I’m already two weeks in, and it feels quite good. And fear not, it’s not that I don’t know about the important news, there are still enough ways to get them from other people. It’s simply that you don’t fetch them actively and don’t fall into the trap of getting caught by another handful of articles. 
 I hope you all can enjoy the end of the year, make it a calm and nice time despite of all the things happening all over the world, despite all the articles here, all the news around us and all the other people sharing their fears, their concerns and annoyances with us. Let’s try to reframe it, show empathy, listen to people and try to understand them. Read you in 2021 again, thanks for everything to you, your support, your money gifts, your article suggestions, your conversations via email. I’m really grateful for this audience.
 Generic
 
 	Eric Bailey on how we can create more eco-friendly websites. Some pretty handy tricks in there, some very simple ones, some that can’t be applied to every project.
 	AVIF, the file format based on the AV1 video codec, is the latest addition to the next-gen image formats. Now this article shares a couple of things we should know before we blindly use this new format for everything. WebP, for example, performs better for thumbnails, and if the picture has a high entropy, JPEG can still deliver better results at a smaller size. Funnily, JPEG2000 which never kicked off really still challenges most of the other formats today. And if you’re now unsure what to do, using mozJPEG is still one of the best compromises you can use today.
 
 UI/UX
 
 	Adam Silver shares why we should be more careful designing tab styles for use cases that aren’t or shouldn’t be tabs.
 	Scale is a nice set of illustrations for free, even for commercial projects. The nice thing? You can set the main color easily in the SVG and adapt them to your brand color.
 
 Tooling
 
 	Blurhash is a nice library that creates blurs for image placeholders. Now here’s alos a pure PHP implementation of it.
 	Plaiceholder is a simple online tool where you can upload an image and get a CSS, SVG, Base64, Blurhash, or React element that acts as blur placeholder until the real image is loaded. A nice tool for when you just need this a once or a couple of times.
 
 Web Performance
 
 	Kilian Valkhof shares concepts and examples of how we can integrate the prefers-reduced-data setting into our websites. With it, we can disable web font loading, not load background images, or using picture and its media query attribute to load smaller images than normal. We can even prevent preloading or autoplay of videos, disable infinite scrolling preloads (we could improve the example using IntersectionObserver). It’s super cool to see these practical implementations which don’t even require much effort but have a huge benefit to users with a limited data budget (international roaming, etc).
 	James Ross shares how we can use Cloudflare Workers and CSS to deliver new image formats like AVIF. It’s pretty cool that James has some real use cases here and even shows the real life impact data.
 	Noam Rosenthal on how they made their Wikipedia page previews fast. A nice insight into JavaScript’s heavy functions and how to easily or not replace them.
 
 HTML &amp; SVG
 
 	Robin Rendle explores what we can do with the &lt;details&gt;/&lt;summary&gt; elements in HTML and CSS. Ever thought that we could de-animate (pause) a gif? We can do inline-block footnotes or build tooltips with it!
 
 JavaScript
 
 	Jeremy Keith created a simple gist to save form input via localStorage to avoid content being lost when something happens to the client browser or the server resets the connection due to security.
 
 CSS
 
 	Ever tried to build a visual calendar with CSS? You may end up with tables and styling a lot of things. What’s much simpler is now that we have CSS Grid using this cool technology to build a basic calendar in three lines of CSS.
 	If you want to have a clickable card interface, things can easily get tricky due to the fact that we can’t nest links in HTML. Christian Heilmann shares a way how to do it the right way but always keep in mind that UX and accessibility may still be impacted by this design pattern which is why nested links aren’t a native thing in HTML.
 	Bramus Van Damme writes how to use the new aspect-ratio CSS property in browsers to replace these ugly padding-top hacks we used to apply to iframe embeds. Unfortunately, this is not baked into stable browsers yet but we can already use the setting and later strip the fallbacks from the codebase.
 
 Go beyond…
 
 	It makes me really happy to see such an amazing project: Field Notes: Snowmaker. A story how to make 99999 unique snow flakes. My dream? Brendan (or someone else) is allowed and paid for drawing all these snow flakes by hand. Nature is amazing, craft is amazing and we should embrace it. Thanks Field Notes for doing this project.
 
 
 Thank you for reading this. I hope you’re doing fine and have a way to stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Writable getters</title>
         <link href="https://lea.verou.me/2020/12/writable-getters/"/>
       <updated>2020-12-23T13:08:17.000Z</updated>
       <content type="text">Setters removing themselves are reminiscent of Ouroboros, the serpent eating its own tail, an ancient symbol. Media credit
 
 
 
 A pattern that has come up a few times in my code is the following: an object has a property which defaults to an expression based on its other properties unless it’s explicitly set, in which case it functions like a normal property. Essentially, the expression functions as a default value. 
 
 
 
 
 
 
 
 Some examples of use cases:
 
 
 
 An object where a default id is generated from its name or title, but can also have custom ids.An object with information about a human, where name can be either specified explicitly or generated from firstName and lastName if not specified.An object with parameters for drawing an ellipse, where ry defaults to rx if not explicitly set.An object literal with date information, and a readable property which formats the date, but can be overwritten with a custom human-readable format.An object representing parts of a Github URL (e.g. username, repo, branch) with an apiCall property which can be either customized or generated from the parts (this is actually the example which prompted this blog post)
 
 
 
 Ok, so now that I convinced you about the utility of this pattern, how do we implement it in JS? Our first attempt may look something like this:
 
 
 
 let lea &#x3D; {
 	name: &quot;Lea Verou&quot;,
 	get id() {
 		return this.name.toLowerCase().replace(/\W+/g, &quot;-&quot;);
 	}
 }
 
 
 
 Note: We are going to use object literals in this post for simplicity, but the same logic applies to variations using Object.create(), or a class Person of which lea is an instance. 
 
 
 
 Our first attempt doesn’t quite work as you might expect:
 
 
 
 lea.id; // &quot;lea-verou&quot;
 lea.id &#x3D; &quot;lv&quot;;
 lea.id; // Still &quot;lea-verou&quot;!
 
 
 
 Why does this happen? The reason is that the presence of the getter turns the property into an accessor, and thus, it cannot also hold data. If it doesn’t have a setter, then simply nothing happens when it is set.
 
 
 
 However, we can have a setter that, when invoked, deletes the accessor and replaces it with a data property:
 
 
 
 let lea &#x3D; {
 	name: &quot;Lea Verou&quot;,
 	get id() {
 		return this.name.toLowerCase().replace(/\W+/g, &quot;-&quot;);
 	},
 	set id(v) {
 		delete this.id;
 		return this.id &#x3D; v;
 	}
 }
 
 
 
 Abstracting the pattern into a helper
 
 
 
 If we find ourselves needing this pattern in more than one places in our codebase, we could abstract it into a helper:
 
 
 
 function writableGetter(o, property, getter, options &#x3D; {}) {
 	Object.defineProperty(o, property, {
 		get: getter,
 		set (v) {
 			delete this[property];
 			return this[property] &#x3D; v;
 		},
 		enumerable: true,
 		configurable: true,
 		...options
 	});
 }
 
 
 
 Note that we used Object.defineProperty() here instead of the succinct get/set syntax. Not only is the former more convenient for augmenting pre-existing objects, but also it allows us to customize enumerability, while the latter just defaults to enumerable: true.
 
 
 
 We’d use the helper like this:
 
 
 
 let lea &#x3D; {name: &quot;Lea Verou&quot;};
 writableGetter(lea, &quot;id&quot;, function() {
 	return this.name.toLowerCase().replace(/\W+/g, &quot;-&quot;);
 }, {enumerable: false});
 
 
 
 Overwriting the getter with a different getter
 
 
 
 This works when we want to overwrite with a static value, but what if we want to overwrite with a different getter? For example, consider the date use case: what if we want to maintain a single source of truth for the date components and only overwrite the format, as a function, so that when the date components change, the formatted date updates accordingly?
 
 
 
 If we are confident that setting the property to an actual function value wouldn’t make sense, we could handle that case specially, and create a new getter instead of a data property:
 
 
 
 function writableGetter(o, property, getter, options &#x3D; {}) {
 	return Object.defineProperty(o, property, {
 		get () {
 			return getter.call(this);
 		},
 		set (v) {
 			if (typeof v &#x3D;&#x3D;&#x3D; &quot;function&quot;) {
 				getter &#x3D; v;
 			}
 			else {
 				delete this[property];
 				return this[property] &#x3D; v;
 			}
 		},
 		enumerable: true,
 		configurable: true,
 		...options
 	});
 }
 
 
 
 Do note that if we set the property to a static value, and try to set it to a function after that, it will just be a data property that creates a function, since we’ve deleted the accessor that handled functions specially. If that is a significant concern, we can maintain the accessor and just update the getter:
 
 
 
 function writableGetter(o, property, getter, options &#x3D; {}) {
 	return Object.defineProperty(o, property, {
 		get () {
 			return getter.call(this);
 		},
 		set (v) {
 			if (typeof v &#x3D;&#x3D;&#x3D; &quot;function&quot;) {
 				getter &#x3D; v;
 			}
 			else {
 				getter &#x3D; () &#x3D;&gt; v;
 			}
 		},
 		enumerable: true,
 		configurable: true,
 		...options
 	});
 }
 
 
 
 Improving the DX of our helper
 
 
 
 While this was the most straightforward way to define a helper, it doesn’t feel very natural to use. Our object definition is now scattered in multiple places, and readability is poor. This is often the case when we start implementing before designing a UI. In this case, writing the helper is the implementation, and its calling code is effectively the UI. 
 
 
 
 It’s always a good practice to start designing functions by writing a call to that function, as if a tireless elf working for us had already written the implementation of our dreams. 
 
 
 
 So how would we prefer to write our object? I’d actually prefer to use the more readable get() syntax, and have everything in one place, then somehow convert that getter to a writable getter. Something like this:
 
 
 
 let lea &#x3D; {
 	name: &quot;Lea Verou&quot;,
 	get id() {
 		return this.name.toLowerCase().replace(/\W+/g, &quot;-&quot;);
 	}
 }
 makeGetterWritable(lea, &quot;id&quot;, {enumerable: true});
 
 
 
 Can we implement something like this? Of course. This is JS, we can do anything!
 
 
 
 The main idea is that we read back the descriptor our get syntax created, fiddle with it, then stuff it back in as a new property:
 
 
 
 function makeGetterWritable(o, property, options) {
 	let d &#x3D; Object.getOwnPropertyDescriptor(o, property);
 	let getter &#x3D; d.get;
 
 	d.get &#x3D; function() {
 		return getter.call(this);
 	};
 	
 	d.set &#x3D; function(v) {
 		if (typeof v &#x3D;&#x3D;&#x3D; &quot;function&quot;) {
 			getter &#x3D; v;
 		}
 		else {
 			delete this[property];
 			return this[property] &#x3D; v;
 		}
 	};
 
 	// Apply any overrides, e.g. enumerable
 	Object.assign(d, options);
 
 	// Redefine the property with the new descriptor
 	Object.defineProperty(o, property, d)
 }
 
 
 
 Other mixed data-accessor properties
 
 
 
 While JS is very firm in its distinction of accessor properties and data properties, the reality is that we often need to combine the two in different ways, and conceptually it’s more of a data-accessor spectrum than two distinct categories. Here are a few more examples where the boundary between data property and accessor property is somewhat …murky:
 
 
 
 “Live” data properties: properties which execute code to produce side effects when they are get or set, but still hold data like a regular data property. This can be faked by having a helper that creates a hidden data property. This idea is the core of Bliss.live(). Lazy evaluation: Properties which are evaluated when they are first read (via a getter), then replace themselves with a regular data property. If they are set before they are read, they function exactly like a writable getter. This idea is the core of Bliss.lazy(). MDN mentions this pattern too.
 
 
 
 Note: Please don’t actually implement id/slug generation with name.toLowerCase().replace(/\W+/g, &quot;-&quot;). That’s very simplistic, to keep examples short. It privileges English/ASCII over other languages and writing systems, and thus, should be avoided. </content>
     </entry>
     <entry>
       <title>Position Statement for the 2020 W3C TAG Election</title>
         <link href="https://lea.verou.me/2020/11/tag/"/>
       <updated>2020-11-30T16:52:00.000Z</updated>
       <content type="text">Update: I got elected!! Thank you so much to every W3C member organization who voted for me.  Now on to making the Web better, alongside fellow TAG members!
 
 
 
 Context: I’m running for one of the four open seats in this year’s W3C TAG election. The W3C Technical Architecture Group (TAG) is the Working Group that ensures that Web Platform technologies are usable and follow consistent design principles, whether they are created inside or outside W3C. It advocates for the needs of everyone who uses the Web and everyone who works on the Web. If you work for a company that is a W3C Member, please consider encouraging your AC rep to vote for me! My candidate statement follows.
 
 
 
 
 
 
 
 Hi, I’m Lea Verou. Equally at home in Web development, the standards process, and programming language design, I bring a rarely-found cross-disciplinary understanding of the full stack of front-end development. 
 
 
 
 I have a thorough and fundamental understanding of all the core technologies of the Web Platform: HTML, CSS, JS, DOM, and SVG. I bring the experience and perspective of having worked as a web designer &amp; developer in the trenches — not in large corporate systems, but on smaller, independent projects for clients, the type of projects that form the majority of the Web. I have started many open source projects, used on millions of websites, large and small. Some of my work has been incorporated in browser dev tools, and some has helped push CSS implementations forwards.
 
 
 
 However, unlike most web developers, I am experienced in working within W3C, both as a longtime member of the CSS Working Group, as well as a W3C Staff alumnus. This experience has given me a fuller grasp of Web technology development: not just the authoring side, but also the needs and constraints of implementation teams, the kinds of problems that tend to show up in our work, and the design principles we apply. I understand in practice how the standards process at W3C addresses the problems and weighs up the necessary compromises — from high-level design changes to minute details — to create successful standards for the Web.
 
 
 
 I have spent over six years doing PhD research at MIT on the intersection of programming language design and human-computer interaction. My research has been published in top-tier peer-reviewed academic venues.  My strong usability background gives me the ability to identify API design pitfalls early on in the design process.
 
 
 
 In addition, I have been teaching web technologies for over a decade, both to professional web developers, through my numerous talks, workshops, and bestselling book, and as an instructor and course co-creator for MIT. This experience helps me to easily identify aspects of API design that can make a technology difficult to learn and conceptualize.
 
 
 
 If elected, I will work with the rest of the TAG to:
 
 
 
 Ensure that web technologies are not only powerful, but also learnable and approachable, with a smooth ease-of-use to complexity curve.Ensure that where possible, commonly needed functionality is available through approachable declarative HTML or CSS syntax and not solely through JS APIs.Work towards making the Web platform more extensible, to allow experienced developers to encapsulate complexity and make it available to novice authors, empowering the latter to create compelling content. Steps have been made in this direction with Web Components and the Houdini specifications, but there are still many gaps that need to be addressed. Record design principles that are often implicit knowledge in standards groups, passed on but never recorded. Explicit design principles help us keep technologies internally consistent, but also assist library developers who want to design APIs that are consistent with the Web Platform and feel like a natural extension of it. A great start has been made with the initial drafts of the Design Principles document, but there is still a lot to be done.Guide those seeking TAG review, some of whom may be new to the standards process, to improve their specifications. 
 
 
 
 Having worn all these hats, I can understand and empathize with the needs of designers and developers, authors and implementers, practitioners and academics, putting me in a unique position to help ensure the Web Platform remains consistent, usable, and inclusive.
 
 
 
 I would like to thank Open JS Foundation and Bocoup for graciously funding my TAG-related travel, in the event that I am elected.
 
 
 
 Selected endorsements
 
 
 
 Tantek Çelik, Mozilla’s AC representative, longtime CSS WG member, and creator of many popular technologies:
 
 
 
 I have had the privilege of working with Lea in the CSS Working Group, and in the broader web development community for many years. Lea is an expert in the practical real-world-web technologies of the W3C, how they fit together, has put them into practice, has helped contribute to their evolution, directly in specs and in working groups. She’s also a passionate user &amp; developer advocate, both of which I think are excellent for the TAG.Source: https://lists.w3.org/Archives/Member/w3c-ac-forum/2021JanMar/0015.html
 
 
 
 Florian Rivoal, CSS WG Invited Expert and editor of several specifications, elected W3C AB member, ex-Opera:
 
 
 
 
 @LeaVerou will be a great addition to the TAG. She&#x27;s fantastic at explaining things, which will be very useful given the TAG&#x27;s role as a coach for everyone else, and her experience on front-end technologies is both wide and deep. Add strong expertise about usability… Go Lea! https://t.co/WIc56sCrr7— Florian (@frivoal) December 10, 2020
 
 
 
 
 Elika Etemad aka fantasai, prolific editor of dozens of W3C specifications, CSS WG member for over 16 years, and elected W3C AB member:
 
 
 
 One TPAC long ago, several members of the TAG on a recruiting spree went around asking people to run for the TAG. I personally turned them down for multiple reasons (including that I’m only a very poor substitute for David Baron), but it occurred to me recently that there was a candidate that they doneed: Lea Verou.Lea is one of those elite developers whose technical expertise ranges across the entire Web platform. She doesn’t just use HTML, CSS, JS, and SVG, she pushes the boundaries of what they’re capable of. Meanwhile her authoring experience spans JS libraries to small site design to CSS+HTML print publications, giving her a personal appreciation of a wide variety of use cases.Unlike most other developers in her class, however, Lea also brings her experience working within W3C as a longtime member of the CSS Working Group.I’ve seen firsthand that she is capable of participating at the deep and excruciatingly detailed level that we operate here, and that her attention is not just on the feature at hand but also the system and its usability and coherence as a whole. She knows how the standards process works, how use cases and implementation constraints drive our design decisions, and how participation in the arcane discussions at W3C can make a real difference in the future usability of the Web.I’m recommending her for the TAG because she’s able to bring a perspective that is needed and frequently missing from our technical discussions which so often revolve around implementers, and because elevating her to TAG would give her both the opportunity and the empowerment to bring that perspective to more of our Web technology development here at W3C and beyond.Source: https://lists.w3.org/Archives/Member/w3c-ac-forum/2020OctDec/0055.html
 
 
 
 Bruce Lawson, Opera alumni, world renowned accessibility expert, speaker, author:
 
 
 
 
 I&#x27;d vote @LeaVerou for @w3ctag on a ticket that promises &quot;powerful, but also learnable and approachable&quot; web tech and &quot;where possible, functionality is available through approachable declarative HTML or CSS syntax and not solely through JS APIs&quot; https://t.co/MFN2BkgYCD— Bruce Lawson (@brucel) December 8, 2020
 
 
 
 
 Brian Kardell, AC representative for both Open JS Foundation and Igalia:
 
 
 
 The OpenJS Foundation is very pleased to nominate and offer our support for Lea Verou to the W3C TAG. We believe that she brings a fresh perspective, diverse background and several kinds of insight that would be exceptionally useful in the TAG’s work. Source: https://www.w3.org/2020/12/07-tag-nominations#lv
 
 
 
 Lea Verou is another easy choice for me. Lea brings a really diverse background, set of perspectives and skills to the table. She’s worked for the W3C, she’s a great communicator to developers (this is definitely a great skill in TAG whose outreach is important), she’s worked with small teams, produced a number of popular libraries and helped drive some interesting standards. The OpenJS Foundation was pleased to nominate her, but Frontiers and several others were also supportive. Lea also deserves “high marks”.Source: https://bkardell.com/blog/TAG-2021.html</content>
     </entry>
     <entry>
       <title>Building TakeNote, a Notes App for Developers With GitHub Sync</title>
         <link href="https://www.taniarascia.com/building-takenote/"/>
       <updated>2020-11-30T00:00:00.000Z</updated>
       <content type="text">Not too long ago, I realized I didn&#x27;t have a really good system for organizing my thoughts, and I wasn&#x27;t happy with any of the note-taking apps I tried. Evernote was too bloated for me, and required a paid account to sync between multiple devices. SimpleNote was closer to what I wanted, but I found it somewhat ugly and clunky. Keep was too simple, Bear didn&#x27;t have a web-based version...and so on.
 Eventually I thought - why not just make exactly what I want? So I started building an app with the technologies I knew.
 
       
     
   
   
     
 That app became TakeNote , which I&#x27;m proud to say has been completed, thanks to over 50 open-source contributors who helped along the way. The source is up on GitHub for anyone who wants to see how it was done.
 What I Wanted It To Do
 A few of my most important wants for the app:
 
 Plain text notes, no WYSIWYG or rich text editor
 Accessible from a web browser
 Authenticates and syncs with GitHub
 Does not require a database
 Has the look and feel of an IDE like Visual Studio Code, with syntax highlighting, markdown preview, and keyboard shortcuts
 Links to internal notes like a wiki
 Drag and drop
 Easy to back up and import
 
 I&#x27;m really happy I saw it through to the end. It took about a full year with some breaks here and there (I started in September, 2019). I learned a lot along the way, much of which became knowledge that I directly used in my job.
 Why I Don&#x27;t Want to Ship It
 TakeNote has two versions - the static, client-side only demo version that only saves to local storage, and a self-hosted complete version that runs on an Express server and authenticates and syncs with GitHub. Originally, I had the full, authenticated version hosted and despite not advertising this app at all, over 1,300 people authenticated with it, but now I have the demo version hosted on Netlify at takenote.dev.
 Although I spent a lot of time on the project and I&#x27;m proud of it, I ultimately decided not to ship it. I spent a long time agonizing about what to do with the project, because I knew I didn&#x27;t want to maintain it or have real users. There were a few reasons for that:
 
 I don&#x27;t want to maintain a server. If I had real users and the app went down, I didn&#x27;t feel comfortable having that knowledge in the back of mind that if there was a DDoS or DigitalOcean was down or for any other reason the server went down, the app would be down for users.
 I don&#x27;t want to be responsible for anyone&#x27;s private data. Although the data was saved in GitHub and not a database of mine, I&#x27;m just really not interested in people&#x27;s private data in any capacity, and a note-taking app is inherently going to contain personal data.
 GitHub OAuth requires full access to all private repositories to get access to any private repository. There is no way around it. The &quot;GitHub App&quot; flow did not pertain to my use case, and all private repos is the only relevant scope for my app. I didn&#x27;t feel comfortable having these permissions for any reason, and nor do many other people, which is understandable.
 If I were to actually host this, it would have users and likely go over the rate limiting allowed very quickly, rendering the app pretty useless.
 After getting far along in the project, I realized it would be impossible to make a good mobile experience for such an involved app, especially since it&#x27;s impossible to disable zoom on iOS devices. I originally planned to just make a responsive web app for mobile, but after realizing I would need to make iOS/Android apps for mobile to work, I realized I wasn&#x27;t willing to double the effort that I had already put in to complete this last step.
 
 What I&#x27;ve ended up with is an excellent desktop web app, that looks and feels exactly like I&#x27;d want a note-taking app to feel. The hosted version is perfectly fine for any private, ephemeral notes anyone wants to take that definitely does not connect to any server or database, and imports/exports to JSON.
 Personally, I decided to settle on Bear for a notes app for myself, as I found that it had so many of the features I implemented in TakeNote, and I appreciated it. My only issue with it is that it&#x27;s not available from the web or for Windows (as I now have a Windows PC for gaming/other activities as well and would like to access notes from there).
 What I Learned While Writing It
 I wrote several articles that were the result of things I learned while writing this app.
 Authentication (OAuth)
 I wrote Client-side Authentication the Right Way (Cookies vs. Local Storage) , which details how to make a secure authentication set up using HTTP Only, Secure, Same-Site cookies with an Express server and a front end. I also learned the OAuth flow, and how easy it is to set up instead of using some third-party system like Auth0, which deceptively sounds similar to OAuth and can easily trick people into thinking they need to use it.
 CI/CD (Docker and Travis)
 I wanted to automate deployments and testing for the app, but I had never done it before from scratch. It&#x27;s easy enough with a static site and something like Netlify or GitHub pages, but it&#x27;s a whole different beast for a site with a server. I wrote Create a CI/CD Pipeline with Docker, which entailed all that I learned while creating an automation pipeline that would test all my code in PRs, and deploy it to a server behind a load balancer every time a merge was made into the master branch.
 State management (Redux)
 Before I started this project, I mostly avoided Redux, preferring to use Context in React for projects at work. As a result of setting up Redux Toolkit and Redux sagas for this project, I wrote An Overview and Walkthrough of Redux which builds demo applications of projects using vanilla Redux and Redux Toolkit. After using Redux, I realized I preferred it greatly to using Context. It does add a lot of boilerplate upfront, but the limitations of Context caused issues in previous projects as they grew.
 Bundling and configuration (webpack)
 I wrote How to Set Up webpack 5 From Scratch based on much of what of what I learned setting up a custom webpack config for the app that would support both an Express-serving-a-frontend app and a static frontend app.
 So the moral of the story here is it&#x27;s always a good idea to reinvent the wheel and build a project because you&#x27;ll learn something along the way. It&#x27;s never a waste of time.
 How I made it
 Here are the decisions and various tech I used to create the app.
 Language (TypeScript)
 I chose to write TakeNote in TypeScript. I had a little bit of experience here and there with TypeScript, but I wanted to make a complete, full-stack app to see if it lives up to the hype. It did get a bit annoying with various React and web types - or using some third-party dependencies, but I did feel confident that a lot of little bugs were prevented by using the strictest settings.
 Front end (React + Redux)
 I used React and Redux for the front end. It was either that, or Vue and Vuex, and although I&#x27;ve used Vue in the past for work, I just feel faster and more productive with React. I used simple SCSS for the styles instead of a CSS-in-JS solution, just because it&#x27;s easier for me and simpler in my opinion.
 Back end (Node + Express)
 I used Node and Express for the full version of the app. I only needed a few endpoints, for authenticating and syncing. I decided against making a real update on every change, and instead only sync all the data at once on an interval. The server is necessary to obtain an authentication token and securely store it to allow a user to remain logged in, as GitHub does not have a PKCE setup that would allow OAuth with a front end only app.
 Data and authentication (GitHub)
 I used GitHub to authenticate, instead of creating a database with users, so all of that security could be handled elsewhere. I also used the GitHub API to make commits on every interval with the changes made to the notes.
 Primary component
 The text editor is built with CodeMirror, which is also used by Chrome and Firefox DevTools, Codepen, GitHub, and basically any other in-browser fiddle environment.
 Behind the scenes, it uses webpack for bundling, Jest and Cypress for testing, ESLint, and a few other open-source helper libraries.
 GitHub Sync
 The part I had the most trouble with in the application was syncing to GitHub. I had the app completed up to that point for a long time before I finally finished it. My original intention was that the app would save all the files exactly like you would organize them in a folder on your computer - all categories would be folders, and all notes would be a .md file within the folder.
 I imagined that every time you sync the data, it would be the equivalent of doing git add . &amp;&amp; git commit &amp;&amp; git push. However, it&#x27;s not so simple to do that with the GitHub API. I spent a lot of time figuring out how to create and edit multiple files at once, and I realized there was really no way to do it without making an API call for every single file. I know it seems like there should be a multi-file create or update API call, but there&#x27;s not.
 What I finally ended up doing is what you see in the sync.ts file, which does the following:
 
 Gets a reference to the current head
 Creates a blob for each file (I decided on just creating one file for notes.json and one file for categories.json, instead of having a file for each note)
 Creates a tree path
 Creates a tree
 Creates a commit
 Updates a reference
 
 With these steps, on every sync a new commit is created for any updates that are made since the last sync.
 Open Source
 
 Nobody should start to undertake a large project. You start with a small trivial project, and you should never expect it to get large. If you do, you&#x27;ll just over design and generally think it is more important than it likely is at that stage. Or worse, you might be scared away by the sheer size of the work you envision.
 So start small, and think about the details. Don&#x27;t think about some big picture and fancy design. If it doesn&#x27;t solve some fairly immediate need, it&#x27;s almost certainly over-designed. And don&#x27;t expect people to jump in and help you. That&#x27;s not how these things work. You need to get something half-way useful first, and then others will say &quot;hey, that almost works for me&quot;, and they&#x27;ll get involved in the project.
 — Linus Torvalds
 
 I just started building this for a few months and once it got to a state where it was recognizable a note-taking app, people started making issues and PRs. It even made it to the trending list on GitHub one day, and I also think there were a lot of Hacktoberfest contributions (most positive, a little spam I had to shut down immediately).
 Ultimately, there were about 250 pull requests and 200 issues created. I divided the issues - everything got a label of a type and a status. Types were: features, bugs, refactors, testing, questions, documentation, and accessibility. Statuses were: blocked, duplicate, won&#x27;t do, invalid, can&#x27;t replicate, in progress, completed, and postponed. Labeling everything kept it all organized and easy to keep track of for me
 I tagged and released the big changes, and I connected the app with SonarQube to track any bugs or problems (very few due to TypeScript) and Cypress/Jest with Travis to run tests before merging PRs. I learned a lot about managing and maintaining a coding project while working on this.
 I took care of the main behavior notes, the authentication, the GitHub sync, and many contributors fixed bugs, designed the logo, set up a testing framework, and suggested and implemented new features.
 Conclusion
 Overall, working on TakeNote was a really positive experience for me. It&#x27;s easy to get excited about a project and never finish it, or get stuck at a road block, but I saw it through to the end making it the fourth big project of mine: Primitive the CSS framework, Laconia the MVC PHP app, a Chip-8 emulator, and now TakeNote.
 Ultimately, I decided not to release the app as a real service, but leave it out there as a fully functioning open-source project for people to see and learn from, and use and extend if they want. For myself, I decided to go with Bear for my own personal note-taking, as I realized I did not want to maintain something and spend more time on the maintenance of the app than the actual intended purpose of it.
 I hope you check out the demo and play around with it! I encourage you to build a project if you have an idea, because it&#x27;s fun and you&#x27;ll learn a lot along the way.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 285: Making CSS more unterstandable, crowdsec, bfcache, web theming and finding details in climate change</title>
         <link href="https://wdrl.info/archive/285"/>
       <updated>2020-11-24T07:00:00.000Z</updated>
       <content type="text">Hey,
 
 Now there we go with a new writing. For me, this year has been quite different so far. And no, this is mostly not about some virus spreading across the world but about changing a lot in my work and personal life. My son keeps teaching me new skills, patience, happiness. My gardening project went well the first year and we’re scaling it next year so we can grow vegetables for more than 70 people. Not to stop here, we created a network for workshops and events where local people share their knowledge to improve farming, gardening and live a more resilient and sustainable life as humans.
 On the other hand, being away from client work for half a year was incredibly useful. I now have a different view on some projects again, I refocused on what’s important in projects and got away from just coding and first ask about the goals, about how to achieve them before starting to dive into code. It was one part that I was missing in my job lately but didn’t realize. By talking to other people out there from a huge variety of professional backgrounds instead of constantly living in the bubble of web developers, I gained a different view of looking at and using websites, of how technology is used and how important (or sometimes unimportant) technology is for people.
 On a last note, I’m currently available for a small frontend/fullstack project so if you know something or need someone like me, talk to me. Bonus if it’s for a sustainable company, project or has a positive environmental impact.  
 In full honesty, Gardening is lot of fun, often really hard work, but also not super well paid. And I think I’m not ready to drop web development entirely, it can be so much fun as well!
 News
 
 	Oh yes, two new Firefox versions are here, v82 and v83 and they bring us DevTool inspection of server-send events, the Media Session API, CSS ::file-selector-button (yay!), conic-gradient, WebRender, Pinch to zoom on desktop, and more.
 	Safari 14 is here and brings Web Extensions, Webpage Translation, Performance improvements, interoperability and better standards and compatibility by improving Web Platform Test passes. Safari now has a privacy report that shows trackers and third party data issues on a webpage to users quite prominently, it brings WebP support, supports CSS image-orientation, :is(), :where() (resetting specificity for the selector to 0, wohoo). JavaScript gets BigInt support and optional chaining via ?..
 
 UI/UX
 
 	Betsy Mikel shares how to write microcopy that improves the user experience even with a strategy how to analyze microcopy and how to collect feedback.
 
 Tooling
 
 	Upptime is an open source uptime and status page system, powered entirely by GitHub Actions and Issues. Cool!
 
 Security
 
 	Crowdsec is an open-source, lightweight service (written in Go) to detect and respond to bad behaviours and uses a global community-wide IP reputation database. Heard of fail2ban? This is a more modern approach of the service.
 
 Web Performance
 
 	Postgres doesn’t stop to surprise me (positively). James Coleman shares some tips on how to save space nearly for free by using a custom Ruby gem.
 
 HTML &amp; SVG
 
 	Tomek Sułkowski gave us a nice tip in a tweet: Using autocomplete&#x3D;&quot;one-time-code&quot; in a form field will auto-fill an 2-FA code from messages or other apps in some browsers like Safari. This is such a nice UX and so simple to build in.
 
 Accessibility
 
 	Here’s Atkinson, a new free and hyperlegible font published by the Braille institute.
 	Nataly Birch on email design accessibility best practices with a good few handful of tips what we can do better in emails and why we and our users benefit from it.
 
 JavaScript
 
 	Philip Walton shares how to build a back/forward (“bfcache” cache for nearly instant loading of pages when a user uses the browser history buttons. To be honest, I didn’t know this exists until I read the article and it’s super interesting. It’s basically a different cache layer that keeps a live capture of the page including the JavaScript heap in a separate cache. But due to this, it’s not always easy to handle, especially if you have events running on the page so this article contains the answers how to observe the bfcache and add handlers to it. In summary it’s a super powerful feature that’s incredibly useful for web apps but a normal website doesn’t need this layer of complexity.
 	This date picker is very nice, flexible, pretty universal and accessible. I know there are many others out there but it’s one that’s working well, working with many coding platforms and is well usable by people.
 	Nice, Apple now lets us integrate Face ID and Touch ID on the web, building it on top of the Web Authentication API. Imagine how this can improve the logging in experience for a good part of your user base.
 	Chris Coyier shares how we can integrate the Web Share API into our websites instead of these ugly lists of social icons. Note that this depends on us how well it’s perceived and working. For example on macOS it’s supported but most apps on the Mac don’t integrate the Framework for it so they don’t show up in the list of sharing services. We should take care that our products support the native frameworks to make the web a better place.
 	Wonder which operator to use in JavaScript for a comparison? Or seen a code snippet and felt weird because you didn’t know what operator this was? Josh W Comeau has built an operator lookup.
 
 CSS
 
 	Stefan Judis shares a nice approach of centering a container with sub-elements like a navigation with the fit-content property in CSS. Unfortunately, this still needs vendor prefixes at the moment to work in major browsers.
 	Ahmad Shadeed explains the CSS Grid minmax() function with some really good examples. Most tutorials explaining this function are very generic and not practical so this was a nice read when I found it. In case you don’t fully understand the minmax syntax yet, give it a read.
 	If you use Tailwind CSS or consider to use it, this amazing post on color theming is for you. But also if you don’t use Tailwind there’s at least a good part of the article that we can learn something from in terms of setting colors in CSS.
 	Josh W. Cameau has a CSS Grid full-bleed layout tutorial that’s pretty complete and includes lots of nice throwbacks into the history of the web.
 	CSS Text effects are rarely useful but sometimes for big teaser headlines it’s what we need. And if we need them, it’s sometimes not so easy to create a nice and really good effect so this article showing five CSS Text effects is great and includes split colored text, video text, and image background text.
 	There are a lot of basic tutorials on CSS Variables but this one contains some pretty handy use cases for variables that go beyond the basic color variable usage but show how we can build a flexible avatar box, defining paddings or gaps by using calc() and variables together.
 	Talking about colors in CSS, we have Josh Bader sharing how we can use CSS variables calc() and rgb() to enforce high contrast colors in our CSS design system.
 	Rachel Andrew shares how we can build a Masonry layout fully in CSS with CSS Grid Level 3.
 	Dave Rupert explores how we should define semantic variable names in the age of light and dark themes.
 
 Work &amp; Life
 
 	Engineering Manager is one of the roles that most people don’t know exactly what it’s about and what these people do. That’s why I found Karl Hughes’ »A day in the life of an Engineering Manager« a really nice one to read. He explains what he does all day and it turns out it’s a role full of soft skills like networking, explaining things or translating between two people, between company departments and to raise awareness around delivery, around process management and recruiting as well as people’s happiness in their jobs. I was Engineering Manager for quite some time as well so I can second what Karl writes here.
 
 Go beyond…
 
 	You know what scares me most when it comes to climate change and how we as humans perceive it? That we don’t notice that our climate is changing slowly but steadily. Every time a big tornado or wildfires destroy lots of things, our habitats, we read dozens of articles in the news, watch it in TV. But we barely notice that the climate already changed for everyone in the world, that temperatures grew already, that the four seasons changed a little bit, that we face more and more inconsistencies in the weather, more things like wildfires.  I’m not sure if it’s the protection mode in our brains that blocks this away or if it’s just too hard to notice these tiny events when so much other stuff is going on in our lives.
 	Who is responsible for climate change? The complexity of the problem makes a simple answer elusive. And yet, responsibility matters—not just for assigning blame, but for finding strategic levers for future change. Here’s insight, here’s evidence, here’s a good article to help us understand our biggest problem of humanity yet a bit better. And remember that most plastic is made out of oil.
 
 
 Thank you all for reading this, I hope you’re doing fine and stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>The case for Weak Dependencies in JS</title>
         <link href="https://lea.verou.me/2020/11/the-case-for-weak-dependencies-in-js/"/>
       <updated>2020-11-19T16:18:51.000Z</updated>
       <content type="text">Earlier today, I was briefly entertaining the idea of writing a library to wrap and enhance querySelectorAll in certain ways. I thought I’d rather not introduce a Parsel dependency out of the box, but only use it to parse selectors properly when it’s available, and use more crude regex when it’s not (which would cover most use cases for what I wanted to do).
 
 
 
 In the olden days, where every library introduced a global, I could just do:
 
 
 
 if (window.Parsel) {
 	let ast &#x3D; Parsel.parse();
 	// rewrite selector properly, with AST
 }
 else {
 	// crude regex replace
 }
 
 
 
 However, with ESM, there doesn’t seem to be a way to detect whether a module is imported, without actually importing it yourself.
 
 
 
 I tweeted about this…
 
 
 
 
 
 
 
 
 One issue with ESM (and other module systems) is that you can&#x27;t have optional dependencies. With globals, you could check if a global existed and branch accordingly. With modules, you can&#x27;t check if a module is available/loaded without importing it from somewhere.— Lea Verou (@LeaVerou) November 19, 2020
 
 
 
 
 I thought this was a common paradigm, and everyone would understand why this was useful. However, I was surprised to find that most people were baffled about my use case. Most of them thought I was either talking about conditional imports, or error recovery after failed imports.
 
 
 
 I suspect it might be because my primary perspective for writing JS is that of a library author, where I do not control the host environment, whereas for most developers, their primary perspective is that of writing JS for a specific app or website.
 
 
 
 After Kyle Simpson asked me to elaborate about the use case, I figured a blog post was in order.
 
 
 
 The use case is essentially progressive enhancement (in fact, I toyed with the idea of titling this blog post “Progressively Enhanced JS”). If library X is loaded already by other code, do a more elaborate thing and cover all the edge cases, otherwise do a more basic thing. It’s for dependencies that are not really dependencies, but more like nice-to-haves.
 
 
 
 We often see modules that do things really well, but use a ton of dependencies and add a lot of weight, even to the simplest of projects, because they need to cater to all the edge cases that we may not care about. We also see modules that are dependency free, but that’s because lots of things are implemented more crudely, or certain features are not there.
 
 
 
 This paradigm gives you the best of both worlds: Dependency free (or low dependency) modules, that can use what’s available to improve how they do things with zero additional impact. 
 
 
 
 Using this paradigm, the size of these dependencies is not a concern, because they are optional peer dependencies, so one can pick the best library for the job without being affected by bundle size. Or even use multiple! One does not even need to pick one dependency for each thing, they can support bigger, more complete libraries when they’re available and fall back to micro-libraries when they are not.
 
 
 
 Some examples besides the one in the first paragraph:
 
 
 
 A Markdown to HTML converter that also syntax highlights blocks of code if Prism is present. Or it could even support multiple different highlighters!A code editor that uses Incrementable to make numbers incrementable via arrow keys, if it’s presentA templating library that also uses Dragula to make items rearrangable via drag &amp; drop, if presentA testing framework that uses Tippy for nice informational popups, when it’s availableA code editor that shows code size (in KB) if a library to measure that is included. Same editor can also show gzipped code size if a gzip library is included.A UI library that uses a custom element if it’s available or the closest native one when it’s not (e.g. a fancy date picker vs &lt;input type&#x3D;&quot;date&quot;&gt; ) when it isn’t. Or Awesomplete for autocomplete when it’s available, and fall back to a simple &lt;datalist&gt; when it isn’t.Code that uses a date formatting library when one is already loaded, and falls back to Intl.DateTimeFormat when it’s not.
 
 
 
 This pattern can even be combined with conditional loading: e.g. we check for all known syntax highlighters and load Prism if none are present.
 
 
 
 To recap, some of the main benefits are:
 
 
 
 Performance: If you’re loading modules over the network HTTP requests are expensive. If you’re pre-bundling it increases bundle size. Even if code size is not a concern, runtime performance is affected if you take the slow but always correct path when you don’t need it and a more crude approach would satisfice.Choice: Instead of picking one library for the thing you need, you can support multiple. E.g. multiple syntax highlighters, multiple Markdown parsers etc. If a library is always needed to do the thing you want, you can load it conditionally, if none of the ones you support are loaded already.
 
 
 
 Are weak dependencies an antipattern?
 
 
 
 Since this article was posted, some of the feedback I got was along the lines of “Weak dependencies are an antipattern because they are unpredictable. What if you have included a library but don’t want another library to use it? You should instead use parameters to explicitly provide references to these libraries.”
 
 
 
 There are several counterpoints to make here.
 
 
 
 First, if weak dependencies are used well, they are only used to enhance the default/basic behavior, so it’s highly unlikely that you’d want to turn that off and fall back to the default behavior.
 
 
 
 Second, weak dependencies and parameter injection are not mutually exclusive. They can work together and complement each other, so that the weak dependencies provide sensible defaults that the parameters can then tweak further (or disable altogether). Only having parameter injection imposes a high upfront cognitive cost for using the library (see Convention over Configuration). Good APIs make simple things easy and complex things possible. The common case is that if you’ve loaded e.g. a syntax highlighter, you’d want to use it to syntax highlight, and if you’ve loaded a parser, you’d prefer it over parsing with regexes. The obscure edge cases where you wouldn’t want to highlight or you want to provide a different parser can still be possible via parameters, but should not be the only way.
 
 
 
 Third, the end user-developer may not even be aware of all the libraries that are being loaded, so they may already have a library loaded for a certain task but not know about it. The weak dependencies pattern operates directly on which modules are loaded so it doesn’t suffer from this problem.
 
 
 
 How could this work with ESM?
 
 
 
 Some people (mostly fellow library authors) *did* understand what I was talking about, and expressed some ideas about how this would work.
 
 
 
 Idea 1: A global module loaded cache could be a low-level way to implement this, and something CJS supports out of the box apparently.
 
 
 
 
 CommonJS exposes the cache … maybe ESM could follow, handy also for cache invalidation and code coverage with tests … indeed my tests are all CommonJS and I *can’t* change that — Andrea Giammarchi  (@WebReflection) November 19, 2020
 
 
 
 
 Idea 2: A global registry where modules can register themselves on, either with an identifier, or a SHA hashIdea 3: An import.whenDefined(moduleURL) promise, though that makes it difficult to deal with the module not being present at all, which is the whole point.
 
 
 
 
 an ugly way could be to:globalThis[Symbol.for(moduleName)] &#x3D; true;in the main module export/fileanother way I use with uce is to pass through customElements.whenDefined(&#x27;uce-lib&#x27;).then(…) so that if nobody imports &#x27;uce&#x27; nothing happensimport.whenDefined(module).then(…)?— Andrea Giammarchi  (@WebReflection) November 19, 2020
 
 
 
 
 
 If it exported a known SHA. a could be solved— James Campbell (@jcampbell_05) November 19, 2020
 
 
 
 
 Idea 4: Monitoring &lt;link rel&#x3D;&quot;modulepreload&quot;&gt;. The problem is that not all modules are loaded this way.
 
 
 
 
 I had a few things in mind:– if the tag is present in the page, you could assume it&#x27;s either present or will be very soon (already loading), so that might/should be a safe enough signal for your use-case– you could also attach a load event handler to the tag to observe it— getify (@getify) November 19, 2020
 
 
 
 
 Idea 5: I was thinking of a function like import() that resolves with the module (same as a regular dynamic import) only when the module is already loaded, or rejects when it’s not (which can be caught). In fact, it could even use the same functional notation, with a second argument, like so:
 
 
 
 import(&quot;https://cool-library&quot;, {weak: true});
 
 
 
 Nearly all of these proposals suffer from one of the following problems.
 
 
 
 Those that are URL based mean that only modules loaded from the same URL would be recognized. The same library loaded over a CDN vs locally would not be recognized as the same library. 
 
 
 
 One way around this is to expose a list of URLs, like the first idea, and allow to listen for changes to it. Then these URLs can be inspected and those which might belong to the module we are looking for can be further inspected by dynamically importing and inspecting their exports (importing already imported modules is a pretty cheap operation, the browser does de-duplicate the request).
 
 
 
 Those that are identifier based, depend on the module to register itself with an identifier, so only modules that want to be exposed, will be. This is the closest to the old global situation, but would suffer in the transitional period until most modules use it. And of course, there is the potential for clashes. Though the API could take care of that, by essentially using a hashtable and adding all modules that register themselves with the same identifier under the same “bucket”. Code reading the registry would then be responsible for filtering.
 
 
 
 </content>
     </entry>
     <entry>
       <title>Building My First PC</title>
         <link href="https://www.taniarascia.com/building-my-first-pc/"/>
       <updated>2020-11-15T00:00:00.000Z</updated>
       <content type="text">I use a Mac for all my software development needs, but recently I decided I wanted to build a PC. I had an old one sitting around for the occasional game, but it was really old - Windows 8 was new when it came out, making it about 8 years old. Also, it wasn&#x27;t built for any sort of performance, being a stock machine meant for home office use.
 Lately, I&#x27;ve wanted to do some extracurricular projects outside of programming and writing about JavaScript. Building a PC seemed like a fun project, as I&#x27;d learn a bit about how a computer works under the hood, it&#x27;s inspired me to set up a sweet battlestation and get some decent gear, and I&#x27;d have a much better machine for when I do play games or if I want to do anything with 3D modeling. So I went ahead and built one.
 
       
     
   
   
     
 After having built one myself now, I can say I&#x27;m really glad I decided to do this. For anyone who uses a PC for any reason, not even just games or development, I&#x27;d highly recommend building your own as well. It&#x27;s been a long, long time since I&#x27;ve used a computer with Windows, but in my experience of buying pre-built PCs, they were always loaded up with all the crapware and antivirus software that the hardware manufacturers ship along with Windows. I&#x27;m amazed at how fast and clean it is when you do it all yourself.
 There was a bit of research I had to do and lingo I had to learn to get up to speed to doing this, so I&#x27;ll share with you all the components needed, the ones I got, how much they cost, and how to set it up. I am by no means an expert, and as a result my write up might be most helpful for absolute beginners.
 Here is my final parts list.
 Getting Started
 When building a PC, PCPartPicker will be your best friend. You can browse through builds others have made, pick from recommended builds for your needs, and create your own part lists. I was overwhelmed by all the parts, what I should get, what build I should go with, and whether or not the parts would be compatible with each other, and the final build I went with ended up varying a bit from what I started off with.
 The questions that will determine your part list are:
 
 What will you be using the PC for?
 What is your price range?
 
 I knew I wanted something around $1200 or below, and I was going to use it for casual gaming, music, and maaaybe a small amount of streaming, 3D modeling, or development work. I ended up paying less than $1200 for the box alone, but the peripherals (monitor, keyboard, webcam, speakers) and other necessities (Windows) made the whole package cost quite a bit more.
 I put off pulling the trigger and doing a build for months and months because I wasn&#x27;t sure if it was going to be worth it or if it was just going to be an extra thing I don&#x27;t use since I prefer Macs. I realized I play games with friends enough to make it worth it, especially during quarantine.
 You can really build something decent for $400-$500 if you don&#x27;t care about gaming at all. The Build Guides on PCPartPicker have a lot of recommended builds, so I used the Excellent AMD Gaming/Streaming Build as a base for mine.
 Parts
 There are 6 essential parts every build needs. There are some additional things you can add, like custom cooling systems and optical drives, but I didn&#x27;t need any of that.
 
 CPU - the central processing unit, the primary component or &quot;brains&quot; of a computer. AMD and Intel are the main brands.
 Motherboard - the main printed circuit board that connects all the parts of a computer together. CPU, RAM, graphics cards, storage, drives, etc. will all hook into the motherboard.
 Memory - the RAM (random access memory) is where a computer stores and retrieves data on a short-term (volatile) basis.
 Storage - long-term (non-volatile) storage for a computer. These used to be stored in spinning hard drives but now frequently use SSD (solid state drive).
 Video card - the GPU (graphics processing unit) or graphics card, responsible for rendering images to the screen. AMD and NVIDIA are the main brands.
 Power Supply - the PSU (power supply unit) converts power from the source (outlet) into the correct format and voltage. This is what plugs into the wall.
 Case - technically not required (I&#x27;ve seen PC builds that are just a bunch of random parts on the wall) but most people want a body to put it all inside.
 
 Okay, that&#x27;s not too bad. I think all the parts make sense - power, long-term data storage, short-term memory, graphics, processing, and the One Board to connect them all.
 My choices
 
       
     
   
   
     
 Here are the choices I made and why.
 
 CPU - AMD Ryzen 5 3600 - honestly, I asked a friend whether or not to go with Intel or AMD, and they recommended AMD. I doubt there&#x27;s much of a different ultimately. This one was highly rated.
 Motherboard - ASRock B550 Phantom Gaming 4 - the motherboard and CPU go together, so you choose based on that compatibility. This one has 8 USB ports. Note that although it has an HDMI port, you will use the HDMI port on the graphics card, as the AMD CPU does not have integrated graphics and instead relies on the graphics card.
 Memory - Crucial Ballistix 32GB Kit (2 x 16GB) DDR4-3600 - the build I selected 16GB RAM, but I went with 32GB (2 sticks of 16GB). More RAM is always better, why not?
 Storage - SSD 970 EVO NVMe M.2 1TB - a solid state drive is the only option, and this NVMe M.2 style was supported on the motherboard and extremely convenient to set up.
 Video card - graphics cards are the big part that people often replace when they want an upgrade to their gaming experience. I didn&#x27;t need anything too intense for my games as I don&#x27;t play anything very intensive. The initial one I wanted (RTX 2070) was sold out everywhere, along with my second choice (Radeon RX 5700 XT). The one I got was a tier behind, but still more than enough for my needs, and I saved $200+, so that was a win! If you care a lot about the stats, there are plenty of sites that do benchmarking between different graphics cards.
 Power supply - EVGA SuperNOVA 650 G+, 80 Plus Gold 650W, Fully Modular - I got a fully modular power supply, meaning you can connect only the parts you need.
 Case - MasterBox NR600 without ODD - ODD is optical disk drive (like DVD), which I didn&#x27;t need to support. I got an inexpensive case that looked tasteful and had nice airflow. I realized it&#x27;s kind of big and I could have gone with a smaller case, but not really a big deal.
 
 Putting it together
 Of course, I had no idea how to put it all together, so I looked it up. All you need is a regular Phillips head screwdriver. I followed this video: How To Build A PC - FULL Beginners Guide + Benchmarking. I was lucky enough that he was using basically the same CPU, motherboard, and storage type as me, and the rest doesn&#x27;t matter. I also felt good that the video was only a few months old and using the parts I chose, so I know I got some decent stuff.
 The steps were as follows:
 
 Put the CPU in the motherboard - the step entails putting the tiny little CPU chip in the right slot on the motherboard and locking it in.
 Attach the fan to the CPU/motherboard - a fan came with my CPU, and I just had to unscrew four screw and screw in the fan in their place.
 Attach the motherboard to the case - just find where all the ports are facing outwards, put it in the right slot, and screw it in.
 Install the RAM - there are four slots for RAM, and I put it in slots 2 and 4.
 Install the SSD - it goes into a slot and screws in with one tiny screw.
 Install the GPU - I took two segments off the side of the case for the display ports, and the GPU just clicks in to one long slot.
 Install the PSU - find all the cables you need and put them into the right slots - CPU, graphics, motherboard. Then you have to hook them up to the correct pins on the motherboard. One important note is that the video I watched had SATA, and my build did not need to use the SATA at all. Also install the fans into the pins and anything else that needs to be hooked up.
 
 The whole thing probably took 2 hours or so, but it was also my first time doing it and I was learning where everything is.
 Turning it on
 
       
     
   
   
     
 At this point I had little to no confidence that this was going to turn on, as it seemed crazy that this could work on the first try and I didn&#x27;t plug something into the wrong slot. I connected the keyboard, mouse, ethernet, and monitor, I flipped the power switch, clicked the on button, and it booted up!
 But it displayed this message:
 Reboot and Select proper Boot device
 or Insert Boot Media in selected Boot device and press a key_
 First of all, what is with the capitalization here? It looks like the constitution. In any case, pressing any key didn&#x27;t work. I forgot to include Windows before turning it on. I just wanted to see if it worked and did anything first.
 I turned it off, inserted the Windows USB, and same message. After looking it up, I found someone suggesting to rapidly press delete 20 times right after turning it on. I did this, and then I was able to go through the Windows installation wizard.
 
       
     
   
   
     
 After that was all set up, I was really blown away by how fast it was. Everything worked great, the computer starts up in 3 seconds and is ready to go, and there&#x27;s no additional start up software. For this reason alone, I&#x27;d recommend anyone who uses a PC to build their own for the next one.
 Peripherals
 Another aspect to consider is the peripherals - keyboard, monitor, mouse, speakers, mic, webcam, headphones, etc. I didn&#x27;t know anything about that stuff, as I was using the cheapest stuff Walgreens could offer until now.
 
 Monitor - I got a gaming monitor that was 27&quot; wide, 1440 pixel resolution, and 165Hz frame rate. I didn&#x27;t really know what any of that meant until I did a bit of research, but I&#x27;m happy with the TUF Gaming VG27AQ Monitor for both work and play.
 Keyboard - I also got the Keychron K1 Wireless Mechanical Keyboard as I wanted something that would work with the Mac and wasn&#x27;t too far off from the tiny clearance on the chiclet keyboard I&#x27;m used to. I still prefer just using the MacBook Pro keyboard and trackpad for working. It&#x27;s just so much more convenient than a separate keyboard and mouse - I never have to move my arm back and forth to the mouse.
 Microphone - I already had a Blue Yeti mic for the occasional music recording needs and games, instead of a headset. And maybe I&#x27;ll make an instructional video some day, who knows.
 Headphones - I also already had a Sony WH-1000XM3, high-quality wireless noise-cancelling headphones, which I use for listening to music, recording music, and games.
 Mouse - my mouse needs are not great, but I wanted something a little more decent than the $9 bargain bin mouse, so I got Logitech G203 Prodigy Programmable RGB Gaming Mouse. I did program it to stay violet-indigoish (I&#x27;m not a fan of it rotating between RGB colors).
 
 For the first time in my life, I have decent gear and a cool set up. It took me months and months - almost a year - of working from home all day long to realize I should set up an actual office area, get an office chair, get a monitor, etc. It&#x27;s still a work in progress - I&#x27;m planning on building a desk and setting it up nicely as well, so I don&#x27;t have a cool &quot;battlestation&quot; to show off yet.
 
       
     
   
   
     
 Conclusion
 I&#x27;ve really enjoyed creating a little space for myself. We&#x27;re all home a lot these days. Once again, I would really recommend anyone who uses a PC to build their own, even if they&#x27;re not a gamer or a developer. It takes a little bit of research and time, but you can really create something that&#x27;s your own. Hopefully this guide helps out some other beginners with the overwhelming world of PC building.</content>
     </entry>
     <entry>
       <title>Simple pie charts with fallback, today</title>
         <link href="https://lea.verou.me/2020/11/simple-pie-charts-with-fallback-today/"/>
       <updated>2020-11-12T14:58:38.000Z</updated>
       <content type="text">Five years ago, I had written this extensive Smashing Magazine article detailing multiple different methods for creating simple pie charts, either with clever use of transforms and pseudo-elements, or with SVG stroke-dasharray. In the end, I mentioned creating pie charts with conic gradients, as a future technique. It was actually a writeup of my “The Missing Slice” talk, and an excerpt of my CSS Secrets book, which had just been published.
 
 
 
 I was reminded of this article today by someone on Twitter:
 
 
 
 
 So it turns out, building pie charts with pure CSS and HTML is insanely difficult! Here&#x27;s how to do it with older browser support: https://t.co/0B4DHmMRU2.Thank you @LeaVerou for such a genius solution.#frontenddev #frontenddevelopment #html #css— Sam (@sam_kent_) November 12, 2020
 
 
 
 
 I suggested conic gradients, since they are now supported in &gt;87% of users’ browsers, but he needed to support IE11. He suggested using my polyfill from back then, but this is not a very good idea today.
 
 
 
 
 What kind of IE11 support? &#x27;cause I would totally go with conic-gradients and fall back to a simpler progress bar kind of thing for browsers that don&#x27;t support itPolyfills can impact performance, and alternative techniques for creating conic gradients are quite hard to maintain— Facundo Corradini (@fcorradini) November 12, 2020
 
 
 
 
 Indeed, unless you really need to display conic gradients, even I would not recommend using the polyfill on a production facing site. It requires -prefix-free, which re-fetches (albeit from cache) your entire CSS and sticks it in a &lt;style&gt; element, with no sourcemaps since those were not a thing back when -prefix-free was written. If you’re already using -prefix-free, the polyfill is great, but if not, it’s way too heavy a dependency.
 
 
 
 Pie charts with fallback (modern browsers)
 
 
 
 Instead, what I would recommend is graceful degradation, i.e. to use the same color stops, but in a linear gradient.
 
 
 
 
 
 
 
 We can use @supports and have quite an elaborate progress bar fallback. For example, take a look at this 40% pie chart:
 
 
 
 .pie {
 	height: 20px;
 	background: linear-gradient(to right, deeppink 40%, transparent 0);
 	background-color: gold;
 }
 
 @supports (background: conic-gradient(white, black)) {
 	.pie {
 		width: 200px; height: 200px;
 		background-image: conic-gradient(deeppink 40%, transparent 0);
 		border-radius: 50%;
 	}
 }
 
 
 
 
 
 
 
 This is what it looks like in Firefox 82 (conic gradients are scheduled to ship unflagged in Firefox 83) or IE11:
 
 
 
 
 
 
 
 Note that because @supports is only used for the pie and not the fallback, the lack of IE11 support for it doesn’t affect us one iota.
 
 
 
 If relatively modern browsers are all we care about, we could even use CSS variables for the percentage and the color stops, to avoid duplication, and to be able to set the percentage from the markup:
 
 
 
 &lt;div class&#x3D;&quot;pie&quot; style&#x3D;&quot;--p: 40%&quot;&gt;&lt;/div&gt;
 
 
 
 .pie {
 	height: 20px;
 	--stops: deeppink var(--p, 0%), transparent 0;
 	background: linear-gradient(to right, var(--stops));
 	background-color: gold;
 }
 
 @supports (background: conic-gradient(white, black)) {
 	.pie {
 		width: 200px; height: 200px;
 		background-image: conic-gradient(var(--stops));
 		border-radius: 50%;
 	}
 }
 
 
 
 You can use a similar approach for 3 or more segments, or for a vertical bar.
 
 
 
 One issue with this approach is that our layout needs to work well with two charts of completely different proportions. To avoid that, we could just use a square:
 
 
 
 .pie {
 	width: 200px; 
 	height: 200px;
 	background: linear-gradient(to right, deeppink 40%, transparent 0) gold;
 }
 
 @supports (background: conic-gradient(white, black)) {
 	.pie {
 		background-image: conic-gradient(deeppink 40%, transparent 0);
 		border-radius: 50%;
 	}
 }
 
 
 
 which produces this in IE11:
 
 
 
 
 
 
 
 Granted, a square progress bar is not the same, but it can still convey the same relationship and is easier to design a layout around it since it always has the same aspect ratio.
 
 
 
 Why not use radial gradients?
 
 
 
 You might be wondering, why not just use a radial gradient, which could use the same dimensions and rounding. Something like this:
 
 
 
 
 
 
 
 There are two problems with this. The first one may be obvious: Horizontal or vertical bars are common for showing the proportional difference between two amounts, albeit less good than a pie chart because it’s harder to compare with 50% at a glance (yes Tufte, pie charts can be better for some things!). Such circular graphs are very uncommon. And for good reason: Drawn naively (e.g. in our case if the radius of the pink circle is 40% of the radius of the yellow circle), their areas do not have the relative relationship we want to depict.
 
 
 
 Why is that? Let r be the radius of the yellow circle. As we know from middle school, the area of the entire circle is πr², so the area of the yellow ring is πr² – (area of pink circle). The area of the pink circle is π(0.4r)² &#x3D; 0.16πr². Therefore, the area of the yellow ring is πr² – 0.16πr² &#x3D; 0.84πr² and their relative ratio is 0.16πr² / 0.84πr² &#x3D; 0.16 / 0.84 ≅ 0.19 which is a far cry from the 40/60 (≅ 0.67) we were looking for!
 
 
 
 Instead, if we wanted to draw a similar visualization to depict the correct relationship, we need to start from the ratio and work our way backwards. Let r be the radius of the yellow circle and kr the radius of the pink circle. Their ratio is π(kr)² / (πr² – π(kr)²) &#x3D; 4/6 ⇒k² / (1 – k²) &#x3D; 4/6 ⇒(1 – k²) / k² &#x3D; 6/4 ⇒1/k² – 1 &#x3D; 6/4 ⇒1/k² &#x3D; 10/4 ⇒k &#x3D; 2 / sqrt(10) ≅ .632Therefore, the radius of the pink circle should be around 63.2% of the radius of the yellow circle, and a more correct chart would look like this:
 
 
 
 
 
 
 
 In the general case where the pink circle is depicting the percentage p, we’d want the radius of the pink circle to be sqrt(1 / p) the size of the yellow circle. That’s a fair bit of calculations that we can’t yet automate (though sqrt() is coming!). Moral of the story: use a bar as your fallback!</content>
     </entry>
     <entry>
       <title>Everyday Systems That Help Me</title>
         <link href="https://www.taniarascia.com/everyday-systems/"/>
       <updated>2020-11-03T00:00:00.000Z</updated>
       <content type="text">At some point in my life, I read A Case Against Optimizing Your Life , and I took it to heart, and I lived by it. Now I think Wise People Have Rules For Themselves is the right approach for me.
 The former basically says, &quot;don&#x27;t worry about trying to set up the perfect system; nothing will ever be perfect, so just relax and get used to chaos, and try to put all your focus on one thing at a time&quot;. The latter says, &quot;set up self-imposed personal rules, because your quality of life will improve when your standards are clear&quot;.
 I agree with that now. I&#x27;ve begun setting up lots of systems - everyday systems - in my life, and it has had a huge positive effect on me. Although it&#x27;s a confusing time and I don&#x27;t really know what I ultimately &quot;want&quot; in life, setting certain boundaries and having certain daily tasks really helps reduce stress and anxiety and leaves me feeling free and open.
 Disclaimer: I don&#x27;t know you, so I don&#x27;t know if none, some, or all of what I write here will apply to you or help you. Just as that first article I read may have worked for someone but ultimately was not the right approach for me. Take all advice with a grain of salt. I&#x27;m just sharing what I actually do.
 So I&#x27;ll talk about boundaries (the things I don&#x27;t do) and systems (the things I do).
 Boundaries
 I set boundaries in my life. I think it&#x27;s useful to preemptively tackle sources of stress rather than hacking away at a problem that&#x27;s already created.
 
 Eliminate infinite scroll
 Drastically reduce social media
 Drastically reduce notifications
 Drastically reduce incoming email
 Separate work from the rest of life
 
 Social media and infinite scroll
 If there&#x27;s anything with infinite scroll, I&#x27;ve trained myself not to use it. No scrolling through Twitter, no scrolling through reddit, no scrolling through Instagram, and I haven&#x27;t had Facebook for many years. No scrolling at all.
 I don&#x27;t want to say something absolute like, &quot;reddit is completely off limits!&quot; because that&#x27;s not the problem. Visiting the &quot;Build a PC&quot; subreddit to find advice on how to build a PC is fine. Infinitely scrolling through the front page or anything else is not. It&#x27;s just a useless waste of time and addiction for me. So although I technically have accounts on some of these platforms, I have none of them installed on my phone and I almost never visit them unless I have an express purpose to.
 Notifications
 I only have notifications on my phone for two things: email and text. (Also calendar events, but since I&#x27;ve hardly had a calendar event in 2020 I almost forgot.) As I mentioned above, I don&#x27;t have any social media apps on my phone. The less amount of dopamine obtained from shallow interactions, the better.
 Twitter is my only connection to the outside world since it&#x27;s where I&#x27;m connected to some other developers, and the only place I promote my work, but I still only check it once every couple of days or once a week. I also keep Slack off my phone.
 Email inbox
 I practice inbox zero. I try to respond to the emails that require a response in a timely manner. I don&#x27;t respond to all or even most emails I receive. Things that I explicitly say I don&#x27;t want on my website (guest posts, sponsors, ad requests) get automatically marked as spam. Websites that sign me up for their newsletter without explicitly asking get unsubscribed and marked as spam. This results in me getting very few emails, and the ones I do get are just genuine people.
 Work hours
 Work is home for me now, as it is for many of us. 9-5 are my work hours, and I don&#x27;t do anything work related after 5 unless there&#x27;s an explicit, special need to do so. No work apps or connections are installed on my phone (Teams, Slack, Outlook, work email).
 Systems
 I&#x27;ve been using various single-purpose apps productively.
 
 One to keep all my personal notes, memories, articles, and lists (Bear)
 One to track the tasks/habits/systems I want to be doing every day (Strides)
 One to track the various to-dos I don&#x27;t want to forget (Todoist)
 
 
 Secret note: I built my own application for handling notes instead of using some other system, because none of the ones I found did exactly what I wanted. In the year I was building this app, I left my thoughts in limbo. I have since realized building and maintaining your own app for something like this is not the best approach. I did learn a ton while doing it, so it was overall a very positive experience.
 
 I don&#x27;t think it really matters if they&#x27;re apps or not - you could use a bullet journal and a diary or random sheets of paper. I don&#x27;t like writing by hand so apps make it easier for me. As long as I have one place I know I can go to retrieve anything I want to remember or work on, and little daily things don&#x27;t fall through the cracks.
 I have always felt like I could not do tedious, every day tasks (budgeting, counting calories, journaling). I had never done it well in the past, and I was always so spontaneous! and free! But it turns out those habits are not so oppressive or difficult, and the benefits far outweigh the positive.
 Of course, you&#x27;ll never be able to always do everything all the time. You&#x27;ll miss a day of exercise here or there. You&#x27;ll be tired and just go to bed. And that&#x27;s fine - just gotta start each day brand new without being discouraged.
 Note, I&#x27;m only discussing the physical things I do here, not any mental thought processes I have.
 Clean up
 Legitimately, every day I do these things:
 
 Make the bed
 Wash all the dishes
 Put away all the clothes
 
 I was never extremely messy, but I might let things go for a day or three - a few dishes pile up, clothes end up on the floor, the bed never gets made. Now I make the bed right after I wake up, I wash the dishes while I&#x27;m making the meal, and I put clothes where they belong right away. The house always feels clean and uncluttered despite almost no amount of time and effort having to go into it because there&#x27;s never more than one day&#x27;s worth of dishes or clothes to deal with at a time.
 Exercise
 Yeah, yeah. Exercise is good. What a revolutionary insight from me. Well, since it&#x27;s cold AF in Chicago, outdoor exercise is pretty much out, and I don&#x27;t feel like getting a gym membership. So I just pick a number and do these every day:
 
 Situps
 Pushups
 Squats
 Take a walk/bike ride (weather permitting)
 
 You really can never have an excuse not to do the first three, as they require no equipment or space. It takes me 45 minutes - 1 hour every day to do the exercises, plus stretching. Can easily be done while multitasking with TV or audio.
 As for taking a walk, I often pick a cafe a mile or two away and walk to it and back to get a coffee. If it&#x27;s biking weather, I try to ride 10 miles (1 hour) a few times/week.
 Sleep
 I would often find myself up at 1am, 2am, just listening to music, playing video games, watching Netflix, or surfing the web. I&#x27;ve also been a habitual snoozer my whole life, and usually set 3 alarms in the morning. Now I have the rule:
 
 In bed by midnight
 Don&#x27;t snooze - wake up to the first alarm
 
 This is a life long struggle but I know every time I live like this, things are better. Sometimes you have to do what sucks in the moment for the good of the rest of the day/week/month/year/life.
 Track net worth
 I&#x27;ve been tracking my net worth for several years now. I see a lot of people budget month-to-month, but I like seeing the big picture. I just add up all my assets (checking/savings accounts, retirement accounts, stocks, crypto) and debts (credit cards, car loans, phone payments, student loans). Yes, that means if you have no retirement accounts or money in the bank and you owe $100,000 on student loans, your net worth is negative. I don&#x27;t like having any debt, so this helped me early on in my life to pay off all my student loans, and buy a cheap car with cash, and so on.
 I just made my own custom Google Sheets page and I input all the numbers at the end of every month. So this is a &quot;once a month&quot; habit as opposed to some of the other &quot;once a day&quot; ones.
 Learning and practice
 I haven&#x27;t necessarily felt the most amount of focus in my life. Art and music don&#x27;t come as naturally to me as they used to. Although I want to practice music, for example, and learn new songs and how to sight read music, I don&#x27;t always feel the motivation to do so. Nonetheless, I&#x27;ve set up a daily thing to do - in this case, 10 minutes of sheet music flash card practice. It has the dual benefit of being a more productive thing to do while waiting in line somewhere than scrolling through the news or reddit, and I&#x27;ll be that much ahead when I do decide to sit down and learn a song.
 So basically it was more productive for me to say &quot;Practice sheet music 10 minutes a day&quot; than &quot;Learn this song&quot; which is bigger and requires more upfront effort. And requires me to do something that isn&#x27;t coding related, which is where I&#x27;ll naturally gravitate for hobby purposes.
 Do the annoying thing immediately
 There is nothing I hate more than dealing with bureaucracy. Paying bills by mail when I can&#x27;t do it any other way, calling up any sort of automated system, stuff like that. I usually put off very simple tasks like paying a bill or renewing car insurance for weeks or months, and it&#x27;s always sitting in the back of my mind as something I know I need to do but I really, really don&#x27;t want to.
 I&#x27;m personally making an effort to put those on my daily to-do list and just getting them done. It&#x27;s so much better than sitting there and knowing there&#x27;s an annoying thing to do for weeks on end.
 Daily journaling
 Recently, Matthew McConaughey was on the JRE podcast and talked about how he has kept a journal every day for basically his entire life. This is something I&#x27;ve never been able to do - I&#x27;ve just kind of been going through it day by day and living life in the moment. But there was some great advice there - when things are good, keep track of what&#x27;s going well. When things are bad, see all the factors of what was going on in your life during those times as well.
 At least for myself, I occasionally have bouts where life just isn&#x27;t going the way I want, and I feel anxious and depressed seemingly out of nowhere. Then at other times, everything is going great, and I don&#x27;t know why. How much of it is within my control? How much is from external factors? Without having really kept track of anything that&#x27;s going on in my life, it&#x27;s really hard to say.
 I&#x27;m aiming to adopt this strategy myself now. I can say things are going pretty well for me right now. I feel good at the moment, even with all the political craziness and lockdown and isolation and having a relatively recent breakup. I don&#x27;t feel anxiety or depression, and I feel a lot of positivity, hope, and potential. I know I won&#x27;t be able to write a detailed diary entry every single day of my life, but I&#x27;m making a point to remember to do it relatively often, and when important or interesting things happen.
 Conclusion
 Once again, this is just me talking about what I&#x27;m doing, not offering advice to you. All situations are unique, plus I don&#x27;t have a husband or children to keep me busy. Not having to commute also opens up time to take care of some of these tasks.
 By the way, there&#x27;s nothing about writing in these systems. I don&#x27;t set any sort of tasks, habits, goals, or systems for writing articles. I just write when I feel like it. People sometimes ask me how I&#x27;ve written so many articles, imagining that I&#x27;m so productive and doing it all the time. Honestly, I sit down like one or two days a month to write. Do that over 5 years with anything and you&#x27;ll have a lot of evidence of your work.
 In conclusion, if I could only implore you to do one thing every single day, it would be floss. Just do it. It takes 15 seconds.</content>
     </entry>
     <entry>
       <title>Understanding Modules, Import and Export in JavaScript</title>
         <link href="https://www.taniarascia.com/javascript-modules-import-export/"/>
       <updated>2020-10-23T00:00:00.000Z</updated>
       <content type="text">In the early days of the Web, websites consisted primarily of HTML and CSS. If any JavaScript loaded into a page at all, it was usually in the form of small snippets that provided effects and interactivity. As a result, JavaScript programs were often written entirely in one file and loaded into a script tag. A developer could break the JavaScript up into multiple files, but all variables and functions would still be added to the global scope.
 But as websites have evolved with the advent of frameworks like Angular, React, and Vue, and with companies creating advanced web applications instead of desktop applications, JavaScript now plays a major role in the browser. As a result, there is a much greater need to use third-party code for common tasks, to break up code into modular files, and to avoid polluting the global namespace.
 The ECMAScript 2015 specification introduced modules to the JavaScript language, which allowed for the use of import and export statements. In this tutorial, you will learn what a JavaScript module is and how to use import and export to organize your code.
 Modular Programming
 Before the concept of modules appeared in JavaScript, when a developer wanted to organize their code into segments, they would create multiple files and link to them as separate scripts. To demonstrate this, create an example index.html file and two JavaScript files, functions.js and script.js.
 The index.html file will display the sum, difference, product, and quotient of two numbers, and link to the two JavaScript files in script tags. Open index.html in a text editor and add the following code:
 index.html
 &lt;!DOCTYPE html&gt;
 &lt;html lang&#x3D;&quot;en&quot;&gt;
   &lt;head&gt;
     &lt;meta charset&#x3D;&quot;utf-8&quot; /&gt;
     &lt;meta name&#x3D;&quot;viewport&quot; content&#x3D;&quot;width&#x3D;device-width, initial-scale&#x3D;1.0&quot; /&gt;
 
     &lt;title&gt;JavaScript Modules&lt;/title&gt;
   &lt;/head&gt;
 
   &lt;body&gt;
     &lt;h1&gt;Answers&lt;/h1&gt;
     &lt;h2&gt;&lt;strong id&#x3D;&quot;x&quot;&gt;&lt;/strong&gt; and &lt;strong id&#x3D;&quot;y&quot;&gt;&lt;/strong&gt;&lt;/h2&gt;
 
     &lt;h3&gt;Addition&lt;/h3&gt;
     &lt;p id&#x3D;&quot;addition&quot;&gt;&lt;/p&gt;
 
     &lt;h3&gt;Subtraction&lt;/h3&gt;
     &lt;p id&#x3D;&quot;subtraction&quot;&gt;&lt;/p&gt;
 
     &lt;h3&gt;Multiplication&lt;/h3&gt;
     &lt;p id&#x3D;&quot;multiplication&quot;&gt;&lt;/p&gt;
 
     &lt;h3&gt;Division&lt;/h3&gt;
     &lt;p id&#x3D;&quot;division&quot;&gt;&lt;/p&gt;
 
     &lt;script src&#x3D;&quot;functions.js&quot;&gt;&lt;/script&gt;
     &lt;script src&#x3D;&quot;script.js&quot;&gt;&lt;/script&gt;
   &lt;/body&gt;
 &lt;/html&gt;
 This HTML will display the value of variables x and y in an h2 header, and the value of operations on those variables in the following p elements. The id attributes of the elements are set for DOM manipulation, which will happen in the script.js file; this file will also set the values of x and y. For more information on HTML, check out our How To Build a Website with HTML series.
 The functions.js file will contain the mathematical functions that will be used in the second script. Open the functions.js file and add the following:
 functions.js
 function sum(x, y) {
   return x + y
 }
 
 function difference(x, y) {
   return x - y
 }
 
 function product(x, y) {
   return x * y
 }
 
 function quotient(x, y) {
   return x / y
 }
 Finally, the script.js file will determine the values of x and y, apply the functions to them, and display the result:
 script.js
 const x &#x3D; 10
 const y &#x3D; 5
 
 document.getElementById(&#x27;x&#x27;).textContent &#x3D; x
 document.getElementById(&#x27;y&#x27;).textContent &#x3D; y
 
 document.getElementById(&#x27;addition&#x27;).textContent &#x3D; sum(x, y)
 document.getElementById(&#x27;subtraction&#x27;).textContent &#x3D; difference(x, y)
 document.getElementById(&#x27;multiplication&#x27;).textContent &#x3D; product(x, y)
 document.getElementById(&#x27;division&#x27;).textContent &#x3D; quotient(x, y)
 After setting up these files and saving them, you can open index.html in a browser to display your website with all the results:
 
       
     
   
   
     
 For websites with a few small scripts, this is an effective way to divide the code. However, there are some issues associated with this approach, including:
 
 Polluting the global namespace: All the variables you created in your scripts—sum, difference, etc.—now exist on the window object. If you attempted to use another variable called sum in another file, it would become difficult to know which value would be used at any point in the scripts, since they would all be using the same window.sum variable. The only way a variable could be private was by putting it within a function scope. There could even be a conflict between an id in the DOM named x and var x.
 Dependency management: Scripts would have to be loaded in order from top to bottom to ensure the correct variables were available. Saving the scripts as different files gives the illusion of separation, but it is essentially the same as having a single inline &lt;script&gt; in the browser page.
 
 Before ES6 added native modules to the JavaScript language, the community attempted to come up with several solutions. The first solutions were written in vanilla JavaScript, such as writing all code in objects or immediately invoked function expressions (IIFEs) and placing them on a single object in the global namespace. This was an improvement on the multiple script approach, but still had the same problems of putting at least one object in the global namespace, and did not make the problem of consistently sharing code between third parties any easier.
 After that, a few module solutions emerged: CommonJS, a synchronous approach that was implemented in Node.js, Asynchronous Module Definition (AMD), which was an asynchronous approach, and Universal Module Definition (UMD), which was intended to be a universal approach that supported both previous styles.
 The advent of these solutions made it easier for developers to share and reuse code in the form of packages, modules that can be distributed and shared, such as the ones found on npm. However, since there were many solutions and none were native to JavaScript, tools like Babel, Webpack, or Browserify had to be implemented to use modules in browsers.
 Due to the many problems with the multiple file approach and the complexity of the solutions proposed, developers were interested in bringing the modular programming approach to the JavaScript language. Because of this, ECMAScript 2015 supports the use of JavaScript modules.
 A module is a bundle of code that acts as an interface to provide functionality for other modules to use, as well as being able to rely on the functionality of other modules. A module exports to provide code and imports to use other code. Modules are useful because they allow developers to reuse code, they provide a stable, consistent interface that many developers can use, and they do not pollute the global namespace.
 Modules (sometimes referred to as ECMAScript modules or ES Modules) are now available natively in JavaScript, and in the rest of this tutorial you will explore how to use and implement them in your code.
 Native JavaScript Modules
 Modules in JavaScript use the import and export keywords:
 
 import: Used to read code exported from another module.
 export: Used to provide code to other modules.
 
 To demonstrate how to use this, update your functions.js file to be a module and export the functions. You will add export in front of each function, which will make them available to any other module.
 Add the following highlighted code to your file:
 functions.js
 export function sum(x, y) {
   return x + y
 }
 
 export function difference(x, y) {
   return x - y
 }
 
 export function product(x, y) {
   return x * y
 }
 
 export function quotient(x, y) {
   return x / y
 }
 Now, in script.js, you will use import to retrieve the code from the functions.js module at the top of the file.
 
 Note: import must always be at the top of the file before any other code, and it is also necessary to include the relative path (./ in this case).
 
 Add the following highlighted code to script.js:
 script.js
 import { sum, difference, product, quotient } from &#x27;./functions.js&#x27;
 
 const x &#x3D; 10
 const y &#x3D; 5
 
 document.getElementById(&#x27;x&#x27;).textContent &#x3D; x
 document.getElementById(&#x27;y&#x27;).textContent &#x3D; y
 
 document.getElementById(&#x27;addition&#x27;).textContent &#x3D; sum(x, y)
 document.getElementById(&#x27;subtraction&#x27;).textContent &#x3D; difference(x, y)
 document.getElementById(&#x27;multiplication&#x27;).textContent &#x3D; product(x, y)
 document.getElementById(&#x27;division&#x27;).textContent &#x3D; quotient(x, y)
 Notice that individual functions are imported by naming them in curly braces.
 In order to ensure this code gets loaded as a module and not a regular script, add type&#x3D;&quot;module&quot; to the script tags in index.html. Any code that uses import or export must use this attribute:
 index.html
 &lt;script 
   type&#x3D;&quot;module&quot; src&#x3D;&quot;functions.js&quot;&gt;
 &lt;/script&gt;
 &lt;script 
   type&#x3D;&quot;module&quot; src&#x3D;&quot;script.js&quot;&gt;
 &lt;/script&gt;
 At this point, you will be able to reload the page with the updates and the website will now use modules. Browser support is very high, but caniuse is available to check which browsers support it. Note that if you are viewing the file as a direct link to a local file, you will encounter this error:
 Access to script at &#x27;file:///Users/your_file_path/script.js&#x27; from origin &#x27;null&#x27; has been blocked by CORS policy: Cross origin requests are only supported for protocol schemes: http, data, chrome, chrome-extension, chrome-untrusted, https.
 Because of the CORS policy, Modules must be used in a server environment, which you can set up locally with http-server or on the internet with a hosting provider.
 Modules are different from regular scripts in a few ways:
 
 Modules do not add anything to the global (window) scope.
 Modules always are in strict mode.
 Loading the same module twice in the same file will have no effect, as modules are only executed once/
 Modules require a server environment.
 
 Modules are still often used alongside bundlers like Webpack for increased browser support and additional features, but they are also available for use directly in browsers.
 Next, you will explore some more ways in which the import and export syntax can be used.
 Named Exports
 As demonstrated earlier, using the export syntax will allow you to individually import values that have been exported by their name. Take for example this simplified version of functions.js:
 functions.js
 export function sum() {}
 export function difference() {}
 This would let you import sum and difference by name using curly braces:
 script.js
 import {sum, difference} from &#x27;./functions.js&#x27;
 It is also possible to use an alias to rename the function. You might do this to avoid naming conflicts within the same module. In this example, sum will be renamed to add and difference will be renamed to subtract.
 script.js
 import {
   sum as add,
   difference as subtract
 } from &#x27;./functions.js&#x27;
 
 add(1, 2) // 3
 Calling add() here will yield the result of the sum() function.
 Using the * syntax, you can import the contents of the entire module into one object. In this case, sum and difference will become methods on the mathFunctions object.
 script.js
 import * as mathFunctions from &#x27;./functions.js&#x27;
 
 mathFunctions.sum(1, 2) // 3
 mathFunctions.difference(10, 3) // 7
 Primitive values, function expressions and definitions, asynchronous functions, classes, and instantiated classes can all be exported, as long as they have an identifier:
 // Primitive values
 export const number &#x3D; 100
 export const string &#x3D; &#x27;string&#x27;
 export const undef &#x3D; undefined
 export const empty &#x3D; null
 export const obj &#x3D; {name: &#x27;Homer&#x27;}
 export const array &#x3D; [&#x27;Bart&#x27;, &#x27;Lisa&#x27;, &#x27;Maggie&#x27;]
 
 // Function expression
 export const sum &#x3D; (x, y) &#x3D;&gt; x + y
 
 // Function defintion
 export function difference(x, y) {
   return x - y
 }
 
 // Asynchronous function
 export async function getBooks() {}
 
 // Class
 export class Book {
   constructor(name, author) {
     this.name &#x3D; name
     this.author &#x3D; author
   }
 }
 
 // Instantiated class
 export const book &#x3D; new Book(&#x27;Lord of the Rings&#x27;, &#x27;J. R. R. Tolkein&#x27;)
 All of these exports can be successfully imported. The other type of export that you will explore in the next section is known as a default export.
 Default Exports
 In the previous examples, you exported multiple named exports and imported them individually or as one object with each export as a method on the object. Modules can also contain a default export, using the default keyword. A default export will not be imported with curly brackets, but will be directly imported into a named identifier.
 Take for example the following contents for the functions.js file:
 functions.js
 export default function sum(x, y) {
   return x + y
 }
 In the script.js file, you could import the default function as sum with the following:
 script.js
 import sum from &#x27;./functions.js&#x27;
 
 sum(1, 2) // 3
 This can be dangerous, as there are no restrictions on what you can name a default export during the import. In this example, the default function is imported as difference although it is actually the sum function:
 script.js
 import difference from &#x27;./functions.js&#x27;
 
 difference(1, 2) // 3
 For this reason, it is often preferred to use named exports. Unlike named exports, default exports do not require an identifier—a primitive value by itself or anonymous function can be used as a default export. Following is an example of an object used as a default export:
 functions.js
 export default {
   name: &#x27;Lord of the Rings&#x27;,
   author: &#x27;J. R. R. Tolkein&#x27;,
 }
 You could import this as book with the following:
 functions.js
 import book from &#x27;./functions.js&#x27;
 Similarly, the following example demonstrates exporting an anonymous arrow function as the default export:
 functions.js
 export default () &#x3D;&gt; &#x27;This function is anonymous&#x27;
 This could be imported with the following script.js:
 script.js
 import anonymousFunction from &#x27;./functions.js&#x27;
 Named exports and default exports can be used alongside each other, as in this module that exports two named values and a default value:
 functions.js
 export const length &#x3D; 10
 export const width &#x3D; 5
 
 export default function perimeter(x, y) {
   return 2 * (x + y)
 }
 You could import these variables and the default function with the following:
 script.js
 import calculatePerimeter, {length, width} from &#x27;./functions.js&#x27;
 
 calculatePerimeter(length, width) // 30
 Now the default value and named values are both available to the script.
 Conclusion
 Modular programming design practices allow you to separate code into individual components that can help make your code reusable and consistent, while also protecting the global namespace. A module interface can be implemented in native JavaScript with the import and export keywords. In this article, you learned about the history of modules in JavaSvript, how to separate JavaScript files into multiple top-level scripts, how to update those files using a modular approach, and the import and export syntax for named and default exports.
 To learn more about modules in JavaScript, read Modules on the Mozilla Developer Network. If you&#x27;d like to explore modules in Node.js, try our How To Create a Node.js Module tutorial.</content>
     </entry>
     <entry>
       <title>webpack Tutorial: How to Set Up webpack 5 From Scratch</title>
         <link href="https://www.taniarascia.com/how-to-use-webpack/"/>
       <updated>2020-10-15T00:00:00.000Z</updated>
       <content type="text">webpack used to be a frustrating and overwhelming beast to me. I felt safe using something like create-react-app to set up a project, but I avoided webpack if at all possible since it seemed complex and confusing.
 If you don&#x27;t feel comfortable setting up webpack from scratch for use with Babel, TypeScript, Sass, React, or Vue, or don&#x27;t know why you might want to use webpack, then this is the perfect article for you. Like all things, once you delve in and learn it you realize it&#x27;s not that scary and there&#x27;s just a few main concepts to learn to get set up.
 In addition to this article, I&#x27;ve created an extremely solid webpack 5 Boilerplate to get you started with any project. I also recommend checking it out if you&#x27;re familiar with webpack 4 but want to see a webpack 5 setup.
 Prerequisites
 
 Basic familiarity with HTML &amp; CSS
 Basic knowledge of JavaScript and programming
 Familiarity with ES6 syntax and features
 Ability to set up a Node.js environment
 Knowledge of the command line
 
 Goals
 
 Learn what webpack is and why you might want to use it
 Set up a development server with webpack
 Set up a production build flow using webpack
 
 Content
 
 What is webpack
 Installation
 Basic configuration
 
 Entry
 Output
 
 
 Plugins
 
 HTML template
 Clean
 
 
 Modules and Loaders
 
 Babel (JavaScript)
 Images
 Fonts and inline
 Styles
 
 
 Development
 
 
 If you&#x27;re upgrading from webpack 4 to webpack 5, here are a few notes:
 
 the webpack-dev-server command is now webpack-serve
 file-loader, raw-loader and url-loader are not necessary, you can use built in asset modules
 Node polyfills are no longer available, so if you get an error for stream, for example, you would add the stream-browserify package as a dependency and add { stream: &#x27;stream-browserify&#x27; } to the alias property in your webpack config.
 
 
 What is webpack?
 For the most part, websites are no longer just written in plain HTML with a bit of optional JavaScript - they&#x27;re often entirely built by JavaScript. So we have to bundle, minify, and transpile the code into something all browsers understand, which is where webpack comes in.
 webpack is a module bundler. It packs all your code neatly for the browser. It allows you to write the latest JavaScript with Babel or use TypeScript, and compile it into something cross-browser compatible and neatly minified. It also allows you to import static assets into your JavaScript.
 For development, webpack also supplies a development server that can update modules and styles on the fly when you save. vue create and create-react-app rely on webpack under the hood, but you can easily set up your own webpack config for them.
 There is much more that webpack can do, but this article will help you get familiar with the concepts and get something set up.
 Installation
 First, create a directory for your project to live and start a Node project. I&#x27;m calling it webpack-tutorial.
 mkdir webpack-tutorial
 cd webpack-tutorial
 npm init -y # creates a default package.json
 To begin, install webpack and webpack-cli. These are the core technologies for getting set up.
 npm i -D webpack webpack-cli
 
 webpack - Module and asset bundler
 webpack-cli - Command line interface for webpack
 
 We&#x27;ll make an src folder to contain all the source files. I&#x27;ll start by creating a simple index.js file.
 src/index.js
 console.log(&#x27;Interesting!&#x27;)
 Alright, so now you have a Node project with the base packages installed and an index file to start. We&#x27;ll begin creating the config files now.
 Basic configuration
 Let&#x27;s start setting up a Webpack build. Create a webpack.config.js in the root of your project.
 Entry
 The first part of setting up a webpack config is defining the entry point, what file or files webpack will look at to compile. In this example, we&#x27;ll set the entry point to the src/index.js.
 webpack.config.js
 const path &#x3D; require(&#x27;path&#x27;)
 
 module.exports &#x3D; {
   entry: {
     main: path.resolve(__dirname, &#x27;./src/index.js&#x27;),
   },
 }
 Output
 The output is where the bundled file will resolve. We&#x27;ll have it output in the dist folder, which is where production code gets built. The [name] in the output will be main, as specified in the entry object.
 webpack.config.js
 module.exports &#x3D; {
   /* ... */
 
   output: {
     path: path.resolve(__dirname, &#x27;./dist&#x27;),
     filename: &#x27;[name].bundle.js&#x27;,
   },
 }
 Now we have the minimum config necessary to build a bundle. In package.json, we can make a build script that runs the webpack command.
 package.json
 &quot;scripts&quot;: {
   &quot;build&quot;: &quot;webpack&quot;
 }
 Now you can run it.
 npm run build
 asset main.bundle.js 19 bytes [emitted] [minimized] (name: main)
 ./src/index.js 18 bytes [built] [code generated]
 webpack 5.1.0 compiled successfully in 152 mss
 You&#x27;ll see that a dist folder has been created with main.bundle.js. Nothing has happened to the file yet, but we now have webpack building successfully.
 Plugins
 webpack has a plugin interface that makes it flexible. Internal webpack code and third party extensions use plugins. There are a few main ones almost every webpack project will use.
 HTML template file
 So we have a random bundle file, but it&#x27;s not very useful to us yet. If we&#x27;re building a web app, we need an HTML page that will load that JavaScript bundle as a script. Since we want the HTML file to automatically bring in the script, we&#x27;ll create an HTML template with html-webpack-plugin.
 
 html-webpack-plugin - Generates an HTML file from a template
 
 Install the plugin.
 npm i -D html-webpack-plugin
 Create a template.html file in the src folder. We can include variables other custom information in the template. We&#x27;ll add a custom title, and otherwise it will look like a regular HTML file with a root div.
 src/template.html
 &lt;!DOCTYPE html&gt;
 &lt;html lang&#x3D;&quot;en&quot;&gt;
   &lt;head&gt;
     &lt;title&gt;&lt;%&#x3D; htmlWebpackPlugin.options.title %&gt;&lt;/title&gt;
   &lt;/head&gt;
 
   &lt;body&gt;
     &lt;div id&#x3D;&quot;root&quot;&gt;&lt;/div&gt;
   &lt;/body&gt;
 &lt;/html&gt;
 Create a plugins property of your config and you&#x27;ll add the plugin, filename to output (index.html), and link to the template file it will be based on.
 webpack.config.js
 const path &#x3D; require(&#x27;path&#x27;)
 const HtmlWebpackPlugin &#x3D; require(&#x27;html-webpack-plugin&#x27;)
 module.exports &#x3D; {
   /* ... */
 
   plugins: [    new HtmlWebpackPlugin({      title: &#x27;webpack Boilerplate&#x27;,      template: path.resolve(__dirname, &#x27;./src/template.html&#x27;), // template file      filename: &#x27;index.html&#x27;, // output file    }),  ],}
 Now run a build again. You&#x27;ll see the dist folder now contains an index.html with the bundle loaded in. Success! If you load that file into a browser, you&#x27;ll see Interesting! in the console.
 Let&#x27;s update it to inject some content into the DOM. Change the index.js entry point to this, and run the build command again.
 src/index.js
 // Create heading node
 const heading &#x3D; document.createElement(&#x27;h1&#x27;)
 heading.textContent &#x3D; &#x27;Interesting!&#x27;
 
 // Append heading node to the DOM
 const app &#x3D; document.querySelector(&#x27;#root&#x27;)
 app.append(heading)
 Now to test it out you can go to the dist folder and start up a server. (Install http-server globally if necessary.)
 http-server
 You&#x27;ll see our JavaScript injected into the DOM, saying &quot;Interesting!&quot;. You&#x27;ll also notice the bundle file is minified.
 Clean
 You&#x27;ll also want to set up clean-webpack-plugin, which clears out anything in the dist folder after each build. This is important to ensure no old data gets left behind.
 
 clean-webpack-plugin - Remove/clean build folders
 
 webpack.config.js
 const path &#x3D; require(&#x27;path&#x27;)
 
 const HtmlWebpackPlugin &#x3D; require(&#x27;html-webpack-plugin&#x27;)
 const { CleanWebpackPlugin } &#x3D; require(&#x27;clean-webpack-plugin&#x27;)
 module.exports &#x3D; {
   /* ... */
 
   plugins: [
     /* ... */
     new CleanWebpackPlugin(),  ],
 }
 Modules and Loaders
 webpack uses loaders to preprocess files loaded via modules. This can be JavaScript files, static assets like images and CSS styles, and compilers like TypeScript and Babel. webpack 5 has a few built-in loaders for assets as well.
 In your project you have an HTML file that loads and brings in some JavaScript, but it still doesn&#x27;t actually do anything. What are the main things we want this webpack config to do?
 
 Compile the latest and greatest JavaScript to a version the browser understands
 Import styles and compile SCSS into CSS
 Import images and fonts
 (Optional) Set up React or Vue
 
 First thing we&#x27;ll do is set up Babel to compile JavaScript.
 Babel (JavaScript)
 Babel is a tool that allows us to use tomorrow&#x27;s JavaScript, today.
 We&#x27;re going to set up a rule that checks for any .js file in the project (outside of node_modules) and uses the babel-loader to transpile. There are a few additional dependencies for Babel as well.
 
 babel-loader - Transpile files with Babel and webpack.
 @babel/core - Transpile ES2015+ to backwards compatible JavaScript
 @babel/preset-env - Smart defaults for Babel
 @babel/plugin-proposal-class-properties - An example of a custom Babel config (use properties directly on a class)
 
 npm i -D babel-loader @babel/core @babel/preset-env @babel/preset-env @babel/plugin-proposal-class-properties
 webpack.config.js
 module.exports &#x3D; {
   /* ... */
 
   module: {    rules: [      // JavaScript      {        test: /\.js$/,        exclude: /node_modules/,        use: [&#x27;babel-loader&#x27;],      },    ],  },}
 
 If you&#x27;re setting up a TypeScript project, you would use the typescript-loader instead of babel-loader for all your JavaScript transpiling needs. You would check for .ts files and use ts-loader.
 
 Now Babel is set up, but our Babel plugin is not. You can demonstrate it not working by adding an example pre-Babel code to index.js.
 src/index.js
 // Create a class property without a constructorclass Game {  name &#x3D; &#x27;Violin Charades&#x27;}const myGame &#x3D; new Game()// Create paragraph nodeconst p &#x3D; document.createElement(&#x27;p&#x27;)p.textContent &#x3D; &#x60;I like ${myGame.name}.&#x60;
 // Create heading node
 const heading &#x3D; document.createElement(&#x27;h1&#x27;)
 heading.textContent &#x3D; &#x27;Interesting!&#x27;
 
 // Append SVG and heading nodes to the DOM
 const app &#x3D; document.querySelector(&#x27;#root&#x27;)
 app.append(heading, p)
 ERROR in ./src/index.js
 Module build failed (from ./node_modules/babel-loader/lib/index.js):
 SyntaxError: /Users/you/webpack-tutorial/src/index.js: Support for the experimental syntax &#x27;classProperties&#x27; isn&#x27;t currently enabled (3:8):
 
   1 | // Create a class property without a constructor
   2 | class Game {
 &gt; 3 |   name &#x3D; &#x27;Violin Charades&#x27;
     |        ^
   4 | }
 To fix this, simply create a .babelrc file in the root of your project. This will add a lot of defaults with preset-env and the plugin we wanted with plugin-proposal-class-properties.
 .babelrc
 {
   &quot;presets&quot;: [&quot;@babel/preset-env&quot;],
   &quot;plugins&quot;: [&quot;@babel/plugin-proposal-class-properties&quot;]
 }
 Now another npm run build and everything will be all set.
 Images
 You&#x27;ll want to be able to import images directly into your JavaScript files, but that&#x27;s not something that JavaScript can do by default. To demonstrate, create src/images and add an image to it, then try to import it into your index.js file.
 src/index.js
 import example from &#x27;./images/example.png&#x27;
 
 /* ... */
 When you run a build, you&#x27;ll once again see an error:
 ERROR in ./src/images/example.png 1:0
 Module parse failed: Unexpected character &#x27;�&#x27; (1:0)
 You may need an appropriate loader to handle this file type, currently no loaders are configured to process this file. See https://webpack.js.org/concepts#loaders
 webpack has some built in asset modules you can use for static assets. For image types, we&#x27;ll use asset/resource. Note that this is a type and not a loader.
 webpack.config.js
 module.exports &#x3D; {
   /* ... */
   module: {
     rules: [
       // Images      {        test: /\.(?:ico|gif|png|jpg|jpeg)$/i,        type: &#x27;asset/resource&#x27;,      },    ],
   },
 }
 You&#x27;ll see the file got output to the dist folder after building.
 Fonts and inline
 webpack also has an asset module to inline some data, like svgs and fonts, using the asset/inline type.
 src/index.js
 import example from &#x27;./images/example.svg&#x27;
 
 /* ... */
 webpack.config.js
 module.exports &#x3D; {
   /* ... */
   module: {
     rules: [
       // Fonts and SVGs      {        test: /\.(woff(2)?|eot|ttf|otf|svg|)$/,        type: &#x27;asset/inline&#x27;,      },    ],
   },
 }
 Styles
 Using a style loader is necessary to be able to do something like import &#x27;file.css&#x27; in your scripts.
 A lot of people these days are using CSS-in-JS, styled-components, and other tools to bring styles into their JavaScript apps.
 Sometimes, just being able to load in a CSS file is sufficient. This website just has a single CSS file. Maybe you want to use PostCSS, which allows you to use all the latest CSS features in any browser. Or maybe you want to use Sass, the CSS preprocessor.
 I want to use all three - write in Sass, process in PostCSS, and compile to CSS. That involves bringing in a few loaders and dependencies.
 
 sass-loader - Load SCSS and compile to CSS
 
 node-sass - Node Sass
 
 
 postcss-loader - Process CSS with PostCSS
 
 postcss-preset-env - Sensible defaults for PostCSS
 
 
 css-loader - Resolve CSS imports
 style-loader - Inject CSS into the DOM
 
 npm i -D sass-loader postcss-loader css-loader style-loader postcss-preset-env node-sass
 Just like with Babel, PostCSS will require a config file, so make that and add it to the root.
 postcss.config.js
 module.exports &#x3D; {
   plugins: {
     &#x27;postcss-preset-env&#x27;: {
       browsers: &#x27;last 2 versions&#x27;,
     },
   },
 }
 In order to test out that Sass and PostCSS are working, I&#x27;ll make a src/styles/main.scss with Sass variables and a PostCSS example (lch).
 src/styles/main.scss
 $font-size: 1rem;
 $font-color: lch(53 105 40);
 
 html {
   font-size: $font-size;
   color: $font-color;
 }
 Now import the file in index.js and add the four loaders. They compile from last to first, so the last one you&#x27;ll want in the list is sass-loader as that needs to compile, then PostCSS, then CSS, and finally style-loader, which will inject the CSS into the DOM.
 src/index.js
 import &#x27;./styles/main.scss&#x27;
 
 /* ... */
 webpack.config.js
 module.exports &#x3D; {
   /* ... */
   module: {
     rules: [
       // CSS, PostCSS, and Sass      {        test: /\.(scss|css)$/,        use: [&#x27;style-loader&#x27;, &#x27;css-loader&#x27;, &#x27;postcss-loader&#x27;, &#x27;sass-loader&#x27;],      },    ],
   },
 }
 Now when you rebuild, you&#x27;ll notice the Sass and PostCSS has been applied.
 
 Note: This is a setup for development. For production, you will use MiniCssExtractPlugin instead of style-loader, which will export the CSS as a minified file. You can this in the webpack 5 boilerplate.
 
 Development
 Running npm run build every single time you make an update is tedious. The bigger your site gets, the longer it will take to build. You&#x27;ll want to set up two configurations for webpack:
 
 a production config, that minifies, optimizes and removes all source maps
 a development config, that runs webpack in a server, updates with every change, and has source maps
 
 Instead of building to a dist file, the development mode will just run everything in memory.
 To set up for development, you&#x27;ll install webpack-dev-server.
 
 webpack-dev-server - Development server for webpack
 
 npm i -D webpack-dev-server
 For demonstrative purposes, we can just add the development config to the current webpack.config.js file we&#x27;re building and test it out. However, you&#x27;ll want to create two config files: one with mode: production and one with mode: development. In the webpack 5 boilerplate, I demonstrate how to use webpack-merge to put all the base webpack config in one file, and any special development or production configs in a webpack.prod.js and webpack.dev.js files.
 const webpack &#x3D; require(&#x27;webpack&#x27;)
 
 module.exports &#x3D;  {
   /* ... */
   mode: &#x27;development&#x27;,
   devServer: {
     historyApiFallback: true,
     contentBase: path.resolve(__dirname, &#x27;./dist&#x27;),
     open: true,
     compress: true,
     hot: true,
     port: 8080,
   },
 
   plugins: [
     /* ... */
     // Only update what has changed on hot reload
     new webpack.HotModuleReplacementPlugin(),
   ],
 })
 We&#x27;re adding mode: development, and creating a devServer property. I&#x27;m setting a few defaults on it - the port will be 8080, it will automatically open a browser window, and uses hot-module-replacement, which requires the webpack.HotModuleReplacementPlugin plugin. This will allow modules to update without doing a complete reload of the page - so if you update some styles, just those styles will change, and you won&#x27;t need to reload the entirety of the JavaScript, which speeds up development a lot.
 Now you&#x27;ll use the webpack serve command to set up the server.
 package.json
 &quot;scripts&quot;: {
   &quot;start&quot;: &quot;webpack serve&quot;
 }
 npm start
 When you run this command, a link to localhost:8080 will automatically pop up in your browser. Now you can update Sass and JavaScript and watch it update on the fly.
 Conclusion
 That should help you get started with webpack. Once again, I&#x27;ve created a production-ready webpack 5 boilerplate, with Babel, Sass, PostCSS, production optimization, and a development server, that has everything from this article but goes into more details. From here, you can easily set up React, Vue, Typescript, or anything else you might want.
 
 webpack 5 boilerplate
 
 Check it out, play around with it, and enjoy!</content>
     </entry>
     <entry>
       <title>The -​-var: ; hack to toggle multiple values with one custom property</title>
         <link href="https://lea.verou.me/2020/10/the-var-space-hack-to-toggle-multiple-values-with-one-custom-property/"/>
       <updated>2020-10-12T15:06:57.000Z</updated>
       <content type="text">What if I told you you could use a single property value to turn multiple different values on and off across multiple different properties and even across multiple CSS rules?
 
 
 
 What if I told you you could turn this flat button into a glossy skeuomorphic button by just tweaking one custom property --is-raised, and that would set its border, background image, box and text shadows in one fell swoop?
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 How, you may ask?
 
 
 
 The crux of this technique is this: There are two custom property values that work almost everywhere there is a var() call with a fallback.
 
 
 
 The more obvious one that you probably already know is the initial value, which makes the property just apply its fallback. So, in the following code:
 
 
 
 background: var(--foo, linear-gradient(white, transparent)) hsl(220 10% 50%);
 border: 1px solid var(--foo, rgb(0 0 0 / .1));
 color: rgb(0 0 0 var(--foo, / .8));
 
 
 
 We can set --foo to initial to enable these “fallbacks” and append these values to the property value, adding a gradient, setting a border-color, and making the text color translucent in one go. But what to do when we want to turn these values off? Any non-initial value for --foo (that doesn’t create cycles) should work. But is there one that works in all three declarations?
 
 
 
 It turns out there is another value that works everywhere, in every property a var() reference is present, and you’d likely never guess what it is (unless you have watched any of my CSS variable talks and have a good memory for passing mentions of things).
 
 
 
 Intrigued?
 
 
 
 It’s whitespace! Whitespace is significant in a custom property. When you write something like this:
 
 
 
 --foo: ;
 
 
 
 This is not an invalid declaration. This is a declaration where the value of --foo is literally one space character. However, whitespace is valid in every CSS property value, everywhere a var() is allowed, and does not affect its computed value in any way. So, we can just set our property to one space (or even a comment) and not affect any other value present in the declaration. E.g. this:
 
 
 
 --foo: ;
 background: var(--foo, linear-gradient(white, transparent)) hsl(220 10% 50%);
 
 
 
 produces the same result as:
 
 
 
 background: hsl(220 10% 50%);
 
 
 
 We can take advantage of this to essentially turn var() into a single-clause if() function and conditionally append values based on a single custom property.
 
 
 
 As a proof of concept, here is the two button demo refactored using this approach:
 
 
 
 
 
 
 
 
 
 
 
 Limitations
 
 
 
 I originally envisioned this as a building block for a technique horrible hack to enable “mixins” in the browser, since @apply is now defunct. However, the big limitation is that this only works for appending values to existing values — or setting a property to either a whole value or initial. There is no way to say “the background should be red if --foo is set and white otherwise”. Some such conditionals can be emulated with clever use of appending, but not most. 
 
 
 
 And of course there’s a certain readability issue: --foo: ; looks like a mistake and --foo: initial looks pretty weird, unless you’re aware of this technique. However, that can easily be solved with comments. Or even constants: 
 
 
 
 :root {
 	--ON: initial;
 	--OFF: ;
 }
 
 button {
 	--is-raised: var(--OFF);
 	/* ... */
 }
 
 #foo {
 	--is-raised: var(--ON);
 }
 
 
 
 Also do note that eventually we will get a proper if() and won’t need such horrible hacks to emulate it, discussions are already underway [w3c/csswg-drafts#5009 w3c/csswg-drafts#4731].
 
 
 
 So what do you think? Horrible hack, useful technique, or both? 
 
 
 
 Prior art
 
 
 
 Turns out this was independently discovered by two people before me:
 
 
 
 First, the brilliant Ana Tudor circa 2017Then James0x57 in April 2020
 
 
 
 And it was called “space toggle hack” in case you want to google it!</content>
     </entry>
     <entry>
       <title>WDRL — Edition 284: AVIF, Lots of CSS and our own Soil.</title>
         <link href="https://wdrl.info/archive/284"/>
       <updated>2020-10-09T08:00:00.000Z</updated>
       <content type="text">Hey,
 
 
 “You can run-away, run-away, run-away… you gotta face this… time is running fast…” — Snoop Lion in his song ‘Rebel Way’
 
 We can apply this quote to a lot of matters in our lives right now. Our time to do something for ourselves, for our relatives, our animals, our planet is running fast. And whether it’s about voting, about the actions to fight a pendemic or to fight to get back our human rights and freedom in face of country lock-downs, or in the fight against global warming which is getting harder each year we don’t do enough.
 Have we considered the impact of our current system to develop, deploy, test and run our Internet services on the planet? How much energy is used to create all these Docker images, to run all the VMs and hardware machines that are just there to run Continuous Integration systems and get rebuilt from scratch each time we push a little piece of code? Now considering that no provider is yet powering their server clusters completely CO2 neutral, not event trying to count the energy and sources we need to build all the hardware in there, this suddenly shines in a not so bright light. At the same time we destroy the biggest value we have as humans: Our soil. By using farming techniques, chemicals that destroy the soil long-term and building more and more houses and business units on the soil, we destroy our ability to produce food that keeps us living. And then we face a pandemic and wonder why… it’s time to act. Not just to watch another video stream. To reduce our impact, to reduce our energy consumption, our car drives, our use of chemicals that hurt ourselves. And instead, we should focus on what’s doing good to us… nature, calmness, friendship, love, kindness.
 News
 
 	Postgres 13 was released and brings performance benefits, application development conveniences such as native UUIDv4 generation, or datetime() support in the JSON path. Here’s an in-depth review including code how to use the new things in v13.
 
 Generic
 
 	Steve Messer explains us why his website is killing the planet — or is it really? We can do something.
 
 UI/UX
 
 	Chris Coyier shows us how to offer options for mailto: and tel: links in a custom popup modal.
 	System UIcons is a set of icons under Unlicense so you can use without attribution for free (but it’s even cooler if you add an attribution).
 
 Tooling
 
 	umami is a simple self-hosted open web analytics platform.
 
 Security
 
 	terjanq provides us with a collection of short XSS payloads that can be used in different contexts. Pretty cool to do some security testing on our applications.
 
 Privacy
 
 	The browser Chrome is changing the default Referrer-Policy and is now using strict-origin-when-cross-origin by default. That will be for the benefit of the users’ privacy. Of course, if you want to enable another type of policy, you can still do.
 
 Web Performance
 
 	Infinite scrolling is used a lot across the web, especially for item listings. Now the key for this cool way to ease a flow for the user is to implement this right and without layout shift or other glitches. Addy Osmani shows how to avoid layout shifts in infinite scolling. And don’t forget to update the URL to the next paging so if something goes wrong and the user needs to reload the page, they see the same state again.
 	AVIF is the new hot image format that we can use on the web. Finally, it’s a standard format that’s going to be implemented by probably all browsers quite quickly. Chrome already has support on Desktop and it’s coming to Android soon, Firefox is currently implementing it and Apple is part of the AV1 working group so they’ll probably add it as well soon. Anyway, you can already use the new image format by using the picture element and content negotiation methods as we do it for WebP or other formats already. But why AVIF? Well, it’s a standard that is accepted by many major software companies and it’s really good in compressing images while still looking good. In many cases it’s much better than WebP, and it’s really good for not only photos but also illustrations.
 
 Accessibility
 
 	Microsoft had it for years already as proprietary solution on their platform, now Mozilla added support for the standard Media Query prefers-contrast in Firefox. This means that some colors are enforced by the browser to ensure better contrast and now we can serve high contrast modes of our websites to more users.
 	Dave Rupert appeals for writing alt attribute values like paragraphs so in case of an image loading failure or a screen reader, the user still gets a really good description of what’s to be seen here. This is good, I do this a lot already but one thing to say is that it’s still an attribute so we can’t really write more than one paragraph there. Alternatively, you can still use figure and figcaption elements for this use case and just leave the img element’s alt attribute empty. Never omit the attribute, please, by the way.
 
 JavaScript
 
 	inclusive-dates is a really well made human-friendly datepicker that’s also fully accessible.
 
 CSS
 
 	Ana Tudor shows the future of CSS masonry solutions with CSS Grid. But as support is very limited and yet to be implemented by many browsers, we need a fallback which Ana shows as well in the tutorial.
 	Wonder how to make a gradient colored text nowadays? Serving it as an image or SVG isn’t necessary, Adam Argyle shows us in a Codepen how it can be solved using background-clip and other CSS properties.
 	While we all use box-shadow a lot, the lesser known drop-shadow property is a really cool one: it creates a shadow around the element’s clippings instead of their box-model. Maybe you remember how to create triangles with just some  borders. Using drop-shadow let’s us define a shadow on such elements.
 	Adam Argyle and Oriol Brufau share how to make custom bullets in a HTML list using the CSS ::marker pseudo-element selectors. This is good because it’s standardizing things we used to hack since years.
 	Stephanie Eckles with a new great tutorial how to apply a custom look to native HTML input fields with the latest methods.
 	Linearly scale font-size with CSS clamp() based on the viewport. The solution we looked for since a long time is now available in browsers.
 	Ahmad Shadeed explains the mindset we should use when coding CSS: Just in case.
 
 Work &amp; Life
 
 	Caring about the climate but don’t find things to help? Climatebase is a platform that offers events and jobs to find in the fight against climate change.
 	Matthias Ott with some simple, honest words on why the change of climate affects us, directly.
 
 Go beyond…
 
 	You remember I posted a link to a website that’s entirely powered by Solar? Here’s the follow-up that shares how sustainable the project is. Well, crazy how unsustainable technology most of the time is, right?
 	Nigeria′s Bayo Akomolafe: “We aren′t ′in control′ of climate crisis” This interesting read gives us some insight into other cultures and how we could treat our nature and living environment differently to how we’re used to do right now.
 	Why soil matters. A good summary of why we should take more care on the soil we live on, we live from. I myself am going to hold workshops and talks beginning next year here in southern Germany about solutions how to take care of soil, how to restore it, preserve it and bind CO2 in the atmosphere in the soil.
 
 
 Thank you all for reading this, I hope you’re doing fine and stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>The failed promise of Web Components</title>
         <link href="https://lea.verou.me/2020/09/the-failed-promise-of-web-components/"/>
       <updated>2020-09-24T16:28:03.000Z</updated>
       <content type="text">Web Components had so much potential to empower HTML to do more, and make web development more accessible to non-programmers and easier for programmers. Remember how exciting it was every time we got new shiny HTML elements that actually do stuff? Remember how exciting it was to be able to do sliders, color pickers, dialogs, disclosure widgets straight in the HTML, without having to include any widget libraries?
 
 
 
 The promise of Web Components was that we’d get this convenience, but for a much wider range of HTML elements, developed much faster, as nobody needs to wait for the full spec + implementation process. We’d just include a script, and boom, we have more elements at our disposal!
 
 
 
 Or, that was the idea. Somewhere along the way, the space got flooded by JS frameworks aficionados, who revel in complex APIs, overengineered build processes and dependency graphs that look like the roots of a banyan tree.
 
 
 
 This is what the roots of a Banyan tree look like. Photo by David Stanley on Flickr (CC-BY). 
 
 
 
 
 
 
 
 Perusing the components on webcomponents.org fills me with anxiety, and I’m perfectly comfortable writing JS — I write JS for a living! What hope do those who can’t write JS have? Using a custom element from the directory often needs to be preceded by a ritual of npm flugelhorn, import clownshoes, build quux, all completely unapologetically because “here is my truckload of dependencies, yeah, what”. Many steps are even omitted, likely because they are “obvious”. Often, you wade through the maze only to find the component doesn’t work anymore, or is not fit for your purpose.
 
 
 
 Besides setup, the main problem is that HTML is not treated with the appropriate respect in the design of these components. They are not designed as closely as possible to standard HTML elements, but expect JS to be written for them to do anything. HTML is simply treated as a shorthand, or worse, as merely a marker to indicate where the element goes in the DOM, with all parameters passed in via JS. I recall a wonderful talk by Jeremy Keith a few years ago about this very phenomenon, where he discussed this e-shop Web components demo by Google, which is the poster child of this practice. These are the entire contents of its &lt;body&gt; element:
 
 
 
 &lt;body&gt;
 	&lt;shop-app unresolved&#x3D;&quot;&quot;&gt;SHOP&lt;/shop-app&gt;
 	&lt;script src&#x3D;&quot;node_assets/@webcomponents/webcomponentsjs/webcomponents-loader.js&quot;&gt;&lt;/script&gt;
 	&lt;script type&#x3D;&quot;module&quot; src&#x3D;&quot;src/shop-app.js&quot;&gt;&lt;/script&gt;
 	&lt;script&gt;window.performance&amp;&amp;performance.mark&amp;&amp;performance.mark(&quot;index.html&quot;);&lt;/script&gt;
 &lt;/body&gt;
 
 
 
 If this is how Google is leading the way, how can we hope for contributors to design components that follow established HTML conventions?
 
 
 
 Jeremy criticized this practice from the aspect of backwards compatibility: when JS is broken or not enabled, or the browser doesn’t support Web Components, the entire website is blank. While this is indeed a serious concern, my primary concern is one of usability: HTML is a lower barrier to entry language. Far more people can write HTML than JS. Even for those who do eventually write JS, it often comes after spending years writing HTML &amp; CSS.
 
 
 
 If components are designed in a way that requires  JS, this excludes thousands of people from using them. And even for those who can write JS, HTML is often easier: you don’t see many people rolling their own sliders or using JS-based ones once &lt;input type&#x3D;&quot;range&quot;&gt; became widely supported, right?
 
 
 
 Even when JS is unavoidable, it’s not black and white. A well designed HTML element can reduce the amount and complexity of JS needed to a minimum. Think of the &lt;dialog&gt; element: it usually does require *some* JS, but it’s usually rather simple JS. Similarly, the &lt;video&gt; element is perfectly usable just by writing HTML, and has a comprehensive JS API for anyone who wants to do fancy custom things.
 
 
 
 The other day I was looking for a simple, dependency free, tabs component. You know, the canonical example of something that is easy to do with Web Components, the example 50% of tutorials mention. I didn’t even care what it looked like, it was for a testing interface. I just wanted something that is small and works like a normal HTML element. Yet, it proved so hard I ended up writing my own!
 
 
 
 Can we fix this?
 
 
 
 I’m not sure if this is a design issue, or a documentation issue. Perhaps for many of these web components, there are easier ways to use them. Perhaps there are vanilla web components out there that I just can’t find. Perhaps I’m looking in the wrong place and there is another directory somewhere with different goals and a different target audience. 
 
 
 
 But if not, and if I’m not alone in feeling this way, we need a directory of web components with strict inclusion criteria:
 
 
 
 Plug and play. No dependencies, no setup beyond including one &lt;script&gt; tag. If a dependency is absolutely needed (e.g. in a map component it doesn’t make sense to draw your own maps), the component loads it automatically if it’s not already loaded.Syntax and API follows conventions established by built-in HTML elements and anything that can be done without the component user writing JS, is doable without JS, per the W3C principle of least power.Accessible by default via sensible ARIA defaults, just like normal HTML elements.Themable via ::part(), selective inheritance and custom properties. Very minimal style by default. Normal CSS properties should just “work” to the the extent possible.Only one component of a given type in the directory, that is flexible and extensible and continuously iterated on and improved by the community. Not 30 different sliders and 15 different tabs that users have to wade through. No branding, no silos of “component libraries”. Only elements that are designed as closely as possible to what a browser would implement in every way the current technology allows.
 
 
 
 I would be up for working on this if others feel the same way, since that is not a project for one person to tackle. Who’s with me?
 
 
 
 UPDATE: Wow this post blew up! Thank you all for your interest in participating in a potential future effort. I’m currently talking to stakeholders of some of the existing efforts to see if there are any potential collaborations before I go off and create a new one. Follow me on Twitter to hear about the outcome!</content>
     </entry>
     <entry>
       <title>Developer priorities throughout their career</title>
         <link href="https://lea.verou.me/2020/09/developer-priorities-throughout-their-career/"/>
       <updated>2020-09-16T17:17:32.000Z</updated>
       <content type="text">I made this chart in the amazing Excalidraw about two weeks ago: 
 
 
 
 
 
 
 
 It only took me 10 minutes! Shortly after, my laptop broke down into repeated kernel panics, and it spent about 10 days in service (I was in a remote place when it broke, so it took some time to get it to service). Yesterday, I was finally reunited with it, turned it on, launched Chrome, and saw it again. It gave me a smile, and I realized I never got to post it, so I tweeted this:
 
 
 
 
 Developer priorities throughout their career pic.twitter.com/juF4peKXCx— Lea Verou (@LeaVerou) September 15, 2020
 
 
 
 
 The tweet kinda blew up! It seems many, many developers identify with it. A few also disagreed with it, especially with the “Does it actually work?” line. So I figured I should write a bit about the rationale behind it. I originally wrote it in a tweet, but then I realized I should probably post it in a less transient medium, that is more well suited to longer text.
 
 
 
 
 
 
 
 When somebody starts coding, getting the code to work is already difficult enough, so there is no space for other priorities. Learning to formalize one’s thought to the degree a computer demands, and then serialize this thinking with an unforgiving syntax, is hard. Writing code that works is THE priority, and whether it’s good code is not even a consideration.
 
 
 
 For more experienced programmers, whether it works is ephemeral: today it works, tomorrow a commit causes a regression, the day after another commit fixes it (yes, even with TDD. No testsuite gets close to 100% coverage). Whereas readability &amp; maintainability do not fluctuate much. If they are not prioritized from the beginning, they are much harder to accomplish when you already have a large codebase full of technical debt.
 
 
 
 Code written by experienced programmers that doesn’t work, can often be fixed with hours or days of debugging. A nontrivial codebase that is not readable can take months or years to rewrite. So one tends to gravitate towards prioritizing what is easier to fix.
 
 
 
 The “peak of drought” and other over-abstractions 
 
 
 
 Many developers identified with the “peak of drought”. Indeed, like other aspects of maintainability, DRY is not even a concern at first. At some point, a programmer learns about the importance of DRY and gradually begins abstracting away duplication. However, you can have too much of a good thing: soon the need to abstract away any duplication becomes all consuming and leads to absurd, awkward abstractions which actually get in the way and produce needless couplings, often to avoid duplicating very little code, once. In my own “peak of drought” (which lasted far longer than the graph above suggests), I’ve written many useless functions, with parameters that make no sense, just to avoid duplicating a few lines of code once. 
 
 
 
 Many articles have been written about this phenomenon, so I’m not going to repeat their arguments here. As a programmer accumulates even more experience, they start seeing the downsides of over-abstraction and over-normalization and start favoring a more moderate approach which prioritizes readability over DRY when they are at odds. 
 
 
 
 A similar thing happens with design patterns too. At some point, a few years in, a developer reads a book or takes a course about design patterns. Soon thereafter, their code becomes so littered with design patterns that it is practically incomprehensible. “When all you have is a hammer, everything looks like a nail”. I have a feeling that Java and Java-like languages are particularly accommodating to this ailment, so this phenomenon tends to proliferate in their codebases. At some point, the developer has to go back to their past code, and they realize themselves that it is unreadable. Eventually, they learn to use design patterns when they are actually useful, and favor readability over design patterns when the two are at odds. 
 
 
 
 What aspects of your coding practice have changed over the years? How has your perspective shifted? What mistakes of the past did you eventually realize?</content>
     </entry>
     <entry>
       <title>Understanding the Event Loop, Callbacks, Promises, and Async/Await in JavaScript</title>
         <link href="https://www.taniarascia.com/asynchronous-javascript-event-loop-callbacks-promises-async-await/"/>
       <updated>2020-09-10T00:00:00.000Z</updated>
       <content type="text">This article was originally written for DigitalOcean.
 Introduction
 In the early days of the internet, websites often consisted of static data in an HTML page. But now that web applications have become more interactive and dynamic, it has become increasingly necessary to do intensive operations like make external network requests to retrieve API data. To handle these operations in JavaScript, a developer must use asynchronous programming techniques.
 Since JavaScript is a single-threaded programming language with a synchronous execution model that proccesses one operation after another, it can only process one statement at a time. However, an action like requesting data from an API can take an indeterminate amount of time, depending on the size of data being requested, the speed of the network connection, and other factors. If API calls were performed in a synchronous manner, the browser would not be able to handle any user input, like scrolling or clicking a button, until that operation completes. This is known as blocking.
 In order to prevent blocking behavior, the browser environment has many Web APIs that JavaScript can access that are asynchronous, meaning they can run in parallel with other operations instead of sequentially. This is useful because it allows the user to continue using the browser normally while the asynchronous operations are being processed.
 As a JavaScript developer, you need to know how to work with asynchronous Web APIs and handle the response or error of those operations. In this article, you will learn about the event loop, the original way of dealing with asynchronous behavior through callbacks, the updated ECMAScript 2015 addition of promises, and the modern practice of using async/await.
 
 Note: This article is focused on client-side JavaScript in the browser environment. The same concepts are generally true in the Node.js environment, however Node.js uses its own C++ APIs as opposed to the browser&#x27;s Web APIs. For more information on asynchronous programming in Node.js, check out How To Write Asynchronous Code in Node.js.
 
 Contents
 
 The Event Loop
 Callback Functions
 
 Nested Callbacks and the Pyramid of Doom
 
 
 Promises
 Async Functions with async/await
 
 The Event Loop
 This section will explain how JavaScript handles asynchronous code with the event loop. It will first run through a demonstration of the event loop at work, and will then explain the two elements of the event loop: the stack and the queue.
 JavaScript code that does not use any asynchronous Web APIs will execute in a synchronous manner—one at a time, sequentially. This is demonstrated by this example code that calls three functions that each print a number to the console:
 // Define three example functions
 function first() {
   console.log(1)
 }
 
 function second() {
   console.log(2)
 }
 
 function third() {
   console.log(3)
 }
 In this code, you define three functions that print numbers with console.log().
 Next, write calls to the functions:
 // Execute the functions
 first()
 second()
 third()
 The output will be based on the order the functions were called: first(), second(), then third().
 1
 2
 3
 When an asynchronous Web API is used, the rules become more complicated. A built-in API that you can test this with is setTimeout, which sets a timer and performs an action after a specified amount of time. setTimeout needs to be asynchronous, otherwise the entire browser would remain frozen during the waiting, which would result in a poor user experience.
 Add setTimeout to the second function to simulate an asynchronous request:
 // Define three example functions, but one of them contains asynchronous code
 function first() {
   console.log(1)
 }
 
 function second() {
   setTimeout(() &#x3D;&gt; {
     console.log(2)
   }, 0)
 }
 
 function third() {
   console.log(3)
 }
 setTimeout takes two arguments: the function it will run asynchronously, and the amount of time it will wait before calling that function. In this code you wrapped console.log in an anonymous function and passed it to setTimeout, then set the function to run after 0 milliseconds.
 Now call the functions, as you did before:
 // Execute the functions
 first()
 second()
 third()
 You might expect with a setTimeout set to 0 that running these three functions would still result in the numbers being printed in sequential order. But because it is asynchronous, the function with the timeout will be printed last:
 1
 3
 2
 Whether you set the timeout to zero seconds or five minutes will make no difference—the console.log called by asynchronous code will execute after the synchronous top-level functions. This happens because the JavaScript host environment, in this case the browser, uses a concept called the event loop to handle concurrency, or parallel events. Since JavaScript can only execute one statement at a time, it needs the event loop to be informed of when to execute which specific statement. The event loop handles this with the concepts of a stack and a queue.
 Stack
 The stack, or call stack, holds the state of what function is currently running. If you&#x27;re unfamiliar with the concept of a stack, you can imagine it as an array with &quot;Last in, first out&quot; (LIFO) properties, meaning you can only add or remove items from the end of the stack. JavaScript will run the current frame (or function call in a specific environment) in the stack, then remove it and move on to the next one.
 For the example only containing synchronous code, the browser handles the execution in the following order:
 
 Add first() to the stack, run first() which logs 1 to the console, remove first() from the stack.
 Add second() to the stack, run second() which logs 2 to the console, remove second() from the stack.
 Add third() to the stack, run third() which logs 3 to the console, remove third() from the stack.
 
 The second example with setTimout looks like this:
 
 Add first() to the stack, run first() which logs 1 to the console, remove first() from the stack.
 Add second() to the stack, run second().
 
 Add setTimeout() to the stack, run the setTimeout() Web API which starts a timer and adds the anonymous function to the queue, remove setTimeout() from the stack.
 
 
 Remove second() from the stack.
 Add third() to the stack, run third() which logs 3 to the console, remove third() from the stack.
 The event loop checks the queue for any pending messages and finds the anonymous function from setTimeout(), adds the function to the stack which logs 2 to the console, then removes it from the stack.
 
 Using setTimeout, an asynchronous Web API, introduces the concept of the queue, which this tutorial will cover next.
 Queue
 The queue, also referred to as message queue or task queue, is a waiting area for functions. Whenever the call stack is empty, the event loop will check the queue for any waiting messages, starting from the oldest message. Once it finds one, it will add it to the stack, which will execute the function in the message.
 In the setTimeout example, the anonymous function runs immediately after the rest of the top-level execution, since the timer was set to 0 seconds. It&#x27;s important to remember that the timer does not mean that the code will execute in exactly 0 seconds or whatever the specified time is, but that it will add the anonymous function to the queue in that amount of time. This queue system exists because if the timer were to add the anonymous function directly to the stack when the timer finishes, it would interrupt whatever function is currently running, which could have unintended and unpredictable effects.
 
 Note: There is also another queue called the job queue or microtask queue that handles promises. Microtasks like promises are handled at a higher priority than macrotasks like setTimeout.
 
 Now you know how the event loop uses the stack and queue to handle the execution order of code. The next task is to figure out how to control the order of execution in your code. To do this, you will first learn about the original way to ensure asynchrnous code is handled correctly by the event loop: callback functions.
 Callback Functions
 In the setTimeout example, the function with the timeout ran after everything in the main top-level execution context. But if you wanted to ensure one of the functions, like the third function, ran after the timeout, then you would have to use asynchronous coding methods. The timeout here can represent an asynchronous API call that contains data. You want to work with the data from the API call, but you have to make sure the data is returned first.
 The original solution to dealing with this problem is using callback functions. Callback functions do not have special syntax; they are just a function that has been passed as an argument to another function. The function that takes another function as an argument is called a higher-order function. According to this definition, any function can become a callback function if it is passed as an argument. Callbacks are not asynchronous by nature, but can be used for asynchronous purposes.
 Here is a syntactic code example of a higher-order function and a callback:
 // A function
 function fn() {
   console.log(&#x27;Just a function&#x27;)
 }
 
 // A function that takes another function as an argument
 function higherOrderFunction(callback) {
   // When you call a function that is passed as an argument, it is referred to as a callback
   callback()
 }
 
 // Passing a function
 higherOrderFunction(fn)
 In this code, you define a function fn, define a function higherOrderFunction that takes a function callback as an argument, and pass fn as a callback to higherOrderFunction.
 Running this code will give the following:
 Just a function
 Let&#x27;s go back to the first, second, and third functions with setTimeout. This is what you have so far:
 function first() {
   console.log(1)
 }
 
 function second() {
   setTimeout(() &#x3D;&gt; {
     console.log(2)
   }, 0)
 }
 
 function third() {
   console.log(3)
 }
 The task is to get the third function to always delay execution until after the asynchronous action in the second function has completed. This is where callbacks come in. Instead of executing first, second, and third at the top-level of execution, you will pass the third function as an argument to second. The second function will execute the callback after the asynchronous action has completed.
 Here are the three functions with a callback applied:
 // Define three functions
 function first() {
   console.log(1)
 }
 
 function second(callback) {  setTimeout(() &#x3D;&gt; {
     console.log(2)
 
     // Execute the callback function
     callback()  }, 0)
 }
 
 function third() {
   console.log(3)
 }
 Now, execute first and second, then pass third as an argument to second:
 first()
 second(third)
 After running this code block, you will receive the following output:
 1
 2
 3
 First 1 will print, and after the timer completes (in this case, zero seconds, but you can change it to any amount) it will print 2 then 3. By passing a function as a callback, you&#x27;ve successfully delayed execution of the function until the asynchronous Web API (setTimeout) completes.
 The key takeaway here is that callback functions are not asynchronous—setTimeout is the asynchronous Web API responsible for handling asynchronous tasks. The callback just allows you to be informed of when an asynchronous task has completed and handles the success or failure of the task.
 Now that you have learned how to use callbacks to handle asynchronous tasks, the next section explains the problems of nesting too many callbacks and creating a &quot;pyramid of doom.&quot;
 Nested Callbacks and the Pyramid of Doom
 Callback functions are an effective way to ensure delayed execution of a function until another one completes and returns with data. However, due to the nested nature of callbacks, code can end up getting messy if you have a lot of consecutive asynchronous requests that rely on each other. This was a big frustration for JavaScript developers early on, and as a result code containing nested callbacks is often called the &quot;pyramid of doom&quot; or &quot;callback hell.&quot;
 Here is a demonstration of nested callbacks:
 function pyramidOfDoom() {
   setTimeout(() &#x3D;&gt; {
     console.log(1)
     setTimeout(() &#x3D;&gt; {
       console.log(2)
       setTimeout(() &#x3D;&gt; {
         console.log(3)
       }, 500)
     }, 2000)
   }, 1000)
 }
 In this code, each new setTimeout is nested inside a higher order function, creating a pyramid shape of deeper and deeper callbacks. Running this code would give the following:
 1
 2
 3
 In practice, with real world asynchronous code, this can get much more complicated. You will most likely need to do error handling in asynchronous code, and then pass some data from each response onto the next request. Doing this with callbacks will make your code difficult to read and maintain.
 Here is a runnable example of a more realistic &quot;pyramid of doom&quot; that you can play around with:
 // Example asynchronous function
 function asynchronousRequest(args, callback) {
   // Throw an error if no arguments are passed
   if (!args) {
     return callback(new Error(&#x27;Whoa! Something went wrong.&#x27;))
   } else {
     return setTimeout(
       // Just adding in a random number so it seems like the contrived asynchronous function
       // returned different data
       () &#x3D;&gt; callback(null, { body: args + &#x27; &#x27; + Math.floor(Math.random() * 10) }),
       500
     )
   }
 }
 
 // Nested asynchronous requests
 function callbackHell() {
   asynchronousRequest(&#x27;First&#x27;, function first(error, response) {
     if (error) {
       console.log(error)
       return
     }
     console.log(response.body)
     asynchronousRequest(&#x27;Second&#x27;, function second(error, response) {
       if (error) {
         console.log(error)
         return
       }
       console.log(response.body)
       asynchronousRequest(null, function third(error, response) {
         if (error) {
           console.log(error)
           return
         }
         console.log(response.body)
       })
     })
   })
 }
 
 // Execute
 callbackHell()
 In this code, you must make every function account for a possible response and a possible error, making the function callbackHell visually confusing.
 Running this code will give you the following:
 First 9
 Second 3
 Error: Whoa! Something went wrong.
     at asynchronousRequest (&lt;anonymous&gt;:4:21)
     at second (&lt;anonymous&gt;:29:7)
     at &lt;anonymous&gt;:9:13
 This way of handling asynchronous code is difficult to follow. As a result, the concept of promises was introduced in ES6. This is the focus of the next section.
 Promises
 A promise represents the completion of an asynchronous function. It is an object that might return a value in the future. It accomplishes the same basic goal as a callback function, but with many additional features and a more readable syntax. As a JavaScript developer, you will likely spend more time consuming promises than creating them, as it is usually asynchronous Web APIs that return a promise for the developer to consume. This tutorial will show you how to do both.
 Creating a Promise
 You can initialize a promise with the new Promise syntax, and you must initialize it with a function. The function that gets passed to a promise has resolve and reject parameters. The resolve and reject functions handle the success and failure of an operation, respectively.
 Write the following line to declare a promise:
 // Initialize a promise
 const promise &#x3D; new Promise((resolve, reject) &#x3D;&gt; {})
 If you inspect the initialized promise in this state with your web browser&#x27;s console, you will find it has a pending status and undefined value:
 __proto__: Promise
 [[PromiseStatus]]: &quot;pending&quot;
 [[PromiseValue]]: undefined
 So far, nothing has been set up for the promise, so it&#x27;s going to sit there in a pending state forever. The first thing you can do to test out a promise is fulfill the promise by resolving it with a value:
 const promise &#x3D; new Promise((resolve, reject) &#x3D;&gt; {
   resolve(&#x27;We did it!&#x27;)})
 Now, upon inspecting the promise, you&#x27;ll find that it has a status of fulfilled, and a value set to the value you passed to resolve:
 __proto__: Promise
 [[PromiseStatus]]: &quot;fulfilled&quot;
 [[PromiseValue]]: &quot;We did it!&quot;
 As stated in the beginning of this section, a promise is an object that may return a value. After being successfully fulfilled, the value goes from undefined to being populated with data.
 A promise can have three possible states: pending, fulfilled, and rejected.
 
 Pending - Initial state before being resolved or rejected
 Fulfilled - Successful operation, promise has resolved
 Rejected - Failed operation, promise has rejected
 
 After being fulfilled or rejected, a promise is settled.
 Now that you have an idea of how promises are created, let&#x27;s look at how a developer may consume these promises.
 Consuming a Promise
 The promise in the last section has fulfilled with a value, but you also want to be able to access the value. Promises have a method called then that will run after a promise reaches resolve in the code. then will return the promise&#x27;s value as a parameter.
 This is how you would return and log the value of the example promise:
 promise.then((response) &#x3D;&gt; {
   console.log(response)
 })
 The promise you created had a [[PromiseValue]] of We did it!. This value is what will be passed into the anonymous function as response:
 We did it!
 So far, the example you created did not involve an asynchronous Web API—it only explained how to create, resolve, and consume a native JavaScript promise. Using setTimeout, you can test out an asynchronous request.
 The following code simulates data returned from an asynchronous request as a promise:
 const promise &#x3D; new Promise((resolve, reject) &#x3D;&gt; {
   setTimeout(() &#x3D;&gt; resolve(&#x27;Resolving an asynchronous request!&#x27;), 2000)
 })
 
 // Log the result
 promise.then((response) &#x3D;&gt; {
   console.log(response)
 })
 Using the then syntax ensures that the response will be logged only when the setTimeout operation is completed after 2000 milliseconds. All this is done without nesting callbacks.
 Now after two seconds, it will resolve the promise value and it will get logged in then:
 Resolving an asynchronous request!
 Promises can also be chained to pass along data to more than one asynchronous operation. If a value is returned in then, another then can be added that will fulfill with the return value of the previous then:
 // Chain a promise
 promise
   .then((firstResponse) &#x3D;&gt; {
     // Return a new value for the next then
     return firstResponse + &#x27; And chaining!&#x27;
   })
   .then((secondResponse) &#x3D;&gt; {
     console.log(secondResponse)
   })
 The fulfilled response in the second then will log the return value:
 Resolving an asynchronous request! And chaining!
 Since then can be chained, it allows the consumption of promises to appear more synchronous than callbacks, as they do not need to be nested. This will allow for more readable code that can be maintained and verified easier.
 Error Handling
 So far, you have only handled a promise with a successful resolve, which puts the promise in a fulfilled state. But frequently with an asynchronous request you also have to handle an error—if the API is down, or a malformed or unauthorized request is sent. A promise should be able to handle both cases. In this section, you will create a function to test out both the success and error case of creating and consuming a promise.
 This getUsers function will pass a flag to a promise, and return the promise.
 function getUsers(onSuccess) {
   return new Promise((resolve, reject) &#x3D;&gt; {
     setTimeout(() &#x3D;&gt; {
       // Handle resolve and reject in the asynchronous API
     }, 1000)
   })
 }
 Set up the code so that if onSuccess is true, the timeout will fulfill with some data. If false, the function will reject with an error.
 function getUsers(onSuccess) {
   return new Promise((resolve, reject) &#x3D;&gt; {
     setTimeout(() &#x3D;&gt; {
       // Handle resolve and reject in the asynchronous API
       if (onSuccess) {        resolve([          { id: 1, name: &#x27;Jerry&#x27; },          { id: 2, name: &#x27;Elaine&#x27; },          { id: 3, name: &#x27;George&#x27; },        ])      } else {        reject(&#x27;Failed to fetch data!&#x27;)      }    }, 1000)  })
 }
 For the successful result, you return JavaScript objects that represent sample user data.
 In order to handle the error, you will use the catch instance method. This will give you a failure callback with the error as a parameter.
 Run the getUser command with onSuccess set to false, using the then method for the success case and the catch method for the error:
 // Run the getUsers function with the false flag to trigger an error
 getUsers(false)
   .then((response) &#x3D;&gt; {
     console.log(response)
   })
   .catch((error) &#x3D;&gt; {
     console.error(error)
   })
 Since the error was triggered, the then will be skipped and the catch will handle the error:
 Failed to fetch data!
 If you switch the flag and resolve instead, the catch will be ignored, and the data will return instead.
 // Run the getUsers function with the true flag to resolve successfully
 getUsers(true)
   .then((response) &#x3D;&gt; {
     console.log(response)
   })
   .catch((error) &#x3D;&gt; {
     console.error(error)
   })
 This will yield the user data:
 (3) [{…}, {…}, {…}]
 0: {id: 1, name: &quot;Jerry&quot;}
 1: {id: 2, name: &quot;Elaine&quot;}
 3: {id: 3, name: &quot;George&quot;}
 For reference, here is a table with the handler methods on Promise objects:
 
 
 
 Method
 Description
 
 
 
 
 then()
 Handles a resolve. Returns a promise, and calls onFulfilled function asynchronously
 
 
 catch()
 Handles a reject. Returns a promise, and calls onRejected function asynchronously
 
 
 finally()
 Called when a promise is settled. Returns a promise, and calls onFinally function asynchronously
 
 
 
 Promises can be confusing, both for new developers and experienced programmers that have never worked in an asynchronous environment before. However as mentioned, it is much more common to consume promises than create them. Usually, a browser&#x27;s Web API or third party library will be providing the promise, and you only need to consume it.
 In the final promise section, this tutorial will cite a common use case of a Web API that returns promises: the Fetch API.
 Using the Fetch API with Promises
 One of the most useful and frequently used Web APIs that returns a promise is the Fetch API, which allows you to make an asynchronous resource request over a network. fetch is a two-part process, and therefore requires chaining then. This example demonstrates hitting the GitHub API to fetch a user&#x27;s data, while also handling any potential error:
 // Fetch a user from the GitHub API
 fetch(&#x27;https://api.github.com/users/octocat&#x27;)
   .then((response) &#x3D;&gt; {
     return response.json()
   })
   .then((data) &#x3D;&gt; {
     console.log(data)
   })
   .catch((error) &#x3D;&gt; {
     console.error(error)
   })
 The fetch request is sent to the https://api.github.com/users/octocat URL, which asynchronously waits for a response. The first then passes the response to an anonymous function that formats the response as JSON data, then passes the JSON to a second then that logs the data to the console. The catch statement logs any error to the console.
 Running this code will yield the following:
 login: &quot;octocat&quot;,
 id: 583231,
 avatar_url: &quot;https://avatars3.githubusercontent.com/u/583231?v&#x3D;4&quot;
 blog: &quot;https://github.blog&quot;
 company: &quot;@github&quot;
 followers: 3203
 ...
 This is the data requested from https://api.github.com/users/octocat, rendered in JSON format.
 This section of the tutorial showed that promises incorporate a lot of improvements for dealing with asynchronous code. But, while using then to handle asynchronous actions is easier to follow than the pyramid of callbacks, some developers still prefer a synchronous format of writing asynchronous code. To address this need, ECMAScript 2016 (ES7) introduced async functions and the await keyword to make working with promises easier.
 Async Functions with async/await
 An async function allows you to handle asynchronous code in a manner that appears synchronous. async functions still use promises under the hood, but have a more traditional JavaScript syntax. In this section, you will try out examples of this syntax.
 You can create an async function by adding the async keyword before a function:
 // Create an async function
 async function getUser() {
   return {}
 }
 Although this function is not handling anything asynchronous yet, it behaves differently than a traditional function. If you execute the function, you&#x27;ll find that it returns a promise with a [[PromiseStatus]] and [[PromiseValue]] instead of a return value.
 Try this out by logging a call to the getUser function:
 console.log(getUser())
 This will give the following:
 __proto__: Promise
 [[PromiseStatus]]: &quot;fulfilled&quot;
 [[PromiseValue]]: Object
 This means you can handle an async function with then in the same way you could handle a promise. Try this out with the following code:
 getUser().then((response) &#x3D;&gt; console.log(response))
 This call to getUser passes the return value to an anonymous function that logs the value to the console.
 You will receive the following when you run this program:
 {}
 An async function can handle a promise called within it using the await operator. await can be used within an async function and will wait until a promise settles before executing the designated code.
 With this knowledge, you can rewrite the Fetch request from the last section using async/await as follows:
 // Handle fetch with async/await
 async function getUser() {
   const response &#x3D; await fetch(&#x27;https://api.github.com/users/octocat&#x27;)
   const data &#x3D; await response.json()
 
   console.log(data)
 }
 
 // Execute async function
 getUser()
 The await operators here ensure that the data is not logged before the request has populated it with data.
 Now the final data can be handled inside the getUser function, without any need for using then. This is the output of logging data:
 login: &quot;octocat&quot;,
 id: 583231,
 avatar_url: &quot;https://avatars3.githubusercontent.com/u/583231?v&#x3D;4&quot;
 blog: &quot;https://github.blog&quot;
 company: &quot;@github&quot;
 followers: 3203
 ...
 
 Note: In many environments, async is necessary to use await—however, some new versions of browsers and Node allow using top-level await, which allows you to bypass creating an async function to wrap the await in.
 
 Finally, since you are handling the fulfilled promise within the asynchronous function, you can also handle the error within the function. Instead of using the catch method with then, you will use the try/catch pattern to handle the exception.
 Add the following highlighted code:
 // Handling success and errors with async/await
 async function getUser() {
   try {    // Handle success in try    const response &#x3D; await fetch(&#x27;https://api.github.com/users/octocat&#x27;)
     const data &#x3D; await response.json()
 
     console.log(data)
   } catch (error) {    // Handle error in catch    console.error(error)  }}
 The program will now skip to the catch block if it receives an error and log that error to the console.
 Modern asynchronous JavaScript code is most often handled with async/await syntax, but it is important to have a working knowledge of how promises work, especially as promises are capable of additional features that cannot be handled with async/await, like combining promises with Promise.all().
 
 Note: async/await can be reproduced by using generators combined with promises to add more flexibility to your code. To learn more, check out our Understanding Generators in JavaScript tutorial.
 
 Conclusion
 Because Web APIs often provide data asynchronously, learning how to handle the result of asynchronous actions is an essential part of being a JavaScript developer. In this article, you learned how the host environment uses the event loop to handle the order of execution of code with the stack and queue. You also tried out examples of three ways to handle the success or failure of an asynchronous event, with callbacks, promises, and async/await syntax. Finally, you used the Fetch Web API to handle asynchronous actions.
 For more information about how the browser handles parallel events, read Concurrency model and the event loop on the Mozilla Developer Network. If you&#x27;d like to learn more about JavaScript, return to our How To Code in JavaScript series.</content>
     </entry>
     <entry>
       <title>REST API: Sorting, Filtering, and Pagination</title>
         <link href="https://www.taniarascia.com/rest-api-sorting-filtering-pagination/"/>
       <updated>2020-09-09T00:00:00.000Z</updated>
       <content type="text">Web applications often have tables of data, whether it&#x27;s the list of items for sale on Amazon, or notes in Evernote, and so on. Usually, users of the application are going to want to filter the results or sort through that data in some way.
 If the dataset is pretty small, maybe a few hundred results, the API can return all the data at once and the front end will handle all the filtering, and no more API calls are required. Most of the time, however, the data could consist of tens of thousands to millions of rows, and it&#x27;s better to just get the data you need from smaller API calls as opposed to trying to request a million results every time the page loads.
 Recently, I made a backend API for some list endpoints, and implemented filtering, sorting, and pagination. There&#x27;s not really a set standard for creating these types of endpoints, and almost every one I&#x27;ve come across is different in some way. I made a few notes on what made sense to me, so this resource could be helpful for someone who is working on designing an API.
 Goals
 In this article I&#x27;ll make an example API endpoint and SQL query for various sort, paginate, and filter APIs, with users as the table for all examples.
 Contents
 
 Response
 Pagination
 Sorting
 
 Ascending vs. Descending
 Single column
 Multiple columns
 
 
 Filtering
 
 String (exact)
 String (exact, multiple)
 String (partial)
 Number (exact)
 Number (greater than)
 Number (less than)
 Number (range)
 Date (range)
 
 
 
 Response
 When using any pagination or filtering in an API, you&#x27;re going to want to know how many results you have, how many results there are total, and what page you&#x27;re on.
 API Response
 {
   content: [], // all the response items will go in this array
   page: 1, // current page
   results_per_page: 5, // how many items available in &quot;content&quot;
   total_results: 100 // total number of items
 }
 From there, you can discern that there are 20 pages with total_results / results_per_page and anything else you might need for the front end.
 Pagination
 Pagination is how you move between the pages when you don&#x27;t want to retrieve all the results at once.
 
 Page and results per page are required inputs
 For the SQL query, offset is equal to (page - 1) * results_per_page
 
 GET /users?page&#x3D;3&amp;results_per_page&#x3D;20
 SELECT * FROM users
 LIMIT 20
 OFFSET 40
 Sorting
 Sorting allows you to order the results by any field, in ascending or descending order.
 Ascending vs. Descending
 I always forget what ascending and descending mean for alphabetical, numerical, and date-based responses, so I wrote this up for reference.
 
 
 
 Type
 Order
 Example
 Description
 
 
 
 
 Alphabetical
 Ascending
 A - Z
 First to last
 
 
 Alphabetical
 Descending
 Z - A
 Last to first
 
 
 Numerical
 Ascending
 1 - 9
 Lowest to highest
 
 
 Numerical
 Descending
 9 - 1
 Highest to lowest
 
 
 Date
 Ascending
 01-01-1970 - Today
 Oldest to newest
 
 
 Date
 Descending
 Today - 01-01-1970
 Newest to oldest
 
 
 
 Single column
 If you only need to sort one column at a time, you could put the column name in sort_by and the sort direction in order.
 GET /users?sort_by&#x3D;first_name&amp;order&#x3D;asc
 SELECT * FROM users
 ORDER BY first_name ASC
 Multiple columns
 If the ability to sort multiple columns is required, you could comma-separate each column:order pair and put it in one sort parameter. This could also be used for a single column if you prefer the syntax.
 GET /users?sort&#x3D;first_name:asc,age:desc
 SELECT * FROM users
 ORDER BY first_name ASC, age DESC
 Filtering
 Filtering is by far the most complex of the three. There are several ways to handle it. Some APIs will use a POST and pass all the data in the body of the request for searching. This might be necessary for advanced searching in some situations, but a GET is preferable.
 Some API will attempt to put everything on a single filter parameter, like this:
 GET users?filter&#x3D;{&quot;first_name&quot;:[&quot;Tania&quot;,&quot;Joe&quot;],&quot;age&quot;:[30,31,32]}
 However, this will have to be URI encoded.
 I&#x27;ve opted for treating each parameter as a column in the database.
 String (exact)
 Exact search by a single column.
 GET /users?first_name&#x3D;Tania
 SELECT * FROM users
 WHERE first_name &#x3D; &#x27;Tania&#x27;
 String (exact, multiple)
 Depending on how you want to handle the API, multiple options for a single column can be handled in different ways. If splitting by comma isn&#x27;t an issue, it might be the easiest. You might also just want to repeat the parameter name or use a custom delimiter.
 GET /users?first_name&#x3D;Tania,Joe
 GET /users?first_name&#x3D;Tania&amp;first_name&#x3D;Joe
 GET /users?first_name[]&#x3D;Tania&amp;first_name[]&#x3D;Joe
 SELECT * FROM users
 WHERE first_name IN (&#x27;Tania&#x27;, &#x27;Joe&#x27;)
 
 Some systems might require using [] for multiple parameters of the same name, and some might now allow [], so I provided both options.
 
 String (partial)
 Often, searches are expected to be partial, so that when I look for &quot;Tan&quot; it will show me &quot;Tania&quot; and &quot;Tanner&quot;. The solution I liked was using like:Tan as value as opposed to modifying the parameter (such as first_name[like]&#x3D;Tan).
 GET /users?first_name&#x3D;like:Tan
 SELECT * FROM users
 WHERE first_name LIKE &#x27;%Tan%&#x27;
 Number (exact)
 Exact number search on a column.
 GET /users?age&#x3D;30
 SELECT * FROM users
 WHERE age &#x3D; 31
 Number (greater than)
 Similar to like:, you can use gt: to handle greater than. Adding the option for gte: (greater than or equal) is also an option.
 GET /users?age&#x3D;gt:21
 SELECT * FROM users
 WHERE age &gt; 21
 Number (less than)
 Same with lt: for less than lte: for less than or equal.
 GET /users?age&#x3D;lt:21
 SELECT * FROM users
 WHERE age &lt; 21
 Number (range)
 If you need a range between two number values, using [and] in between them could be one option. This one could get complicated, depending on if you want to allow both greater than and greater than or equal, or other options.
 GET /users?age&#x3D;gt:12[and]lt:20
 SELECT * FROM users
 WHERE age &gt; 12 AND age &lt; 20
 Date (range)
 If you need a range between two dates, you can use start and end, or since and to.
 GET /users?start&#x3D;01-01-1970&amp;end&#x3D;09-09-2020
 SELECT * FROM users
 WHERE created_at BETWEEN &#x27;01-01-1970&#x27; AND &#x27;09-09-2020&#x27;
 Conclusion
 These examples are pretty simple and cover basic use cases. If your API is very complicated, you might need to change it up to add more options, particularly with ranges, and various combinations of &quot;and&quot; and &quot;or&quot;. Hopefully this will be a helpful starting point!</content>
     </entry>
     <entry>
       <title>Parsel: A tiny, permissive CSS selector parser</title>
         <link href="https://lea.verou.me/2020/09/parsel-a-tiny-permissive-css-selector-parser/"/>
       <updated>2020-09-07T09:22:39.000Z</updated>
       <content type="text">I’ve posted before about my work for the Web Almanac this year. To make it easier to calculate the stats about CSS selectors, we looked to use an existing selector parser, but most were too big and/or had dependencies or didn’t account for all selectors we wanted to parse, and we’d need to write our own walk and specificity methods anyway. So I did what I usually do in these cases: I wrote my own!
 
 
 
 You can find it here: https://projects.verou.me/parsel/ 
 
 
 
 
 
 
 
 
 
 
 
 It not only parses CSS selectors, but also includes methods to walk the AST produced, as well as calculate specificity as an array and convert it to a number for easy comparison.
 
 
 
 It is one of my first libraries released as an ES module, and there are instructions about both using it as a module, and as a global, for those who would rather not deal with ES modules yet, because convenient as ESM are, I wouldn’t want to exclude those less familiar with modern JS.
 
 
 
 Please try it out and report any bugs! We plan to use it for Almanac stats in the next few days, so if you can spot bugs sooner rather than later, you can help that volunteer effort. I’m primarily interested in (realistic) valid selectors that are parsed incorrectly. I’m aware there are many invalid selectors that are parsed weirdly, but that’s not a focus (hence the “permissive” aspect, there are many invalid selectors it won’t throw on, and that’s by design to keep the code small, the logic simple, and the functionality future-proof).
 
 
 
 How it works
 
 
 
 If you’re just interested in using this selector parser, read no further. This section is about how the parser works, for those interested in this kind of thing. 
 
 
 
 I first started by writing a typical parser, with character-by-character gobbling and different modes, with code somewhat inspired by my familiarity with jsep. I quickly realized that was a more fragile approach for what I wanted to do, and would result in a much larger module. I also missed the ease and flexibility of doing things with regexes. 
 
 
 
 However, since CSS selectors include strings and parens that can be nested, parsing them with regexes is a fool’s errand. Nested structures are not regular languages as my CS friends know. You cannot use a regex to find the closing parenthesis that corresponds to an opening parenthesis, since you can have other nested parens inside it. And it gets even more complex when there are other tokens that can nest, such as strings or comments. What if you have an opening paren that contains a string with a closing paren, like e.g. (&quot;foo)&quot;)? A regex would match the closing paren inside the string. In fact, parsing the language of nested parens (strings like (()(()))) with regexes is one of the typical (futile) exercises in a compilers course. Students struggle to do it because it’s an impossible task, and learn the hard way that not everything can be parsed with regexes.
 
 
 
 Unlike a typical programming language with lots of nested structures however, the language of CSS selectors is more limited. There are only two nested structures: strings and parens, and they only appear in specific types of selectors (namely attribute selectors, pseudo-classes and pseudo-elements).  Once we get those out of the way, everything else can be easily parsed by regexes. So I decided to go with a hybrid approach: The selector is first looked at character-by-character, to extract strings and parens. We only extract top-level parens, since anything inside them can be parsed separately (when it’s a selector), or not at all. The strings are replaced by a single character, as many times as the length of the string, so that any character offsets do not change, and the strings themselves are stored in a stack. Same with parens. 
 
 
 
 After that point, this modified selector language is a regular language that can be parsed with regexes. To do so, I follow an approach inspired by the early days of Prism: An object literal of tokens in the order they should be matched in, and a function that tokenizes a string by iteratively matching tokens from an object literal. In fact, this function was taken from an early version of Prism and modified.
 
 
 
 After we have the list of tokens as a flat array, we can restore strings and parens, and then nest them appropriately to create an AST.
 
 
 
 Also note that the token regexes use the new-ish named capture groups feature in ES2018, since it’s now supported pretty widely in terms of market share. For wider support, you can transpile </content>
     </entry>
     <entry>
       <title>WDRL — Edition 283: We decide what’s our future.</title>
         <link href="https://wdrl.info/archive/283"/>
       <updated>2020-09-04T19:00:00.000Z</updated>
       <content type="text">Hey,
 
 Wow, time flies and I realise that I not only negelected this newsletter for a long time but since I started the draft for this edition, nearly two months of life happened. I’ll have to see how often I can write this again but the goal is to just publish it when I write the draft and not let it wait in the queue for such a long time. If you want to know what happened recently in my life, at least a part of it can be seen here and there are some notes on my Garden (in German only for now). My own blog has been neglected as well but maybe I’ll find more time over the next weeks to give back some life to it. Now over to this weeks topic:
 It’s our money that rules the world. We together still own more money than anyone else on the planet. What we spend our money on is going to shape the direction where we as society go. We influence whether there will be a war, how many cars are produced, how many pesticides are sprayed across our planet, how many trees in the rainforests are cut, how many planes are polluting  the air we breathe. Every single cent is spent either to a good cause or a bad cause and if we’d bear that in mind every day, we probably would change our shopping behaviour.
 We can opt for renewable energy in our homes, at the workplace, we can buy natural, organic, non processed food. We don’t need to buy a new iPhone every year, a new car every two or five years, we don’t need to fly to far away destinations just for vacation. We don’t need to buy cheap meat, plastics, a pair of Jeans manufactured by poor children who breath chemicals and colors every day because of what we buy.
 We’re a small dot in evolution but we can change where we are on the line. More importantly, we, and only we or maximum our children will decide whether the line continues or whether humans go extinct. Like dinosaurs in the past.
 Generic
 
 	The Climate Strike Github organisation created an open source license we can use which prevents oil and gas companies from co-opting our hard work to extracting more fossil fuels.
 
 UI/UX
 
 	Ahmad Shadeed on how to align a logo image correctly in layouts.
 
 Security
 
 	iOS 14 will bring App-Bound Domains to WKWebView, a new mechanism that will offer better privacy for users in an app’s web view. Until now, the app could monitor any user activity in a web view; with the new App-Bound domain concept that won’t be the case anymore. That comes with the Intelligent Tracking Prevention (ITP) being enabled in WKWebView starting with iOS14 as well.
 	Did you know that you can capture CSP violation events in the browser with JavaScript? The SecurityPolicyViolationEvent does this and tells you all the details we want to know about such an issue. With that, we can easily build a simple reporting tool for our services.
 
 Privacy
 
 	“Being a Facebook-Free Business means your customers can trust that you aren’t collaborators with the Facebook machine. That when you spend your money with a Facebook-Free Business, none of that money will find its way back to Facebook’s coffers.”—DHH in Become A Facebook-Free Business. I personally highly value companies that don’t spend money on Facebook (incl Insta, etc.) advertising.
 	A good example of why one browser alone isn’t making a good ecosystem for us and only diversity, transparency can create a safer product for us users. Chrome’s marketplace was recently cleaned from browser extensions that were installed on 32million browser instances. The fatal thing here is that they didn’t serve the purpose they advertised but instead captured all the browser history and user activity and sent it to shady databases for spam, advertising and retargeting purposes. Better don’t install any extension even from a “trusted marketplace” like Google’s.
 
 Accessibility
 
 	Learn how to design beautiful and accessible link focus states in this tutorial by Ethan Marcotte.
 
 JavaScript
 
 	Ada Rose Cannon shares how we can animate DOM changes in JavaScript.
 	If we want to look for a specific index in JavaScript we don’t need to use any complex loop with a stop anymore but instead can use Array.prototype.findIndex() which returns the index of the first match of our testing function.
 	Wenson Hsieh shares how to use the Async Clipboard API and the differences and advantages over the methods we used in the past.
 	iOS Safari 13.4+ allows us to block back and forward navigation gestures. This is very useful for media galleries and image croppers where users interact with elements on the side of the viewport and most likely don’t want to navigate to another page, as Rik Schennink explains.
 	Fluor.js is a cool small library that lets us animate a lot of details on websites. Quite nice!
 
 CSS
 
 	Chris Coyier shares how we can use the new shiny CSS property aspect-ratio in our layouts and with that finally replace various hacks we used to use for over a decade now.
 	Lea Verou with a guide on CSS Variables that handles the basic concepts and real practical use cases.
 	Bram Van Damme shares how to use CSS Grid layout Level 2 to create a Masonry layout.
 	Sara Soueidan on how she uses global and component style settings with CSS Custom Properties.
 
 Work &amp; Life
 
 	After a month of trial, the Buffer team now is moving entirely to a 4-day work week as the results of significantly reduced stress, higher autonomy, and higher work happiness, more motivation, better collaboration while productivity surprisingly didn’t suffer.
 	Why it’s important to connect productivity with our purpose to make it essential and meaningful productivity. Because what doesn’t serve the mission is burning ourselves out which happens if we just strive for more productivity when it doesn’t serve our own deepest mission.
 	Finally, there’s a resource to find ethical job sites for freelancers instead of ending up on sites that try to exploit us.
 	Lucas Miller is neuroscientist and he sees Slack as a particularly “scary offender” in stopping people getting their work done because it encourages them to be constantly distracted. Of course that isn’t only about Slack but about all the messenger style chat systems we use — no matter if made by an independent company, Microsoft, Facebook or any other.
 
 Go beyond…
 
 	If we frame our own life a little bit different, in evolutionary context, everything we do, everything we see as given or as inherited by our parents can be seen a bit different. We’re then only a small dot in evolution and yet, we are the only ones who can still decide whether the human species will continue to be part of evolution on our planet or not. Evolution is a quite simple concept.
 	Let’s try to understand why biodiversity is collapsing worldwide at the moment. It means that less species of animals are around in our ecosystem and climate change is fueling it while our agriculture is actively killing the living ground and plants these species need to live. Weirdly, we fail to understand that we won’t get any food, wood and other things that are vital to our health and ability to live if there is no biodiversity anymore. Technology won’t save us in that case, it can only help understand the issue and maybe help improve the situation but the major impact is how we act, what we buy and how we farm.
 	We are facing one of the biggest decisions of humankind — a decision on how to face the climate crisis. This is a huge responsibility and at the same time an opportunity. Our chance to work together to initiate a comprehensive change towards a more just society. Here’s the Grassroots Climate Plan with a solid plan what we need and can do to change the current state of burning down our own habitat.
 
 
 Thank you all for reading this, I hope you’re doing fine and stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Adding Comments to My Blog (via Utterances)</title>
         <link href="https://www.taniarascia.com/adding-comments-to-my-blog/"/>
       <updated>2020-08-04T00:00:00.000Z</updated>
       <content type="text">I&#x27;ve gone back and forth about whether or not to have comments on the site. Most of all, I&#x27;ve liked having absolutely no server or external scripts on the site, and not having to moderate comments that are publicly facing on the site.
 A bit ago, I implemented my own comments REST API server with Node and wrote about it here, which was awesome, but I didn&#x27;t implement any user system or authentication, so the barrier to posting was just too low.
 I couldn&#x27;t keep ignoring how awesome Utterances looks, an open source project by Jeremy Danyow. Not only does it look absolutely fantastic, but it took me about 15 minutes to set up completely on my Gatsby site.
 I made a comments repo to host all the issues and responses. Connecting to the Utterances GitHub app is required to leave a comment through the site, though you can also comment directly through GitHub if the issue exists. I imagine spam and obnoxious comments will be much more rare through this system, and people will actually be able to participate in discussions and leave helpful comments once again.</content>
     </entry>
     <entry>
       <title>Understanding Arrow Functions in JavaScript</title>
         <link href="https://www.taniarascia.com/understanding-arrow-functions-in-javascript/"/>
       <updated>2020-08-01T00:00:00.000Z</updated>
       <content type="text">This article was originally written for DigitalOcean.
 Introduction
 The 2015 edition of the ECMAScript specification (ES6) added arrow function expressions to the JavaScript language. Arrow functions are a new way to write anonymous function expressions, and are similar to lambda functions in some other programming languages, such as Python.
 Arrow functions differ from traditional functions in a number of ways, including the way their scope is determined and how their syntax is expressed. Because of this, arrow functions are particularly useful when passing a function as a parameter to a higher-order function, such as when you are looping over an array with built-in iterator methods. Their syntactic abbreviation can also allow you to improve the readability of your code.
 In this article, you will review function declarations and expressions, learn about the differences between traditional function expressions and arrow function expressions, learn about lexical scope as it pertains to arrow functions, and explore some of the syntactic shorthand permitted with arrow functions.
 Defining Functions
 Before delving into the specifics of arrow function expressions, this tutorial will briefly review traditional JavaScript functions in order to better show the unique aspects of arrow functions later on.
 The How To Define Functions in JavaScript tutorial earlier in this series introduced the concept of function declarations and function expressions. A function declaration is a named function written with the function keyword. Function declarations load into the execution context before any code runs. This is known as hoisting, meaning you can use the function before you declare it.
 Here is an example of a sum function that returns the sum of two parameters:
 function sum(a, b) {
   return a + b
 }
 You can execute the sum function before declaring the function due to hoisting:
 sum(1, 2)
 
 function sum(a, b) {
   return a + b
 }
 Running this code would give the following output:
 3
 You can find the name of the function by logging the function itself:
 console.log(sum)
 This will return the function, along with its name:
 ƒ sum(a, b) {
   return a + b
 }
 A function expression is a function that is not pre-loaded into the execution context, and only runs when the code encounters it. Function expressions are usually assigned to a variable, and can be anonymous, meaning the function has no name.
 In this example, write the same sum function as an anonymous function expression:
 const sum &#x3D; function (a, b) {
   return a + b
 }
 You&#x27;ve now assigned the anonymous function to the sum constant. Attempting to execute the function before it is declared will result in an error:
 sum(1, 2)
 
 const sum &#x3D; function (a, b) {
   return a + b
 }
 Running this will give:
 Uncaught ReferenceError: Cannot access &#x27;sum&#x27; before initialization
 Also, note that the function does not have a named identifier. To illustrate this, write the same anonymous function assigned to sum, then log sum to the console:
 const sum &#x3D; function (a, b) {
   return a + b
 }
 
 console.log(sum)
 This will show you the following:
 ƒ (a, b) {
   return a + b
 }
 The value of sum is an anonymous function, not a named function.
 You can name function expressions written with the function keyword, but this is not popular in practice. One reason you might want to name a function expression is to make error stack traces easier to debug.
 Consider the following function, which uses an if statement to throw an error if the function parameters are missing:
 const sum &#x3D; function namedSumFunction(a, b) {
   if (!a || !b) throw new Error(&#x27;Parameters are required.&#x27;)
 
   return a + b
 }
 
 sum()
 The highlighted section gives the function a name, and then the function uses the or || operator to throw an error object if either of the parameters is missing.
 Running this code will give you the following:
 Uncaught Error: Parameters are required.
     at namedSumFunction (&lt;anonymous&gt;:3:23)
     at &lt;anonymous&gt;:1:1
 In this case, naming the function gives you a quick idea of where the error is.
 An arrow function expression is an anonymous function expression written with the &quot;fat arrow&quot; syntax (&#x3D;&gt;).
 Rewrite the sum function with arrow function syntax:
 const sum &#x3D; (a, b) &#x3D;&gt; {
   return a + b
 }
 Like traditional function expressions, arrow functions are not hoisted, and so you cannnot call them before you declare them. They are also always anonymous—there is no way to name an arrow function. In the next section, you will explore more of the syntactical and practical differences between arrow functions and traditional functions.
 Arrow Functions
 Arrow functions have a few important distinctions in how they work that distinguish them from traditional functions, as well as a few syntactic enhancements. The biggest functional differences are that arrow functions do not have their own this binding or prototype and cannot be used as a constructor. Arrow functions can also be written as a more compact alternative to traditional functions, as they grant the ability to omit parentheses around parameters and add the concept of a concise function body with implicit return.
 In this section, you will go through examples that illustrate each of these cases.
 Lexical this
 The keyword this is often considered a tricky topic in JavaScript. The article Understanding This, Bind, Call, and Apply in JavaScript explains how this works, and how this can be implicitly inferred based on whether the program uses it in the global context, as a method within an object, as a constructor on a function or class, or as a DOM event handler.
 Arrow functions have lexical this, meaning the value of this is determined by the surrounding scope (the lexical environment).
 The next example will demonstrate the difference between how traditional and arrow functions handle this. In the following printNumbers object, there are two properties: phrase and numbers. There is also a method on the object, loop, which should print the phrase string and the current value in numbers:
 const printNumbers &#x3D; {
   phrase: &#x27;The current value is:&#x27;,
   numbers: [1, 2, 3, 4],
 
   loop() {
     this.numbers.forEach(function (number) {
       console.log(this.phrase, number)
     })
   },
 }
 One might expect the loop function to print the string and current number in the loop on each iteraton. However, in the result of running the function the phrase is actually undefined:
 printNumbers.loop()
 This will give the following:
 undefined 1
 undefined 2
 undefined 3
 undefined 4
 As this shows, this.phrase is undefined, indicating that this within the anonymous function passed into the forEach method does not refer to the printNumbers object. This is because a traditional function will not determine its this value from the scope of the environment, which is the printNumbers object.
 In older versions of JavaScript, you would have had to use the bind method, which explicitly sets this. This pattern can be found often in some earlier versions of frameworks, like React, before the advent of ES6.
 Use bind to fix the function:
 const printNumbers &#x3D; {
   phrase: &#x27;The current value is:&#x27;,
   numbers: [1, 2, 3, 4],
 
   loop() {
     // Bind the &#x60;this&#x60; from printNumbers to the inner forEach function
     this.numbers.forEach(
       function (number) {
         console.log(this.phrase, number)
       }.bind(this),
     )
   },
 }
 
 printNumbers.loop()
 This will give the expected result:
 The current value is: 1
 The current value is: 2
 The current value is: 3
 The current value is: 4
 Arrow functions can give a more direct way of dealing with this. Since their this value is determined based on the lexical scope, the inner function called in forEach can now access the properties of the outer printNumbers object, as demonstrated:
 const printNumbers &#x3D; {
   phrase: &#x27;The current value is:&#x27;,
   numbers: [1, 2, 3, 4],
 
   loop() {
     this.numbers.forEach((number) &#x3D;&gt; {
       console.log(this.phrase, number)
     })
   },
 }
 
 printNumbers.loop()
 This will give the expected result:
 The current value is: 1
 The current value is: 2
 The current value is: 3
 The current value is: 4
 These examples establish that using arrow functions in built-in array methods like forEach, map, filter, and reduce can be more intuitive and easier to read, making this strategy more likely to fulfill expectations.
 Arrow Functions as Object Methods
 While arrow functions are excellent as parameter functions passed into array methods, they are not effective as object methods because of the way they use lexical scoping for this. Using the same example as before, take the loop method and turn it into an arrow function to discover how it will execute:
 const printNumbers &#x3D; {
   phrase: &#x27;The current value is:&#x27;,
   numbers: [1, 2, 3, 4],
 
   loop: () &#x3D;&gt; {
     this.numbers.forEach((number) &#x3D;&gt; {
       console.log(this.phrase, number)
     })
   },
 }
 In this case of an object method, this should refer to properties and methods of the printNumbers object. However, since an object does not create a new lexical scope, an arrow function will look beyond the object for the value of this.
 Call the loop() method:
 printNumbers.loop()
 This will give the following:
 Uncaught TypeError: Cannot read property &#x27;forEach&#x27; of undefined
 Since the object does not create a lexical scope, the arrow function method looks for this in the outer scope–Window in this example. Since the numbers property does not exist on the Window object, it throws an error. As a general rule, it is safer to use traditional functions as object methods by default.
 Arrow Functions Have No constructor or prototype
 The Understanding Prototypes and Inheritance in JavaScript tutorial earlier in this series explained that functions and classes have a prototype property, which is what JavaScript uses as a blueprint for cloning and inheritance.
 To illustrate this, create a function and log the automatically assigned prototype property:
 function myFunction() {
   this.value &#x3D; 5
 }
 
 // Log the prototype property of myFunction
 console.log(myFunction.prototype)
 This will print the following to the console:
 {constructor: ƒ}
 This shows that in the prototype property there is an object with a constructor. This allows you to use the new keyword to create an instance of the function:
 const instance &#x3D; new myFunction()
 
 console.log(instance.value)
 This will yield the value of the value property that you defined when you first declared the function:
 5
 In contrast, arrow functions do not have a prototype property. Create a new arrow function and try to log its prototype:
 const myArrowFunction &#x3D; () &#x3D;&gt; {}
 
 // Attempt to log the prototype property of myArrowFunction
 console.log(myArrowFunction.prototype)
 This will give the following:
 undefined
 As a result of the missing prototype property, the new keyword is not available and you cannot construct an instance from the arrow function:
 const arrowInstance &#x3D; new myArrowFunction()
 
 console.log(arrowInstance)
 This will give the following error:
 Uncaught TypeError: myArrowFunction is not a constructor
 This is consistent with our earlier example: Since arrow functions do not have their own this value, it follows that you would be unable to use an arrow function as a constructor.
 As shown here, arrow functions have a lot of subtle changes that make them operate differently from traditional functions in ES5 and earlier. There have also been a few optional syntactical changes that make writing arrow functions quicker and less verbose. The next section will show examples of these syntax changes.
 Implicit Return
 The body of a traditional function is contained within a block using curly brackets ({}) and ends when the code encounters a return keyword. The following is what this implementation looks like as an arrow function:
 const sum &#x3D; (a, b) &#x3D;&gt; {
   return a + b
 }
 Arrow functions introduce concise body syntax, or implicit return. This allows the omission of the curly brackets and the return keyword.
 const sum &#x3D; (a, b) &#x3D;&gt; a + b
 Implicit return is useful for creating succinct one-line operations in map, filter, and other common array methods. Note that both the brackets and the return keyword must be omitted. If you cannot write the body as a one-line return statement, then you will have to use the normal block body syntax.
 In the case of returning an object, syntax requires that you wrap the object literal in parentheses. Otherwise, the brackets will be treated as a function body and will not compute a return value.
 To illustrate this, find the following example:
 const sum &#x3D; (a, b) &#x3D;&gt; ({result: a + b})
 
 sum(1, 2)
 This will give the following output:
 {result: 3}
 Omitting Parentheses Around a Single Parameter
 Another useful syntactic enhancement is the ability to remove parentheses from around a single parameter in a function. In the following example, the square function only operates on one parameter, x:
 const square &#x3D; (x) &#x3D;&gt; x * x
 As a result, you can omit the parentheses around the parameter, and it will work just the same:
 const square &#x3D; x &#x3D;&gt; x * x
 
 square(10)
 This will give the following:
 100
 Note that if a function takes no parameters, parentheses will be required:
 const greet &#x3D; () &#x3D;&gt; &#x27;Hello!&#x27;
 
 greet()
 Calling greet() will work as follows:
 &#x27;Hello!&#x27;
 Some codebases choose to omit parentheses wherever possible, and others choose to always keep parentheses around parameters no matter what, particularly in codebases that use TypeScript and require more information about each variable and parameter. When deciding how to write your arrow functions, check the style guide of the project to which you are contributing.
 Conclusion
 In this article, you reviewed traditional functions and the difference between function declarations and function expressions. You learned that arrow functions are always anonymous, do not have a prototype or constructor, cannot be used with the new keyword, and determine the value of this through lexical scope. Finally, you explored the new syntactic enhancements available to arrow functions, such as implicit return and parentheses omission for single parameter functions.
 For a review of basic functions, read How To Define Functions in JavaScript. To read more about the concept of scope and hoisting in JavaScript, read Understanding Variables, Scope, and Hoisting in JavaScript.</content>
     </entry>
     <entry>
       <title>Introspecting CSS via the CSS OM: Getting supported properties, shorthands, longhands</title>
         <link href="https://lea.verou.me/2020/07/introspecting-css-via-the-css-om-getting-supported-properties-shorthands-longhands/"/>
       <updated>2020-07-27T05:10:13.000Z</updated>
       <content type="text">For some of the statistics we are going to study for this year’s Web Almanac we may end up needing a list of CSS shorthands and their longhands. Now this is typically done by maintaining a data structure by hand or guessing based on property name structure. But I knew that if we were going to do it by hand, it’s very easy to miss a few of the less popular ones, and the naming rule where shorthands are a prefix of their longhands has failed to get standardized and now has even more exceptions than it used to. And even if we do an incredibly thorough job, next year the data structure will be inaccurate, because CSS and its implementations evolve fast. The browser knows what the shorthands are, surely we should be able to get the information from it  …right? Then we could use it directly if this is a client-side library, or in the case of the Almanac, where code needs to be fast because it will run on millions of websites, paste the precomputed result into whatever script we run.
 
 
 
 
 
 
 
 There are essentially two steps for this:
 
 
 
 Get a list of all CSS propertiesFigure out how to test if a given property is a shorthand and how to get its longhands if so.
 
 
 
 I decided to tell this story in the inverse order. In my exploration, I first focused on figuring out shorthands (2), because I had coded getting a list of properties many times before, but since (1) is useful in its own right (and probably in more use cases), I felt it makes more sense to examine that first.
 
 
 
 Note: I’m using document.body instead of a dummy element in these examples, because I like to experiment in about:blank, and it’s just there and because this way you can just copy stuff to the console and try it wherever, even right here while reading this post. However, if you use this as part of code that runs on a real website, it goes without saying that you should create and test things on a dummy element instead!
 
 
 
 Getting a list of all CSS properties from the browser
 
 
 
 In Chrome and Safari, this is as simple as Object.getOwnPropertyNames(document.body.style). However, in Firefox, this doesn’t work. Why is that? To understand this (and how to work around it), we need to dig a bit deeper.
 
 
 
 In Chrome and Safari, element.style is a CSSStyleDeclaration instance. In Firefox however, it is a CSS2Properties instance, which inherits from CSSStyleDeclaration. CSS2Properties is an older interface, defined in the DOM 2 Specification, which is now obsolete. In the current relevant specification, CSS2Properties is gone, and has been merged with CSSStyleDeclaration. However, Firefox hasn’t caught up yet.
 
 
 
 Firefox on the left, Safari on the right. Chrome behaves like Safari.
 
 
 
 Since the properties are on CSSStyleDeclaration, they are not own properties of element.style, so Object.getOwnPropertyNames() fails to return them. However, we can extract the CSSStyleDeclaration instance by using __proto__ or Object.getPrototypeOf(), and then Object.getOwnPropertyNames(Object.getPrototypeOf(document.body.style)) gives us what we want! 
 
 
 
 So we can combine the two to get a list of properties regardless of browser:
 
 
 
 let properties &#x3D; Object.getOwnPropertyNames(
 	style.hasOwnProperty(&quot;background&quot;)? 
 	style : style.__proto__
 );
 
 
 
 And then, we just drop non-properties, and de-camelCase:
 
 
 
 properties &#x3D; properties.filter(p &#x3D;&gt; style[p] &#x3D;&#x3D;&#x3D; &quot;&quot;) // drop functions etc
 	.map(prop &#x3D;&gt; { // de-camelCase
 		prop &#x3D; prop.replace(/[A-Z]/g, function($0) { return &#x27;-&#x27; + $0.toLowerCase() });
 
 		if (prop.indexOf(&quot;webkit-&quot;) &gt; -1) {
 			prop &#x3D; &quot;-&quot; + prop;
 		}
 
 		return prop;
 	});
 
 
 
 You can see a codepen with the result here:
 
 
 
 
 
 
 
 
 
 Testing if a property is a shorthand and getting a list of longhands
 
 
 
 The main things to note are:
 
 
 
 When you set a shorthand on an element’s inline style, you are essentially setting all its longhands.element.style is actually array-like, with numerical properties and .length that gives you the number of properties set on it. This means you can use the spread operator on it:
 
 
 
 &gt; document.body.style.background &#x3D; &quot;red&quot;;
 &gt; [...document.body.style] 
 &lt; [
 	&quot;background-image&quot;, 
 	&quot;background-position-x&quot;, 
 	&quot;background-position-y&quot;, 
 	&quot;background-size&quot;, 
 	&quot;background-repeat-x&quot;, 
 	&quot;background-repeat-y&quot;, 
 	&quot;background-attachment&quot;, 
 	&quot;background-origin&quot;, 
 	&quot;background-clip&quot;, 
 	&quot;background-color&quot;
 ]
 
 
 
 Interestingly, document.body.style.cssText serializes to background: red and not all the longhands.
 
 
 
 There is one exception: The all property. In Chrome, it does not quite behave as a shorthand:
 
 
 
 &gt; document.body.style.all &#x3D; &quot;inherit&quot;;
 &gt; [...document.body.style]
 &lt; [&quot;all&quot;]
 
 
 
 Whereas in Safari and Firefox, it actually returns every single property that is not a shorthand!
 
 
 
 Firefox and Safari expand all to literally all non-shorthand properties.
 
 
 
 While this is interesting from a trivia point of view, it doesn’t actually matter for our use case, since we don’t typically care about all when constructing a list of shorthands, and if we do we can always add or remove it manually.
 
 
 
 So, to recap, we can easily get the longhands of a given shorthand:
 
 
 
 function getLonghands(property) {
 	let style &#x3D; document.body.style;
 	style[property] &#x3D; &quot;inherit&quot;; // a value that works in every property
 	let ret &#x3D; [...style];
 	style.cssText &#x3D; &quot;&quot;; // clean up
 	return ret;
 }
 
 
 
 Putting the pieces together
 
 
 
 You can see how all the pieces fit together (and the output!) in this codepen:
 
 
 
 
 
 
 
 
 
 How many of these shorthands did you already know? </content>
     </entry>
     <entry>
       <title>Import non-ESM libraries in ES Modules, with client-side vanilla JS</title>
         <link href="https://lea.verou.me/2020/07/import-non-esm-libraries-in-es-modules-with-client-side-vanilla-js/"/>
       <updated>2020-07-20T20:43:15.000Z</updated>
       <content type="text">In case you haven’t heard, ECMAScript modules (ESM) are now supported everywhere!
 
 
 
 While I do have some gripes with them, it’s too late for any of these things to change, so I’m embracing the good parts and have cautiously started using them in new projects. I do quite like that I can just use import statements and dynamic import() for dependencies with URLs right from my JS, without module loaders, extra &lt;script&gt; tags in my HTML, or hacks with dynamic &lt;script&gt; tags and load events (in fact, Bliss has had a helper for this very thing that I’ve used extensively in older projects). I love that I don’t need any libraries for this, and I can use it client-side, anywhere, even in my codepens.
 
 
 
 Once you start using ESM, you realize that most libraries out there are not written in ESM, nor do they include ESM builds. Many are still using globals, and those that target Node.js use CommonJS (CJS). What can we do in that case? Unfortunately, ES Modules are not really designed with any import (pun intended) mechanism for these syntaxes, but, there are some strategies we could employ.
 
 
 
 
 
 
 
 Libraries using globals
 
 
 
 Technically, a JS file can be parsed as a module even with no imports or exports. Therefore, almost any library that uses globals can be fair game, it can just be imported as a module with no exports! How do we do that?
 
 
 
 While you may not see this syntax a lot, you don’t actually need to name anything in the import statement. There is a syntax to import a module entirely for its side effects:
 
 
 
 import &quot;url/to/library.js&quot;;
 
 
 
 This syntax works fine for libraries that use globals, since declaring a global is essentially a side effect, and all modules share the same global scope. For this to work, the imported library needs to satisfy the following conditions:
 
 
 
 It should declare the global as a property on window (or self), not via var Foo or this. In modules top-level variables are local to the module scope, and this is undefined, so the last two ways would not work.Its code should not violate strict modeThe URL is either same-origin or CORS-enabled. While &lt;script&gt; can run cross-origin resources, import sadly cannot.
 
 
 
 Basically, you are running a library as a module that was never written with the intention to be run as a module. Many are written in a way that also works in a module context, but not all. ExploringJS has an excellent summary of the differences between the two. For example, here is a trivial codepen loading jQuery via this method.
 
 
 
 Libraries using CJS without dependencies
 
 
 
 I dealt with this today, and it’s what prompted this post. I was trying to play around with Rework CSS, a CSS parser used by the HTTPArchive for analyzing CSS in the wild. However, all its code and documentation assumes Node.js. If I could avoid it, I’d really rather not have to make a Node.js app to try this out, or have to dive in module loaders to be able to require CJS modules in the browser. Was there anything I could do to just run this in a codepen, no strings attached?
 
 
 
 After a little googling, I found this issue. So there was a JS file I could import and get all the parser functionality. Except …there was one little problem. When you look at the source, it uses module.exports. If you just import that file, you predictably get an error that module is not defined, not to mention there are no ESM exports.
 
 
 
 My first thought was to stub module as a global variable, import this as a module, and then read module.exports and give it a proper name:
 
 
 
 window.module &#x3D; {};
 import &quot;https://cdn.jsdelivr.net/gh/reworkcss/css@latest/lib/parse/index.js&quot;;
 console.log(module.exports);
 
 
 
 However, I was still getting the error that module was not defined. How was that possible?! They all share the same global context!! *pulls hair out* After some debugging, it dawned on me: static import statements are hoisted; the “module” was getting executed before the code that imports it and stubs module.
 
 
 
 Dynamic imports to the rescue! import() is executed exactly where it’s called, and returns a promise. So this actually works:
 
 
 
 window.module &#x3D; {};
 import(&quot;https://cdn.jsdelivr.net/gh/reworkcss/css@latest/lib/parse/index.js&quot;).then(_ &#x3D;&gt; {
 	console.log(module.exports);
 });
 
 
 
 We could even turn it into a wee function, which I cheekily called require():
 
 
 
 async function require(path) {
 	let _module &#x3D; window.module;
 	window.module &#x3D; {};
 	await import(path);
 	let exports &#x3D; module.exports;
 	window.module &#x3D; _module; // restore global
 	return exports;
 }
 
 (async () &#x3D;&gt; { // top-level await cannot come soon enough…
 
 let parse &#x3D; await require(&quot;https://cdn.jsdelivr.net/gh/reworkcss/css@latest/lib/parse/index.js&quot;);
 console.log(parse(&quot;body { color: red }&quot;));
 
 })();
 
 
 
 You can fiddle with this code in a live pen here.
 
 
 
 Do note that this technique will only work if the module you’re importing doesn’t import other CJS modules. If it does, you’d need a more elaborate require() function, which is left as an exercise for the reader. Also, just like the previous technique, the code needs to comply with strict mode and not be cross-origin.
 
 
 
 A similar technique can be used to load AMD modules via import(), just stub define() and you’re good to go.
 
 
 
 So, with this technique I was able to quickly whip up a ReworkCSS playground. You just edit the CSS in CodePen and see the resulting AST, and you can even fork it to share a specific AST with others! 
 
 
 
 
 
 
 
 
 
 Update: CJS with static imports
 
 
 
 After this article was posted, a clever hack was pointed out to me on Twitter:
 
 
 
 
 You can shim the window.module variable without dynamic import() if you do it in a separate module, imported before the library: https://t.co/BIqlHCOdV2— Justin Fagnani (@justinfagnani) July 20, 2020
 
 
 
 
 While this works great if you can have multiple separate files, it doesn’t work when you’re e.g. quickly trying out a pen. Data URIs to the rescue! Turns out you can import a module from a data URI!
 
 
 
 So let’s adapt our Rework example to use this:
 
 
 
 
 
 
 
 
 
 Addendum: ESM gripes
 
 
 
 Since I was bound to get questions about what my gripes are with ESM, I figured I should mention them pre-emptively.
 
 
 
 First off, a little context. Nearly all of the JS I write is for libraries. I write libraries as a hobby, I write libraries as my job, and sometimes I write libraries to help me do my job. My job is usability (HCI) research (and specifically making programming easier), so I’m very sensitive to developer experience issues. I want my libraries to be usable not just by seasoned developers, but by novices too.
 
 
 
 ESM has not been designed with novices in mind. It evolved from the CJS/UMD/AMD ecosystem, in which most voices are seasoned developers. 
 
 
 
 My main gripe with them, is how they expect full adoption, and settle for nothing less. There is no way to create a bundle of a library that can be used both traditionally, with a global, or as an ES module. There is also no standard way to import older libraries, or libraries using other module patterns (yes, this very post is about doing that, but essentially these are hacks, and there should be a better way). I understand the benefits of static analysis for imports and exports, but I wish there was a dynamic alternative to export, analogous to the dynamic import().
 
 
 
 In terms of migrating to ESM, I also dislike how opinionated they are: strict mode is great, but forcing it doesn’t help people trying to migrate older codebases. Restricting them to cross-origin is also a pain, using &lt;script&gt;s from other domains made it possible to quickly experiment with various libraries, and I would love for that to be true for modules too. 
 
 
 
 But overall, I’m excited that JS now natively supports a module mechanism, and I expect any library I release in the future to utilize it.</content>
     </entry>
     <entry>
       <title>Releasing MaVoice: A free app to vote on repo issues</title>
         <link href="https://lea.verou.me/2020/07/releasing-mavoice-a-free-app-to-vote-on-repo-issues/"/>
       <updated>2020-07-11T22:40:38.000Z</updated>
       <content type="text">First off, some news: I agreed to be this year’s CSS content lead for the Web Almanac! One of the first things to do is to flesh out what statistics we should study to answer the question “What is the state of CSS in 2020?”. You can see last year’s chapter to get an idea of what kind of statistics could help answer that question.
 
 
 
 Of course, my first thought was “We should involve the community! People might have great ideas of statistics we could study!”. But what should we use to vote on ideas and make them rise to the top?
 
 
 
 
 
 
 
 I wanted to use a repo to manage all this, since I like all the conveniences for managing issues. However, there is not much on Github for voting. You can add  reactions, but not sort by them, and voting itself is tedious: you need to open the comment, click on the reaction, then go back to the list of issues, rinse and repeat. Ideally, I wanted something like UserVoice, which lets you vote with one click, and sorts proposals by votes.
 
 
 
 And then it dawned on me: I’ll just build a Mavo app on top of the repo issues, that displays them as proposals to be voted on and sorts by  reactions, UserVoice-style but without the UserVoice price tag.  In fact, I had started such a Mavo app a couple years ago, and never finished or released it. So, I just dug it up and resurrected it from its ashes! It’s — quite fittingly I think — called MaVoice.
 
 
 
 
 
 
 
 You can set it to any repo via the repo URL parameter, and any label via the labels URL param (defaults to enhancement) to create a customized URL for any repo you want in seconds! For example, here’s the URL for the css-almanac repo, which only displays issues with the label “proposed stat”: https://leaverou.github.io/mavoice/?repo&#x3D;leaverou/css-almanac&amp;labels&#x3D;proposed%20stat
 
 
 
 While this did need some custom JS, unlike other Mavo apps which need none, I’m still pretty happy I could spin up this kind of app with &lt; 100 lines of JS 
 
 
 
 Yes, it’s still rough around the edges, and I’m sure you can find many things that could be improved, but it does the job for now, and PRs are always welcome 
 
 
 
 The main caveat if you decide to use this for your own repo: Because (to my knowledge) Github API still does not provide a way to sort issues by  reactions, or even reactions in general (in either the v3 REST API, or the GraphQL API), issues are instead requested sorted by comment count, and are sorted by  reactions client-side, right before render. Due to API limitations, this API call can only fetch the top 100 results. This means that if you have more than 100 issues to display (i.e. more than 100 open issues with the given label), it could potentially be inaccurate, especially if you have issues with many reactions and few comments.
 
 
 
 Another caveat is that because this is basically reactions on Github issues, there is no limit on how many issues someone can vote on. In theory, if they’re a bad actor (or just overexcited), they can just vote on everything. But I suppose that’s an intrinsic problem with using reactions to vote for things, having a UI for it just reveals the existing issue, it doesn’t create it.
 
 
 
 Hope you enjoy, and don’t forget to vote on which CSS stats we should study!</content>
     </entry>
     <entry>
       <title>The Cicada Principle, revisited with CSS variables</title>
         <link href="https://lea.verou.me/2020/07/the-cicada-principle-revisited-with-css-variables/"/>
       <updated>2020-07-07T17:50:11.000Z</updated>
       <content type="text">Many of today’s web crafters were not writing CSS at the time Alex Walker’s landmark article The Cicada Principle and Why it Matters to Web Designers was published in 2011. Last I heard of it was in 2016, when it was used in conjunction with blend modes to pseudo-randomize backgrounds even further.
 
 
 
 So what is the Cicada Principle and how does it relate to web design in a nutshell? It boils down to: when using repeating elements (tiled backgrounds, different effects on multiple elements etc), using prime numbers for the size of the repeating unit maximizes the appearance of organic randomness. Note that this only works when the parameters you set are independent.
 
 
 
 When I recently redesigned my blog, I ended up using a variation of the Cicada principle to pseudo-randomize the angles of code snippets. I didn’t think much of it until I saw this tweet:
 
 
 
 
 
 
 
 
 digging the look of code blocks in @LeaVerou’s redesign:(from this article: https://t.co/PAzOuZ1zjW ) pic.twitter.com/RT4IsWMhyL— From The Book of Saw Robson (@StuRobson) June 19, 2020
 
 
 
 
 This made me think: hey, maybe I should actually write a blog post about the technique. After all, the technique itself is useful for way more than angles on code snippets.
 
 
 
 The main idea is simple: You write your main rule using CSS variables, and then use :nth-of-*() rules to set these variables to something different every N items. If you use enough variables, and choose your Ns for them to be prime numbers, you reach a good appearance of pseudo-randomness with relatively small Ns.
 
 
 
 In the case of code samples, I only have two different top cuts (going up or going down) and two different bottom cuts (same), which produce 2*2 &#x3D; 4 different shapes. Since I only had four shapes, I wanted to maximize the pseudo-randomness of their order. A first attempt looks like this:
 
 
 
 pre {
 	clip-path: polygon(var(--clip-top), var(--clip-bottom));
 	--clip-top: 0 0, 100% 2em;
 	--clip-bottom: 100% calc(100% - 1.5em), 0 100%;
 }
 
 pre:nth-of-type(odd) {
 	--clip-top: 0 2em, 100% 0;
 }
 
 pre:nth-of-type(3n + 1) {
 	--clip-bottom: 100% 100%, 0 calc(100% - 1.5em);
 }
 
 
 
 This way, the exact sequence of shapes repeats every 2 * 3 &#x3D; 6 code snippets. Also, the alternative --clip-bottom doesn’t really get the same visibility as the others, being present only 33.333% of the time. However, if we just add one more selector:
 
 
 
 pre {
 	clip-path: polygon(var(--clip-top), var(--clip-bottom));
 	--clip-top: 0 0, 100% 2em;
 	--clip-bottom: 100% calc(100% - 1.5em), 0 100%;
 }
 
 pre:nth-of-type(odd) {
 	--clip-top: 0 2em, 100% 0;
 }
 
 pre:nth-of-type(3n + 1),
 pre:nth-of-type(5n + 1) {
 	--clip-bottom: 100% 100%, 0 calc(100% - 1.5em);
 }
 
 
 
 Now the exact same sequence of shapes repeats every 2 * 3 * 5 &#x3D; 30 code snippets, probably way more than I will have in any article. And it’s more fair to the alternate --clip-bottom, which now gets 1/3 + 1/5 – 1/15 &#x3D; 46.67%, which is almost as much as the alternate --clip-top gets!
 
 
 
 You can explore this effect in this codepen:
 
 
 
 
 
 
 
 
 
 Or, to better explore how different CSS creates different pseudo-randomness, you can use this content-less version with three variations:
 
 
 
 
 
 
 
 
 
 Of course, the illusion of randomness is much better with more shapes, e.g. if we introduce a third type of edge we get 3 * 3 &#x3D; 9 possible shapes:
 
 
 
 
 
 
 
 
 
 I also used primes 7 and 11, so that the sequence repeats every 77 items. In general, the larger primes you use, the better the illusion of randomness, but you need to include more selectors, which can get tedious.
 
 
 
 Other examples
 
 
 
 So this got me thinking: What else would this technique be cool on? Especially if we include more values as well, we can pseudo-randomize the result itself better, and not just the order of only 4 different results.
 
 
 
 So I did a few experiments.
 
 
 
 Pseudo-randomized color swatches
 
 
 
 
 
 Pseudo-randomized color swatches, with variables for hue, saturation, and lightness.
 
 
 
 And an alternative version:
 
 
 
 
 
 
 
 
 
 Which one looks more random? Why do you think that is?
 
 
 
 Pseudo-randomized border-radius
 
 
 
 Admittedly, this one can be done with just longhands, but since I realized this after I had already made it, I figured eh, I may as well include it 
 
 
 
 
 
 
 
 
 
 It is also really cool when combined with pseudo-random colors (just hue this time):
 
 
 
 
 
 
 
 
 
 Pseudo-randomized snowfall
 
 
 
 Lots of things here:
 
 
 
 Using translate and transform together to animate them separately without resorting to CSS.registerPropery()Pseudo-randomized horizontal offset, animation-delay, font-sizeTechnically we don’t need CSS variables to pseudo-randomize font-size, we can just set the property itself. However, variables enable us to pseudo-randomize it via a multiplier, in order to decouple the base font size from the pseudo-randomness, so we can edit them independently. And then we can use the same multiplier in animation-duration to make smaller snowflakes fall slower!
 
 
 
 
 
 
 
 
 
 Conclusions
 
 
 
 In general, the larger the primes you use, the better the illusion of randomness. With smaller primes, you will get more variation, but less appearance of randomness. 
 
 
 
 There are two main ways to use primes to create the illusion of randomness with :nth-child() selectors: 
 
 
 
 The first way is to set each trait on :nth-child(pn + b) where p is a prime that increases with each value and b is constant for each trait, like so:
 
 
 
 :nth-child(3n + 1)  { property1: value11; }
 :nth-child(5n + 1)  { property1: value12; }
 :nth-child(7n + 1)  { property1: value13; }
 :nth-child(11n + 1) { property1: value14; }
 ...
 :nth-child(3n + 2)  { property2: value21; }
 :nth-child(5n + 2)  { property2: value22; }
 :nth-child(7n + 2)  { property2: value23; }
 :nth-child(11n + 2) { property2: value24; }
 ...
 
 
 
 The benefit of this approach is that you can have as few or as many values as you like. The drawback is that because primes are sparse, and become sparser as we go, you will have a lot of “holes” where your base value is applied.
 
 
 
 The second way (which is more on par with the original Cicada principle) is to set each trait on :nth-child(pn + b) where p is constant per trait, and b increases with each value:
 
 
 
 :nth-child(5n + 1) { property1: value11; }
 :nth-child(5n + 2) { property1: value12; }
 :nth-child(5n + 3) { property1: value13; }
 :nth-child(5n + 4) { property1: value14; }
 ...
 :nth-child(7n + 1) { property2: value21; }
 :nth-child(7n + 2) { property2: value22; }
 :nth-child(7n + 3) { property2: value23; }
 :nth-child(7n + 4) { property2: value24; }
 ...
 
 
 
 This creates a better overall impression of randomness (especially if you order the values in a pseudo-random way too) without “holes”, but is more tedious, as you need as many values as the prime you’re using.
 
 
 
 What other cool examples can you think of?</content>
     </entry>
     <entry>
       <title>Using Git Submodules for Private Content</title>
         <link href="https://www.taniarascia.com/git-submodules-private-content/"/>
       <updated>2020-07-01T00:00:00.000Z</updated>
       <content type="text">My website has been open source for as long as it has existed. Originally, it was a WordPress site, but only the layout was out there for everyone to see since the data was saved in a database. Once I moved to Gatsby, I kept all the images and posts in a content directory. This was way better, as my content is all conveniently stored in one easy-to-save folder and the posts are all in beautiful markdown.
 However, people often like my layout and want to use it, so they clone and deploy this site. Sometimes they will just leave up all the posts and images and update the name and image. Although I subscribe to the Zenhabits Uncopyright philosophy towards content - my content is out there for the world to see and do what they want with it, and it doesn&#x27;t bother me - I don&#x27;t think I should make it quite so easy to just clone everything I&#x27;ve written in a moment. If you&#x27;re going to plagiarize, you should at least have to do a bit of work.
 So I decided to store my content in a private git submodule. If you go to the repo for this site now, you&#x27;ll see a folder that looks like content @ &lt;hash&gt;. If you click on it, you&#x27;ll be taken to a 404 page. If I click on it, I&#x27;ll be taken to a separate, private repo that contains all my images and posts.
 
       
     
   
   
     
 A lot of people have asked me how to use private git submodules, so I&#x27;ll go over it here. Note that this is not a deep-dive into submodules, but just the basics of adding, updating, and cloning a repo with submodules.
 Git Submodules
 Git submodules allow you to keep a git repository as a subdirectory of another git repository.
 This could be useful if you have a lot of projects within a project. One example of this is the Dracula code theme repo. Every folder is a git submodule. This allows people to add a new theme for a new program by creating their own repo, and the owner of the parent repository only needs to reference the child repos. You can tell they&#x27;re all submodules because of the @ &lt;hash&gt; after each subdirectory name.
 Before doing anything with submodules, I would recommend running this command to update the config and set submodule.recurse to true, which allows git clone and git pull to automatically update submodules.
 git config --global submodule.recurse true
 
 
 
 Command
 Description
 
 
 
 
 git submodule add &lt;repo&gt;
 Add a submodule within a repository
 
 
 git submodule update
 Update existing submodules within a repository (add --remote to pull from a remote location)
 
 
 git submodule init
 Initialize local submodules file (only necessary if repo not cloned with --recurse-submodules)
 
 
 
 Adding a submodule
 Let&#x27;s imagine that you want a public blog, located on the blog repo, to contain a submodule with all the posts, located in the posts repo. So it will look like:
 
 A public repo at github.com/you/blog
 A private repo at github.com/you/posts
 
 
 I&#x27;m just using GitHub as an example, it doesn&#x27;t matter where the repo is hosted. Also, git submodules can also be used for both private and public repos.
 
 First you can add the submodule. From the root of blog, you would run this command.
 git submodule add https://github.com/you/posts
 This would clone the posts repo into a folder in blog.
 Cloning into &#x27;/Users/you/blog/posts&#x27;...
 You will now have two new entries into the blog repo, a .gitmodules file, and the new posts subdirectory.
 git status
 Changes to be committed:
   (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage)
 	new file:   .gitmodules
 	new file:   posts
 .gitmodules will look like this:
 .gitmodules
 [submodule &quot;posts&quot;]
 	path &#x3D; posts
 	url &#x3D; https://github.com/you/posts
 At this point, you have a reference to the posts repo as a submodule, so the directory structure will look like this:
 .git
 .gitmodules
 posts/
 
 As a note, if you cd into .git, you&#x27;ll see a modules directory. This will contain a folder called posts, and this is where git is storing references and other data about your submodules.
 
 Updating a submodule
 To update submodule content, you&#x27;ll pull in any changes made to the remote submodule repo with the update command. Since you would be updating content from a remote location, you&#x27;ll add the --remote flag. From the root of the blog repo, you would run the command:
 git submodule update --remote
 It&#x27;s important to note that when working with submodules, you shouldn&#x27;t work on or commit your local version of the submodule repo. If you made any changes locally, your version would now be out-of-sync with the submodule repo.
 
 You just want to treat a submodule as an entirely separate repo, but linked. This is much like code found in node_modules for an node project, where the references to the projects are listed in package.json and you know any local changes you make to a dependency in node_modules will not be persisted.
 
 Modified content
 If you make changes locally and run a git status, you will see modified content next to the modified submodule.
 Changes not staged for commit:
   (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)
   (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory)
   (commit or discard the untracked or modified content in submodules)
 	modified:   posts (modified content)
 New commits
 If you make changes to the submodule and bring those commits in properly and run a git status, you will see new commits next to the modified submodule.
 Changes not staged for commit:
   (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)
   (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory)
 	modified:   posts (new commits)
 If you see (new commits), you can commit those changes on the parent repo. You can also check this by viewing the diff.
 git diff
 diff --git a/posts b/posts
 index abc..def 160000
 --- a/posts
 +++ b/posts
 @@ -1 +1 @@
 -Subproject commit abc...
 +Subproject commit def...
 Cloning a repo with submodules
 If you clone an existing repository and it has submodules within it, you&#x27;ll have to init and update to pull in all the submodule content.
 git clone https://github.com/you/posts
 cd posts &amp;&amp; git submodule init &amp;&amp; git submodule update
 You can bypass this by either having the submodules.recurse setting set, or by using the --recurse-submodules flag.
 git clone --recurse-submodules https://github.com/you/posts
 This will clone the directory along with all submodule content.
 Deployment
 For my site, I&#x27;ve made the content submodule private. If you have a Netlify site and want to know what to do to allow Netlify to pull from the private repo, here is an overview of the steps.
 
 Generate a deploy key from Netlify.
 Add the key as a read-only deploy key on the settings for your private repo (found at github.com/you/repo/settings/keys).
 Netlify will now have permissions to fetch the submodules that it reads from your .gitmodules file.
 
 Summary
 Here are the main points from the article:
 
 Submodules are used when a subdirectory in a repo should consist of all the data from another repo.
 You can add a submodule to a project with git add submodule &lt;submodule-repo&gt;.
 You can update submodules within a project with git update submodule --remote.
 You should clone a project that has submodules with --recurse-submodules or set submodule.recurse in your config to do this by default.
 You should not work on any submodule files directly within the parent repo. The submodule directory should be treated only as a reference to another existing repo.
 
 My current process for updating the site looks like:
 
 Make changes to content repo.
 Commit changes to content and push to private submodule hosted on GitHub: git commit &amp;&amp; git push.
 Pull new updates into local taniarascia.com repo: git submodule update --remote.
 Commit the new submodule changes and push to public GitHub repo: git commit &amp;&amp; git push.
 Netlify deploys the new site.
 </content>
     </entry>
     <entry>
       <title>Understanding Template Literals in JavaScript</title>
         <link href="https://www.taniarascia.com/understanding-template-literals/"/>
       <updated>2020-06-30T00:00:00.000Z</updated>
       <content type="text">This article was originally written for DigitalOcean.
 Introduction
 The 2015 edition of the ECMAScript specification (ES6) added template literals to the JavaScript language. Template literals are a new form of making strings in JavaScript that add a lot of powerful new capabilities, such as creating multi-line strings more easily and using placeholders to embed expressions in a string. In addition, an advanced feature called tagged template literals allows you to perform operations on the expressions within a string. All of these capabilities increase your options for string manipulation as a developer, letting you more easily generate dynamic strings that could be used for URLs or functions that customize HTML elements.
 In this article, you will go over the differences between single/double-quoted strings and template literals, running through the various ways to declare strings of different shape, including multi-line strings and dynamic strings that change depending on the value of a variable or expression. You will then learn about tagged templates and see some real-world examples of projects using them.
 Declaring Strings
 This section will review how to declare strings with single quotes and double quotes, and will then show how to do the same with template literals.
 In JavaScript, a string can be written with single quotes (&#x27; &#x27;):
 const single &#x3D; &#x27;Every day is a good day when you paint.&#x27;
 A string can also be written with double quotes (&quot; &quot;):
 const double &#x3D; &quot;Be so very light. Be a gentle whisper.&quot;
 There is no major difference in JavaScript between single- or double-quoted strings, unlike other languages that might allow interpolation in one type of string but not the other. In this context, interpolation refers to the evaluation of a placeholder as a dynamic part of a string.
 The use of single- or double-quoted strings mostly comes down to personal preference and convention, but used in conjunction, each type of string only needs to escape its own type of quote:
 // Escaping a single quote in a single-quoted string
 const single &#x3D; &#x27;&quot;We don\&#x27;t make mistakes. We just have happy accidents.&quot; - Bob Ross&#x27;
 
 // Escaping a double quote in a double-quoted string
 const double &#x3D; &quot;\&quot;We don&#x27;t make mistakes. We just have happy accidents.\&quot; - Bob Ross&quot;
 
 console.log(single)
 console.log(double)
 The result of the log() method here will print the same two strings to the console:
 &quot;We don&#x27;t make mistakes. We just have happy accidents.&quot; - Bob Ross
 &quot;We don&#x27;t make mistakes. We just have happy accidents.&quot; - Bob Ross
 Template literals, on the other hand, are written by surrounding the string with the backtick character, or grave accent (&#x60;):
 const template &#x3D; &#x60;Find freedom on this canvas.&#x60;
 They do not need to escape single or double quotes:
 const template &#x3D; &#x60;&quot;We don&#x27;t make mistakes. We just have happy accidents.&quot; - Bob Ross&#x60;
 However, they do still need to escape backticks:
 const template &#x3D; &#x60;Template literals use the \&#x60; character.&#x60;
 Template literals can do everything that regular strings can, so you could possibly replace all strings in your project with them and have the same functionality. However, the most common convention in codebases is to only use template literals when using the additional capabilities of template literals, and consistently using the single or double quotes for all other simple strings. Following this standard will make your code easier to read if examined by another developer.
 Now that you&#x27;ve seen how to declare strings with single quotes, double quotes, and backticks, you can move on to the first advantage of template literals: writing multi-line strings.
 Multi-line Strings
 In this section, you will first run through the way strings with multiple lines were declared before ES6, then see how template literals make this easier.
 Originally, if you wanted to write a string that spans multiple lines in your text editor, you would use the concatenation operator. However, this was not always a straight-forward process. The following string concatenation seemed to run over multiple lines:
 const address &#x3D; &#x27;Homer J. Simpson&#x27; + &#x27;742 Evergreen Terrace&#x27; + &#x27;Springfield&#x27;
 This might allow you to break up the string into smaller lines and include it over multiple lines in the text editor, but it has no effect on the output of the string. In this case, the strings will all be on one line and not separated by newlines or spaces. If you logged address to the console, you would get the following:
 Homer J. Simpson742 Evergreen TerraceSpringfield
 The backslash character (\) can be used to continue the string onto multiple lines:
 const address &#x3D;
   &#x27;Homer J. Simpson\
   742 Evergreen Terrace\
   Springfield&#x27;
 This will retain any indentation as whitespace, but the string will still be on one line in the output:
 Homer J. Simpson  742 Evergreen Terrace  Springfield
 Using the newline character (\n), you can create a true multi-line string:
 const address &#x3D; &#x27;Homer J. Simpson\n&#x27; + &#x27;742 Evergreen Terrace\n&#x27; + &#x27;Springfield&#x27;
 When logged to the console, this will display the following:
 Homer J. Simpson
 742 Evergreen Terrace
 Springfield
 Using newline characters to designate multi-line strings can be counterintuitive, however. In contrast, creating a multi-line string with template literals can be much more straight-forward. There is no need to concatenate, use newline characters, or use backslashes. Just pressing enter and writing the string across multiple lines works by default:
 const address &#x3D; &#x60;Homer J. Simpson
 742 Evergreen Terrace
 Springfield&#x60;
 The output of logging this to the console is the same as the input:
 Homer J. Simpson
 742 Evergreen Terrace
 Springfield
 Any indentation will be preserved, so it&#x27;s important not to indent any additional lines in the string if that is not desired. For example, consider the following:
 const address &#x3D; &#x60;Homer J. Simpson
                  742 Evergreen Terrace
                  Springfield&#x60;
 Although this style of writing the line might make the code more human readable, the output will not be:
 Homer J. Simpson
                  742 Evergreen Terrace
                  Springfield
 With multi-line strings now covered, the next section will deal with how expressions are interpolated into their values with the different string declarations.
 Expression Interpolation
 In strings before ES6, concatenation was used to create a dynamic string with variables or expressions:
 const method &#x3D; &#x27;concatenation&#x27;
 const dynamicString &#x3D; &#x27;This string is using &#x27; + method + &#x27;.&#x27;
 When logged to the console, this will yield the following:
 This string is using concatenation.
 With template literals, an expression can be embedded in a placeholder. A placeholder is represented by ${}, with anything within the curly brackets treated as JavaScript and anything outside the brackets treated as a string:
 const method &#x3D; &#x27;interpolation&#x27;
 const dynamicString &#x3D; &#x60;This string is using ${method}.&#x60;
 When dynamicString is logged to the console, the console will show the following:
 This string is using interpolation.
 One common example of embedding values in a string might be for creating dynamic URLs. With concatenation, this can be cumbersome. For example, the following declares a function to generate an OAuth access string:
 function createOAuthString(host, clientId, scope) {
   return host + &#x27;/login/oauth/authorize?client_id&#x3D;&#x27; + clientId + &#x27;&amp;scope&#x3D;&#x27; + scope
 }
 
 createOAuthString(&#x27;https://github.com&#x27;, &#x27;abc123&#x27;, &#x27;repo,user&#x27;)
 Logging this function will yield the following URL to the console:
 https://github.com/login/oauth/authorize?client_id&#x3D;abc123&amp;scope&#x3D;repo,user
 Using string interpolation, you no longer have to keep track of opening and closing strings and concatenation operator placement. Here is the same example with template literals:
 function createOAuthString(host, clientId, scope) {
   return &#x60;${host}/login/oauth/authorize?client_id&#x3D;${clientId}&amp;scope&#x3D;${scope}&#x60;
 }
 
 createOAuthString(&#x27;https://github.com&#x27;, &#x27;abc123&#x27;, &#x27;repo,user&#x27;)
 This will have the same output as the concatenation example:
 https://github.com/login/oauth/authorize?client_id&#x3D;abc123&amp;scope&#x3D;repo,user
 The trim() method can also be used on a template literal to remove any whitespace at the beginning or end of the string. For example, the following uses an arrow function to create an HTML &lt;li&gt; element with a customized link:
 const menuItem &#x3D; (url, link) &#x3D;&gt;
   &#x60;
 &lt;li&gt;
   &lt;a href&#x3D;&quot;${url}&quot;&gt;${link}&lt;/a&gt;
 &lt;/li&gt;
 &#x60;.trim()
 
 menuItem(&#x27;https://google.com&#x27;, &#x27;Google&#x27;)
 The result will be trimmed of all the whitespace, ensuring that the element will be rendered correctly:
 &lt;li&gt;
   &lt;a href&#x3D;&quot;https://google.com&quot;&gt;Google&lt;/a&gt;
 &lt;/li&gt;
 Entire expressions can be interpolated, not just variables, such as in this example of the sum of two numbers:
 const sum &#x3D; (x, y) &#x3D;&gt; x + y
 const x &#x3D; 5
 const y &#x3D; 100
 const string &#x3D; &#x60;The sum of ${x} and ${y} is ${sum(x, y)}.&#x60;
 
 console.log(string)
 This code defines the sum function and the variables x and y, then uses both the function and the variables in a string. The logged result will show the following:
 The sum of 5 and 100 is 105.
 This can be particularly useful with ternary operators, which allow conditionals within a string:
 const age &#x3D; 19
 const message &#x3D; &#x60;You can ${age &lt; 21 ? &#x27;not&#x27; : &#x27;&#x27;} view this page&#x60;
 console.log(message)
 The logged message here will change depnding on whether the value of age is over or under 21. Since it is 19 in this example, the following output will be logged:
 You can not view this page
 Now you have an idea of how template literals can be useful when used to interpolate expressions. The next section will take this a step further by examining tagged template literals to work with the expressions passed into placeholders.
 Tagged Template Literals
 An advanced feature of template literals is the use of tagged template literals, sometimes referred to as template tags. A tagged template starts with a tag function that parses a template literal, allowing you more control over manipulating and returning a dynamic string.
 In this example, you&#x27;ll create a tag function to use as the function operating on a tagged template. The string literals are the first parameter of the function, named strings here, and any expressions interpolated into the string are packed into the second parameter using rest parameters. You can console out the parameter to see what they will contain:
 function tag(strings, ...expressions) {
   console.log(strings)
   console.log(expressions)
 }
 Use the tag function as the tagged template function and parse the string as follows:
 const string &#x3D; tag&#x60;This is a string with ${true} and ${false} and ${100} interpolated inside.&#x60;
 Since you&#x27;re console logging strings and expressions, this will be the output:
 (4) [&quot;This is a string with &quot;, &quot; and &quot;, &quot; and &quot;, &quot; interpolated inside.&quot;
 (3) [true, false, 100]
 The first parameter, strings, is an array containing all the string literals:
 
 &quot;This is a string with &quot;
 &quot; and &quot;
 &quot; and &quot;
 &quot; interpolated inside.&quot;
 
 There is also a raw property available on this argument at strings.raw, which contains the strings without any escape sequences being processed. For example, /n would just be the character /n and not be escaped into a newline.
 The second parameter, ...expressions, is a rest parameter array consisting of all the expressions:
 
 true
 false
 100
 
 The string literals and expressions are passed as parameters to the tagged template function tag. Note that the tagged template does not need to return a string; it can operate on those values and return any type of value. For example, we can have the function ignore everything and return null, as in this returnsNull function:
 function returnsNull(strings, ...expressions) {
   return null
 }
 
 const string &#x3D; returnsNull&#x60;Does this work?&#x60;
 console.log(string)
 Logging the string variable will return:
 null
 An example of an action that can be performed in tagged templates is applying some change to both sides of each expression, such as if you wanted to wrap each expression in an HTML tag. Create a bold function that will add &lt;strong&gt; and &lt;/strong&gt; to each expression:
 function bold(strings, ...expressions) {
   let finalString &#x3D; &#x27;&#x27;
 
   // Loop through all expressions
   expressions.forEach((value, i) &#x3D;&gt; {
     finalString +&#x3D; &#x60;${strings[i]}&lt;strong&gt;${value}&lt;/strong&gt;&#x60;
   })
 
   // Add the last string literal
   finalString +&#x3D; strings[strings.length - 1]
 
   return finalString
 }
 
 const string &#x3D; bold&#x60;This is a string with ${true} and ${false} and ${100} interpolated inside.&#x60;
 
 console.log(string)
 This code uses the forEach method to loop over the expressions array and add the bolding element:
 This is a string with &lt;strong&gt;true&lt;/strong&gt; and &lt;strong&gt;false&lt;/strong&gt; and &lt;strong&gt;100&lt;/strong&gt; interpolated inside.
 There are a few examples of tagged template literals in popular JavaScript libraries. The graphql-tag library uses the gql tagged template to parse GraphQL query strings into the abstract syntax tree (AST) that GraphQL understands:
 import gql from &#x27;graphql-tag&#x27;
 
 // A query to retrieve the first and last name from user 5
 const query &#x3D; gql&#x60;
   {
     user(id: 5) {
       firstName
       lastName
     }
   }
 &#x60;
 Another library that uses tagged template functions is styled-components, which allows you to create new React components from regular DOM elements and apply additional CSS styles to them:
 import styled from &#x27;styled-components&#x27;
 
 const Button &#x3D; styled.button&#x60;
   color: magenta;
 &#x60;
 
 // &lt;Button&gt; can now be used as a custom component
 The built-in String.raw method can also be used on tagged template literals to prevent any escape sequences from being processed:
 const rawString &#x3D; String.raw&#x60;I want to write /n without it being escaped.&#x60;
 console.log(rawString)
 This will log the folowing:
 I want to write /n without it being escaped.
 Conclusion
 In this article, you reviewed single- and double-quoted string literals and you learned about template literals and tagged template literals. Template literals make a lot of common string tasks simpler by interpolating expressions in strings and creating multi-line strings without any concatenation or escaping. Template tags are also a useful advanced feature of template literals that many popular libraries have used, such as GraphQL and styled-components.
 To learn more about strings in JavaScript, read How To Work with Strings in JavaScript and How To Index, Split, and Manipulate Strings in JavaScript.</content>
     </entry>
     <entry>
       <title>Another Website Redesign - and Making the Internet Better</title>
         <link href="https://www.taniarascia.com/another-website-redesign/"/>
       <updated>2020-06-26T00:00:00.000Z</updated>
       <content type="text">At the age of 9, when I was too shy, young, inexperienced, and insecure to know how to make friends in the real world, I discovered the internet - and it was awesome.
 Maybe I&#x27;m looking back at it all through the rose-tinted glasses of nostalgia - conveniently forgetting pop-under ads, horrible dial-up speeds, falling snowflake gifs, and the infinitely nested BBS.
 But I think anyone who was there remembers it as a truly special time. I feel immensely lucky that I grew up right in that transitionary era, a sliver of time between two completely different universes. I experienced a childhood in which my choices were limited, I went to the library to learn new things, and I discovered music on analog cassettes and LPs. Then I spent the most formidable years of my adolescence discovering the world and myself through the internet when it was the wild west.
 To be honest, I don&#x27;t really like the internet anymore. Of course, it&#x27;s still an incredibly useful tool and there are still pockets of what made it amazing to begin with, but it&#x27;s getting harder and harder to find amidst all the noise.
 When I use the internet now, I feel like I&#x27;m just a piece of data to be exploited. Some data to be analyzed and marketed to. You can&#x27;t look up a recipe without being prompted with desperate attempts to influence your behavior, a video pop up, a cookie consent notice, dozens of ads, and clickbait littered throughout the page. Try looking up just about anything without getting &quot;7 ways to...&quot; and &quot;12 surprising examples of...&quot; type of shit headlines meant to drive traffic to their useless website.
 In the beginning, social media was a place to be creative and make your little corner of the web even if you didn&#x27;t know any HTML. It became a place to connect with your friends, share a bit of your life, and do stupid little quizzes and games. Now, all social media is useless. Just another medium through which ads and far too many opinions can be injected into your brain.
 Although the tide is against me, I want to make the internet a better place. Even if it&#x27;s just right here. I hope I will inspire others to make their own creative corner of the web.
 The Things We Don&#x27;t Need
 I decided to redesign my website and start from scratch and make sure it was a clean, easy-to-navigate site that didn&#x27;t have a single thing it didn&#x27;t need. I tagged this latest version 3.0 (previously I wrote about version 2.0 and version 4.0, and version 1 was actually like 200 versions, so I&#x27;m not very good at versioning this site). In my sidebar, I list a lot of the things this site doesn&#x27;t have, and I&#x27;ll reiterate them a bit here.
 Before anyone freaks out at me, I&#x27;m not judging anyone who has one or more of these things on their site. I just don&#x27;t want them. And I do wish less websites had them, but if you&#x27;re actually making a living from ads that&#x27;s none of my business.
 No ads
 One of my favorite websites is Hyperbole and a Half. And I remember reading her thoughts about having ads on the FAQ a long time ago:
 &quot;I feel that having advertisements on my page creates a sort of weirdness about my motivation for writing. I think some people feel used when a site they enjoy is plastered with ads, and I don&#x27;t want to make you guys feel like that. I&#x27;m more comfortable having just the one little button for my store. It feels less intrusive and it lets people choose whether they want to support me or not.&quot; — Allie Brosh
 I distinctly remember reading that and thinking...you have such a successful blog, why wouldn&#x27;t you want ads? It&#x27;s free money. The fact that someone with one of the most visited blogs on the internet wouldn&#x27;t want ads really stuck with me.
 From the first day this site was published in September 2015, there has never been an ad on it. I have, over the years, made a few thousand dollars in donations and support. For the first few years, I didn&#x27;t have any donate button at all because I thought it seemed weird, spammy, and needy to have one, but I realized when you do something cool, people want to show their appreciation.
 I&#x27;m obviously not trying to (and would be unable to) make a living from this site, but the readers and I have a symbiotic relationship where they get quality content, I have fun making the content I want, and if anyone would like to support it they can. I don&#x27;t care about spamming my website out, because I don&#x27;t get money from visits.
 I have always thought to myself (and I discussed this on with Joel Hooks on the egghead.io podcast) that if I resorted to ads, I would be giving up. I still believe eventually I can write a book, course, or some other content that people will voluntarily pay for (some day!).
 No social media
 This site doesn&#x27;t need a link to Twitter, or LinkedIn, or Facebook, or anything else. Nobody needs to know how many followers I do or don&#x27;t have. Of course, I think this extends beyond just the website - I deleted Facebook years ago among other social media platforms and I would highly recommend it to everyone. I stopped going on Twitter (and reddit) except for the occasional poll to the community and to share new posts, but even that limited exposure is starting to feel like too much. No matter how hard I try to curate my feed, it&#x27;s impossible not to see angry mobs and brigades. I have made some good friends and discovered some opportunities through Twitter as well, so it&#x27;s difficult.
 I can anticipate at this point that some people will see me as pretentious and stuck-up, thinking I&#x27;m so superior because I don&#x27;t do social media. Honestly, if a real platform existed where I could just communicate with my friends, make new friends, and read blogs by real people with no corporations or ads, I would love to be a part of that. I just don&#x27;t even know if it&#x27;s possible anymore. I don&#x27;t know if any centralized platform can do that in 2020. Fortunately, we have our decentralized websites, and we can connect with each other this way.
 No tracking or analytics
 I removed Google Analytics from the site. I had it on there for years, and I know somewhere between 200,000 and 300,000 people visit the site per month, and 80% or so is organic search. I glanced at it maybe twice a year, and I never made any decisions based on the data on there. Since I&#x27;m not trying to drive any traffic to my site for ad clicks, it doesn&#x27;t really matter how many hits to the site I get. So from this point forth, I&#x27;m removing it. And on that note...
 No third party scripts
 Aside from Google Analytics, I think I only had one third party script, which was a little script to pull the follower count from GitHub for a site badge. It was cute, but unnecessary. I thought that having the follower count added a little bit of credibility to my name at first glance for anyone who doesn&#x27;t know me, but ultimately I don&#x27;t want to be motivated by numbers.
 Update: I&#x27;m testing out Utterances, which is a really cool open-source project that uses the GitHub issue API for comments, so I can&#x27;t strictly say I have no third party scripts at all anymore, though I think this one is very clean and useful.
 No sponsored posts
 Not a day goes by that I don&#x27;t get an email from someone asking to put a sponsored post on my website. I&#x27;m not sure where anyone ever got the idea that I would put a sponsored or guest post on the site. First of all, it&#x27;s taniarascia.com. It would be pretty weird if every other post was written by someone else.
 No affiliate links
 My first ever paid webhost was NearlyFreeSpeech.net, and somewhere on their FAQ they posted &quot;Affiliate programs make it difficult for a web designer to make objective recommendations about what&#x27;s good for your business. So good web designers generally don&#x27;t participate in affiliate programs, and you can rely on their advice. We don&#x27;t have an affiliate program, so when someone recommends us, you can be comfortable that it&#x27;s because they like us and think we&#x27;re a good fit for you.&quot; I thought that was pretty cool. I&#x27;m not selling anything, but I wouldn&#x27;t feel comfortable doing any sort of affiliate program.
 No paywall
 All the content on this site is free and freely accessible. It&#x27;s incredibly annoying to be looking something up and find a post that matches what you&#x27;re looking for, only to discover it was posted on Medium, so you have to go through a bunch of hoops to view it without making an account. Also, Medium doesn&#x27;t have syntax highlighting for code blocks, so I wish people would use a different publishing platform for dev content in general.
 Comments?
 This site originally had comments, and there were some pros to this. Comments gave a lot of proof to the website. For example, one of my first popular posts was the WordPress theme development guide - that post alone had over a thousand comments, two or three years ago. It was easy for someone to quickly scan the comments, see that there were so many and that they were all positive, and know that the article would probably be helpful. I don&#x27;t have anything like that anymore, only the reputation that I hope I&#x27;ve built over the last 5 years. Comments also made the site interactive, and potentially gave people a reason to come back.
 Update: I&#x27;m testing out comments now, so it remains to be seen whether or not they stay.
 So What&#x27;s Left?
 I thought the old design was pretty good, but it was a little too simple.
 Organization
 I haven&#x27;t written much lately. I haven&#x27;t had time to write the long form articles that I would like to, and I keep feeling like small snippets, notes, opinions, musings, and so on didn&#x27;t have a place in the old blog. So now, I&#x27;ve divided the site into two sections - the blog, which contains everything, and guides, which contain my complete tutorials, walkthroughs, and reference guides. I also tagged everything nicely. This was my biggest reason for redesigning the blog, and I think I&#x27;ll feel more comfortable to make posts about anything I want now.
 I want the site to be less of a blog and more of a digital garden, which I read about through Joel&#x27;s site. I want to have more fun and whimsy with it. It&#x27;s not completely there yet, but I feel more free to add whatever I want, wherever I want.
 Search
 The search was extremely lame - I quickly hacked it together and all it did was search the titles in the DOM. The biggest new thing I&#x27;ve added to the site is flexsearch, so the search bar actually looks through the whole post body as well as tags, metadata, etc.
 Originality
 My site is open source, and the old design was up for a few years. As such, many people cloned the site, so it didn&#x27;t feel very unique to me anymore. The design of this one isn&#x27;t much different, because I love that simple minimalist look, but the structure is a bit different.
 Simplicity
 I wanted to be absolutely certain that no dependencies or code existed in my codebase that weren&#x27;t necessary. I also wanted to use the latest version of Node (14.4, as of this writing), so I redid everything from scratch. I also wrote all the CSS in one style.css file, without any Sass or libraries or anything else.
 Speed
 Just having the essentials has kept the site extremely fast.
 
       
     
   
   
     
 A fast site is extremely important to me. I highly recommend Gatsby and Netlify.
 Here are two pictures of what it looks like, as I&#x27;m sure in a year I&#x27;ll have a new layout once again.
 
       
     
   
   
     
 
       
     
   
   
     
 Final Thought
 I hope you like my site. Having a place to call my own on the internet has always been important to me. When I created this blog, I finally found something that I love doing that helps others as much as it helps me. And it has created so much opportunity in my life. If you&#x27;re thinking of making a website, I hope you do it. If you&#x27;re thinking of deleting social media, I hope you do it.
 Here are a few sources of inspiration that I used for the site.
 
 Derek Sivers
 Zander Martineau
 Joel Hooks
 Christian Tietze
 Leo Babauta
 </content>
     </entry>
     <entry>
       <title>Refactoring optional chaining into a large codebase: lessons learned</title>
         <link href="https://lea.verou.me/2020/06/refactoring-optional-chaining-into-a-large-codebase-lessons-learned/"/>
       <updated>2020-06-18T14:56:08.000Z</updated>
       <content type="text">Chinese translation by Coink Wang
 
 
 
 
 
 
 
 Now that optional chaining is supported across the board, I decided to finally refactor Mavo to use it (yes, yes, we do provide a transpiled version as well for older browsers, settle down). This is a moment I have been waiting for a long time, as I think optional chaining is the single most substantial JS syntax improvement since arrow functions and template strings. Yes, I think it’s more significant than async/await, just because of the mere frequency of code it improves. Property access is literally everywhere.
 
 
 
 
 
 
 
 First off, what is optional chaining, in case you haven’t heard of it before?
 
 
 
 You know how you can’t just do foo.bar.baz() without checking if foo exists, and then if foo.bar exists, and then if foo.bar.baz exists because you’ll get an error? So you have to do something awkward like:
 
 
 
 if (foo &amp;&amp; foo.bar &amp;&amp; foo.bar.baz) {
 	foo.bar.baz();
 }
 
 
 
 Or even:
 
 
 
 foo &amp;&amp; foo.bar &amp;&amp; foo.bar.baz &amp;&amp; foo.bar.baz();
 
 
 
 Some even contort object destructuring to help with this. With optional chaining, you can just do this:
 
 
 
 foo?.bar?.baz?.()
 
 
 
 It supports normal property access, brackets (foo?.[bar]), and even function invocation (foo?.()). Sweet, right??
 
 
 
 Yes, mostly. Indeed, there is SO MUCH code that can be simplified with it, it’s incredible. But there are a few caveats.
 
 
 
 Patterns to search for
 
 
 
 Suppose you decided to go ahead and refactor your code as well. What to look for?
 
 
 
 There is of course the obvious foo &amp;&amp; foo.bar that becomes foo?.bar.
 
 
 
 There is also the conditional version of it, that we described in the beginning of this article, which uses if() for some or all of the checks in the chain.
 
 
 
 There are also a few more patterns.
 
 
 
 Ternary
 
 
 
 foo? foo.bar : defaultValue
 
 
 
 Which can now be written as:
 
 
 
 foo?.bar || defaultValue
 
 
 
 or, using the other awesome new operator, the nullish coalescing operator:
 
 
 
 foo?.bar ?? defaultValue
 
 
 
 Array checking
 
 
 
 if (foo.length &gt; 3) {
 	foo[2]
 }
 
 
 
 which now becomes:
 
 
 
 foo?.[2]
 
 
 
 Note that this is no substitute for a real array check, like the one done by Array.isArray(foo). Do not go about replacing proper array checking with duck typing because it’s shorter. We stopped doing that over a decade ago.
 
 
 
 Regex match
 
 
 
 Forget about things like this:
 
 
 
 let match &#x3D; &quot;#C0FFEE&quot;.match(/#([A-Z]+)/i);
 let hex &#x3D; match &amp;&amp; match[1];
 
 
 
 Or even things like that:
 
 
 
 let hex &#x3D; (&quot;#C0FFEE&quot;.match(/#([A-Z]+)/i) || [,])[1];
 
 
 
 Now it’s just:
 
 
 
 let hex &#x3D; &quot;#C0FFEE&quot;.match(/#([A-Z]+)/i)?.[1];
 
 
 
 In our case, I was able to even remove two utility functions and replace their invocations with this.
 
 
 
 Feature detection
 
 
 
 In simple cases, feature detection can be replaced by ?.. For example:
 
 
 
 if (element.prepend) element.prepend(otherElement);
 
 
 
 becomes:
 
 
 
 element.prepend?.(otherElement);
 
 
 
 Don’t overdo it
 
 
 
 While it may be tempting to convert code like this:
 
 
 
 if (foo) {
 	something(foo.bar);
 	somethingElse(foo.baz);
 	andOneLastThing(foo.yolo);
 }
 
 
 
 to this:
 
 
 
 something(foo?.bar);
 somethingElse(foo?.baz);
 andOneLastThing(foo?.yolo);
 
 
 
 Don’t. You’re essentially having the JS runtime check foo three times instead of one. You may argue these things don’t matter much anymore performance-wise, but it’s the same repetition for the human reading your code: they have to mentally process the check for foo three times instead of one. And if they need to add another statement using property access on foo, they need to add yet another check, instead of just using the conditional that’s already there.
 
 
 
 Caveats
 
 
 
 You still need to check before assignment
 
 
 
 You may be tempted to convert things like:
 
 
 
 if (foo &amp;&amp; foo.bar) {
 	foo.bar.baz &#x3D; someValue;
 }
 
 
 
 to:
 
 
 
 foo?.bar?.baz &#x3D; someValue;
 
 
 
 Unfortunately, that’s not possible and will error. This was an actual snippet from our codebase:
 
 
 
 if (this.bar &amp;&amp; this.bar.edit) {
 	this.bar.edit.textContent &#x3D; this._(&quot;edit&quot;);
 }
 
 
 
 Which I happily refactored to:
 
 
 
 if (this.bar?.edit) {
 	this.bar.edit.textContent &#x3D; this._(&quot;edit&quot;);
 }
 
 
 
 All good so far, this works nicely. But then I thought, wait a second… do I need the conditional at all? Maybe I can just do this:
 
 
 
 this.bar?.edit?.textContent &#x3D; this._(&quot;edit&quot;);
 
 
 
 Nope. Uncaught SyntaxError: Invalid left-hand side in assignment. Can’t do that. You still need the conditional. I literally kept doing this, and I’m glad I had ESLint in my editor to warn me about it without having to actually run the code.
 
 
 
 It’s very easy to put the ?. in the wrong place or forget some ?.
 
 
 
 Note that if you’re refactoring a long chain with optional chaining, you often need to insert multiple ?. after the first one, for every member access that may or may not exist, otherwise you will get errors once the optional chaining returns undefined.
 
 
 
 Or, sometimes you may think you do, because you put the ?. in the wrong place.
 
 
 
 Take the following real example. I originally refactored this:
 
 
 
 this.children[index]? this.children[index].element : this.marker
 
 
 
 into this:
 
 
 
 this.children?.[index].element ?? this.marker
 
 
 
 then got a TypeError: Cannot read property &#x27;element&#x27; of undefined. Oops! Then I fixed it by adding an additional ?.:
 
 
 
 this.children?.[index]?.element ?? this.marker
 
 
 
 This works, but is superfluous, as pointed out in the comments. I just needed to move the ?.:
 
 
 
 this.children.[index]?.element ?? this.marker
 
 
 
 Note that as pointed out in the comments be careful about replacing array length checks with optional access to the index. This might be bad for performance, because out-of-bounds access on an array is de-optimizing the code in V8 (as it has to check the prototype chain for such a property too, not only decide that there is no such index in the array).
 
 
 
 It can introduce bugs if you’re not careful
 
 
 
 If, like me, you go on a refactoring spree, it’s easy after a certain point to just introduce optional chaining in places where it actually ends up changing what your code does and introducing subtle bugs. 
 
 
 
 null vs undefined
 
 
 
 Possibly the most common pattern is replacing foo &amp;&amp; foo.bar with foo?.bar. While in most cases these work equivalently, this is not true for every case. When foo is null, the former returns null, whereas the latter returns undefined. This can cause bugs to creep up in cases where the distinction matters and is probably the most common way to introduce bugs with this type of refactoring.
 
 
 
 Equality checks
 
 
 
 Be careful about converting code like this:
 
 
 
 if (foo &amp;&amp; bar &amp;&amp; foo.prop1 &#x3D;&#x3D;&#x3D; bar.prop2) { /* ... */ }
 
 
 
 to code like this:
 
 
 
 if (foo?.prop1 &#x3D;&#x3D;&#x3D; bar?.prop2) { /* ... */ }
 
 
 
 In the first case, the condition will not be true, unless both foo and bar are truthy. However, in the second case, if both foo and bar are nullish, the conditional will be true, because both operands will return undefined!
 
 
 
 The same bug can creep in even if the second operand doesn’t include any optional chaining, as long as it could be undefined you can get unintended matches.
 
 
 
 Operator precedence slips
 
 
 
 One thing to look out for is that optional chaining has higher precedence than &amp;&amp;. This becomes particularly significant when you replace an expression using &amp;&amp; that also involves equality checks, since the (in)equality operators are sandwiched between ?. and &amp;&amp;, having lower precedence than the former and higher than the latter. 
 
 
 
 if (foo &amp;&amp; foo.bar &#x3D;&#x3D;&#x3D; baz) { /* ... */ }
 
 
 
 What is compared with baz here? foo.bar or foo &amp;&amp; foo.bar? Since &amp;&amp; has lower precedence than &#x3D;&#x3D;&#x3D;, it’s as if we had written:
 
 
 
 if (foo &amp;&amp; (foo.bar &#x3D;&#x3D;&#x3D; baz)) { /* ... */ }
 
 
 
 Note that the conditional cannot ever be executed if foo is falsy. However, once we refactor it to use optional chaining, it is now as if we were comparing (foo &amp;&amp; foo.bar) to baz:
 
 
 
 if (foo?.bar &#x3D;&#x3D;&#x3D; baz) { /* ... */ }
 
 
 
 An obvious case where the different semantics affect execution is when baz is undefined. In that case, we can enter the conditional when foo is nullish, since then optional chaining will return undefined, which is basically the case we described above. In most other cases this doesn’t make a big difference. It can however be pretty bad when instead of an equality operator, you have an inequality operator, which still has the same precedence. Compare this:
 
 
 
 if (foo &amp;&amp; foo.bar !&#x3D;&#x3D; baz) { /* ... */ }
 
 
 
 with this:
 
 
 
 if (foo?.bar !&#x3D;&#x3D; baz) { /* ... */ }
 
 
 
 Now, we are going to enter the conditional every time foo is nullish, as long as baz is not undefined! The difference is not noticeable in an edge case anymore, but in the average case! 
 
 
 
 Return statements
 
 
 
 Rather obvious after you think about it, but it’s easy to forget return statements in the heat of the moment. You cannot replace things like this:
 
 
 
 if (foo &amp;&amp; foo.bar) {
 	return foo.bar();
 }
 
 
 
 with:
 
 
 
 return foo?.bar?.();
 
 
 
 In the first case, you return conditionally, whereas in the second case you return always. This will not introduce any issues if the conditional is the last statement in your function, but it will change the control flow if it’s not.
 
 
 
 Sometimes, it can fix bugs too!
 
 
 
 Take a look at this code I encountered during my refactoring:
 
 
 
 /**
  * Get the current value of a CSS property on an element
  */
 getStyle: (element, property) &#x3D;&gt; {
 	if (element) {
 		var value &#x3D; getComputedStyle(element).getPropertyValue(property);
 
 		if (value) {
 			return value.trim();
 		}
 	}
 },
 
 
 
 Can you spot the bug? If value is an empty string (and given the context, it could very well be), the function will return undefined, because an empty string is falsy! Rewriting it to use optional chaining fixes this:
 
 
 
 if (element) {
 	var value &#x3D; getComputedStyle(element).getPropertyValue(property);
 
 	return value?.trim();
 }
 
 
 
 Now, if value is the empty string, it will still return an empty string and it will only return undefined when value is nullish.
 
 
 
 Finding usages becomes trickier
 
 
 
 This was pointed out by Razvan Caliman on Twitter:
 
 
 
 
 Thank you for sharing your experience! When looking to use the optional chaining operator within Firefox DevTools, one thing we had to consider is how it affects project-wide search results for uses of a method, like &#x60;object.destroy()&#x60; vs &#x60;object.destroy?.()&#x60;, when refactoring.— Razvan Caliman (@razvancaliman) June 18, 2020
 
 
 
 
 Bottom line
 
 
 
 In the end, this refactor made Mavo about 2KB lighter and saved 37  lines of code. It did however make the transpiled version 79 lines and  9KB (!) heavier.
 
 
 
 Here is the relevant commit, for your perusal. I tried my best to exercise restraint and not introduce any unrelated refactoring in this commit, so that the diff is chock-full of optional chaining examples. It has 104 additions and 141 deletions, so I’d wager it has about 100 examples of optional chaining in practice. Hope it’s helpful!</content>
     </entry>
     <entry>
       <title>WDRL — Edition 282: Welcome back again, the web has changed, work as well.</title>
         <link href="https://wdrl.info/archive/282"/>
       <updated>2020-06-10T15:00:00.000Z</updated>
       <content type="text">Hey,
 
 Hello my fellow readers! After months of silence I’m back. Since my last update in February a lot has changed, not only in my life but in everyones’. A virus makes us living differently than we are  used to and work has changed with it. A lot of people work from home these days, others lost their jobs and many are facing a change of rules in any part of life. Kids can’t go to school, vacation is not allowed for a specific time or in other cases is enforced to use during the calm times.
 For myself, my life changed quite a bit in March and April. I started with a new field to grow vegetables nearby in March and it keeps me busy for most of the week right now. It’s a CSA and we can already give ten families including our own nearly enough vegetables for the week with the amount growing every week. In April, my son was born, right in the middle of a weird time that enabled us to welcome him calm and easy to our world.
 The Web… where to start? For me, now just watching from outside as I laid down all my jobs during the past months to enjoy family time and do the garden, it felt weird to read about all the technological changes during a virus that changes the lives of many people. On the other hand, life goes on, work goes on, and it’s great to see that some people continue to build amazing things, to make better browsers, to invent technologies and services that help all the other people out there struggling right now.
 Our own species to me remains a mystery: While during the crisis and lockdowns, we achieved to lower our greenhouse gas emissions, this quickly goes back or even worse since we “recover” from the lockdowns. I lost my hope that people will fly less now for private and business purposes and may rethink their lives and get on a more sustainable track. I read a lot about how to prevent getting sick, how to stay sane during this heavy, exhausting times and it’s all about your inner peace, about health and about happiness. Many solutions how to achieve it are going outside right where you are, meditate, get rid of stuff you don’t use or need. Most people in my country know less about cities and villages or areas in their own country than in Italy or France or even more abroad. Why is it still not cool to go on vacation in your own country or neighbour country? It takes you only 1 – 3 hours by train instead of a flight that takes 16 hours just to enjoy another beach that’s no different to what you could visit also with a 2 – 3 hours flight. We all watch nature and outdoor adventure documentaries on Netflix but we don’t do it ourselves. I’ve never been truly happier at work than in the past months being outdoors on my field every day, no matter which weather. My back does not hurt anymore, I’m less stressed out and overall way happier. I don’t earn a lot of money right now and we all know that earning money with growing  vegetables by hand and with love doesn’t pay a lot of bills. But I decided now that it just needs to pay my bills. I’ll continue to be a freelance developer to earn a baseline of money but splitting my work into two jobs was by far the best I could do for myself lately.
 As always, my thoughts are thought as a trigger to you. Disregard it if you don’t want to think about it or don’t have the time right now. Reply to me if you think you want to say something or have a question. Share it, if you think you know someone who would benefit from it or write your own thoughts to the public if you think they’ll benefit others.
 News
 
 	Uhm, so there’s Firefox 73, 74, 75, 76, and 77 out since my last update. The most important news are: Better WebRTC privacy protections, faster search bar, locally cached trusted Web PKI Certificate Authority certificates, support for the loading attribute on img elements, Picture-in-picture support, Audio Worklet support, and support for the JavaScript API String.prototype.replaceAll().
 	WebKit in the meanwhile introduced Full Third-Party Cookie Blocking to improve privacy for users. Quite a big change although we could see it coming.
 	Chrome 80, 81, 83 now supports Module Workers, optional chaining in JavaScript, trusted types, new default form styles, support for the native file system API, and new cross-origin policies.
 	Safari 13.1 adds support for the Web Animation API, Async Clipboard API, JavaScript replaceAll, nullish coalesing operator, ResizeObserver, the enterkeyhint attribute for form inputs, Picture-in-Picture API, and the RemotePlayback API.
 
 Generic
 
 	José Manrique López de la Fuente on Ethics and Open Source Software Development. The plea for an ethical license for copyleft comes up again.
 
 UI/UX
 
 	Alex Holachek shares her experiences on how to make form inputs better for mobile users by choosing the right input types, attributes, validators.
 	“It is not uncommon that I raise an accessibility or usability issue with a client’s design or implementation and am met with either ‘But Google does this,’ or ‘But Apple does this.’” Adrian Roselli in a revealing, open statement on why we really shouldn’t care about what the big companies do because they don’t know either.
 
 Tooling
 
 	Tobias Tom shares how to use bash operators effectively and shows that there’s more than just the &amp;&amp; operator.
 
 Security
 
 	deSEC is a free and secure DNS hosting running on open source software and sounds pretty cool. It even has an anycast so it’s fast around the world while offering more security than most other services that you pay for.
 
 Web Performance
 
 	Tim Kadlec on building web solutions with friction and why he loves the concept of Bundlephobia, a tool that warns you about your JavaScript bundle size.
 	Danny van Kooten is the author of a very common Wordpress plugin. He made some simple changes and with it saved probably about 59000kg of CO2, all by removing a 20kb JavaScript dependency. There’s little chance you can similarly easy cut your personal emissions so this gives us a clue how important it is to not be careless about what we do on the web.
 	How fast is PHP 8 going to be? Although there are only prereleases available yet, it’s probably a lot faster than version 7.
 
 Accessibility
 
 	Here are the Standards for Writing Code Accessibly.
 
 JavaScript
 
 	Arrays, arrays, arrays, one of the most common things we use in programming languages. Here’s Rebecca Uranga explaining the most common modern JavaScript Array methods and when to use which one.
 	Arrow functions are handy but they’re also not very great when we need to debug them. But Dmitri Pavlutin shares how to write better arrow functions.
 
 CSS
 
 	Jhey Tompkins shows us how we can create a responsive CSS Motion Path.
 	Diagonal layouts are pretty common in the past years, but they often were hacked together quite ugly to achieve the look. However, in 2020 we can do it quite nicely with modern CSS techniques and Nils Binder shares how.
 	Sometimes the specification is the best documentation. Inside the CSS Values and Units Module Level 4 you can learn about the min(), max() but more importantly about the clamp() function in CSS that allows you to e.g. force the font-size to stay between 12px and 100px by stating: font-size: clamp(12px, 10 * (1vw + 1vh) / 2, 100px); All we ever wanted for fonts, right?
 	Oh, what if we could style form inputs nicely and with simple CSS? Aaron Iker shares how we can do it.
 
 Work &amp; Life
 
 	This news made me very unhappy. It’s one of the main reasons I chose to change my profession, to do something actively against climate change, to become a climate farmer: “One in five children have had nightmares about climate change while more than half do not think they are being listened to on the subject, a survey has found”. It’s upon us, parents, uncles/aunts, and friends of parents to fix this problem, so that these kids don’t need to suffer from bad dreams.
 	Leo Babauta on the heartbreaking effects of being only partly committed to most things.
 	“When you bought your first smartphone, did you know you would spend more than 1,000 hours a year looking at it? […] If we wasted money the way we waste time, we’d all be bankrupt.” — Seth Godin
 
 Go beyond…
 
 	Ericsson gives us a “quick guide” to our digital carbon footprint that shows our impact of using all the online services. And while the subtitle says it’s little, it also reveals that there’s potential to reduce the impact by 80 percent if providers would use greener energy and infrastructure.
 	So who would think about farming when we’re talking about air quality and clean air? Well, it seems that California’s regulations help farms produce more food since the trees’ photosynthesis is more efficient when there’s less ground ozone.
 	How does climate change impact Europe? Because common news mostly include other areas in the world, we know little about it. But this great interactive maps model shows the change of weather in a simple way. Heavy rains, forest fires, agricultural issues, draughts will all become more serious and more frequent, it seems.
 	As a company you can stand up and protect your clients, you can be political. See the example of Basecamp who now deny to offer their products to employee-surveillance products and services.
 
 
 Thank you all for reading this, I hope you’re doing fine and stay positive, find your way to make an impact and help build a better society, better friendships and make our world a little better. The next edition will hopefully be served to your inboxes way sooner again and I already have a couple of thoughts and links in the queue.
 If you want to support my work, you can give back via PayPal or Stripe. Thank you!Anselm</content>
     </entry>
     <entry>
       <title>Hybrid positioning with CSS variables and max()</title>
         <link href="https://lea.verou.me/2020/06/hybrid-positioning-with-css-variables-and-max/"/>
       <updated>2020-06-05T04:17:23.000Z</updated>
       <content type="text">Notice how the navigation on the left behaves wrt scrolling: It’s like absolute at first that becomes fixed once the header scrolls out of the viewport.
 
 
 
 One of my side projects these days is a color space agnostic color conversion &amp; manipulation library, which I’m developing together with my husband, Chris Lilley (you can see a sneak peek of its docs above). He brings his color science expertise to the table, and I bring my JS &amp; API design experience, so it’s a great match and I’m really excited about it! (if you’re serious about color and you’re building a tool or demo that would benefit from it contact me, we need as much early feedback on the API as we can get! )
 
 
 
 For the documentation, I wanted to have the page navigation on the side (when there is enough space), right under the header when scrolled all the way to the top, but I wanted it to scroll with the page (as if it was absolutely positioned) until the header is out of view, and then stay at the top for the rest of the scrolling (as if it used fixed positioning).
 
 
 
 
 
 
 
 It sounds very much like a case for position: sticky, doesn’t it? However, an element with position: sticky behaves like it’s relatively positioned when it’s in view and like it’s using position: fixed when its scrolled out of view but its container is still in view. What I wanted here was different. I basically wanted position: absolute while the header was in view and position: fixed after. Yes, there are ways I could have contorted position: sticky to do what I wanted, but was there another solution?
 
 
 
 In the past, we’d just go straight to JS, slap position: absolute on our element, calculate the offset in a scroll event listener and set a top CSS property on our element. However, this is flimsy and violates separation of concerns, as we now need to modify Javascript to change styling. Pass!
 
 
 
 What if instead we had access to the scroll offset in CSS? Would that be sufficient to solve our use case? Let’s find out!
 
 
 
 As I pointed out in my Increment article about CSS Variables last month, and in my CSS Variables series of talks a few years ago, we can use JS to set &amp; update CSS variables on the root that describe pure data (mouse position, input values, scroll offset etc), and then use them as-needed throughout our CSS, reaching near-perfect separation of concerns for many common cases. In this case, we write 3 lines of JS to set a --scrolltop variable:
 
 
 
 let root &#x3D; document.documentElement;
 document.addEventListener(&quot;scroll&quot;, evt &#x3D;&gt; {
 	root.style.setProperty(&quot;--scrolltop&quot;, root.scrollTop);
 });
 
 
 
 Then, we can position our navigation absolutely, and subtract var(--scrolltop) to offset any scroll (11rem is our header height):
 
 
 
 #toc {
 	position: fixed;
 	top: calc(11rem - var(--scrolltop) * 1px);
 }
 
 
 
 This works up to a certain point, but once scrolltop exceeds the height of the header, top becomes negative and our navigation starts drifting off screen:
 
 
 
 Just subtracting --scrolltop essentially implements absolute positioning with position: fixed.
 
 
 
 We’ve basically re-implemented absolute positioning with position: fixed, which is not very useful! What we really want is to cap the result of the calculation to 0 so that our navigation always remains visible. Wouldn’t it be great if there was a max-top attribute, just like max-width so that we could do this?
 
 
 
 One thought might be to change the JS and use Math.max() to cap --scrolltop to a specific number that corresponds to our header height. However, while this would work for this particular case, it means that --scrolltop cannot be used generically anymore, because it’s tailored to our specific use case and does not correspond to the actual scroll offset. Also, this encodes more about styling in the JS than is ideal, since the clamping we need is presentation-related — if our style was different, we may not need it anymore. But how can we do this without resorting to JS?
 
 
 
 Thankfully, we recently got implementations for probably the one feature I was pining for the most in CSS, for years: min(), max() and clamp() functions, which bring the power of min/max constraints to any CSS property! And even for width and height, they are strictly more powerful than min/max-* because you can have any number of minimums and maximums, whereas the min/max-* properties limit you to only one.
 
 
 
 While brower compatibility is actually pretty good, we can’t just use it with no fallback, since this is one of the features where lack of support can be destructive. We will provide a fallback in our base style and use @supports to conditonally override it:
 
 
 
 #toc {
 	position: fixed;
 	top: 11em;
 }
 
 @supports (top: max(1em, 1px)) {
 	#toc {
 		top: max(0em, 11rem - var(--scrolltop) * 1px);
 	}
 }
 
 
 
 Aaand that was it, this gives us the result we wanted!
 
 
 
 And because --scrolltop is sufficiently generic, we can re-use it anywhere in our CSS where we need access to the scroll offset. I’ve actually used exactly the scame --scrolltop setting JS code in my blog, to keep the gradient centerpoint on my logo while maintaining a fixed background attachment, so that various elements can use the same background and having it appear continuous, i.e. not affected by their own background positioning area:
 
 
 
 The website header and the post header are actually different element. The background appears continuous because it’s using background-attachment: fixed, and the scrolltop variable is used to emulate background-attachment: scroll while still using the viewport as the background positioning area for both backgrounds.
 
 
 
 Appendix: Why didn’t we just use the cascade?
 
 
 
 You might wonder, why do we even need @supports? Why not use the cascade, like we’ve always done to provide fallbacks for values without sufficiently universal support? I.e., why not just do this:
 
 
 
 #toc {
 	position: fixed;
 	top: 11em;
 	top: max(0em, 11rem - var(--scrolltop) * 1px);
 }
 
 
 
 The reason is that when you use CSS variables, this does not work as expected. The browser doesn’t know if your property value is valid until the variable is resolved, and by then it has already processed the cascade and has thrown away any potential fallbacks.
 
 
 
 So, what would happen if we went this route and max() was not supported? Once the browser realizes that the second value is invalid due to using an unknown function, it will make the property invalid at computed value time, which essentially equates to the initial keyword, and for the top property, the initial value is 0. This would mean your navigation would overlap the header when scrolled close to the top, which is terrible!</content>
     </entry>
     <entry>
       <title>New decade, new theme</title>
         <link href="https://lea.verou.me/2020/06/new-decade-new-theme/"/>
       <updated>2020-06-03T06:25:12.000Z</updated>
       <content type="text">It has been almost a decade since this blog last saw a redesign.
 
 
 
 This blog’s theme 2011 – 2020. RIP!
 
 
 
 In these 9 years, my life changed dramatically. I joined and left W3C, joined the CSS WG, went to MIT for a PhD, published a book, got married, had a baby, among other things. I designed dozens of websites for dozens of projects, but this theme remained constant, with probably a hasty tweak here and there but nothing more than that. Even its mobile version was a few quick media queries to make it palatable on mobile.
 
 
 
 
 
 
 
 To put this into perspective, when I designed that theme:
 
 
 
 CSS gradients were still cutting edgeWe were still using browser prefixes all over the placeRSS was still a thing that websites advertisedSkeuomorphism was all the rageWebsites were desktop first, and often desktop-only.Opera was a browser we tested in.IE8 was the latest IE version. It didn’t support SVG, gradients, border-radius, shadows, web fonts (except .eot), transforms, &lt;video&gt;, &lt;audio&gt;, &lt;canvas&gt;We were still hacking layout with floats, clearfix and overflow: hidden
 
 
 
 Over the course of these years, I kept saying “I need to update my website’s theme”, but never got around to it, there was always something more high priority.
 
 
 
 The stroke that broke the camel’s back was this Monday. I came up with a nice CSS tip on another website I was working on, and realized I was hesitating to blog about it because I was embarrassed at how my website looked. This is it, I thought. If it has gotten so bad that I avoid blogging because I don’t want people to be reminded of how old my website looks, I need to get my shit together and fix this, I told myself.
 
 
 
 My plan was to design something entirely from scratch, like I had done the previous time (the previous theme used a blank HTML5 starter theme as its only starting point). However, when I previewed the new WordPress default (Twenty Twenty), I fell in love, especially with its typography: it used a very Helvetica-esque variable font as its heading typeface, and Hoefler Text for body text. 
 
 
 
 It would surely be very convenient to be able to adapt an existing theme, but on the other hand, isn’t it embarrassing to be known for CSS and use the default theme or something close to it?
 
 
 
 In the end, I kept the things I liked about it and it certainly still looks a lot like Twenty Twenty, but I think I’ve made enough tweaks that it’s also very Lea. And of course there are animated conic gradients in it, because duh.  
 
 
 
 Do keep in mind that this is just a day’s work, so it will be rough around the edges and still very much a work in progress. Let me know about any issues you find in the comments!
 
 
 
 PS: Yes, yes, I will eventually get around to enforcing https://!</content>
     </entry>
     <entry>
       <title>Today’s Javascript, from an outsider’s perspective</title>
         <link href="https://lea.verou.me/2020/05/todays-javascript-from-an-outsiders-perspective/"/>
       <updated>2020-05-25T23:53:44.000Z</updated>
       <content type="text">Today I tried to help a friend who is a great computer scientist, but not a JS person use a JS module he found on Github. Since for the past 6 years my day job is doing usability research &amp; teaching at MIT, I couldn’t help but cringe at the slog that this was. Lo and behold, a pile of unnecessary error conditions, cryptic errors, and lack of proper feedback. And I don’t feel I did a good job communicating the frustration he went through in the one hour or so until he gave up. 
 
 
 
 
 
 
 
 It went a bit like this…
 
 
 
 Note: Names of packages and people have been changed to protect their identity. I’ve also omitted a few issues he faced that were too specific to the package at hand. Some of the errors are reconstructed from memory, so let me know if I got anything wrong!
 
 
 
 John: Hey, I want to try out this algorithm I found on Github, it says to use import functionName from packageName and then call functionName(arguments). Seems simple enough! I don’t really need a UI, so I’m gonna use Node!
 
 
 
 Lea: Sure, Node seems appropriate for this!
 
 
 
 John runs npm install packageName --save as recommended by the package’s READMEJohn runs node index.js
 
 
 
 Node:
 
 
 
 Warning: To load an ES module, set &quot;type&quot;: &quot;module&quot; in the package.json or use the .mjs extension.
 SyntaxError: Cannot use import statement outside a module
 
 
 
 John: But I don’t have a package.json…Lea: Run npm init, it will generate it for you!
 
 
 
 John runs npm init, goes through the wizard, adds type: &quot;module&quot; manually to the generated package.json.John runs node index.js
 
 
 
 Node:
 
 
 
 SyntaxError: Cannot use import statement outside a module
 
 
 
 Oddly, the error was thrown from an internal module of the project this time. WAT?!
 
 
 
 Lea: Ok, screw this, just run it in a browser, it’s an ES6 module and it’s just a pure JS algorithm that doesn’t use any Node APIs, it should work.
 
 
 
 John makes a simple index.html with a &lt;script type&#x3D;&quot;module&quot; src&#x3D;&quot;index.js&quot;&gt;John loads index.html in a browser
 
 
 
 Nothing in the console. Nada. Crickets. 
 
 
 
 Lea: Oh, you need to adjust your module path to import packageName. Node does special stuff to resolve based on node_modules, now you’re in a browser you need to specify an explicit path yourself.
 
 
 
 John looks, at his filesystem, but there was no node_modules directory.
 
 
 
 Lea: Oh, you ran npm install before you had a package.json, that’s probably it! Try it again!
 
 
 
 John runs npm install packageName --save again
 
 
 
 John: Oh yeah, there is a node_modules now!
 
 
 
 John desperately looks in node_modules to find the entry pointJohn edits his index.js accordingly, reloads index.html
 
 
 
 Firefox: 
 
 
 
 Incorrect MIME type: text/html
 
 
 
 Lea: Oh, you’re in file://! Dude, what are you doing these days without a localhost? Javascript is severely restricted in file:// today.
 
 
 
 John: But why do I… ok fine, I’m going to start a localhost.
 
 
 
 John starts localhost, visits his index.html under http://localhost:80
 
 
 
 Firefox:
 
 
 
 Incorrect MIME type: text/html
 
 
 
 John: Sigh. Do I need to configure my localhost to serve JS files with a text/javascript MIME type?Lea: What? No! It knows this. Um… look at the Networks tab, I suspect it can’t find your module, so it’s returning an HTML page for the 404, then it complains because the MIME type of the error page is not text/javascript.
 
 
 
 Looks at node_modules again, corrects path. Turns out VS Code collapses folders with only 1 subfolder, which is why we hadn’t noticed. 
 
 
 
 FWIW I do think this is a good usability improvement on VS Code’s behalf, it improves efficiency, but they need to make it more visible that this is what has happened.
 
 
 
 Firefox:
 
 
 
 SyntaxError: missing ) after formal parameters
 
 
 
 Lea: What? That’s coming from the package source, it’s not your fault. I don’t understand… can we look at this line?
 
 
 
 John clicks at line throwing the error
 
 
 
 Lea: Oh my goodness. This is not Javascript, it’s Typescript!! With a .js extension!!John: I just wanted to run one line of code to test this algorithm… 
 
 
 
 John gives up. Concludes never to touch Node, npm, or ES6 modules with a barge pole.
 
 
 
 The End.
 
 
 
 Note that John is a computer scientist that knows a fair bit about the Web: He had Node &amp; npm installed, he knew what MIME types are, he could start a localhost when needed. What hope do actual novices have?</content>
     </entry>
     <entry>
       <title>Understanding Destructuring, Rest Parameters, and Spread Syntax</title>
         <link href="https://www.taniarascia.com/understanding-destructuring-rest-spread/"/>
       <updated>2020-05-14T00:00:00.000Z</updated>
       <content type="text">This article was originally written for DigitalOcean.
 Introduction
 Many new features for working with arrays and objects have been made available to the JavaScript language since the 2015 Edition of the ECMAScript specification. A few of the notable ones that you will learn in this article are destructuring, rest parameters, and spread syntax. These features provide more direct ways of accessing the members of an array or an object, and can make working with these data structures quicker and more succinct.
 Many other languages do not have corresponding syntax for destructuring, rest parameters, and spread, so these features may have a learning curve both for new JavaScript developers and those coming from another language. In this article, you will learn how to destructure objects and arrays, how to use the spread operator to unpack objects and arrays, and how to use rest parameters in function calls.
 Destructuring
 Destructuring assignment is a syntax that allows you to assign object properties or array items as variables. This can greatly reduce the lines of code necessary to manipulate data in these structures. There are two types of destructuring: Object destructuring and Array destructuring.
 Object Destructuring
 Object destructuring allows you to create new variables using an object property as the value.
 Consider this example, an object that represents a note with an id, title, and date:
 const note &#x3D; {
   id: 1,
   title: &#x27;My first note&#x27;,
   date: &#x27;01/01/1970&#x27;,
 }
 Traditionally, if you wanted to create a new variable for each property, you would have to assign each variable individually, with a lot of repetition:
 // Create variables from the Object properties
 const id &#x3D; note.id
 const title &#x3D; note.title
 const date &#x3D; note.date
 With object destructuring, this can all be done in one line. By surrounding each variable in curly brackets {}, JavaScript will create new variables from each property with the same name:
 // Destructure properties into variables
 const {id, title, date} &#x3D; note
 Now, console.log() the new variables:
 console.log(id)
 console.log(title)
 console.log(date)
 You will get the original property values as output:
 1
 My first note
 01/01/1970
 
 Note: Destructuring an object does not modify the original object. You could still call the original note with all its entries intact.
 
 The default assignment for object destructuring creates new variables with the same name as the object property. If you do not want the new variable to have the same name as the property name, you also have the option of renaming the new variable by using a colon (:) to decide a new name, as seen with noteId in the following:
 // Assign a custom name to a destructured value
 const {id: noteId, title, date} &#x3D; note
 Log the new variable noteId to the console:
 console.log(noteId)
 You will receive the following output:
 1
 You can also destructure nested object values. For example, update the note object to have a nested author object:
 const note &#x3D; {
   id: 1,
   title: &#x27;My first note&#x27;,
   date: &#x27;01/01/1970&#x27;,
   author: {
     firstName: &#x27;Sherlock&#x27;,
     lastName: &#x27;Holmes&#x27;,
   },
 }
 Now you can destructure note, then destructure once again to create variables from the author properties:
 // Destructure nested properties
 const {
   id,
   title,
   date,
   author: {firstName, lastName},
 } &#x3D; note
 Next, log the new variables firstName and lastName using template literals:
 console.log(&#x60;${firstName} ${lastName}&#x60;)
 This will give the following output:
 Sherlock Holmes
 Note that in this example, though you have access to the contents of the author object, the author object itself is not accessible. In order to access an object as well as its nested values, you would have to declare them separately:
 // Access object and nested values
 const {
   author,
   author: {firstName, lastName},
 } &#x3D; note
 
 console.log(author)
 This code will output the author object:
 {firstName: &quot;Sherlock&quot;, lastName: &quot;Holmes&quot;}
 Because of this property, destructuring an object is not only useful for reducing the amount of code that you have to write; it also allows you to target your access to the properties you care about.
 Finally, destructuring can be used to access the object properties of primitive values. For example, String is a global object for strings, and has a length property:
 const {length} &#x3D; &#x27;A string&#x27;
 This will find the inherent length property of a string and set it equal to the length variable. Log length to see if this worked:
 console.log(length)
 You will get the following output:
 8
 The string A string was implicitly converted into an object here to retrieve the length property.
 Array Destructuring
 Array destructuring allows you to create new variables using an array item as a value. Consider this example, an array with the various parts of a date:
 const date &#x3D; [&#x27;1970&#x27;, &#x27;12&#x27;, &#x27;01&#x27;]
 Arrays in JavaScript are guaranteed to preserve their order, so in this case the first index will always be a year, the second will be the month, and so on. Knowing this, you can create variables from the items in the array:
 // Create variables from the Array items
 const year &#x3D; date[0]
 const month &#x3D; date[1]
 const day &#x3D; date[2]
 But doing this manually can take up a lot of space in your code. With array destructuring, you can unpack the values from the array in order and assign them to their own variables, like so:
 // Destructure Array values into variables
 const [year, month, day] &#x3D; date
 Now log the new variables:
 console.log(year)
 console.log(month)
 console.log(day)
 You will get the following output:
 1970
 12
 01
 Values can be skipped by leaving the destructuring syntax blank between commas:
 // Skip the second item in the array
 const [year, , day] &#x3D; date
 
 console.log(year)
 console.log(day)
 Running this will give the value of year and day:
 1970
 01
 Nested arrays can also be destructured. First, create a nested array:
 // Create a nested array
 const nestedArray &#x3D; [1, 2, [3, 4], 5]
 Then destructure that array and log the new variables:
 // Destructure nested items
 const [one, two, [three, four], five] &#x3D; nestedArray
 
 console.log(one, two, three, four, five)
 You will receive the following output:
 1 2 3 4 5
 Destructuring syntax can be applied to destructure the parameters in a function. To test this out, you will destructure the keys and values out of Object.entries().
 First, declare the note object:
 const note &#x3D; {
   id: 1,
   title: &#x27;My first note&#x27;,
   date: &#x27;01/01/1970&#x27;,
 }
 Given this object, you could list the key-value pairs by destructuring arguments as they are passed to the forEach() method:
 // Using forEach
 Object.entries(note).forEach(([key, value]) &#x3D;&gt; {
   console.log(&#x60;${key}: ${value}&#x60;)
 })
 Or you could accomplish the same thing using a for loop:
 // Using a for loop
 for (let [key, value] of Object.entries(note)) {
   console.log(&#x60;${key}: ${value}&#x60;)
 }
 Either way, you will receive the following:
 id: 1
 title: My first note
 date: 01/01/1970
 Object destructuring and array destructuring can be combined in a single destructuring assignment. Default parameters can also be used with destructuring, as seen in this example that sets the default date to new Date().
 First, declare the note object:
 const note &#x3D; {
   title: &#x27;My first note&#x27;,
   author: {
     firstName: &#x27;Sherlock&#x27;,
     lastName: &#x27;Holmes&#x27;,
   },
   tags: [&#x27;personal&#x27;, &#x27;writing&#x27;, &#x27;investigations&#x27;],
 }
 Then destructure the object, while also setting a new date variable with the default of new Date():
 const {
   title,
   date &#x3D; new Date(),
   author: {firstName},
   tags: [personalTag, writingTag],
 } &#x3D; note
 
 console.log(date)
 console.log(date) will then give output similar to the following:
 Fri May 08 2020 23:53:49 GMT-0500 (Central Daylight Time)
 As shown in this section, the destructuring assignment syntax adds a lot of flexibility to JavaScript and allows you to write more succinct code. In the next section, you will see how spread syntax can be used to expand data structures into their constituent data entries.
 Spread
 Spread syntax (...) is another helpful addition to JavaScript for working with arrays, objects, and function calls. Spread allows objects and iterables (such as arrays) to be unpacked, or expanded, which can be used to make shallow copies of data structures to increase the ease of data manipulation.
 Spread with Arrays
 Spread can simplify common tasks with arrays. For example, let&#x27;s say you have two arrays and want to combine them:
 // Create an Array
 const tools &#x3D; [&#x27;hammer&#x27;, &#x27;screwdriver&#x27;]
 const otherTools &#x3D; [&#x27;wrench&#x27;, &#x27;saw&#x27;]
 Originally you would use concat() to concatenate the two arrays:
 // Concatenate tools and otherTools together
 const allTools &#x3D; tools.concat(otherTools)
 Now you can also use spread to unpack the arrays into a new array:
 // Unpack the tools Array into the allTools Array
 const allTools &#x3D; [...tools, ...otherTools]
 
 console.log(allTools)
 Running this would give the following:
 [&quot;hammer&quot;, &quot;screwdriver&quot;, &quot;wrench&quot;, &quot;saw&quot;]
 This can be particularly helpful with immutability. For example, you might be working with an app that has users stored in an array of objects:
 // Array of users
 const users &#x3D; [
   {id: 1, name: &#x27;Ben&#x27;},
   {id: 2, name: &#x27;Leslie&#x27;},
 ]
 You could use push to modify the existing array and add a new user, which would be the mutable option:
 // A new user to be added
 const newUser &#x3D; {id: 3, name: &#x27;Ron&#x27;}
 
 users.push(newUser)
 But this changes the user array, which we might want to preserve.
 Spread allows you to create a new array from the existing one and add a new item to the end:
 const updatedUsers &#x3D; [...users, newUser]
 
 console.log(users)
 console.log(updatedUsers)
 Now the new array, updatedUsers, has the new user, but the original users array remains unchanged:
 [{id: 1, name: &quot;Ben&quot;}
  {id: 2, name: &quot;Leslie&quot;}]
 
 [{id: 1, name: &quot;Ben&quot;}
  {id: 2, name: &quot;Leslie&quot;}
  {id: 3, name: &quot;Ron&quot;}]
 Creating copies of data instead of changing existing data can help prevent unexpected changes. In JavaScript, when you create an object or array and assign it to another variable, you are not actually creating a new object—you are passing a reference.
 Take this example, in which an array is created and assigned to another variable:
 // Create an Array
 const originalArray &#x3D; [&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;]
 
 // Assign Array to another variable
 const secondArray &#x3D; originalArray
 Removing the last item of the second Array will modify the first one:
 // Remove the last item of the second Array
 secondArray.pop()
 
 console.log(originalArray)
 This will give the output:
 [&quot;one&quot;, &quot;two&quot;]
 Spread allows you to make a shallow copy of an array or object, meaning that any top level properties will be cloned, but nested objects will still be passed by reference. For simple arrays or objects, a shallow copy may be all you need.
 If you write the same example code, but copy the array with spread, and the original Array will no longer be modified:
 // Create an Array
 const originalArray &#x3D; [&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;]
 
 // Use spread to make a shallow copy
 const secondArray &#x3D; [...originalArray]
 
 // Remove the last item of the second Array
 secondArray.pop()
 
 console.log(originalArray)
 The following will be logged to the console:
 [&quot;one&quot;, &quot;two&quot;, &quot;three&quot;]
 Spread can also be used to convert a set, or any other iterable to an Array.
 Create a new set and add some entries to it:
 // Create a set
 const set &#x3D; new Set()
 
 set.add(&#x27;octopus&#x27;)
 set.add(&#x27;starfish&#x27;)
 set.add(&#x27;whale&#x27;)
 Next, use the spread operator with set and log the results:
 // Convert Set to Array
 const seaCreatures &#x3D; [...set]
 
 console.log(seaCreatures)
 This will give the following:
 [&quot;octopus&quot;, &quot;starfish&quot;, &quot;whale&quot;]
 This can also be useful for creating an array from a string:
 const string &#x3D; &#x27;hello&#x27;
 
 const stringArray &#x3D; [...string]
 
 console.log(stringArray)
 This will give an array with each character as an item in the array:
 [&quot;h&quot;, &quot;e&quot;, &quot;l&quot;, &quot;l&quot;, &quot;o&quot;]
 Spread with Objects
 When working with objects, spread can be used to copy and update objects.
 Originally, Object.assign() was used to copy an object:
 // Create an Object and a copied Object with Object.assign()
 const originalObject &#x3D; {enabled: true, darkMode: false}
 const secondObject &#x3D; Object.assign({}, originalObject)
 The secondObject will now be a clone of the originalObject.
 This is simplified with the spread syntax—you can shallow copy an object by spreading it into a new one:
 // Create an object and a copied object with spread
 const originalObject &#x3D; {enabled: true, darkMode: false}
 const secondObject &#x3D; {...originalObject}
 
 console.log(secondObject)
 This will result in the following:
 {enabled: true, darkMode: false}
 Just like with arrays, this will only create a shallow copy, and nested objects will still be passed by reference.
 Adding or modifying properties on an existing object in an immutable fashion is simplified with spread. In this example, the isLoggedIn property is added to the user object:
 const user &#x3D; {
   id: 3,
   name: &#x27;Ron&#x27;,
 }
 
 const updatedUser &#x3D; {...user, isLoggedIn: true}
 
 console.log(updatedUser)
 This will output the following:
 {id: 3, name: &quot;Ron&quot;, isLoggedIn: true}
 One important thing to note with updating objects via spread is that any nested object will have to be spread as well. For example, let&#x27;s say that in the user object there is a nested organization object:
 const user &#x3D; {
   id: 3,
   name: &#x27;Ron&#x27;,
   organization: {
     name: &#x27;Parks &amp; Recreation&#x27;,
     city: &#x27;Pawnee&#x27;,
   },
 }
 If you tried to add a new item to organization, it would overwrite the existing fields:
 const updatedUser &#x3D; {...user, organization: {position: &#x27;Director&#x27;}}
 
 console.log(updatedUser)
 This would result in the following:
 id: 3
 name: &quot;Ron&quot;
 organization: {position: &quot;Director&quot;}
 If mutability is not an issue, the field could be updated directly:
 user.organization.position &#x3D; &#x27;Director&#x27;
 But since we are seeking an immutable solution, we can spread the inner object to retain the existing properties:
 const updatedUser &#x3D; {
   ...user,
   organization: {
     ...user.organization,
     position: &#x27;Director&#x27;,
   },
 }
 
 console.log(updatedUser)
 This will give the following:
 id: 3
 name: &quot;Ron&quot;
 organization: {name: &quot;Parks &amp; Recreation&quot;, city: &quot;Pawnee&quot;, position: &quot;Director&quot;}
 Spread with Function Calls
 Spread can also be used with arguments in function calls.
 As an example, here is a multiply function that takes three parameters and multiplies them:
 // Create a function to multiply three items
 function multiply(a, b, c) {
   return a * b * c
 }
 Normally, you would pass three values individually as arguments to the function call, like so:
 multiply(1, 2, 3)
 This would give the following:
 6
 However, if all the values you want to pass to the function already exist in an array, the spread syntax allows you to use each item in an array as an argument:
 const numbers &#x3D; [1, 2, 3]
 
 multiply(...numbers)
 This will give the same result:
 6
 
 Note: Without spread, this can be accomplished by using apply():
 
 multiply.apply(null, [1, 2, 3])
 This will give:
 6
 Now that you have seen how spread can shorten your code, you can take a look at a different use of the ... syntax: rest parameters.
 Rest Parameters
 The last feature you will learn in this article is the rest parameter syntax. The syntax appears the same as spread (...) but has the opposite effect. Instead of unpacking an array or object into individual values, the rest syntax will create an array of an indefinite number of arguments.
 In the function restTest for example, if we wanted args to be an array composed of an indefinite number of arguments, we could have the following:
 function restTest(...args) {
   console.log(args)
 }
 
 restTest(1, 2, 3, 4, 5, 6)
 All the arguments passed to the restTest function are now available in the args array:
 [1, 2, 3, 4, 5, 6]
 Rest syntax can be used as the only parameter or as the last parameter in the list. If used as the only parameter, it will gather all arguments, but if it&#x27;s at the end of a list, it will gather every argument that is remaining, as seen in this example:
 function restTest(one, two, ...args) {
   console.log(one)
   console.log(two)
   console.log(args)
 }
 
 restTest(1, 2, 3, 4, 5, 6)
 This will take the first two arguments individually, then group the rest into an array:
 1
 2
 [3, 4, 5, 6]
 In older code, the arguments variable could be used to gather all the arguments passed through to a function:
 function testArguments() {
   console.log(arguments)
 }
 
 testArguments(&#x27;how&#x27;, &#x27;many&#x27;, &#x27;arguments&#x27;)
 This would give the following output:
 Arguments(3) [&quot;how&quot;, &quot;many&quot;, &quot;arguments&quot;]
 However, this has a few disadvantages. First, the arguments variable cannot be used with arrow functions.
 const testArguments &#x3D; () &#x3D;&gt; {
   console.log(arguments)
 }
 
 testArguments(&#x27;how&#x27;, &#x27;many&#x27;, &#x27;arguments&#x27;)
 This would yield an error:
 Uncaught ReferenceError: arguments is not defined
 Additionally, arguments is not a true array and cannot use methods like map and filter without first being converted to an array. It also will collect all arguments passed instead of just the rest of the arguments, as seen in the restTest(one, two, ...args) example.
 Rest can be used when destructuring arrays as well:
 const [firstTool, ...rest] &#x3D; [&#x27;hammer&#x27;, &#x27;screwdriver&#x27;, &#x27;wrench&#x27;]
 
 console.log(firstTool)
 console.log(rest)
 This will give:
 hammer
 [&quot;screwdriver&quot;, &quot;wrench&quot;]
 Rest can also be used when destructuring objects:
 const {isLoggedIn, ...rest} &#x3D; {id: 1, name: &#x27;Ben&#x27;, isLoggedIn: true}
 
 console.log(isLoggedIn)
 console.log(rest)
 Giving the following output:
 true
 {id: 1, name: &quot;Ben&quot;}
 In this way, rest syntax provides efficient methods for gathering an indeterminate amount of items.
 Conclusion
 In this article, you learned about destructuring, spread syntax, and rest parameters. In summary:
 
 Destructuring is used to create varibles from array items or object properties.
 Spread syntax is used to unpack iterables such as arrays, objects, and function calls.
 Rest parameter syntax will create an array from an indefinite number of values.
 
 Destructuring, rest parameters, and spread syntax are useful features in JavaScript that help keep your code succinct and clean.
 If you would like to see destructuring in action, take a look at How To Customize React Components with Props, which uses this syntax to destructure data and pass it to custom front-end components. If you&#x27;d like to learn more about JavaScript, return to our How To Code in JavaScript series page.</content>
     </entry>
     <entry>
       <title>Writing an Emulator in JavaScript (Chip-8)</title>
         <link href="https://www.taniarascia.com/writing-an-emulator-in-javascript-chip8/"/>
       <updated>2020-04-13T00:00:00.000Z</updated>
       <content type="text">I spent a good portion of my childhood playing emulated NES and SNES games on my computer, but I never imagined I&#x27;d write an emulator myself one day. However, Vanya Sergeev challenged me to write a Chip-8 interpreter to learn some of the basic concepts of lower-level programming languages and how a CPU works, and the end result is a Chip-8 emulator in JavaScript that I wrote with his guidance.
 Although there are endless implementations of the Chip-8 interpreter in every programming language imaginable, this one is a bit unique. My Chip8.js code interfaces with not just one but three environments, existing as a web app, a CLI app, and a native app.
 You can take a look at the web app demo and the source here:
 
 Demo
 Source code
 
 There are plenty of guides on how to make a Chip-8 emulator, such as Mastering Chip8, How to Write an Emulator, and most importantly, Cowgod&#x27;s Chip-8 Technical Reference, the primary resource used for my own emulator, and a website so old it ends in .HTM. As such, this isn&#x27;t intended to be a how-to guide, but an overview of how I built the emulator, what major concepts I learned, and some JavaScript specifics for making a browser, CLI, or native app.
 
       
     
   
   
     
 Contents
 
 What is Chip-8
 What Goes Into a Chip-8 Interpreter?
 Decoding Chip-8 Instructions
 Reading the ROM
 The Instruction Cycle - Fetch, Decode, Execute
 Creating a CPU Interface for I/O
 
 CLI App - Interfacing with the Terminal
 Web App - Interfacing with the Browser
 Native App - Interfacing with the Native Platform
 
 
 
 What is Chip-8?
 I had never heard of Chip-8 before embarking on this project, so I assume most people haven&#x27;t either unless they&#x27;re already into emulators. Chip-8 is a very simple interpreted programming language that was developed in the 1970s for hobbyist computers. People wrote basic Chip-8 programs that mimicked popular games of the time, such as Pong, Tetris, Space Invaders, and probably other unique games lost to the annuls of time.
 A virtual machine that plays these games is actually a Chip-8 interpreter, not technically an emulator, as an emulator is software that emulates the hardware of a specific machine, and Chip-8 programs aren&#x27;t tied to any hardware in specific. Often, Chip-8 interpreters were used on graphing calculators.
 Nonetheless, it&#x27;s close enough to being an emulator that it&#x27;s usually the starting project for anyone who wants to learn how to build an emulator, since it&#x27;s significantly more simple than creating an NES emulator or anything beyond that. It&#x27;s also a good starting point for a lot of CPU concepts in general, like memory, stacks, and I/O, things I deal with on a daily basis in the infinitely more complex world of a JavaScript runtime.
 What Goes Into a Chip-8 Interpreter?
 There was a lot of pre-learning I had to do to even get started understanding what I was working with, since I had never learned about computer science basics before. So I wrote Understanding Bits, Bytes, Bases, and Writing a Hex Dump in JavaScript which goes over much of that.
 To summarize, there are two major takeaways of that article:
 
 Bits and Bytes - A bit is a binary digit - 0 or 1, true or false, on or off. Eight bits is a byte, which is the basic unit of information that computers work with.
 Number Bases - Decimal is the base number system we&#x27;re most used to dealing with, but computers usually work with binary (base 2) or hexadecimal (base 16). 1111 in binary, 15 in decimal, and f in hexadecimal are all the same number.
 Nibbles - Also, 4 bits is a nibble, which is cute, and I had to deal with them a bit in this project.
 Prefixes - In JavaScript, 0x is a prefix for a hex number, and 0b is a prefix for a binary number.
 
 I also wrote a CLI snake game in preparation of figuring out how to work with pixels in the terminal for this project.
 A CPU is the main processor of a computer that executes the instructions of a program. In this case, it consists of various bits of state, described below, and an instruction cycle with fetch, decode, and execute steps.
 
 Memory
 Program counter
 Registers
 Index register
 Stack
 Stack pointer
 Key input
 Graphical output
 Timers
 
 Memory
 Chip-8 can access up to 4 kilobytes of memory (RAM). (That&#x27;s 0.002% of the storage space on a floppy disk.) The vast majority of data in the CPU is stored in memory.
 4kb is 4096 bytes, and JavaScript has some helpful typed arrays, like Uint8Array which is a fixed-size array of a certain element - in this case 8-bits.
 let memory &#x3D; new Uint8Array(4096)
 You can access and use this array like a regular array, from memory[0] to memory[4095] and set each element to a value up to 255. Anything above that will fall back to that (for example, memory[0] &#x3D; 300 would result in memory[0] &#x3D;&#x3D;&#x3D; 255).
 Program counter
 The program counter stores the address of the current instruction as an 16-bit integer. Every single instruction in Chip-8 will update the program counter (PC) when it is done to go on to the next instruction, by accessing memory with PC as the index.
 In the Chip-8 memory layout, 0x000 to 0x1FF in memory is reserved, so it starts at 0x200.
 let PC &#x3D; 0x200 // memory[PC] will access the address of  the current instruvtion
 *You&#x27;ll notice the memory array is 8-bit and the PC is a 16-bit integer, so two program codes will be combined to make a big endian opcode.
 Registers
 Memory is generally used for long-term storage and program data, so registers exist as a kind of &quot;short-term memory&quot; for immediate data and computations. Chip-8 has 16 8-bit registers. They&#x27;re referred as V0 through VF.
 let registers &#x3D; new Uint8Array(16)
 Index register
 There is a special 16-bit register that accesses a specific point in memory, referred to as I. The I register exists mostly for reading and writing to memory in general, since the addressable memory is 16-bit as well.
 let I &#x3D; 0
 Stack
 Chip-8 has the ability to go into subroutines, and a stack for keeping track of where to return to. The stack is 16 16-bit values, meaning the program can go into 16 nested subroutines before experiencing a &quot;stack overflow&quot;.
 let stack &#x3D; new Uint16Array(16)
 Stack pointer
 The stack pointer (SP) is an 8-bit integer that points to a location in the stack. It only needs to be 8-bit even though the stack is 16-bit because it&#x27;s only referencing the index of the stack, so only needs to be 0 thorough 15.
 let SP &#x3D; -1
 
 // stack[SP] will access the current return address in the stack
 Timers
 Chip-8 is capable of a glorious single beep as far as sound goes. To be honest, I didn&#x27;t bother implementing an actual output for the &quot;music&quot;, though the CPU itself is all set up to interface properly with it. There are two timers, both 8-bit registers - a sound timer (ST) for deciding when to beep and a delay timer (DT) for timing some events throughout the game. They count down at 60 Hz.
 let DT &#x3D; 0
 let ST &#x3D; 0
 Key input
 Chip-8 was set up to interface with the amazing hex keyboard. It looked like this:
 ┌───┬───┬───┬───┐
 │ 1 │ 2 │ 3 │ C │
 │ 4 │ 5 │ 6 │ D │
 │ 7 │ 8 │ 9 │ E │
 │ A │ 0 │ B │ F │
 └───┴───┴───┴───┘
 In practice, only a few of the keys seem to be used, and you can map them to whatever 4x4 grid you want, but they&#x27;re pretty inconsistent between games.
 Graphical output
 Chip-8 uses a monochromatic 64x32 resolution display. Each pixel is either on or off.
 Sprites that can be saved in memory are 8x15 - eight pixels wide by fifteen pixels high. Chip-8 also comes with a font set, but it only contains the characters in the hex keyboard, so not overall the most useful font set.
 CPU
 Put it all together, and you get the CPU state.
 CPU
 class CPU {
   constructor() {
     this.memory &#x3D; new Uint8Array(4096)
     this.registers &#x3D; new Uint8Array(16)
     this.stack &#x3D; new Uint16Array(16)
     this.ST &#x3D; 0
     this.DT &#x3D; 0
     this.I &#x3D; 0
     this.SP &#x3D; -1
     this.PC &#x3D; 0x200
   }
 }
 Decoding Chip-8 Instructions
 Chip-8 has 36 instructions. All the instructions are listed here. All instructions are 2 bytes (16-bits) long. Each instruction is encoded by an opcode (operation code) and operand, the data being operated on.
 An example of an instruction could be like this operation on two variables:
 x &#x3D; 1
 y &#x3D; 2
 
 ADD x, y
 In which ADD is the opcode and x, y are the operands. This type of language is known as an assembly language. This instruction would map to:
 x &#x3D; x + y
 With this instruction set I&#x27;ll have to store this data in 16-bits, so every instrution ends up being a number from 0x0000 to 0xffff. Each digit position in these sets is a nibble (4-bit).
 So how can I get from nnnn to something like ADD x, y, that is a little more understandable? Well, I&#x27;ll start by looking at one of the instructions from Chip-8, which is basically the same as the above example:
 
 
 
 Instruction
 Description
 
 
 
 
 8xy4
 ADD Vx, Vy
 
 
 
 So what are we dealing with here? There&#x27;s one keyword, ADD, and two arguments, Vx and Vy, which we&#x27;ve established above are registers.
 There are several opcode mnemonics (which are like keywords), such as:
 
 ADD (add)
 SUB (subtract)
 JP (jump)
 SKP (skip)
 RET (return)
 LD (load)
 
 And there are several type of operand values, such as:
 
 Address (I)
 Register (Vx, Vy)
 Constant (N or NN for nibble or byte)
 
 The next step is to find a way to interpret the 16-bit opcode as these more understandable instructions.
 Bit Masking
 Each instruction contains a pattern that will always be the same, and variables that can change. For 8xy4, the pattern is 8__4. The two nibbles in the middle are the variables. By creating a bitmask for that pattern, I can determine the instruction.
 To mask, you use the bitwise AND (&amp;) with a mask and match it to a pattern. So if the instruction 8124 came up, you would want to make sure the nibble in position 1 and 4 are on (passed through) and the nibble in position 2 and 3 are off (masked out). The mask then becomes f00f.
 const opcode &#x3D; 0x8124
 const mask &#x3D; 0xf00f
 const pattern &#x3D; 0x8004
 
 const isMatch &#x3D; (opcode &amp; mask) &#x3D;&#x3D;&#x3D; pattern // true
   8124
 &amp; f00f
   &#x3D;&#x3D;&#x3D;&#x3D;
   8004
 Similarly, 0f00 and 00f0 will mask the variables, and right-shifting (&gt;&gt;) them will access the correct nibble.
 const x &#x3D; (0x8124 &amp; 0x0f00) &gt;&gt; 8 // 1
 
 // (0x8124 &amp; 0x0f00) is 100000000 in binary
 // right shifting by 8 (&gt;&gt; 8) will remove 8 zeroes from the right
 // This leaves us with 1
 
 const y &#x3D; (0x8124 &amp; 0x00f0) &gt;&gt; 4 // 2
 // (0x8124 &amp; 0x00f0) is 100000 in binary
 // right shifting by 4 (&gt;&gt; 4) will remove 4 zeroes from the right
 // This leaves us with 10, the binary equivalent of 2
 So for each of the 36 instructions, I made an object with a unique identifier, mask, pattern, and arguments.
 const instruction &#x3D; {
   id: &#x27;ADD_VX_VY&#x27;,
   name: &#x27;ADD&#x27;,
   mask: 0xf00f,
   pattern: 0x8004,
   arguments: [
     { mask: 0x0f00, shift: 8, type: &#x27;R&#x27; },
     { mask: 0x00f0, shift: 4, type: &#x27;R&#x27; },
   ],
 }
 Now that I have these objects, each opcode can be disassembled into a unique identifier, and the values of the arguments can be determined. I made an INSTRUCTION_SET array containing all these instructions and a disassembler. I also wrote tests for every one to ensure they all worked correctly.
 Disassembler
 function disassemble(opcode) {
   // Find the instruction from the opcode
   const instruction &#x3D; INSTRUCTION_SET.find(
     (instruction) &#x3D;&gt; (opcode &amp; instruction.mask) &#x3D;&#x3D;&#x3D; instruction.pattern
   )
   // Find the argument(s)
   const args &#x3D; instruction.arguments.map((arg) &#x3D;&gt; (opcode &amp; arg.mask) &gt;&gt; arg.shift)
 
   // Return an object containing the instruction data and arguments
   return { instruction, args }
 }
 Reading the ROM
 Since we&#x27;re considering this project an emulator, each Chip-8 program file can be considered a ROM. The ROM is just binary data, and we&#x27;re writing the program to interpret it. We can imagine the Chip8 CPU to be a virtual console, and a Chip-8 ROM to be a virtual game cartridge.
 The ROM buffer will take the raw binary file and translate it into 16-bit big endian words (a word is a unit of data consisting of a set amount of bits). This is where that hex dump article comes in handy. I&#x27;m collecting the binary data and converting it into blocks that I can use, in this case the 16-bit opcodes. Big endian means that the most significant byte will be first in the buffer, so when it encounters the two bytes 12 34, it will create a 1234 16-bit code. A little endian code would look like 3412.
 RomBuffer.js
 class RomBuffer {
   /**
    * @param {binary} fileContents ROM binary
    */
   constructor(fileContents) {
     this.data &#x3D; []
 
     // Read the raw data buffer from the file
     const buffer &#x3D; fileContents
 
     // Create 16-bit big endian opcodes from the buffer
     for (let i &#x3D; 0; i &lt; buffer.length; i +&#x3D; 2) {
       this.data.push((buffer[i] &lt;&lt; 8) | (buffer[i + 1] &lt;&lt; 0))
     }
   }
 }
 The data returned from this buffer is the &quot;game&quot;.
 The CPU will have a load() method - like loading a cartridge into a console - that will take the data from this buffer and place it into memory. Both the buffer and memory act as arrays in JavaScript, so loading the memory is just a matter of looping through the buffer and placing the bytes into the memory array.
 The Instruction Cycle - Fetch, Decode, Execute
 Now I have the instruction set and game data all ready to be interpreted. The CPU just needs to do something with it. The instruction cycle consists of three steps - fetch, decode, and execute.
 
 Fetch - Get the data stored in memory using the program counter
 Decode - Disassemble the 16-bit opcode to get the decoded instruction and argument values
 Execute - Perform the operation based on the decoded instruction and update the program counter
 
 Here&#x27;s a condensed and simplified version of how load, fetch, decode, and execute work in the code. These CPU cycle methods are private and not exposed.
 The first step, fetch, will access the current opcode from memory.
 Fetch
 // Get address value from memory
 function fetch() {
   return memory[PC]
 }
 The next step, decode, will disassemble the opcode into the more understandable instruction set I created earlier.
 Decode
 // Decode instruction
 function decode(opcode) {
   return disassemble(opcode)
 }
 The last step, execute, will consist of a switch with all 36 instructions as cases, and perform the relevant operation for the one it finds, then update the program counter so the next fetch cycle finds the next opcode. Any error handling will go here as well, which will halt the CPU.
 Execute
 // Execute instruction
 function execute(instruction) {
   const { id, args } &#x3D; instruction
 
   switch (id) {
     case &#x27;ADD_VX_VY&#x27;:
       // Perform the instruction operation
       registers[args[0]] +&#x3D; registers[args[1]]
 
       // Update program counter to next instruction
       PC &#x3D; PC + 2
       break
     case &#x27;SUB_VX_VY&#x27;:
     // etc...
   }
 }
 What I end up with is the CPU, with all the state and the instruction cycle. There are two methods exposed on the CPU - load, which is the equivalent of loading a cartridge into a console with the romBuffer as the game, and step, which is the three functions of the instruction cycle (fetch, decode, execute). step will run in an infinite loop.
 CPU.js
 class CPU {
   constructor() {
     this.memory &#x3D; new Uint8Array(4096)
     this.registers &#x3D; new Uint8Array(16)
     this.stack &#x3D; new Uint16Array(16)
     this.ST &#x3D; 0
     this.DT &#x3D; 0
     this.I &#x3D; 0
     this.SP &#x3D; -1
     this.PC &#x3D; 0x200
   }
 
   // Load buffer into memory
   load(romBuffer) {
     this.reset()
 
     romBuffer.forEach((opcode, i) &#x3D;&gt; {
       this.memory[i] &#x3D; opcode
     })
   }
 
   // Step through each instruction
   step() {
     const opcode &#x3D; this._fetch()
     const instruction &#x3D; this._decode(opcode)
 
     this._execute(instruction)
   }
 
   _fetch() {
     return this.memory[this.PC]
   }
 
   _decode(opcode) {
     return disassemble(opcode)
   }
 
   _execute(instruction) {
     const { id, args } &#x3D; instruction
 
     switch (id) {
       case &#x27;ADD_VX_VY&#x27;:
         this.registers[args[0]] +&#x3D; this.registers[args[1]]
         this.PC &#x3D; this.PC + 2
         break
     }
   }
 }
 Only one aspect of the project is missing now, and a pretty important one - the ability to actually play and see the game.
 Creating a CPU Interface for I/O
 So now I have this CPU that&#x27;s interpreting and executing instructions and updating all its own state, but I can&#x27;t do anything with it yet. In order to play a game, you have to see it and be able to interact with it.
 This is where input/output, or I/O, comes in. I/O is the communication between the CPU and the outside world.
 
 Input is data received by the CPU
 Output is data sent from the CPU
 
 So for me, the input will be through the keyboard, and the output will be graphics onto the screen.
 I could just mix the I/O code in with the CPU directly, but then I would be tied to one environment. By creating a generic CPU interface to connect the I/O and the CPU, I can interface with any system.
 The first thing to do was look through the instructions and find any that have to do with I/O. A few examples of those instructions:
 
 CLS - Clear the screen
 LD Vx, K - Wait for a key press, store the value of the key in Vx.
 DRW Vx, Vy, nibble - Display n-byte sprite starting at memory location I
 
 Based on that, we&#x27;ll want the interface to have methods like:
 
 clearDisplay()
 waitKey()
 drawPixel() (drawSprite would have been 1:1, but it ended up to be easier doing it pixel-by-pixel from the interface)
 
 JavaScript doesn&#x27;t really have a concept of an abstract class as far as I could find, but I created one by making a class that could not itself be instantiated, with methods that can only be used from classes that extend it. Here are all the interface methods on the class:
 CpuInterface.js
 // Abstract CPU interface class
 class CpuInterface {
   constructor() {
     if (new.target &#x3D;&#x3D;&#x3D; CpuInterface) {
       throw new TypeError(&#x27;Cannot instantiate abstract class&#x27;)
     }
   }
 
   clearDisplay() {
     throw new TypeError(&#x27;Must be implemented on the inherited class.&#x27;)
   }
 
   waitKey() {
     throw new TypeError(&#x27;Must be implemented on the inherited class.&#x27;)
   }
 
   getKeys() {
     throw new TypeError(&#x27;Must be implemented on the inherited class.&#x27;)
   }
 
   drawPixel() {
     throw new TypeError(&#x27;Must be implemented on the inherited class.&#x27;)
   }
 
   enableSound() {
     throw new TypeError(&#x27;Must be implemented on the inherited class.&#x27;)
   }
 
   disableSound() {
     throw new TypeError(&#x27;Must be implemented on the inherited class.&#x27;)
   }
 }
 Here&#x27;s how it will work: the interface will be loaded into the CPU on initialization, and the CPU will be able to access methods on the interface.
 class CPU {
   // Initialize the interface
   constructor(cpuInterface) {
     this.interface &#x3D; cpuInterface
   }
 
   _execute(instruction) {
     const { id, args } &#x3D; instruction
 
     switch (id) {
       case &#x27;CLS&#x27;:
         // Use the interface while executing an instruction
         this.interface.clearDisplay()
   }
 }
 Before setting up the interface with any real environment (web, terminal, or native) I created a mock interface for tests. It doesn&#x27;t actually hook up to any I/O but it helped me to set up the state of the interface and prepare it for real data. I&#x27;ll ignore the sound ones, because that was never implemented with actual speaker output, so that leaves the keyboard and screen.
 Screen
 The screen has a resolution of 64 pixels wide by 32 pixels tall. So as far as the CPU and interface is concerned, its a 64x32 grid of bits that are either on or off. To set up an empty screen, I can just make a 3D array of zeroes to represent all pixels being off. A frame buffer is a portion of memory containing a bitmapped image that will be rendered to a display.
 MockCpuInterface.js
 // Interface for testing
 class MockCpuInterface extends CpuInterface {
   constructor() {
     super()
 
     // Store the screen data in the frame buffer
     this.frameBuffer &#x3D; this.createFrameBuffer()
   }
 
   // Create 3D array of zeroes
   createFrameBuffer() {
     let frameBuffer &#x3D; []
 
     for (let i &#x3D; 0; i &lt; 32; i++) {
       frameBuffer.push([])
       for (let j &#x3D; 0; j &lt; 64; j++) {
         frameBuffer[i].push(0)
       }
     }
 
     return frameBuffer
   }
 
   // Update a single pixel with a value (0 or 1)
   drawPixel(x, y, value) {
     this.frameBuffer[y][x] ^&#x3D; value
   }
 }
 So I end up with something like this to represent the screen (when printing it as a newline-separated string):
 0000000000000000000000000000000000000000000000000000000000000000
 0000000000000000000000000000000000000000000000000000000000000000
 0000000000000000000000000000000000000000000000000000000000000000
 ...etc...
 In the DRW function, the CPU will loop through the sprite it pulled from memory and update each pixel in the sprite (some details left out for brevity).
 case &#x27;DRW_VX_VY_N&#x27;:
   // The interpreter reads n bytes from memory, starting at the address stored in I
   for (let i &#x3D; 0; i &lt; args[2]; i++) {
     let line &#x3D; this.memory[this.I + i]
       // Each byte is a line of eight pixels
       for (let position &#x3D; 0; position &lt; 8; position++) {
         // ...Get value, x, and y...
         this.interface.drawPixel(x, y, value)
       }
     }
 The clearDisplay() function is the only other method that will be used for interacting with the screen. This is all the CPU interface needs for interacting with the screen.
 Keys
 For keys, I mapped the original hex keyboard to the following 4x4 grid of keys:
 ┌───┬───┬───┬───┐
 │ 1 │ 2 │ 3 │ 4 │
 │ Q │ W │ E │ R │
 │ A │ S │ D │ F │
 │ Z │ X │ C │ V │
 └───┴───┴───┴───┘
 I put the keys in an array.
 // prettier-ignore
 const keyMap &#x3D; [
   &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;,
   &#x27;q&#x27;, &#x27;w&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, 
   &#x27;a&#x27;, &#x27;s&#x27;, &#x27;d&#x27;, &#x27;f&#x27;, 
   &#x27;z&#x27;, &#x27;x&#x27;, &#x27;c&#x27;, &#x27;v&#x27;
 ]
 And create some state to store the currently pressed keys.
 this.keys &#x3D; 0
 In the interface, keys is a binary number consisting of 16 digits where each index represents a key. Chip-8 just wants to know at any given time which keys are pressed out of the 16 and makes a decision based on that. A few examples below:
 0b1000000000000000 // V is pressed (keyMap[15], or index 15)
 0b0000000000000011 // 1 and 2 are pressed (index 0, 1)
 0b0000000000110000 // Q and W are pressed (index 4, 5)
 Now if, for example, V is pressed (keyMap[15]) and the operand is 0xf (decimal 15), the key is pressed. Left shifting (&lt;&lt;) with 1 will create a binary number with a 1 followed by as many zeroes as are in the left shift.
 case &#x27;SKP_VX&#x27;:
   // Skip next instruction if key with the value of Vx is pressed
   if (this.interface.getKeys() &amp; (1 &lt;&lt; this.registers[args[0]])) {
    // Skip instruction
   } else {
     // Go to next instruction
   }
 There is one other key method, waitKey, where the instruction is to wait for a keypress and return the key once pressed.
 CLI App - Interfacing with the Terminal
 The first interface I made was for the terminal. This was less familiar to me than working with the DOM as I have never made any sort of graphical app in terminal, but it wasn&#x27;t too difficult.
 
       
     
   
   
     
 Curses is a library used to create text user interfaces in the terminal. Blessed is a library wrapping curses for Node.js.
 Screen
 The frame buffer that contains the bitmap of the screen data is the same for all implementations, but the way the screen interfaces with each environment will be different.
 With blessed, I just defined a screen object:
 this.screen &#x3D; blessed.screen({ smartCSR: true })
 And used fillRegion or clearRegion on the pixel with a full unicode block to fill it out, using the frameBuffer as the data source.
 drawPixel(x, y, value) {
   this.frameBuffer[y][x] ^&#x3D; value
 
   if (this.frameBuffer[y][x]) {
     this.screen.fillRegion(this.color, &#x27;█&#x27;, x, x + 1, y, y + 1)
   } else {
     this.screen.clearRegion(x, x + 1, y, y + 1)
   }
 
   this.screen.render()
 }
 Keys
 The key handler was not too different from what I would expect with the DOM. If a key is pressed, the handler passes the key along, which I can then use to find the index and update the keys object with any new additional keys that have been pressed.
 this.screen.on(&#x27;keypress&#x27;, (_, key) &#x3D;&gt; {
   const keyIndex &#x3D; keyMap.indexOf(key.full)
 
   if (keyIndex) {
     this._setKeys(keyIndex)
   }
 })
 The only particularly weird thing was that blessed didn&#x27;t have a any keyup event that I could use, so I had to just simulate one by setting an interval that would periodically clear the keys.
 setInterval(() &#x3D;&gt; {
   // Emulate a keyup event to clear all pressed keys
   this._resetKeys()
 }, 100)
 Entrypoint
 Everything is set up now - the rom buffer to convert the binary data to opcodes, the interface to connect I/O, the CPU containing state, the instruction cycle, and two exposed methods - one to load the game, and one to step through a cycle. So I create a cycle function which will run the CPU instructions in an infinite loop.
 terminal.js
 const fs &#x3D; require(&#x27;fs&#x27;)
 const { CPU } &#x3D; require(&#x27;../classes/CPU&#x27;)
 const { RomBuffer } &#x3D; require(&#x27;../classes/RomBuffer&#x27;)
 const { TerminalCpuInterface } &#x3D; require(&#x27;../classes/interfaces/TerminalCpuInterface&#x27;)
 
 // Retrieve the ROM file
 const fileContents &#x3D; fs.readFileSync(process.argv.slice(2)[0])
 
 // Initialize the terminal interface
 const cpuInterface &#x3D; new TerminalCpuInterface()
 
 // Initialize the CPU with the interface
 const cpu &#x3D; new CPU(cpuInterface)
 
 // Convert the binary code into opcodes
 const romBuffer &#x3D; new RomBuffer(fileContents)
 
 // Load the game
 cpu.load(romBuffer)
 
 function cycle() {
   cpu.step()
 
   setTimeout(cycle, 3)
 }
 
 cycle()
 There is also a delay timer in the cycle function, but I removed it from the example for clarity.
 Now I can run a script of the terminal entrypoint file and pass a ROM as the argument to play the game.
 npm run play:terminal roms/PONG
 Web App - Interfacing with the Browser
 The next interface I made was for the web, communicating with the browser and the DOM. I made this version of the emulator a bit more fancy, since the browser is more of my familiar environment and I can&#x27;t resist the urge to make retro looking websites. This one also allows you to switch between games.
 
       
     
   
   
     
 Screen
 For the screen, I used the Canvas API, which uses CanvasRenderingContext2D for the drawing surface. Using fillRect with canvas was basically the same as fillRegion in blessed.
 this.screen &#x3D; document.querySelector(&#x27;canvas&#x27;)
 this.context &#x3D; this.screen.getContext(&#x27;2d&#x27;)
 this.context.fillStyle &#x3D; &#x27;black&#x27;
 this.context.fillRect(0, 0, this.screen.width, this.screen.height)
 One slight difference I made here is I multiplied all the pixels by 10 so the screen would be more visible.
 this.multiplier &#x3D; 10
 this.screen.width &#x3D; DISPLAY_WIDTH * this.multiplier
 this.screen.height &#x3D; DISPLAY_HEIGHT * this.multiplier
 This made the drawPixel command more verbose, but otherwise the same concept.
 drawPixel(x, y, value) {
   this.frameBuffer[y][x] ^&#x3D; value
 
   if (this.frameBuffer[y][x]) {
     this.context.fillStyle &#x3D; COLOR
     this.context.fillRect(
       x * this.multiplier,
       y * this.multiplier,
       this.multiplier,
       this.multiplier
     )
   } else {
     this.context.fillStyle &#x3D; &#x27;black&#x27;
     this.context.fillRect(
       x * this.multiplier,
       y * this.multiplier,
       this.multiplier,
       this.multiplier
     )
   }
 }
 Keys
 I had access to a lot more key event handlers with the DOM, so I was able to easily handle the keyup and keydown events without any hacks.
 // Set keys on key down
 document.addEventListener(&#x27;keydown&#x27;, event &#x3D;&gt; {
   const keyIndex &#x3D; keyMap.indexOf(event.key)
 
   if (keyIndex) {
     this._setKeys(keyIndex)
   }
 })
 
 // Reset keys on keyup
 document.addEventListener(&#x27;keyup&#x27;, event &#x3D;&gt; {
   this._resetKeys()
 })
 }
 Entrypoint
 I handled working with the modules by importing all of them and setting them to the global object, then using Browserify to use them in the browser. Setting them to the global makes them available on the window so I could use the code output in a browser script. Nowadays I might use Webpack or something else for this, but it was quick and simple.
 web.js
 const { CPU } &#x3D; require(&#x27;../classes/CPU&#x27;)
 const { RomBuffer } &#x3D; require(&#x27;../classes/RomBuffer&#x27;)
 const { WebCpuInterface } &#x3D; require(&#x27;../classes/interfaces/WebCpuInterface&#x27;)
 
 const cpuInterface &#x3D; new WebCpuInterface()
 const cpu &#x3D; new CPU(cpuInterface)
 
 // Set CPU and Rom Buffer to the global object, which will become window in the
 // browser after bundling.
 global.cpu &#x3D; cpu
 global.RomBuffer &#x3D; RomBuffer
 The web entrypoint uses the same cycle function as the terminal implementation, but has a function to fetch each ROM and reset the display every time a new one is selected. I&#x27;m used to working with json data and fetch, but in this case I fetched the raw arrayBuffer from the response.
 // Fetch the ROM and load the game
 async function loadRom() {
   const rom &#x3D; event.target.value
   const response &#x3D; await fetch(&#x60;./roms/${rom}&#x60;)
   const arrayBuffer &#x3D; await response.arrayBuffer()
   const uint8View &#x3D; new Uint8Array(arrayBuffer)
   const romBuffer &#x3D; new RomBuffer(uint8View)
 
   cpu.interface.clearDisplay()
   cpu.load(romBuffer)
 }
 
 // Add the ability to select a game
 document.querySelector(&#x27;select&#x27;).addEventListener(&#x27;change&#x27;, loadRom)
 The HTML includes a canvas and a select.
 &lt;canvas&gt;&lt;/canvas&gt;
 &lt;select&gt;
   &lt;option disabled selected&gt;Load ROM...&lt;/option&gt;
   &lt;option value&#x3D;&quot;CONNECT4&quot;&gt;Connect4&lt;/option&gt;
   &lt;option value&#x3D;&quot;PONG&quot;&gt;Pong&lt;/option&gt;
 &lt;/select&gt;
 Then I just deployed the code onto GitHub pages because it&#x27;s static.
 Native App - Interfacing with the Native Platform
 I also made an experimental native UI implementation. I used Raylib, a programming library for programming simple games that had bindings for Node.js.
 
       
     
   
   
     
 I consider this version experimental just because it&#x27;s really slow compared to the other ones, so it&#x27;s less usable, but everything works correctly with the keys and screen.
 Entrypoint
 Raylib works a bit differently than the other implementations because Raylib itself runs in a loop, meaning I won&#x27;t end up using the cycle function.
 const r &#x3D; require(&#x27;raylib&#x27;)
 
 // As long as the window shouldn&#x27;t close...
 while (!r.WindowShouldClose()) {
   // Fetch, decode, execute
   cpu.step()
 
   r.BeginDrawing()
   // Paint screen with amy changes
   r.EndDrawing()
 }
 
 r.CloseWindow()
 Screen
 Within the beginDrawing() and endDrawing() methods, the screen will update. For the Raylib implementation I accessed the interface directly from the script instead of keeping everything contained in the interface, but it works.
 r.BeginDrawing()
 
 cpu.interface.frameBuffer.forEach((y, i) &#x3D;&gt; {
   y.forEach((x, j) &#x3D;&gt; {
     if (x) {
       r.DrawRectangleRec({ x, y, width, height }, r.GREEN)
     } else {
       r.DrawRectangleRec({ x, y, width, height }, r.BLACK)
     }
   })
 })
 
 r.EndDrawing()
 Keys
 Getting the keys to work on Raylib was the last thing I worked on. It was more difficult to figure out because I had to do everything in the IsKeyDown method - there was a GetKeyPressed method, but it had side effects and caused problems. So instead of just waiting for a keypress like the other implementations, I had to loop through all keys and check if they were down, and append them to the key bitmask if so.
 let keyDownIndices &#x3D; 0
 // Run through all possible keys
 for (let i &#x3D; 0; i &lt; nativeKeyMap.length; i++) {
   const currentKey &#x3D; nativeKeyMap[i]
   // If key is already down, add index to key down map
   // This will also lift up any keys that aren&#x27;t pressed
   if (r.IsKeyDown(currentKey)) {
     keyDownIndices |&#x3D; 1 &lt;&lt; i
   }
 }
 
 // Set all pressed keys
 cpu.interface.setKeys(keyDownIndices)
 That&#x27;s it for the native implementation. It was more of a challenge than the other ones, but I&#x27;m glad I did it to round out the interface and see how well it would work on drastically different platforms.
 Conclusion
 So that&#x27;s my Chip-8 project! Once again, you can check out the source on GitHub. I learned a lot about lower level programming concepts and how a CPU operates, and also about the capabilities of JavaScript outside of a browser app or REST API server. I still have a few things left to do in this project, like attempt to make a (very) simple game, but the emulator is complete, and I&#x27;m proud to have finished it.</content>
     </entry>
     <entry>
       <title>LCH colors in CSS: what, why, and how?</title>
         <link href="https://lea.verou.me/2020/04/lch-colors-in-css-what-why-and-how/"/>
       <updated>2020-04-04T13:23:31.000Z</updated>
       <content type="text">I was always interested in color science. In 2014, I gave a talk about CSS Color 4 at various conferences around the world called “The Chroma Zone”. Even before that, in 2009, I wrote a color picker that used a hidden Java applet to support ICC color profiles to do CMYK properly, a first on the Web at the time (to my knowledge). I never released it, but it sparked this angry rant.
 
 
 
 Color is also how I originally met my now husband, Chris Lilley: In my first CSS WG meeting in 2012, he approached me to ask a question about CSS and Greek, and once he introduced himself I said “You’re Chris Lilley, the color expert?!? I have questions for you!”. I later discovered that he had done even more cool things (he was a co-author of PNG and started SVG ), but at the time, I only knew of him as “the W3C color expert”, that’s how much into color I was (I got my color questions answered much later, in 2015 that we actually got together).
 
 
 
 My interest in color science was renewed in 2019, after I became co-editor of CSS Color 5, with the goal of fleshing out my color modification proposal, which aims to allow arbitrary tweaking of color channels to create color variations, and combine it with Una’s color modification proposal. LCH colors in CSS is something I’m very excited about, and I strongly believe designers would be outraged we don’t have them yet if they knew more about them.
 
 
 
 
 
 
 
 What is LCH?
 
 
 
 CSS Color 4 defines lch() colors, among other things, and as of recently, all major browsers have started implementing them or are seriously considering it:
 
 
 
 Safari is already implementing, Chrome is about to,and Firefox is discussing it. 
 
 
 
 LCH is a color space that has several advantages over the RGB/HSL colors we’re familiar with in CSS. In fact, I’d go as far as to call it a game-changer, and here’s why.
 
 
 
 1. We actually get access to about 50% more colors.
 
 
 
 This is huge. Currently, every CSS color we can specify, is defined to be in the sRGB color space. This was more than sufficient a few years ago, since all but professional monitors had gamuts smaller than sRGB. However, that’s not true any more. Today, the gamut (range of possible colors displayed) of most monitors is closer to P3, which has a 50% larger volume than sRGB. CSS right now cannot access these colors at all. Let me repeat: We have no access to one third of the colors in most modern monitors. And these are not just any colors, but the most vivid colors the screen can display. Our websites are washed out because monitor hardware evolved faster than CSS specs and browser implementations.
 
 
 
 Gamut volume of sRGB vs P3
 
 
 
 2. LCH (and Lab) is perceptually uniform
 
 
 
 In LCH, the same numerical change in coordinates produces the same perceptual color difference. This property of a color space is called “perceptual uniformity”. RGB or HSL are not perceptually uniform. A very illustrative example is the following [example source]:
 
 
 
 Both the colors in the first row, as well as the colors in the second row, only differ by 20 degrees in hue. Is the perceptual difference between them equal?
 
 
 
 3. LCH lightness actually means something
 
 
 
 In HSL, lightness is meaningless. Colors can have the same lightness value, with wildly different perceptual lightness. My favorite examples are yellow and blue. Believe it or not, both have the same HSL lightness!
 
 
 
 Both of these colors have a lightness of 50%, but they are most certainly not equally light. What does HSL lightness actually mean then?
 
 
 
 You might argue that at least lightness means something for constant hue and saturation, i.e. for adjustments within the same color. It is true that we do get a lighter color if we increase the HSL lightness and a darker one if we decrease it, but it’s not necessarily the same color:
 
 
 
 Both of these have the same hue and saturation, but do they really look like darker and lighter variants of the same color?
 
 
 
 With LCH, any colors with the same lightness are equally perceptually light, and any colors with the same chroma are equally perceptually saturated.
 
 
 
 How does LCH work?
 
 
 
 LCH stands for “Lightness Chroma Hue”. The parameters loosely correspond to HSL’s, however there are a few crucial differences:
 
 
 
 The hue angles don’t fully correspond to HSL’s hues. E.g. 0 is not red, but more of a magenta and 180 is not turquoise but more of a bluish green, and is exactly complementary. 
 
 
 
 Note how these colors, while wildly different in hue, perceptually have the same lightness.
 
 
 
 In HSL, saturation is a neat 0-100 percentage, since it’s a simple transformation of RGB into polar coordinates. In LCH however, Chroma is theoretically unbounded. LCH (like Lab) is designed to be able to represent the entire spectrum of human vision, and not all of these colors can be displayed by a screen, even a P3 screen. Not only is the maximum chroma different depending on screen gamut, it’s actually different per color. 
 
 
 
 This may be better understood with an example. For simplicity, assume you have a screen whose gamut exactly matches the sRGB color space (for comparison, the screen of a 2013 MacBook Air was about 60% of sRGB, although most modern screens are about 150% of sRGB, as discussed above). For L&#x3D;50 H&#x3D;180 (the cyan above), the maximum Chroma is only 35! For L&#x3D;50 H&#x3D;0 (the magenta above), Chroma can go up to 77 without exceeding the boundaries of sRGB. For L&#x3D;50 H&#x3D;320 (the purple above), it can go up to 108!
 
 
 
 While the lack of boundaries can be somewhat unsettling (in people and in color spaces), don’t worry: if you specify a color that is not displayable in a given monitor, it will be scaled down so that it becomes visible while preserving its essence. After all, that’s not new: before monitors got gamuts wider than sRGB, this is what was happening with regular CSS colors when they were displayed in monitors with gamuts smaller than sRGB.
 
 
 
 An LCH color picker
 
 
 
 Hopefully, you are now somewhat excited about LCH, but how to visualize it?
 
 
 
 I actually made this a while ago, primarily to help me, Chris, Adam, and Una in wrapping our heads around LCH sufficiently to edit CSS Color 5. It’s different to know the theory, and it’s different to be able to play with sliders and see the result. I even bought a domain, css.land, to host similar demos eventually. We used it a fair bit, and Chris got me to add a few features too, but I never really posted about it, so it was only accessible to us, and anybody that noticed its Github repo.
 
 
 
 
 
 
 
 Why not just use an existing LCH color picker?
 
 
 
 The conversion code for this is written by Chris, and he was confident the math is at least intended to be correct (i.e. if it’s wrong it’s a bug in the code, not a gap in understanding)The Chroma is not 0-100 like in some color pickers we foundWe wanted to allow inputting arbitrary CSS colors (the “Import…” button above)We wanted to allow inputting decimals (the sliders only do integers, but the black number inputs allow any number)I wanted to be able to store colors, and see how they interpolate.We wanted to be able to see whether the LCH color was within sRGB, P3, (or Rec.2020, an even larger color space).We wanted alphaAnd lastly, because it’s fun! Especially since it’s implemented with Mavo (and a little bit of JS, this is not a pure Mavo HTML demo).
 
 
 
 Recently, Chris posted it in a whatwg/html issue thread and many people discovered it, so it nudged me to post about it, so, here it is: css.land/lch
 
 
 
 FAQ
 
 
 
 Based on the questions I got after I posted this article, I should clarify a few common misconceptions.
 
 
 
 “You said that these colors are not implemented yet, but I see them in your article”
 
 
 
 All of the colors displayed in this article are within the sRGB gamut, exactly because we can’t display those outside it yet. sRGB is a color space, not a syntax. E.g. rgb(255 0 0) and lch(54.292% 106.839 40.853) specify the same color.
 
 
 
 “How does the LCH picker display colors outside sRGB?”
 
 
 
 It doesn’t. Neither does any other on the Web (to my knowledge). The color picker is implemented with web technologies, and therefore suffers from the same issues. It has to scale them down to display something similar, that is within sRGB (it used to just clip the RGB components to 0-100%, but thanks to this PR from Tab it now uses a far superior algorithm: it just reduces the Chroma until the color is within sRGB). This is why increasing the Chroma doesn’t produce a brighter color beyond a certain point: because that color cannot be displayed with CSS right now.
 
 
 
 “I’ve noticed that Firefox displays more vivid colors than Chrome and Safari, is that related?”
 
 
 
 Firefox does not implement the spec that restricts CSS colors to sRGB. Instead, it just throws the raw RGB coordinates on the screen, so e.g. rgb(100% 0% 0%) is the brightest red your screen can display. While this may seem like a superior solution, it’s incredibly inconsistent: specifying a color is approximate at best, since every screen displays it differently. By restricting CSS colors to a known color space (sRGB) we gained device independence. LCH and Lab are also device independent as they are based on actual measured color.
 
 
 
 What about color(display-p3 r g b)? Safari supports that since 2017!
 
 
 
 I was notified of this after I posted this article. I was aware Safari was implementing this syntax a while ago, but somehow missed that they shipped it. In fact, WebKit published an article about this syntax last month!  How exciting!
 
 
 
 color(colorspaceid params) is another syntax added by CSS Color 4 and is the swiss army knife of color management in CSS: in its full glory it allows specifying an ICC color profile and colors from it (e.g. you want real CMYK colors on a webpage? You want Pantone? With color profiles, you can do that too!). It also supports some predefined color spaces, of which display-p3 is one. So, for example, color(display-p3 0 1 0) gives us the brightest green in the P3 color space.  You can use this test case to test support: you’ll see red if color() is not supported and bright green if it is.
 
 
 
 
 
 
 
 Exciting as it may be (and I should tweak the color picker to use it when available!), do note that it only addresses the first issue I mentioned: getting to all gamut colors. However, since it’s RGB-based, it still suffers from the other issues of RGB. It is not perceptually uniform, and is difficult to create variants (lighter or darker, more or less vivid etc) by tweaking its parameters. 
 
 
 
 Furthermore, it’s a short-term solution. It works now, because screens that can display a wider gamut than P3 are rare. Once hardware advances again, color(display-p3 ...) will have the same problem as sRGB colors have today. LCH and Lab are device independent, and can represent the entire gamut of human vision so they will work regardless of how hardware advances.
 
 
 
 How does LCH relate to the Lab color space that I know from Photoshop and other applications?
 
 
 
 LCH is the same color space as Lab, just viewed differently! Take a look at the following diagram that I made for my students:
 
 
 
 
 
 
 
 The L in Lab and LCH is exactly the same (perceptual Lightness). For a given lightness L, in Lab, a color has cartesian coordinates (L, a, b) and polar coordinates (L, C, H). Chroma is just the length of the line from 0 to point (a, b) and Hue is the angle of that ray. Therefore, the formulae to convert Lab to LCH are trivial one liners: C is sqrt(a² + b²) and H is atan(b/a) (with different handling if a &#x3D; 0). atan() is just the reverse of tan(), i.e. tan(H) &#x3D; b/a.</content>
     </entry>
     <entry>
       <title>Understanding Default Parameters in JavaScript</title>
         <link href="https://www.taniarascia.com/default-parameters-javascript/"/>
       <updated>2020-03-31T00:00:00.000Z</updated>
       <content type="text">This article was originally written for DigitalOcean.
 In ECMAScript 2015, default function parameters were introduced to the JavaScript language. These allow developers to initialize a function with default values if the arguments are not supplied to the function call. Initializing function parameters in this way will make your functions easier to read and less error-prone, and will provide default behavior for your functions. This will help you avoid errors that stem from passing in undefined arguments and destructuring objects that don&#x27;t exist.
 In this article, you will review the difference between parameters and arguments, learn how to use default parameters in functions, see alternate ways to support default parameters, and learn what types of values and expressions can be used as default parameters. You will also run through examples that demonstrate how default parameters work in JavaScript.
 Arguments and Parameters
 Before explaining default function parameters, it is important to know what it is that parameters can default to. Because of this, we will first review the difference between arguments and parameters in a function. If you would like to learn more about this distinction, check out our earlier article in the JavaScript series, How to Define Functions in JavaScript.
 In the following code block, you will create a function that returns the cube of a given number, defined as x:
 // Define a function to cube a number
 function cube(x) {
   return x * x * x
 }
 The x variable in this example is a parameter—a named variable passed into a function. A parameter must always be contained in a variable and must never have a direct value.
 Now take a look at this next code block, which calls the cube function you just created:
 // Invoke cube function
 cube(10)
 This will give the following output:
 1000
 In this case, 10 is an argument—a value passed to a function when it is invoked. Often the value will be contained in a variable as well, such as in this next example:
 // Assign a number to a variable
 const number &#x3D; 10
 
 // Invoke cube function
 cube(number)
 This will yield the same result:
 1000
 If you do not pass an argument to a function that expects one, the function will implicitly use undefined as the value:
 // Invoke the cube function without passing an argument
 cube()
 This will return:
 NaN
 In this case, cube() is trying to calculate the value of undefined * undefined * undefined, which results in NaN, or &quot;not a number&quot;. For more on this, take a look at the number section of Understanding Data Types in JavaScript.
 This automatic behavior can sometimes be a problem. In some cases, you might want the parameter to have a value even if no argument was passed to the function. That&#x27;s where the default parameters feature comes in handy, a topic that you will cover in the next section.
 Default Parameter Syntax
 With the addition of default parameters in ES2015, you can now assign a default value to any parameter, which the function will use instead of undefined when called without an argument. This section will first show you how to do this manually, and then will guide you through setting default parameters.
 Without default parameters, you would have to explicitly check for undefined values in order to set defaults, as is shown in this example:
 // Check for undefined manually
 function cube(x) {
   if (typeof x &#x3D;&#x3D;&#x3D; &#x27;undefined&#x27;) {
     x &#x3D; 5
   }
 
   return x * x * x
 }
 
 cube()
 This uses a conditional statement to check if the value has been automatically provided as undefined, then sets the value of x as 5. This will result in the following output:
 125
 In contrast, using default parameters accomplishes the same goal in much less code. You can set a default value to the parameter in cube by assigning it with the equality assignment operator (&#x3D;), as highlighted here:
 // Define a cube function with a default value
 function cube(x &#x3D; 5) {
   return x * x * x
 }
 Now when the cube function is invoked without an argument, it will assign 5 to x and return the calculation instead of NaN:
 // Invoke cube function without an argument
 cube()
 125
 It will still function as intended when an argument is passed, ignoring the default value:
 // Invoke cube function with an argument
 cube(2)
 8
 However, one important caveat to note is that the default parameter value will also override an explicit undefined passed as an argument to a function, as demonstrated here:
 // Invoke cube function with undefined
 cube(undefined)
 This will give the calculation with x equal to 5:
 125
 In this case, the default parameter values were calculated, and an explicit undefined value did not override them.
 Now that you have an idea of the basic syntax of default parameters, the next section will show how default parameters work with different data types.
 Default Parameter Data Types
 Any primitive value or object can be used as a default parameter value. In this section, you will see how this flexibility increases the ways in which default parameters can be used.
 First, set parameters using a number, string, boolean, object, array, and null value as a default value. This example will use arrow function syntax:
 // Create functions with a default value for each data type
 const defaultNumber &#x3D; (number &#x3D; 42) &#x3D;&gt; console.log(number)
 const defaultString &#x3D; (string &#x3D; &#x27;Shark&#x27;) &#x3D;&gt; console.log(string)
 const defaultBoolean &#x3D; (boolean &#x3D; true) &#x3D;&gt; console.log(boolean)
 const defaultObject &#x3D; (object &#x3D; { id: 7 }) &#x3D;&gt; console.log(object)
 const defaultArray &#x3D; (array &#x3D; [1, 2, 3]) &#x3D;&gt; console.log(array)
 const defaultNull &#x3D; (nullValue &#x3D; null) &#x3D;&gt; console.log(nullValue)
 When these functions are invoked without parameters, they will all use the default values:
 // Invoke each function
 defaultNumber()
 defaultString()
 defaultBoolean()
 defaultObject()
 defaultArray()
 defaultNull()
 42
 &quot;Shark&quot;
 true
 {id: 7}
 (3) [1, 2, 3]
 null
 Note that any object created in a default parameter will be created every time the function is called. One of the common use cases for default parameters is to use this behavior to obtain values out of an object. If you try to destructure or access a value from an object that doesn&#x27;t exist, it will throw an error. However, if the default parameter is an empty object, it will simply give you undefined values instead of throwing an error:
 // Define a settings function with a default object
 function settings(options &#x3D; {}) {
   const { theme, debug } &#x3D; options
 
   // Do something with settings
 }
 This will avoid the error caused by destructuring objects that don&#x27;t exist.
 Now that you&#x27;ve seen how default parameters operate with different data types, the next section will explain how multiple default parameters can work together.
 Using Multiple Default Parameters
 You can use as many default parameters as you want in a function. This section will show you how to do this, and how to use it to manipulate the DOM in a real-world example.
 First, declare a sum() function with multiple default parameters:
 // Define a function to add two values
 function sum(a &#x3D; 1, b &#x3D; 2) {
   return a + b
 }
 
 sum()
 This will result in the following default calculation:
 3
 Additionally, the value used in a parameter can be used in any subsequent default parameter, from left to right. For example, this createUser function creates a user object userObj as the third parameter, and all the function itself does is return userObj with the first two parameters:
 // Define a function to create a user object using parameters
 function createUser(name, rank, userObj &#x3D; { name, rank }) {
   return userObj
 }
 
 // Create user
 const user &#x3D; createUser(&#x27;Jean-Luc Picard&#x27;, &#x27;Captain&#x27;)
 If you call user here, you will get the following:
 {name: &quot;Jean-Luc Picard&quot;, rank: &quot;Captain&quot;}
 It is usually recommended to put all default parameters at the end of a list of parameters, so that you can easily leave off optional values. If you use a default parameter first, you will have to explicitly pass undefined to use the default value.
 Here is an example with the default parameter at the beginning of the list:
 // Define a function with a default parameter at the start of the list
 function defaultFirst(a &#x3D; 1, b) {
   return a + b
 }
 When calling this function, you would have to call defaultFirst() with two arguments:
 defaultFirst(undefined, 2)
 This would give the following:
 3
 Here is an example with the default parameter at the end of the list:
 // Define a function with a default parameter at the end of the list
 function defaultLast(a, b &#x3D; 1) {
   return a + b
 }
 
 defaultLast(2)
 This would yield the same value:
 3
 Both functions have the same result, but the one with the default value last allows a much cleaner function call.
 For a real-world example, here is a function that will create a DOM element, and add a text label and classes, if they exist.
 // Define function to create an element
 function createNewElement(tag, text, classNames &#x3D; []) {
   const el &#x3D; document.createElement(tag)
   el.textContent &#x3D; text
 
   classNames.forEach((className) &#x3D;&gt; {
     el.classList.add(className)
   })
 
   return el
 }
 You can call the function with some classes in an array:
 const greeting &#x3D; createNewElement(&#x27;p&#x27;, &#x27;Hello!&#x27;, [&#x27;greeting&#x27;, &#x27;active&#x27;])
 Calling greeting will give the following value:
 &lt;p class&#x3D;&quot;greeting active&quot;&gt;Hello!&lt;/p&gt;
 However, if you leave the classNames array out of the function call, it will still work.
 const greeting2 &#x3D; createNewElement(&#x27;p&#x27;, &#x27;Hello!&#x27;)
 greeting2 now has the following value:
 &lt;p&gt;Hello!&lt;/p&gt;
 In this example, forEach() can be used on an empty array without an issue. If that empty array were not set in the default parameter, you would get the following error:
 VM2673:5 Uncaught TypeError: Cannot read property &#x27;forEach&#x27; of undefined
     at createNewElement (&lt;anonymous&gt;:5:14)
     at &lt;anonymous&gt;:12:18
 Now that you have seen how multiple default parameters can interact, you can move on to the next section to see how function calls work as default parameters.
 Function Calls as Default Parameters
 In addition to primitives and objects, the result of calling a function can be used as a default parameter.
 In this code block, you will create a function to return a random number, and then use the result as the default parameter value in a cube function:
 // Define a function to return a random number from 1 to 10
 function getRandomNumber() {
   return Math.floor(Math.random() * 10)
 }
 
 // Use the random number function as a default parameter for the cube function
 function cube(x &#x3D; getRandomNumber()) {
   return x * x * x
 }
 Now invoking the cube function without a parameter will have potentially different results every time you call it:
 // Invoke cube function twice for two potentially different results
 cube()
 cube()
 The output from these function calls will vary:
 512
 64
 You can even use built-in methods, like those on the Math object, and use the value returned in one function call as a parameter in another function.
 In the following example, a random number is assigned to x, which is used as the parameter in the cube function you created. The y parameter will then calculate the cube root of the number and check to see if x and y are equal:
 // Assign a random number to x
 // Assign the cube root of the result of the cube function and x to y
 function doesXEqualY(x &#x3D; getRandomNumber(), y &#x3D; Math.cbrt(cube(x))) {
   return x &#x3D;&#x3D;&#x3D; y
 }
 
 doesXEqualY()
 This will give the following:
 true
 A default parameter can even be a function definition, as seen in this example, which defines a parameter as the inner function and returns the function call of parameter:
 // Define a function with a default parameter that is an anonymous function
 function outer(
   parameter &#x3D; function inner() {
     return 100
   }
 ) {
   return parameter()
 }
 
 // Invoke outer function
 outer()
 100
 This inner function will be created from scratch every time the outer function is invoked.
 Conclusion
 In this article, you learned what default function parameters are and how to use them. Now you can use default parameters to help keep your functions clean and easy to read. You can also assign empty objects and arrays to parameters upfront to reduce both complexity and lines of code when dealing with situations such as retrieving values from an object or looping through an array.
 If you would like to learn more about JavaScript, check out the homepage for our How To Code in JavaScript series, or browse our How to Code in Node.js series for articles on back-end development.</content>
     </entry>
     <entry>
       <title>Redux Tutorial: An Overview and Walkthrough</title>
         <link href="https://www.taniarascia.com/redux-react-guide/"/>
       <updated>2020-03-09T00:00:00.000Z</updated>
       <content type="text">Do you have experience using React? Have you heard of Redux, but you&#x27;ve put off learning it because it looks very complicated and all the guides seem overwhelming? If that&#x27;s the case, this is the article for you! Contain your fear of containing state and come along with me on this relatively painless journey.
 Prerequisites
 You must already know how to use React for this tutorial, as I will not be explaining any aspects of React itself.
 
 Familiarity with HTML &amp; CSS.
 Familiarity with ES6 syntax and features.
 Knowledge of React terminology: JSX, State, Components, Props, Lifecycle and Hooks
 Knowledge of React Router
 Knowledge of asynchronous JavaScript and making API calls
 
 Also, download Redux DevTools for Chrome or for FireFox.
 Goals
 In this tutorial, we will build a small blog app. It will fetch posts and comments from an API. I&#x27;ve created the same app with both plain Redux, and Redux Toolkit (RTK), the officially sanctioned toolset for Redux. Here are the links to the source and demos of both the plain and RTK versions.
 React + Redux Application (Plain Redux)
 
 Source
 Demo Application
 
 React + Redux Toolkit Application
 
 Source
 Demo Application
 
 
 Note: The applications are pulling from a real API via JSON Placeholder API. Due to rate limiting on CodeSandbox, the API may appear slow, but it has nothing to do with the Redux application itself. You can also clone the repository locally.
 
 We will learn:
 
 What is Redux and why you might want to use it
 The terminology of Redux: actions, reducers, store, dispatch, connect, and container
 Making asynchronous API calls with Redux Thunk
 How to make a small, real-world application with React and Redux
 How to use Redux Toolkit to simplify Redux app development
 
 What is Redux?
 Redux is a state container for JavaScript applications. Normally with React, you manage state at a component level, and pass state around via props. With Redux, the entire state of your application is managed in one immutable object. Every update to the Redux state results in a copy of sections of the state, plus the new change.
 Redux was originally created by Dan Abramov and Andrew Clark.
 Why should I use Redux?
 
 Easily manage global state - access or update any part of the state from any Redux-connected component
 Easily keep track of changes with Redux DevTools - any action or state change is tracked and easy to follow with Redux. The fact that the entire state of the application is tracked with each change means you can easily do time-travel debugging to move back and forth between changes.
 
 The downside to Redux is that there&#x27;s a lot of initial boilerplate to set up and maintain (especially if you use plain Redux without Redux Toolkit). A smaller application may not need Redux and may instead benefit from simply using the Context API for global state needs.
 In my personal experience, I set up an application with Context alone, and later needed to convert everything over to Redux to make it more maintainable and organized.
 Terminology
 Usually I don&#x27;t like to just make a list of terms and definitions, but Redux has a few that are likely unfamiliar, so I&#x27;m just going to define them all up front to make it easy to refer back to them. Although you can skip to the beginning of the tutorial section, I think it would be good to read through all the definitions just to get exposure and an idea of them in your head first.
 
 Actions
 Reducers
 Store
 Dispatch
 Connect
 
 I&#x27;ll just use the typical todo application, and the action of deleting a todo, for the examples.
 Actions
 An action sends data from your application to the Redux store. An action is conventionally an object with two properties: type and (optional) payload. The type is generally an uppercase string (assigned to a constant) that describes the action. The payload is additional data that may be passed.
 Action Type
 const DELETE_TODO &#x3D; &#x27;posts/deleteTodo&#x27;
 Action
 {
   type: DELETE_TODO,
   payload: id,
 }
 Action creators
 An action creator is a function that returns an action.
 Action Creator
 const deleteTodo &#x3D; (id) &#x3D;&gt; ({ type: DELETE_TODO, payload: id })
 Reducers
 A reducer is a function that takes two parameters: state and action. A reducer is immutable and always returns a copy of the entire state. A reducer typically consists of a switch statement that goes through all the possible action types.
 Reducer
 const initialState &#x3D; {
   todos: [
     { id: 1, text: &#x27;Eat&#x27; },
     { id: 2, text: &#x27;Sleep&#x27; },
   ],
   loading: false,
   hasErrors: false,
 }
 
 function todoReducer(state &#x3D; initialState, action) {
   switch (action.type) {
     case DELETE_TODO:
       return {
         ...state,
         todos: state.todos.filter((todo) &#x3D;&gt; todo.id !&#x3D;&#x3D; action.payload),
       }
     default:
       return state
   }
 }
 Store
 The Redux application state lives in the store, which is initialized with a reducer. When used with React, a &lt;Provider&gt; exists to wrap the application, and anything within the Provider can have access to Redux.
 Store
 import { createStore } from &#x27;redux&#x27;
 import { Provider } from &#x27;react-redux&#x27;
 import reducer from &#x27;./reducers&#x27;
 
 const store &#x3D; createStore(reducer)
 
 render(
   &lt;Provider store&#x3D;{store}&gt;
     &lt;App /&gt;
   &lt;/Provider&gt;,
   document.getElementById(&#x27;root&#x27;)
 )
 Dispatch
 dispatch is a method available on the store object that accepts an object which is used to update the Redux state. Usually, this object is the result of invoking an action creator.
 const Component &#x3D; ({ dispatch }) &#x3D;&gt; {
   useEffect(() &#x3D;&gt; {
     dispatch(deleteTodo())
   }, [dispatch])
 }
 Connect
 The connect() function is one typical way to connect React to Redux. A connected component is sometimes referred to as a container.
 Okay, that about covers it for the major terms of Redux. It can be overwhelming to read the terminology without any context, so let&#x27;s begin.
 Getting Started
 For ease of getting started quickly, my example uses Create React App to set up the environment.
 npx create-react-app redux-tutorial
 cd redux-tutorial
 Redux requires a few dependencies.
 
 Redux - Core library
 React Redux - React bindings for Redux
 Redux Thunk - Async middleware for Redux
 Redux DevTools Extension - Connects Redux app to Redux DevTools
 
 You can yarn add or npm i them, and I&#x27;ll be using react-router-dom as well, but that&#x27;s it for extra dependencies.
 npm i \
 redux \
 react-redux \
 redux-thunk \
 redux-devtools-extension \
 react-router-dom
 And delete all the boilerplate. We&#x27;ll add everything we need from scratch instead.
 cd src &amp;&amp; rm * # move to src and delete all files within
 We&#x27;ll make directories for Redux reducers and Redux actions, as well as pages and components which you should already be familiar with from React.
 mkdir actions components pages reducers
 And we&#x27;ll bring back index.js, App.js, and index.css.
 touch index.js index.css App.js
 So at this point your project directory tree looks like this.
 └── src/
     ├── actions/
     ├── components/
     ├── pages/
     ├── reducers/
     ├── App.js
     ├── index.css
     └── index.js
 For the index.css file, just take the contents of this gist and paste it. I intend only to go over functionality and not anything about style, so I just wrote some very basic styles to ensure the site looks decent enough.
 Now we have enough boilerplate to get started, so we&#x27;ll begin working on the entrypoint.
 Setting up the Redux Store
 When I first started learning Redux, it seemed so overwhelming because every app I looked at had index.js set up a bit differently. After looking at a lot of the more up-to-date apps and taking the aspects that were common across all of them, I got a good feel for what should really be in a Redux app, and what is just people moving things around to be unique.
 There are plenty of tutorials out there that show you how to get a very basic Redux store with todos set up, but I don&#x27;t find that very useful for knowing how to make a production level setup, so I&#x27;m going to set it up with everything you need from the get-go. Even so, there will be some opinionated aspects because Redux is very flexible.
 In index.js, we&#x27;ll be bringing in a few things.
 
 createStore, to create the store that will maintain the Redux state
 applyMiddleware, to be able to use middleware, in this case thunk
 Provider, to wrap the entire application in Redux
 thunk, a middleware that allows us to make asynchronous actions in Redux
 composeWithDevTools, code that connects your app to Redux DevTools
 
 index.js
 // External imports
 import React from &#x27;react&#x27;
 import { render } from &#x27;react-dom&#x27;
 import { createStore, applyMiddleware } from &#x27;redux&#x27;
 import { Provider } from &#x27;react-redux&#x27;
 import thunk from &#x27;redux-thunk&#x27;
 import { composeWithDevTools } from &#x27;redux-devtools-extension&#x27;
 
 // Local imports
 import App from &#x27;./App&#x27;
 import rootReducer from &#x27;./reducers&#x27;
 
 // Assets
 import &#x27;./index.css&#x27;
 
 const store &#x3D; createStore(rootReducer, composeWithDevTools(applyMiddleware(thunk)))
 
 render(
   &lt;Provider store&#x3D;{store}&gt;
     &lt;App /&gt;
   &lt;/Provider&gt;,
   document.getElementById(&#x27;root&#x27;)
 )
 Put a component in App.js. We&#x27;ll modify this later, but we just want to get the app up and running for now.
 App.js
 import React from &#x27;react&#x27;
 
 const App &#x3D; () &#x3D;&gt; {
   return &lt;div&gt;Hello, Redux&lt;/div&gt;
 }
 
 export default App
 Bringing in reducers
 The last thing to do is bring in the reducer. A reducer is a function that determines changes to Redux state. It is a pure function that returns a copy of the state with the new change.
 A neat feature of Redux is that we can have many reducers, and combine them all into one root reducer that the store uses, using combineReducers. This leads to us being able to easily organize our code while still having everything in one root state tree.
 Since this app will be like a blog, it will have a list of posts, and we&#x27;ll put that in the postsReducer in a moment. Having this combineReducers method allows us to bring whatever we want in - a commentsReducer, an authReducer, and so on.
 In reducers/index.js, create the file that will combine all reducers.
 reducers/index.js
 import { combineReducers } from &#x27;redux&#x27;
 
 import postsReducer from &#x27;./postsReducer&#x27;
 
 const rootReducer &#x3D; combineReducers({
   posts: postsReducer,
 })
 
 export default rootReducer
 Finally, we&#x27;ll make the postsReducer. We can set it up with an initial state. Just like you might expect from a regular React component, we&#x27;ll have a loading and hasErrors state, as well as a posts array, where all the posts will live. First we&#x27;ll set it up with no actions in the switch, just a default case that returns the entire state.
 reducers/postsReducer.js
 export const initialState &#x3D; {
   posts: [],
   loading: false,
   hasErrors: false,
 }
 
 export default function postsReducer(state &#x3D; initialState, action) {
   switch (action.type) {
     default:
       return state
   }
 }
 Now we at least have enough setup that the application will load without crashing.
 Redux DevTools
 With the application loading and the Redux &lt;Provider&gt; set up, we can take a look at Redux DevTools. After downloading it, it&#x27;ll be a tab in your Developer Tools. If you click on State, you&#x27;ll see the entire state of the application so far.
 
       
     
   
   
     
 There&#x27;s not much in here yet, but Redux DevTools is amazing once you get to having a lot of reducers and actions.
 It keeps track of all changes to your app and makes debugging a breeze compared to plain React.
 Setting up Redux Actions
 So now we have a reducer for posts, but we don&#x27;t have any actions, meaning the reducer will only return the state without modifying it in any way. Actions are how we communicate with the Redux store. For this blog app, we&#x27;re going to want to fetch posts from an API and put them in our Redux state.
 Since fetching posts is an asynchronous action, it will require the use of Redux thunk. Fortunately, we don&#x27;t have to do anything special to use thunk beyond setting it up in the store, which we already did.
 Create a actions/postsActions.js. First, we&#x27;ll define the action types as constants. This is not necessary, but is a common convention, and makes it easy to export the actions around and prevent typos. We want to do three things:
 
 getPosts - begin telling Redux we&#x27;re going to fetch posts from an API
 getPostsSuccess - pass the posts to Redux on successful API call
 getPostsFailure - inform Redux that an error was encountered during Redux on failed API call
 
 actions/postsActions.js
 // Create Redux action types
 export const GET_POSTS &#x3D; &#x27;GET_POSTS&#x27;
 export const GET_POSTS_SUCCESS &#x3D; &#x27;GET_POSTS_SUCCESS&#x27;
 export const GET_POSTS_FAILURE &#x3D; &#x27;GET_POSTS_FAILURE&#x27;
 Then create action creators, functions that return an action, which consists of the type and an optional payload containing data.
 actions/postsActions.js
 // Create Redux action creators that return an action
 export const getPosts &#x3D; () &#x3D;&gt; ({
   type: GET_POSTS,
 })
 
 export const getPostsSuccess &#x3D; (posts) &#x3D;&gt; ({
   type: GET_POSTS_SUCCESS,
   payload: posts,
 })
 
 export const getPostsFailure &#x3D; () &#x3D;&gt; ({
   type: GET_POSTS_FAILURE,
 })
 Finally, make the asynchronous thunk action that combines all three of the above actions. When called, it will dispatch the initial getPosts() action to inform Redux to prepare for an API call, then in a try/catch, do everything necessary to get the data, and dispatch a success or failure action as necessary.
 actions/postsActions.js
 // Combine them all in an asynchronous thunk
 export function fetchPosts() {
   return async (dispatch) &#x3D;&gt; {
     dispatch(getPosts())
 
     try {
       const response &#x3D; await fetch(&#x27;https://jsonplaceholder.typicode.com/posts&#x27;)
       const data &#x3D; await response.json()
 
       dispatch(getPostsSuccess(data))
     } catch (error) {
       dispatch(getPostsFailure())
     }
   }
 }
 Great, we&#x27;re all done with creating actions now! All that&#x27;s left to do is tell the reducer what to do with the state on each action.
 Responding to actions
 Back at our post reducer, we have a switch that isn&#x27;t doing anything yet.
 reducers/postsReducer.js
 export default function postsReducer(state &#x3D; initialState, action) {
   switch (action.type) {
     default:
       return state
   }
 }
 Now that we have actions, we can bring them in from the postsActions page.
 // Import all actions
 import * as actions from &#x27;../actions/postsActions&#x27;
 For each action, we&#x27;ll make a case, that returns the entire state plus whatever change we&#x27;re making to it. For GET_POSTS, for example, all we want to do is tell the app to set loading to true since we&#x27;ll be making an API call.
 case actions.GET_POSTS:
   return { ...state, loading: true }
 
 GET_POSTS - begin loading
 GET_POSTS_SUCCESS - the app has posts, no errors, and should stop loading
 GET_POSTS_FAILURE - the app has errors and should stop loading
 
 Here&#x27;s the whole reducer.
 reducers/postsReducer.js
 import * as actions from &#x27;../actions/postsActions&#x27;
 
 export const initialState &#x3D; {
   posts: [],
   loading: false,
   hasErrors: false,
 }
 
 export default function postsReducer(state &#x3D; initialState, action) {
   switch (action.type) {
     case actions.GET_POSTS:
       return { ...state, loading: true }
     case actions.GET_POSTS_SUCCESS:
       return { posts: action.payload, loading: false, hasErrors: false }
     case actions.GET_POSTS_FAILURE:
       return { ...state, loading: false, hasErrors: true }
     default:
       return state
   }
 }
 Now our actions and reducers are ready, so all that&#x27;s left to do is connect everything to the React app.
 Connecting Redux to React Components
 Since the demo app I&#x27;ve created uses React Router to have a few routes - a dashboard, a listing of all posts, and an individual posts page, I&#x27;ll bring React Router in now. I&#x27;ll just bring in the dashboard and all posts listing for this demo.
 App.js
 import React from &#x27;react&#x27;
 import { BrowserRouter as Router, Switch, Route, Redirect } from &#x27;react-router-dom&#x27;
 
 import DashboardPage from &#x27;./pages/DashboardPage&#x27;
 import PostsPage from &#x27;./pages/PostsPage&#x27;
 
 const App &#x3D; () &#x3D;&gt; {
   return (
     &lt;Router&gt;
       &lt;Switch&gt;
         &lt;Route exact path&#x3D;&quot;/&quot; component&#x3D;{DashboardPage} /&gt;
         &lt;Route exact path&#x3D;&quot;/posts&quot; component&#x3D;{PostsPage} /&gt;
         &lt;Redirect to&#x3D;&quot;/&quot; /&gt;
       &lt;/Switch&gt;
     &lt;/Router&gt;
   )
 }
 
 export default App
 We can create the dashboard page, which is just a regular React component.
 pages/DashboardPage.js
 import React from &#x27;react&#x27;
 import { Link } from &#x27;react-router-dom&#x27;
 
 const DashboardPage &#x3D; () &#x3D;&gt; (
   &lt;section&gt;
     &lt;h1&gt;Dashboard&lt;/h1&gt;
     &lt;p&gt;This is the dashboard.&lt;/p&gt;
 
     &lt;Link to&#x3D;&quot;/posts&quot; className&#x3D;&quot;button&quot;&gt;
       View Posts
     &lt;/Link&gt;
   &lt;/section&gt;
 )
 
 export default DashboardPage
 For each post, let&#x27;s make a Post component that will display the title and an excerpt of the text of the article. Make a Post.js in the components subdirectory.
 components/Post.js
 import React from &#x27;react&#x27;
 
 export const Post &#x3D; ({ post }) &#x3D;&gt; (
   &lt;article className&#x3D;&quot;post-excerpt&quot;&gt;
     &lt;h2&gt;{post.title}&lt;/h2&gt;
     &lt;p&gt;{post.body.substring(0, 100)}&lt;/p&gt;
   &lt;/article&gt;
 )
 
 Components that do not connect to Redux are still important and useful for smaller, reusable areas, such as this Post component.
 
 Now the interesting part comes in for the posts page - bringing Redux into React. To do this we&#x27;ll use connect from react-redux. First, we&#x27;ll just make a regular component for the page.
 pages/PostsPage.js
 import React from &#x27;react&#x27;
 
 const PostsPage &#x3D; () &#x3D;&gt; {
   return (
     &lt;section&gt;
       &lt;h1&gt;Posts&lt;/h1&gt;
     &lt;/section&gt;
   )
 }
 
 export default PostsPage
 Then we&#x27;ll bring in connect. The connect function is a higher-order function that connects the Redux store to a React component. We&#x27;ll pass a parameter called mapStateToProps to connect. This aptly named function will take any state from the Redux store and pass it to the props of the React component. We&#x27;ll bring in loading, posts, and hasErrors from the Redux postsReducer.
 pages/PostsPage.js
 import React from &#x27;react&#x27;
 import { connect } from &#x27;react-redux&#x27;
 // Redux state is now in the props of the component
 const PostsPage &#x3D; ({ loading, posts, hasErrors }) &#x3D;&gt; {
   return (
     &lt;section&gt;
       &lt;h1&gt;Posts&lt;/h1&gt;
     &lt;/section&gt;
   )
 }
 
 // Map Redux state to React component propsconst mapStateToProps &#x3D; (state) &#x3D;&gt; ({  loading: state.posts.loading,  posts: state.posts.posts,  hasErrors: state.posts.hasErrors,})// Connect Redux to Reactexport default connect(mapStateToProps)(PostsPage)
 
 Since this component uses state from the same reducer, we could also write state &#x3D;&gt; state.posts. However, learning how to write it the long way is useful to know in case you need to bring multiple reducers into the same component.
 
 Finally, we&#x27;ll bring in the asynchronous fetchPosts from the actions, which is the action that combines the whole lifecycle of fetching all posts into one. Using useEffect from React, we&#x27;ll dispatch fetchPosts when the component mounts. dispatch will automatically be available on a connected component.
 pages/PostsPage.js
 import React, { useEffect } from &#x27;react&#x27;import { connect } from &#x27;react-redux&#x27;
 
 // Bring in the asynchronous fetchPosts actionimport { fetchPosts } from &#x27;../actions/postsActions&#x27;
 const PostsPage &#x3D; ({ dispatch, loading, posts, hasErrors }) &#x3D;&gt; {  useEffect(() &#x3D;&gt; {    dispatch(fetchPosts())  }, [dispatch])
   return (
     &lt;section&gt;
       &lt;h1&gt;Posts&lt;/h1&gt;
     &lt;/section&gt;
   )
 }
 
 const mapStateToProps &#x3D; (state) &#x3D;&gt; ({
   loading: state.posts.loading,
   posts: state.posts.posts,
   hasErrors: state.posts.hasErrors,
 })
 
 export default connect(mapStateToProps)(PostsPage)
 All that&#x27;s left to do at this point is display all three possible states of the page - whether it&#x27;s loading, has an error, or successfully retrieved the posts from the API.
 pages/PostsPage.js
 import React, { useEffect } from &#x27;react&#x27;
 import { connect } from &#x27;react-redux&#x27;
 
 import { fetchPosts } from &#x27;../actions/postsActions&#x27;
 import { Post } from &#x27;../components/Post&#x27;
 const PostsPage &#x3D; ({ dispatch, loading, posts, hasErrors }) &#x3D;&gt; {
   useEffect(() &#x3D;&gt; {
     dispatch(fetchPosts())
   }, [dispatch])
 
   // Show loading, error, or success state  const renderPosts &#x3D; () &#x3D;&gt; {    if (loading) return &lt;p&gt;Loading posts...&lt;/p&gt;    if (hasErrors) return &lt;p&gt;Unable to display posts.&lt;/p&gt;    return posts.map((post) &#x3D;&gt; &lt;Post key&#x3D;{post.id} post&#x3D;{post} /&gt;)  }
   return (
     &lt;section&gt;
       &lt;h1&gt;Posts&lt;/h1&gt;
       {renderPosts()}    &lt;/section&gt;
   )
 }
 
 const mapStateToProps &#x3D; (state) &#x3D;&gt; ({
   loading: state.posts.loading,
   posts: state.posts.posts,
   hasErrors: state.posts.hasErrors,
 })
 
 export default connect(mapStateToProps)(PostsPage)
 And that&#x27;s all - we now have a connected component, and are bringing in data from an API to our Redux store. Using Redux DevTools, we can see each action as it happens, and the changes (diff) after each state change.
 
       
     
   
   
     
 The End
 This is where the tutorial for creating an application with plain Redux ends. If you look at the source code of the demo application, you&#x27;ll see a lot has been added - a reducer and actions for a single post, and for comments.
 I would recommend completing your project so that it matches the demo app. There are no new concepts to be learned, but you will create two more reducers and actions, and see how to bring two states into one component for the single post page, which brings in one post as well as comments for that post.
 Redux Toolkit
 There is one more thing I want to cover - Redux Toolkit. Redux Toolkit, or RTK, is a newer and easier official way to use Redux. You may notice that Redux has a lot of boilerplate for setup and requires many more folders and files than plain React would. Some patterns have emerged to attempt to mitigate all that, such as Redux ducks pattern, but we can simplify it even more.
 View the source of the demo Redux Toolkit application, which is the same application we just created with Redux, but using RTK. It is much simpler, with a drastic reduction in lines of code for all the same functionality.
 Using RTK just requires one dependency, @reduxjs/toolkit.
 npm i @reduxjs/toolkit
 And no longer requires you to install the redux-thunk or redux-devtools-extension dependencies.
 Advantages to Redux Toolkit
 The main advantages to using RTK are:
 
 Easier to set up (less dependencies)
 Reduction of boilerplate code (one slice vs. many files for actions and reducers)
 Sensible defaults (Redux Thunk, Redux DevTools built-in)
 The ability to use direct state mutation, since RTK uses immer under the hood. This means you no longer need to return { ...state } with every reducer.
 
 Store
 Since Redux Toolkit comes with a lot built-in already, like Redux DevTools and Redux Thunk, we no longer have to bring them into the index.js file. Now we only need configureStore, instead of createStore.
 index.js
 import React from &#x27;react&#x27;
 import { render } from &#x27;react-dom&#x27;
 import { configureStore } from &#x27;@reduxjs/toolkit&#x27;import { Provider } from &#x27;react-redux&#x27;
 
 import App from &#x27;./App&#x27;
 import rootReducer from &#x27;./slices&#x27;
 
 import &#x27;./index.css&#x27;
 
 const store &#x3D; configureStore({ reducer: rootReducer })
 render(
   &lt;Provider store&#x3D;{store}&gt;
     &lt;App /&gt;
   &lt;/Provider&gt;,
   document.getElementById(&#x27;root&#x27;)
 )
 Slices
 Instead of dealing with reducers, actions, and all as separate files and individually creating all those action types, RTK gives us the concept of slices. A slice automatically generates reducers, action types, and action creators. As such, you&#x27;ll only have to create one folder - slices.
 initialState will look the same.
 slices/posts.js
 import { createSlice } from &#x27;@reduxjs/toolkit&#x27;
 
 export const initialState &#x3D; {
   loading: false,
   hasErrors: false,
   posts: [],
 }
 The names of the reducers in the slice will also be the same - getPosts, getPostsSuccess, and getPostsFailure. We&#x27;ll make all the same changes, but note that we&#x27;re no longer returning the entire state - we&#x27;re just mutating state. It&#x27;s still immutable under the hood, but this approach may be easier and faster for some. If preferred, you can still return the whole state as an object.
 slices/posts.js
 // A slice for posts with our three reducers
 const postsSlice &#x3D; createSlice({
   name: &#x27;posts&#x27;,
   initialState,
   reducers: {
     getPosts: (state) &#x3D;&gt; {
       state.loading &#x3D; true
     },
     getPostsSuccess: (state, { payload }) &#x3D;&gt; {
       state.posts &#x3D; payload
       state.loading &#x3D; false
       state.hasErrors &#x3D; false
     },
     getPostsFailure: (state) &#x3D;&gt; {
       state.loading &#x3D; false
       state.hasErrors &#x3D; true
     },
   },
 })
 The actions that get generated are the same, we just don&#x27;t have to write them out individually anymore. From the same file, we can export all the actions, the reducer, the asynchronous thunk, and one new thing - a selector, which we&#x27;ll use to access any of the state from a React component instead of using connect.
 slices/posts.js
 // Three actions generated from the slice
 export const { getPosts, getPostsSuccess, getPostsFailure } &#x3D; postsSlice.actions
 
 // A selector
 export const postsSelector &#x3D; (state) &#x3D;&gt; state.posts
 
 // The reducer
 export default postsSlice.reducer
 
 // Asynchronous thunk action
 export function fetchPosts() {
   return async (dispatch) &#x3D;&gt; {
     dispatch(getPosts())
 
     try {
       const response &#x3D; await fetch(&#x27;https://jsonplaceholder.typicode.com/posts&#x27;)
       const data &#x3D; await response.json()
 
       dispatch(getPostsSuccess(data))
     } catch (error) {
       dispatch(getPostsFailure())
     }
   }
 }
 Selecting Redux state in a React component
 The traditional approach, as we just learned, is to use mapStateToProps with the connect() function. This is still common in codebases and therefore worth learning. You can still use this approach with RTK, but the newer, React Hooks way of going about it is to use useDispatch and useSelector from react-redux. This approach requires less code overall as well.
 As you can see in the updated PostsPage.js file below, the Redux state is no longer available as props on the connected component, but from the selector we exported in the slice.
 pages/PostsPage.js
 import React, { useEffect } from &#x27;react&#x27;
 import { useDispatch, useSelector } from &#x27;react-redux&#x27;
 import { fetchPosts, postsSelector } from &#x27;../slices/posts&#x27;
 import { Post } from &#x27;../components/Post&#x27;
 
 const PostsPage &#x3D; () &#x3D;&gt; {
   const dispatch &#x3D; useDispatch()  const { posts, loading, hasErrors } &#x3D; useSelector(postsSelector)
   useEffect(() &#x3D;&gt; {
     dispatch(fetchPosts())
   }, [dispatch])
 
   const renderPosts &#x3D; () &#x3D;&gt; {
     if (loading) return &lt;p&gt;Loading posts...&lt;/p&gt;
     if (hasErrors) return &lt;p&gt;Unable to display posts.&lt;/p&gt;
 
     return posts.map((post) &#x3D;&gt; &lt;Post key&#x3D;{post.id} post&#x3D;{post} excerpt /&gt;)
   }
 
   return (
     &lt;section&gt;
       &lt;h1&gt;Posts&lt;/h1&gt;
       {renderPosts()}
     &lt;/section&gt;
   )
 }
 
 export default PostsPage
 Now we have the same app as before with a few updates from Redux Toolkit, and a lot less code to maintain.
 Conclusion
 We did it! If you followed along with me through this whole tutorial, you should have a really good feel for Redux now, both the old-fashioned way and using Redux Toolkit to simplify things. To summarize, Redux allows us to easily manage global state in a React application. We can access and update the state from anywhere, and easily debug the entire state of an application with Redux Devtools.
 You can place most of the state of your application in Redux, but certain areas of an app, such as forms as they are being updated, still make sense to keep in the React component state itself until the form is officially submitted.
 I hope you enjoyed this article! It was a lot of work to put together two complete demo applications and run through the whole thing here, and the article ran pretty long, but hopefully this is your one-stop shop for learning all beginner and intermediate Redux concepts. Please let me know what you think and share the article if it helped you out, and donations are always welcome!</content>
     </entry>
     <entry>
       <title>Understanding Generators in JavaScript</title>
         <link href="https://www.taniarascia.com/understanding-generators-in-javascript/"/>
       <updated>2020-02-29T00:00:00.000Z</updated>
       <content type="text">This article was originally written for DigitalOcean.
 In ECMAScript 2015, generators were introduced to the JavaScript language. A generator is a process that can be paused and resumed and can yield multiple values. A generator in JavaScript consists of a generator function, which returns an iterable Generator object.
 Generators are a powerful addition to JavaScript. They can maintain state, providing an efficient way to make iterators, and are capable of dealing with infinite data streams, which can be used to implement infinite scroll on the frontend of a web application, to operate on sound wave data, and more. Additionally, when used with Promises, generators can mimic the async/await functionality, which allows us to deal with asynchronous code in a more straightforward and readable manner. Although async/await is a more prevalent way to deal with common, simple asynchronous use cases, like fetching data from an API, generators have more advanced features that make learning how to use them worthwhile.
 In this article, we&#x27;ll cover how to create generator functions, how to iterate over Generator objects, the difference between yield and return inside a generator, and other aspects of working with generators.
 Generator Functions
 A generator function is a function that returns a Generator object, and is defined by the function keyword followed by an asterisk (*), as shown in the following:
 // Generator function declaration
 function* generatorFunction() {}
 Occasionally, you will see the asterisk next to the function name, as opposed to the function keyword, such as function *generatorFunction(). This works the same, but function* is a more widely accepted syntax.
 Generator functions can also be defined in an expression, like regular functions:
 // Generator function expression
 const generatorFunction &#x3D; function* () {}
 Generators can even be the methods of an object or class:
 // Generator as the method of an object
 const generatorObj &#x3D; {
   *generatorMethod() {},
 }
 
 // Generator as the method of a class
 class GeneratorClass {
   *generatorMethod() {}
 }
 The examples throughout this article will use the generator function declaration syntax.
 
 Note: Unlike regular functions, generators cannot be constructed with the new keyword, nor can they be used in conjunction with arrow functions.
 
 Now that you know how to declare generator functions, lets look at the iterable Generator objects that they return.
 Generator Objects
 Traditionally, functions in JavaScript run to completion, and calling a function will return a value when it arrives at the return keyword. If the return keyword is omitted, a function will implicitly return undefined.
 In the following code, for example, we declare a sum() function that returns a value that is the sum of two integer arguments:
 // A regular function that sums two values
 function sum(a, b) {
   return a + b
 }
 Calling the function returns a value that is the sum of the arguments:
 const value &#x3D; sum(5, 6) // 11
 A generator function, however, does not return a value immediately, and instead returns an iterable Generator object. In the following example, we declare a function and give it a single return value, like a standard function:
 // Declare a generator function with a single return value
 function* generatorFunction() {
   return &#x27;Hello, Generator!&#x27;
 }
 When we invoke the generator function, it will return the Generator object, which we can assign to a variable:
 // Assign the Generator object to generator
 const generator &#x3D; generatorFunction()
 If this were a regular function, we would expect generator to give us the string returned in the function. However, what we actually get is an object in a suspended state. Calling generator will therefore give output similar to the following:
 generatorFunction {&lt;suspended&gt;}
   __proto__: Generator
   [[GeneratorLocation]]: VM272:1
   [[GeneratorStatus]]: &quot;suspended&quot;
   [[GeneratorFunction]]: ƒ* generatorFunction()
   [[GeneratorReceiver]]: Window
   [[Scopes]]: Scopes[3]
 The Generator object returned by the function is an iterator. An iterator is an object that has a next() method available, which is used for iterating through a sequence of values. The next() method returns an object with value and done properties. value represent the returned value, and done indicates whether the iterator has run through all its values or not.
 Knowing this, let&#x27;s call next() on our generator and get the current value and state of the iterator:
 // Call the next method on the Generator object
 generator.next()
 This will give the following output:
 {value: &quot;Hello, Generator!&quot;, done: true}
 The value returned from calling next() is Hello, Generator!, and the state of done is true, because this value came from a return that closed out the iterator. Since the iterator is done, the generator function&#x27;s status will change from suspended to closed. Calling generator again will give the following:
 generatorFunction {&lt;closed&gt;}
 As of right now, we&#x27;ve only demonstrated how a generator function can be a more complex way to get the return value of a function. But generator functions also have unique features that distinguish them from normal functions. In the next section, we&#x27;ll learn about the yield operator and see how a generator can pause and resume execution.
 yield Operators
 Generators introduce a new keyword to JavaScript: yield. yield can pause a generator function and return the value that follows yield, providing a lightweight way to iterate through values.
 In this example, we&#x27;ll pause the generator function three times with different values, and return a value at the end. Then we will assign our Generator object to the generator variable.
 // Create a generator function with multiple yields
 function* generatorFunction() {
   yield &#x27;Neo&#x27;
   yield &#x27;Morpheus&#x27;
   yield &#x27;Trinity&#x27;
 
   return &#x27;The Oracle&#x27;
 }
 
 const generator &#x3D; generatorFunction()
 Now, when we call next() on the generator function, it will pause every time it encounters yield. done will be set to false after each yield, indicating that the generator has not finished. Once it encounters a return, or there are no more yields encountered in the function, done will flip to true, and the generator will be finished.
 Use the next() method four times in a row:
 // Call next four times
 generator.next()
 generator.next()
 generator.next()
 generator.next()
 These will give the following four lines of output in order:
 {value: &quot;Neo&quot;, done: false}
 {value: &quot;Morpheus&quot;, done: false}
 {value: &quot;Trinity&quot;, done: false}
 {value: &quot;The Oracle&quot;, done: true}
 Note that a generator does not require a return; if omitted, the last iteration will return {value: undefined, done: true}, as will any subsequent calls to next() after a generator has completed.
 Iterating Over a Generator
 Using the next() method, we manually iterated through the Generator object, receiving all the value and done properties of the full object. However, just like Array, Map, and Set, a Generator follows the iteration protocol, and can be iterated through with for...of:
 // Iterate over Generator object
 for (const value of generator) {
   console.log(value)
 }
 This will return the following:
 Neo
 Morpheus
 Trinity
 The spread operator can also be used to assign the values of a Generator to an array.
 // Create an array from the values of a Generator object
 const values &#x3D; [...generator]
 
 console.log(values)
 This will give the following array:
 (3) [&quot;Neo&quot;, &quot;Morpheus&quot;, &quot;Trinity&quot;]
 Both spread and for...of will not factor the return into the values (in this case, it would have been &#x27;The Oracle&#x27;).
 
 Note: While both of these methods are effective for working with finite generators, if a generator is dealing with an infinite data stream, it won&#x27;t be possible to use spread or for...of directly without creating an infinite loop.
 
 Closing a Generator
 As we&#x27;ve seen, a generator can have its done property set to true and its status set to closed by iterating through all its values. There are two additional ways to immediately cancel a generator: with the return() method, and with the throw() method.
 With return(), the generator can be terminated at any point, just as if a return statement had been in the function body. You can pass an argument into return(), or leave it blank for an undefined value.
 To demonstrate return(), we&#x27;ll create a generator with a few yield values but no return in the function definition:
 function* generatorFunction() {
   yield &#x27;Neo&#x27;
   yield &#x27;Morpheus&#x27;
   yield &#x27;Trinity&#x27;
 }
 
 const generator &#x3D; generatorFunction()
 The first next() will give us &#x27;Neo&#x27;, with done set to false. If we invoke a return() method on the Generator object right after that, we&#x27;ll now get the passed value and done set to true. Any additional call to next() will give the default completed generator response with an undefined value.
 To demonstrate this, run the following three methods on generator:
 generator.next()
 generator.return(&#x27;There is no spoon!&#x27;)
 generator.next()
 This will give the three following results:
 {value: &quot;Neo&quot;, done: false}
 {value: &quot;There is no spoon!&quot;, done: true}
 {value: undefined, done: true}
 The return() method forced the Generator object to complete and to ignore any other yield keywords. This is particularly useful in asynchronous programming when you need to make functions cancelable, such as interrupting a web request when a user wants to perform a different action, as it is not possible to directly cancel a Promise.
 If the body of a generator function has a way to catch and deal with errors, you can use the throw() method to throw an error into the generator. This starts up the generator, throws the error in, and terminates the generator.
 To demonstrate this, we will put a try...catch inside the generator function body and log an error if one is found:
 // Define a generator function
 function* generatorFunction() {
   try {
     yield &#x27;Neo&#x27;
     yield &#x27;Morpheus&#x27;
   } catch (error) {
     console.log(error)
   }
 }
 
 // Invoke the generator and throw an error
 const generator &#x3D; generatorFunction()
 Now, we will run the next() method, followed by throw():
 generator.next()
 generator.throw(new Error(&#x27;Agent Smith!&#x27;))
 This will give the following output:
 {value: &quot;Neo&quot;, done: false}
 Error: Agent Smith!
 {value: undefined, done: true}
 Using throw(), we injected an error into the generator, which was caught by the try...catch and logged to the console.
 Generator Object Methods and States
 The following table shows a list of methods that can be used on Generator objects:
 
 
 
 Method
 Description
 
 
 
 
 next()
 Returns the next value in a generator
 
 
 return()
 Returns a value in a generator and finishes the generator
 
 
 throw()
 Throws an error and finishes the generator
 
 
 
 The next table lists the possible states of a Generator object:
 
 
 
 Status
 Description
 
 
 
 
 suspended
 Generator has halted execution but has not terminated
 
 
 closed
 Generator has terminated by either encountering an error, returning, or iterating through all values
 
 
 
 yield Delegation
 In addition to the regular yield operator, generators can also use the yield* expression to delegate further values to another generator. When the yield* is encountered within a generator, it will go inside the delegated generator and begin iterating through all the yields until that generator is closed. This can be used to separate different generator functions to semantically organize your code, while still having all their yields be iterable in the right order.
 To demonstrate, we can create two generator functions, one of which will yield* operate on the other:
 // Generator function that will be delegated to
 function* delegate() {
   yield 3
   yield 4
 }
 
 // Outer generator function
 function* begin() {
   yield 1
   yield 2
   yield* delegate()
 }
 Next, let&#x27;s iterate through the begin() generator function:
 // Iterate through the outer generator
 const generator &#x3D; begin()
 
 for (const value of generator) {
   console.log(value)
 }
 This will give the following values in the order they are generated:
 1
 2
 3
 4
 The outer generator yielded the values 1 and 2, then delegated to the other generator with yield*, which returned 3 and 4.
 yield* can also delegate to any object that is iterable, such as an Array or a Map. Yield delegation can be helpful in organizing code, since any function within a generator that wanted to use yield would also have to be a generator.
 Infinite Data Streams
 One of the useful aspects of generators is the ability to work with infinite data streams and collections. This can be demonstrated by creating an infinite loop inside a generator function that increments a number by one.
 In the following code block, we define this generator function and then initiate the generator:
 // Define a generator function that increments by one
 function* incrementer() {
   let i &#x3D; 0
 
   while (true) {
     yield i++
   }
 }
 
 // Initiate the generator
 const counter &#x3D; incrementer()
 Now, iterate through the values using next():
 // Iterate through the values
 counter.next()
 counter.next()
 counter.next()
 counter.next()
 This will give the following output:
 {value: 0, done: false}
 {value: 1, done: false}
 {value: 2, done: false}
 {value: 3, done: false}
 The function returns successive values in the infinite loop while the done property remains false, ensuring that it will not finish.
 With generators, you don&#x27;t have to worry about creating an infinite loop, because you can halt and resume execution at will. However, you still have to have caution with how you invoke the generator. If you use spread or for...of on an infinite data stream, you will still be iterating over an infinite loop all at once, which will cause the environment to crash.
 For a more complex example of an infinite data stream, we can create a Fibonacci generator function. The Fibonacci sequence, which continuously adds the two previous values together, can be written using an infinite loop within a generator as follows:
 // Create a fibonacci generator function
 function* fibonacci() {
   let prev &#x3D; 0
   let next &#x3D; 1
 
   yield prev
   yield next
 
   // Add previous and next values and yield them forever
   while (true) {
     const newVal &#x3D; next + prev
 
     yield newVal
 
     prev &#x3D; next
     next &#x3D; newVal
   }
 }
 To test this out, we can loop through a finite number and print the Fibonacci sequence to the console.
 // Print the first 10 values of fibonacci
 const fib &#x3D; fibonacci()
 
 for (let i &#x3D; 0; i &lt; 10; i++) {
   console.log(fib.next().value)
 }
 This will give the following:
 0
 1
 1
 2
 3
 5
 8
 13
 21
 34
 The ability to work with infinite data sets is one part of what makes generators so powerful. This can be useful for examples like implementing infinite scroll on the frontend of a web application, or operating on sound wave data.
 Passing Values in Generators
 Throughout this article, we&#x27;ve used generators as iterators, and we&#x27;ve yielded values in each iteration. In addition to producing values, generators can also consume values from next(). In this case, yield will contain a value.
 It&#x27;s important to note that the first next() that is called will not pass a value, but will only start the generator. To demonstrate this, we can log the value of yield and call next() a few times with some values.
 function* generatorFunction() {
   console.log(yield)
   console.log(yield)
 
   return &#x27;The end&#x27;
 }
 
 const generator &#x3D; generatorFunction()
 
 generator.next()
 generator.next(100)
 generator.next(200)
 This will give the following output:
 100
 200
 {value: &quot;The end&quot;, done: true}
 It is also possible to seed the generator with an initial value. In the following example, we&#x27;ll make a for loop and pass each value into the next() method, but pass an argument to the inital function as well:
 function* generatorFunction(value) {
   while (true) {
     value &#x3D; yield value * 10
   }
 }
 
 // Initiate a generator and seed it with an initial value
 const generator &#x3D; generatorFunction(0)
 
 for (let i &#x3D; 0; i &lt; 5; i++) {
   console.log(generator.next(i).value)
 }
 We&#x27;ll retrieve the value from next() and yield a new value to the next iteration, which is the previous value times ten. This will give the following:
 0
 10
 20
 30
 40
 Another way to deal with starting up a generator is to wrap the generator in a function that will always call next() once before doing anything else.
 async/await with Generators
 An asynchronous function is a type of function available in ES6+ JavaScript that makes working with asynchronous data simpler and easier to understand by making it appear synchronous. Generators have a more extensive array of capabilities than asynchronous functions, but are capable of replicating similar behavior. Implementing asynchronous programming in this way can increase the flexibility of your code.
 In this section, we will demonstrate an example of reproducing async/await with generators.
 Let&#x27;s build an asynchronous function that uses the Fetch API to get data from the JSONPlaceholder API (which provides example JSON data for testing purposes) and logs the response to the console.
 Start out by defining an asynchronous function called getUsers that fetches data from the API and returns an array of objects, then call getUsers:
 const getUsers &#x3D; async function () {
   const response &#x3D; await fetch(&#x27;https://jsonplaceholder.typicode.com/users&#x27;)
   const json &#x3D; await response.json()
 
   return json
 }
 
 // Call the getUsers function and log the response
 getUsers().then((response) &#x3D;&gt; console.log(response))
 This will give the following JSON data:
 
 [ {id: 1, name: &quot;Leanne Graham&quot; ...},
   {id: 2, name: &quot;Ervin Howell&quot; ...},
   {id: 3, name&quot;: &quot;Clementine Bauch&quot; ...},
   {id: 4, name: &quot;Patricia Lebsack&quot;...},
   {id: 5, name: &quot;Chelsey Dietrich&quot;...},
   {id: 6, name: &quot;Mrs. Dennis Schulist&quot;...},
   {id: 7, name: &quot;Kurtis Weissnat&quot;...},
   {id: 8, name: &quot;Nicholas Runolfsdottir V&quot;...},
   {id: 9, name: &quot;Glenna Reichert&quot;...},
   {id: 10, name: &quot;Clementina DuBuque&quot;...}]
 Using generators, we can create something almost identical that does not use the async/await keywords. Instead, it will use a new function we create, and yield values instead of await promises.
 In the following code block, we define a function called getUsers that uses our new asyncAlt function (which we will write later on) to mimic async/await.
 const getUsers &#x3D; asyncAlt(function* () {
   const response &#x3D; yield fetch(&#x27;https://jsonplaceholder.typicode.com/users&#x27;)
   const json &#x3D; yield response.json()
 
   return json
 })
 
 // Invoking the function
 getUsers().then((response) &#x3D;&gt; console.log(response))
 As we can see, it looks almost identical to the async/await implementation, except that there is a generator function being passed in that yields values.
 Now we can create an asyncAlt function that resembles an asynchronous function. asyncAlt has a generator function as a parameter, which is our function that yields the promises that fetch returns. asyncAlt returns a function itself, and resolves every promise it finds until the last one:
 // Define a function named asyncAlt that takes a generator function as an argument
 function asyncAlt(generatorFunction) {
   // Return a function
   return function () {
     // Create and assign the generator object
     const generator &#x3D; generatorFunction()
 
     // Define a function that accepts the next iteration of the generator
     function resolve(next) {
       // If the generator is closed and there are no more values to yield,
       // resolve the last value
       if (next.done) {
         return Promise.resolve(next.value)
       }
 
       // If there are still values to yield, they are promises and
       // must be resolved.
       return Promise.resolve(next.value).then((response) &#x3D;&gt; {
         return resolve(generator.next(response))
       })
     }
 
     // Begin resolving promises
     return resolve(generator.next())
   }
 }
 This will give the same output as the async/await version:
 [ {id: 1, name: &quot;Leanne Graham&quot; ...},
   {id: 2, name: &quot;Ervin Howell&quot; ...},
   {id: 3, name&quot;: &quot;Clementine Bauch&quot; ...},
   {id: 4, name: &quot;Patricia Lebsack&quot;...},
   {id: 5, name: &quot;Chelsey Dietrich&quot;...},
   {id: 6, name: &quot;Mrs. Dennis Schulist&quot;...},
   {id: 7, name: &quot;Kurtis Weissnat&quot;...},
   {id: 8, name: &quot;Nicholas Runolfsdottir V&quot;...},
   {id: 9, name: &quot;Glenna Reichert&quot;...},
   {id: 10, name: &quot;Clementina DuBuque&quot;...}]
 Note that this implementation is for demonstrating how generators can be used in place of async/await, and is not a production-ready design. It does not have error handling set up, nor does it have the ability to pass parameters into the yielded values. Though this method can add flexibility to your code, often async/await will be a better choice, since it abstracts implementation details away and lets you focus on writing productive code.
 Conclusion
 Generators are processes that can halt and resume execution. They are a powerful, versatile feature of JavaScript, although they are not commonly used. In this tutorial, we learned about generator functions and generator objects, methods available to generators, the yield and yield* operators, and using generators with finite and infinite data sets. We also explored one way to implement asynchronous code without nested callbacks or long promise chains.
 If you would like to learn more about JavaScript syntax, take a look at our Understanding This, Bind, Call, and Apply in JavaScript and Understanding Map and Set Objects in JavaScript tutorials.</content>
     </entry>
     <entry>
       <title>macOS Monterey: Setting up a Mac for Development</title>
         <link href="https://www.taniarascia.com/setting-up-a-brand-new-mac-for-development/"/>
       <updated>2020-02-17T00:00:00.000Z</updated>
       <content type="text">Last Updated: 1/28/2022
 I have to set up a MacBook Pro fairly often - when starting a new job and when buying a new personal computer. I created this article back in 2015 when I got my first Mac and have been updating it ever since with whatever I need as my job evolves. I&#x27;m primarily a full-stack web developer, so most of my needs will revolve around JavaScript/Node.js.
 Getting Started
 The setup assistant will launch once you turn the computer on. Enter your language, time zone, Apple ID, and so on. The first thing you should do is update macOS to get the latest security updates and patches.
 
 Install apps via Homebrew
 Shell setup with zsh
 Set up Node via nvm
 Set up git config
 Set up SSH keys and config
 macOS settings
 Application settings
 
 Homebrew
 Install the Homebrew package manager. This will allow you to install almost any app from the command line.
 /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot;
 Make sure everything is up to date.
 brew update
 
 Note: On the M1 install, there will be a few errors at the end, you&#x27;ll have to run the commands to add to path, then run brew doctor and run the commands listed.
 
 Install Apps
 Here are a few shell programs I always use:
 
 
 
 Shell Program
 Purpose
 
 
 
 
 git
 Version control
 
 
 exa
 ls alternative
 
 
 bat
 cat alternative
 
 
 fzf
 Search
 
 
 tldr
 man alternative
 
 
 
 Here are some the applications I always install (cask flag in Homebrew):
 
 Do not install Node.js through Homebrew. Use nvm (below).
 
 
 
 
 Application
 Purpose
 
 
 
 
 Visual Studio Code
 text editor
 
 
 Google Chrome
 web browser
 
 
 Firefox
 web browser
 
 
 Rectangle
 window resizing
 
 
 iTerm2
 terminal
 
 
 Docker
 development
 
 
 Discord
 communication
 
 
 Slack
 communication
 
 
 Spotify
 music
 
 
 Postgres
 database
 
 
 Postico
 database UI
 
 
 Postman
 API tool
 
 
 Bear
 Notes
 
 
 Todoist
 Todos
 
 
 
 App installation
 ## Shell Programs
 brew install \
   git \
   bat \
   exa \
   tldr \
   fzf &amp;&amp;
 
 # GUI programs
 brew install --cask \
   visual-studio-code \
   google-chrome \
   firefox \
   rectangle \
   iterm2 \
   docker \
   discord \
   slack \
   spotify \
   postgres \
   postico \
   bear \
   todoist \
 Shell
 Catalina comes with zsh as the default shell. Install Oh My Zsh for sensible defaults.
 sh -c &quot;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;
 I add a few alises.
 alias cat&#x3D;&quot;bat&quot;
 alias ls&#x3D;&quot;exa&quot;
 Node.js
 Use Node Version Manager (nvm) to install Node.js. This allows you to easily switch between Node versions, which is essential.
 curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
 Install
 Install the latest version.
 nvm install node
 Restart terminal and run the final command.
 nvm use node
 Confirm that you are using the latest version of Node and npm.
 node -v &amp;&amp; npm -v
 Update
 For later, here&#x27;s how to update nvm.
 nvm install node --reinstall-packages-from&#x3D;node
 Change version
 Here&#x27;s how to switch to another version and use it.
 nvm install xx.xx
 nvm use xx.xx
 And to set the default:
 nvm alias default xx.xx
 Git
 The first thing you should do with Git is set your global configuration.
 touch ~/.gitconfig
 Input your config and create some aliases.
 .gitconfig
 [user]
   name   &#x3D; Firstname Lastname
   email  &#x3D; you@example.com
 [github]
   user   &#x3D; username
 [alias]
   a      &#x3D; add
   ca     &#x3D; commit -a
   cam    &#x3D; commit -am
   cm     &#x3D; commit -m
   s      &#x3D; status
   p      &#x3D; push
   pom    &#x3D; push origin master
   puom   &#x3D; pull origin master
   cob    &#x3D; checkout -b
   co     &#x3D; checkout
   fp     &#x3D; fetch --prune --all
   l      &#x3D; log --oneline --decorate --graph
   lall   &#x3D; log --oneline --decorate --graph --all
   ls     &#x3D; log --oneline --decorate --graph --stat
   lt     &#x3D; log --graph --decorate --pretty&#x3D;format:&#x27;%C(yellow)%h%Creset%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)%an%Creset&#x27;
 With the above aliases, I can run git s instead of git status, for example. The less I have to type, the happier I am.
 SSH
 Generate SSH key
 You can generate an SSH key to distribute.
 ssh-keygen -t ed25519 -C &quot;your_email@example.com&quot;
 Start ssh-agent.
 eval &quot;$(ssh-agent -s)&quot;
 Add key.
 ssh-add -K ~/.ssh/id_rsa
 Config
 Simplify the process of SSHing into other boxes with your SSH config file. Create ~/.ssh/config if it does not already exist.
 Add the following contents, changing the variables for any hosts that you connect to. Using the below will be the same as running ssh -i ~/.ssh/key.pem user@example.com.
 ~/.ssh/config
 Host *
   AddKeysToAgent yes
   UseKeychain yes
   IdentityFile ~/.ssh/id_ed25519
 
 Host myssh
   HostName example.com
   User user
   IdentityFile ~/.ssh/key.pem
 Now just run the alias to connect.
 ssh myssh
 macOS Settings
 I don&#x27;t like a lot of the Apple defaults so here are the things I always change.
 To get the Home folder in the finder, press CMD + SHIFT + H and drag the home folder to the sidebar.
 General
 
 Set Dark mode
 Make Google Chrome default browser
 
 Dock
 
 Automatically hide and show Dock
 Show indicators for open applications
 
 Keyboard
 
 Key Repeat -&gt; Fast
 Delay Until Repeat -&gt; Short
 Disable &quot;Correct spelling automatically&quot;
 Disable &quot;Capitalize words automatically&quot;
 Disable &quot;Add period with double-space&quot;
 Disable &quot;Use smart quotes and dashes&quot;
 
 Security and Privacy
 
 Allow apps downloaded from App Store and identified developers
 Turn FileVault On (makes sure SSD is securely encrypted)
 
 Sharing
 
 Change computer name
 Make sure all file sharing is disabled
 
 Users &amp; Groups
 
 Add &quot;Rectangle&quot; to Login items
 
 Defaults
 A few more commands to change some defaults.
 # Show Library folder
 chflags nohidden ~/Library
 
 # Show hidden files
 defaults write com.apple.finder AppleShowAllFiles YES
 
 # Show path bar
 defaults write com.apple.finder ShowPathbar -bool true
 
 # Show status bar
 defaults write com.apple.finder ShowStatusBar -bool true
 
 # Prevent left and right swipe through history in Chrome
 defaults write com.google.Chrome AppleEnableSwipeNavigateWithScrolls -bool false
 Application Settings
 Chrome
 
 Install uBlock Origin
 Install React DevTools
 Install Redux DevTools
 Install JSONView
 Install Duplicate Tab Shortcut
 Settings -&gt; Set theme to &quot;Dark&quot;
 
 Visual Studio Code
 
 Press CMD + SHIFT + P and click &quot;Install code command in PATH&quot;.
 View Dotfiles for keyboard shortcuts and settings
 Install New Moon Theme
 Install GitLens
 Install Highlight Matching Tag
 Install ESLint
 Install Prettier
 Install Jest
 Install Jest Runner
 
 Rectangle
 
 Full Screen: CMD + SHIFT + &#x27; (prevents messing with other commands)
 Left Half: CMD + OPTION + LEFT
 Right Half: CMD + OPTION + RIGHT
 
 iTerm2
 
 Use ⌥← and ⌥→ to jump forwards / backwards
 
 Change⌥← via &quot;Send Escape Sequence&quot; with b
 Change⌥→ via &quot;Send Escape Sequence&quot; with f
 
 
 
 Conclusion
 That sums it up for my current preferences on setting up a MacBook Pro. I hope it helped speed up your process or gave you ideas for the next time you&#x27;re setting one up.</content>
     </entry>
     <entry>
       <title>WDRL — Edition 281: Progressive Image element, striving for enough, SpiderFoot and the architecture of a web search engine today</title>
         <link href="https://wdrl.info/archive/281"/>
       <updated>2020-02-05T16:00:00.000Z</updated>
       <content type="text">Hey,
 
 There’s so much potential in all of us. There’s so much distraction in our lives today. We tend to continue doing not something different because we have too much on our plates, we are too distracted. We don’t have time to focus on what we really want to do, focus on what we want to change, focus on helping other people or focus on self-care.
 In 2020 I opened a Community Supported Agriculture to provide local, naturally grown and healthy food to people nearby. No need for them to buy it in the supermarket where it was transported many hundred kilometers through the country and isn’t fresh anymore. My action to foster local community, to do something for the better, for the climate, for other people with a real impact.
 I will also take three months of parental leave time for my family (my wife and soon my newborn child) and for myself, to get focus on the things that are in my mind but never find time to think properly about. This means no work, no WDRL, no other developer related stuff during that time so an entirely different lifestyle for a limited time. Let’s see what this brings.
 News
 
 	PHP 7.4 is the latest version but v8 is in the works and Brent shares what’s new in PHP 8: We’ll likely get Union types, a Just in time compiler, a static return type, weak maps, ::class on objects as easier form of writing get_class(), it lets us create DateTime objects from an interface, and some more. What’s changing?  Concatenation precedence, error reporting types, and error silencing via @ suppression.
 
 Generic
 
 	Hugo Giraudel shares the path of how they transformed a very MVP app into a great, modern web app that’s scalable, fast and accessible: Lessons from building N26 for the web.
 	The Architecture of a Large-Scale Web Search Engine, circa 2019. A deep dive into Microservices, Kubernetes and more. Real time indexing with Kafka, Cassandra; Deployments with Ansible, Salt and Terraform. A lot and this is still just a snapshot of a fast changing ecosystem.
 
 UI/UX
 
 	Jonathon Colman analyzed mobile app designs and 36% is the right amount of text in mobile apps. An interesting design journey through countless apps that shows the importance of text in design.
 
 Tooling
 
 	Now this site is incredibly useful for configuring PostgreSQL as it’s a complete config guide written for humans.
 	SpiderFoot is an open source intelligence (OSINT) automation tool. What’s that? A tool that integrates with every data source available and analyses them, making that data easy to navigate in a UI.
 
 Accessibility
 
 	Marcus Herrmann shows us how to build accessible routing in Vue.js.
 
 JavaScript
 
 	This is &lt;progressive-image&gt;,  a custom element that progressively enhances image placeholders once they are in the viewport, made by André Ruffert. It lazy loads, has a dave data option, produces no reflow, and has no dependencies.
 	Philip Walton shares how we can serve way smaller HTML payloads in applications by using Service Workers for them.
 
 CSS
 
 	It’s interesting to see where we started with CSS over twenty years ago and where we are now. I myself built my first website in 2001 with Framesets and inline CSS and HTML 4 style attributes. Quickly after, I learned a bit how to use tables to create layouts (Photoshop could write most of the code, huh!) but gladly this didn’t take long until we had CSS2.1 which allowed us to create way better and more semantic layouts.
 
 Work &amp; Life
 
 	Enough is enough. But what’s enough? Having less stuff (aka “minimalism”) can’t be successful for us long-term, it can only serve as a path to sufficiency and satisfaction. The goal needs to be that we solve enough for us, that we realize and incorporate that we don’t need more, that we have enough.
 	Frank Chimero with a list of things that foster burnout: Our achievement culture, hyperactive comparison, isolation, the lack of work ethics, unnecessary self-improvement thoughts, and a couple more. It’s the first step to identify these points and then take care that not too many in parallel are true for our lives.
 	A new guide &#x27;With Great Tech Comes Great Responsibility&#x27; arrives during a period of backlash on campuses, where tech companies like Amazon and Palantir recruit. It’s by Mozilla and it wants young people to consider ‘Ethical Issues’ before taking jobs in tech. It’s on healthy work places, healthy company and startup culture, working on real problems in the world and not causing any harm with our work we do.
 
 Go beyond…
 
 	How your clothes become microfibre pollution in the sea. We buy plastic fibre clothes, wash them and the plastic is slowly dissolving into microplastic in the washing mashine and the sewage treatment plants can’t filter it so it gets into the rivers which end up in the sea.
 	Paris, France, 2020: The mayor announces that they plan to make the city fully cyclist friendly in the next four years. But what does it mean and how will the city change? It’s a great plan that ensures cycling is )the most efficient way to navigate from A to B.
 	Simon Weckert is an artist and he has a fun project where he hacks Google’s traffic jam algorithm by walking on empty streets with a dozen phones using Google Maps directions. The result? More empty streets because Google reroutes other cars to avoid the jam.
 	The open climate science group has analyzed the past 2k years and this shows the development of global temperature in a visual way between year 1 and 2019 AD. It’s super interesting to see that how unusual the current period of the past 150–200 years is in terms of our planet’s climate. See, humans exists since way longer than 1AD so this visualization also shows us where the solution to fix climate change would be. We have to look back and adapt our lifestyle a bit. Local, following the year rythm, slower, and happier.
 
 
 Finally, let me recommend you the printed books of Cabins that are simple and serve the one purpose we all strive for: A space that spellbind us in its warmth and ingenious simplicity.
 If you want to support produce WDRL, contribute via PayPal or Stripe. Every small amount you pledge will help pay my costs. Thanks a lot!Anselm</content>
     </entry>
     <entry>
       <title>Issue closing stats for any repo</title>
         <link href="https://lea.verou.me/2019/12/issue-closing-stats-for-any-repo/"/>
       <updated>2019-12-13T23:05:38.000Z</updated>
       <content type="text">tl;dr: If you just want to quickly get stats for a repo, you can find the app here. The rest of this post explains how it’s built with Mavo HTML, CSS, and 0 lines of JS. Or, if you’d prefer, you can just View Source — it’s all there!
 
 
 
 The finished app we’re going to make, find it at https://leaverou.github.io/issue-closing
 
 
 
 One of the cool things about Mavo is how it enables one to quickly build apps that utilize the Github API. At some point I wanted to compute stats about how quickly (or rather, slowly…) Github issues are closed in the Mavo repo. And what better way to build this than a Mavo app? It was fairly easy to build a prototype for that.
 
 
 
 
 
 
 
 Displaying a list of the last 100 closed issues and the time it took to close them
 
 
 
 To render the last 100 closed issues in the Mavo app, I first looked up the appropriate API call in Github’s API documentation, then used it in the mv-source attribute on the Mavo root, i.e. the element with mv-app that encompasses everything in my app:
 
 
 
 &lt;div mv-app&#x3D;&quot;issueClosing&quot;
      mv-source&#x3D;&quot;https://api.github.com/repos/mavoweb/mavo/issues?state&#x3D;closed&amp;sort&#x3D;updated&amp;per_page&#x3D;100&quot;
      mv-mode&#x3D;&quot;read&quot;&gt;
 	&lt;!-- app here --&gt;
 &lt;/div&gt;
 
 
 
  Then, I displayed a list of these issues with:
 
 
 
 &lt;div mv-multiple property&#x3D;&quot;issue&quot;&gt;
 	&lt;a class&#x3D;&quot;issue-number&quot; href&#x3D;&quot;https://github.com/mavoweb/mavo/issues/[number]&quot; title&#x3D;&quot;[title]&quot; target&#x3D;&quot;_blank&quot;&gt;#[number]&lt;/a&gt;
 	took [closed_at - created_at] ms
 &lt;/div&gt;
 
 
 
 
   See the Pen 
   Step 1 – Issue Closing App Tutorial by Lea Verou (@leaverou)
   on CodePen.
 
 
 
 
 
 This would work, but the way it displays results is not very user friendly (e.g. “#542 took 149627000 ms”). We need to display the result in a more readable way.
 
 
 
 We can use the duration() function to display a readable duration such as “1 day”:
 
 
 
 &lt;div mv-multiple property&#x3D;&quot;issue&quot;&gt;
 	&lt;a class&#x3D;&quot;issue-number&quot; href&#x3D;&quot;https://github.com/mavoweb/mavo/issues/[number]&quot; title&#x3D;&quot;[title]&quot; target&#x3D;&quot;_blank&quot;&gt;#[number]&lt;/a&gt;
 	took [duration(closed_at - created_at)]
 &lt;/div&gt;
 
 
 
 
   See the Pen 
   Step 2 – Issue Closing App Tutorial by Lea Verou (@leaverou)
   on CodePen.
 
 
 
 
 
 Displaying aggregate statistics
 
 
 
 However, a list of issues is not very easy to process. What’s the overall picture? Does this repo close issues fast or not? Time for some statistics! We want to calculate average, median, minimum and maximum issue closing time. To calculate these statistics, we need to use the times we have displayed in the previous step.
 
 
 
 First, we need to give our calculation a name, so we can refer to its value in expressions:
 
 
 
 &lt;span property&#x3D;&quot;timeToClose&quot;&gt;[duration(closed_at - created_at)]&lt;/span&gt;
 
 
 
 However, as it currently stands, the value of this property is text (e.g. “1 day”, “2 months” etc). We cannot compute averages and medians on text! We need the property value to be a number. We can hide the actual raw value in an attribute and use the nicely formatted value as the visible content of the element, like so (we use the content attribute here but you can use any, e.g. a data-* attribute would work just as well):
 
 
 
 &lt;span property&#x3D;&quot;timeToClose&quot; mv-attribute&#x3D;&quot;content&quot; content&#x3D;&quot;[closed_at - created_at]&quot;&gt;[duration(timeToClose)]&lt;/span&gt;
 
 
 
 Note: There is a data formatting feature in the works which would simplify this kind of thing by allowing you to separate the raw value and its presentation without having to use separate attributes for them.
 
 
 
 We can also add a class to color it red, green, or black depending on whether the time is longer than a month, shorter than a day, or in-between respectively:
 
 
 
 &lt;span property&#x3D;&quot;timeToClose&quot; mv-attribute&#x3D;&quot;content&quot; content&#x3D;&quot;[closed_at - created_at]&quot; class&#x3D;&quot;[if(timeToClose &gt; month(), &#x27;long&#x27;, if (timeToClose &lt; day(), &#x27;short&#x27;))]&quot;&gt;[duration(timeToClose)]&lt;/span&gt;
 
 
 
 Now, on to calculate our statistics! We take advantage of the fact that timeToClose outside the issue collection gives us all the times, so we can compute aggregates on them. Therefore, the stats we want to calculate are simply average(timeToClose), median(timeToClose), min(timeToclose), and max(timeToClose). We put all these in a definition list:
 
 
 
 &lt;dl&gt;
 	&lt;dt&gt;Median&lt;/dt&gt;
 	&lt;dd&gt;[duration(median(timeToClose))]&lt;/dd&gt;
 	&lt;dt&gt;Average&lt;/dt&gt;
 	&lt;dd&gt;[duration(average(timeToClose))]&lt;/dd&gt;
 	&lt;dt&gt;Slowest&lt;/dt&gt;
 	&lt;dd&gt;[duration(max(timeToClose))]&lt;/dd&gt;
 	&lt;dt&gt;Fastest&lt;/dt&gt;
 	&lt;dd&gt;[duration(min(timeToClose))]&lt;/dd&gt;
 &lt;/dl&gt;
 
 
 
 
   See the Pen 
   Step 3 – Issue Closing App Tutorial by Lea Verou (@leaverou)
   on CodePen.
 
 
 
 
 
 Making repo a variable
 
 
 
 Now that all the functionality of my app was in place, I realized this could be useful for more repos as well. Why not make the repo a property that can be changed? So I added an input for specifying the repo: &lt;input property&#x3D;&quot;repo&quot; mv-default&#x3D;&quot;mavoweb/mavo&quot;&gt; and then replaced mavoweb/mavo with [repo] everywhere else, i.e. mv-source became https://api.github.com/repos/[repo]/issues?state&#x3D;closed&amp;sort&#x3D;updated&amp;per_page&#x3D;100.
 
 
 
 Avoid reload on every keystroke
 
 
 
 This worked, but since Mavo properties are reactive, it kept trying to reload data with every single keystroke, which was annoying and wasteful. Therefore, I needed to do a bit more work so that there is a definite action that submits the change. Enter Mavo Actions!
 
 
 
 I created two properties: repo for the actual repo and repoInput for the input. repoInput still changes on every keystroke, but it’s repo that is actually being used in the app. I wrapped the input with a &lt;form&gt; and added an action on the form that does this (mv-action&#x3D;&quot;set(repo, repoInput)&quot;). I also added a submit button. Since Mavo actions on forms are triggered when the form is submitted, it doesn’t matter if I press Enter on the input, or click the Submit button, both work.
 
 
 
 Setting the repo via a URL parameter
 
 
 
 Eventually I also wanted to be able to set the repo from the URL, so I also added a hidden repoDefault property: &lt;meta property&#x3D;&quot;repoDefault&quot; content&#x3D;&quot;[url(&#x27;repo&#x27;) or &#x27;mavoweb/mavo&#x27;]&quot;&gt;, and then changed the hardcoded mv-default&#x3D;&quot;mavoweb/mavo&quot; to mv-default&#x3D;&quot;[repoDefault]&quot; on both the repo and the repoInput properties. That way one can link to stats for a specific repo, e.g. https://leaverou.github.io/issue-closing/?repo&#x3D;prismjs/prism
 
 
 
 Why a repoDefault property and not just mv-default&#x3D;&quot;[url(&#x27;repo&#x27;) or &#x27;mavoweb/mavo&#x27;]? Just keeping things DRY and avoiding having to repeat the same expression twice.
 
 
 
 
   See the Pen 
   Step 5 – Issue Closing App Tutorial by Lea Verou (@leaverou)
   on CodePen.
 
 
 
 
 
 Filtering by label
 
 
 
 At some point I wondered: What would the issue closing times be if we only counted bugs? What if we only counted enhancements? Surely these would be different: When looking at issue closing times for a repo, one primarily cares about how fast bugs are fixed, not how quickly every random feature suggestion is implemented. Wouldn’t it be cool to also have a label filter?
 
 
 
 For that, I added a series of radio buttons:
 
 
 
 Show:
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; property&#x3D;&quot;labels&quot; name&#x3D;&quot;labels&quot; checked value&#x3D;&quot;&quot;&gt; All&lt;/label&gt;
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;bug&quot;&gt; Bugs only&lt;/label&gt;
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;enhancement&quot;&gt; Enhancements only&lt;/label&gt;
 
 
 
 Then, I modified mv-source to also use this value in its API call: mv-source&#x3D;&quot;https://api.github.com/repos/[repo]/issues?state&#x3D;closed&amp;sort&#x3D;updated&amp;labels&#x3D;[labels]&amp;per_page&#x3D;100&quot;.
 
 
 
 Note that when turning radio buttons into a Mavo property you only use the property attribute on the first one. This is important because Mavo has special handling when you use the property attribute with the same name multiple times in the same group, which we don’t want here. You can add the property attribute on any of the radio buttons, it doesn’t have to be the first. Just make sure it’s only one of them.
 
 
 
 Then I became greedy: Why not also allow filtering by custom labels too? So I added another radio with an input:
 
 
 
 Show:
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; property&#x3D;&quot;labels&quot; name&#x3D;&quot;labels&quot; checked value&#x3D;&quot;&quot;&gt; All&lt;/label&gt;
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;bug&quot;&gt; Bugs only&lt;/label&gt;
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;enhancement&quot;&gt; Enhancements only&lt;/label&gt;
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;[customLabel]&quot;&gt; Label &lt;input property&#x3D;&quot;customLabel&quot;&gt;&lt;/label&gt;
 
 
 
 Note that since this is a text field, when the last value is selected, we’d have the same problem as we did with the repo input: Every keystroke would fire a new request. We can solve this in the same way as we solved it for the repo property, by having an intermediate property and only setting labels when the form is actually submitted:
 
 
 
 Show:
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; property&#x3D;&quot;labelFilter&quot; name&#x3D;&quot;labels&quot; checked value&#x3D;&quot;&quot;&gt; All&lt;/label&gt;
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;bug&quot;&gt; Bugs only&lt;/label&gt;
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;enhancement&quot;&gt; Enhancements only&lt;/label&gt;
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;[customLabel]&quot;&gt; Label &lt;input property&#x3D;&quot;customLabel&quot;&gt;&lt;/label&gt;
 &lt;meta property&#x3D;&quot;labels&quot; content&#x3D;&quot;&quot;&gt;
 
 
 
 Adding label autocomplete
 
 
 
 Since we now allow filtering by a custom label, wouldn’t it be cool to allow autocomplete too? HTML allows us to offer autocomplete in our forms via &lt;datalist&gt; and we can use Mavo to populate the contents!
 
 
 
 First, we add a &lt;datalist&gt; and link it with our custom label input, like so:
 
 
 
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;[customLabel]&quot;&gt; Label &lt;input property&#x3D;&quot;customLabel&quot; list&#x3D;&quot;label-suggestions&quot;&gt;&lt;/label&gt;
 &lt;datalist id&#x3D;&quot;label-suggestions&quot;&gt;
 &lt;/datalist&gt;
 
 
 
 Currently, our suggestion list is empty. How do we populate it with the labels that have actually been used in this repo? Looking at the API documentation, we see that each returned issue has a labels field with its labels as an object, and each of these objects has a name field with the textual label. This means that if we use issue.labels.name in Mavo outside of the issues collection, we get a list with all of these values, which we can then use to populate our &lt;datalist&gt; by passing it on to mv-value which allows us to create dynamic collections:
 
 
 
 &lt;label&gt;&lt;input type&#x3D;&quot;radio&quot; name&#x3D;&quot;labels&quot; value&#x3D;&quot;[customLabel]&quot;&gt; Label &lt;input property&#x3D;&quot;customLabel&quot; list&#x3D;&quot;label-suggestions&quot;&gt;&lt;/label&gt;
 &lt;datalist id&#x3D;&quot;label-suggestions&quot;&gt;
 	&lt;option mv-multiple mv-value&#x3D;&quot;unique(issue.labels.name)&quot;&gt;&lt;/option&gt;
 &lt;/datalist&gt;
 
 
 
 Note that we also used unique() to eliminate duplicates, since otherwise each label would appear as many times as it is used.
 
 
 
 
   See the Pen 
   Issue Closing App – Tutorial Step 6 by Lea Verou (@leaverou)
   on CodePen.
 
 
 
 
 
 Adding a visual summary graphic
 
 
 
 Now that we got the functionality down, we can be a little playful and add some visual flourish. How about a bar chart that summarizes the proportion of long vs short vs normal closing times? We start by setting the CSS variables we are going to need for our graphic, i.e. the number of issues in each category:
 
 
 
 &lt;summary style&#x3D;&quot;--short: [count(timeToClose &lt; day())]; --long: [count(timeToClose &gt; month())]; --total: [count(issue)];&quot;&gt;
 	Based on [count(issue)] most recently updated issues
 &lt;/summary&gt;
 
 
 
 Then, we draw our graphic:
 
 
 
 summary::before {
 	content: &quot;&quot;;
 	position: fixed;
 	bottom: 0;
 	left: 0;
 	right: 0;
 	z-index: 1;
 	height: 5px;
 	background: linear-gradient(to right, var(--short-color) calc(var(--short, 0) / var(--total) * 100%), hsl(220, 10%, 75%) 0, hsl(220, 10%, 75%) calc(100% - var(--long, 0) / var(--total) * 100%), var(--long-color) 0) bottom / auto 100% no-repeat border-box;
 }
 
 
 
 Now, wouldn’t it be cool to also show a small pie chart next to the heading, if conic gradients are supported so we can draw it? The color stops would be the same, so we define a --summary-stops variable on summary, so we can reuse them across both gradients:
 
 
 
 summary {
 	--summary-stops: var(--short-color) calc(var(--short, 0) / var(--total) * 100%), hsl(220, 10%, 75%) 0, hsl(220, 10%, 75%) calc(100% - var(--long, 0) / var(--total) * 100%), var(--long-color) 0;
 }
 
 	summary::before {
 		content: &quot;&quot;;
 		position: fixed;
 		bottom: 0;
 		left: 0;
 		right: 0;
 		z-index: 1;
 		height: 5px;
 		background: linear-gradient(to right, var(--summary-stops)) bottom / auto 100% no-repeat border-box;
 	}
 
 	@supports (background: conic-gradient(red, red)) {
 		summary::after {
 			content: &quot;&quot;;
 			display: inline-block;
 			vertical-align: middle;
 			width: 1.2em;
 			height: 1.2em;
 			margin-left: .3em;
 			border-radius: 50%;
 			background: conic-gradient(var(--summary-stops));
 		}
 	}
 
 
 
 
   See the Pen 
   Issue Closing App – Tutorial Step 7 by Lea Verou (@leaverou)
   on CodePen.
 
 </content>
     </entry>
     <entry>
       <title>Utility: Convert SVG path to all-relative or all-absolute commands</title>
         <link href="https://lea.verou.me/2019/05/utility-convert-svg-path-to-all-relative-or-all-absolute-commands/"/>
       <updated>2019-05-06T10:19:19.000Z</updated>
       <content type="text">I like hand-editing my SVGs. Often I will create an initial version in Illustrator, and then export and continue with hand editing. Not only is it a bit of a meditative experience and it satisfies my obsessive-compulsive tendencies to clean up the code, it has actual practical benefits when you need to make certain changes or introduce animation. Some things are easier to do in a GUI, and others are easier to do in code, and I like having the flexibility to pick which one fits my use case best.
 
 
 
 However, there was always a thing that was a PITA: modifying paths. Usually if I need anything more complicated than just moving them, I’d do it in Illustrator, but even moving them can be painful if they are not all relative (and no, I don’t like introducing pointless transforms for things that should really be in the d attribute). 
 
 
 
 For example, this was today’s result of trying to move an exported “a” glyph from Raleway Bold by modifying its first M command:
 
 
 
 Trying to move a path by changing its first M command when not all of its commands are relative.
 
 
 
 This happened because even though most commands were exported as relative, several were not and I had not noticed. I have no idea why some commands were exported as absolute, it seems kind of random.
 
 
 
 When all commands are relative, moving a path is as simple as manipulating its initial M command and the rest just adapts, because that’s the whole point of relative commands. Same with manipulating every other part of the path, the rest of it just adapts. It’s beautiful. I honestly have no idea why anybody would favor absolute commands. And yet, googling “convert SVG path to relative” yields one result, whereas there are plenty of results about converting paths to absolute. No idea why that’s even desirable, ever (?).
 
 
 
 I remembered I had come across that result before. Thankfully, there’s also a fiddle to go with it, which I had used in the past to convert my path. I love it, it uses this library called Snap.svg which supports converting paths to relative as a just-add-water utility method. However, that fiddle is a quick demo to answer a StackOverflow question, so the UI is not super pleasant to use (there is no UI: you just manipulate the path in the SVG and wait for the fiddle to run). This time around, I needed to convert multiple paths, so I needed a more efficient UI.
 
 
 
 So I created this demo which is also based on Snap.svg, but has a slightly more efficient UI. You just paste your path in a textarea and it both displays it and instantly converts it to all-relative and all-absolute paths (also using Snap.svg). It also displays both your original path and the two converted ones, so you can make sure they still look the same. It even follows a pending-delete pattern so you can just focus on the output textarea and hit Cmd-C in one fell swoop.
 
 
 
 
 
 
 
 I wasn’t sure about posting this or just tweeting it (it literally took less than 30 minutes — including this blog post — and I tend to only post small things like that on my twitter), but I thought it might be useful to others googling the same thing, so I may as well post it here for posterity. Enjoy!
 
 
 
 
 
 
 
 </content>
     </entry>
     <entry>
       <title>ReferenceError: x is not defined?</title>
         <link href="https://lea.verou.me/2018/12/referenceerror-x-is-not-defined/"/>
       <updated>2018-12-14T05:46:25.000Z</updated>
       <content type="text">Today for a bit of code I was writing, I needed to be able to distinguish “x is not defined” ReferenceErrors from any other error within a try...catch block and handle them differently.
 
 
 
 Now I know what you’re thinking. Trying to figure out exactly what kind of error you have programmatically is a well-known fool’s errand. If you express a desire to engage in such a risky endeavor, any JS veteran in sight will shake their head in remembrance of their early days, but have the wisdom to refrain from trying to convince you otherwise; they know that failing will teach you what it taught them when they were young and foolish enough to attempt such a thing.
 
 
 
 Despite writing JS for 13 years, today I was feeling adventurous. “But what if, just this once, I could get it to work? It’s a pretty standard error message! What if I tested in so many browsers that I would be confident I’ve covered all cases?”
 
 
 
 I made a simple page on my server that just prints out the error message written in a way that would maximize older browser coverage. Armed with that, I started visiting every browser in my BrowserStack account. Here are my findings for anyone interested:
 
 
 
 Chrome (all versions, including mobile): x is not definedFirefox (all versions, including mobile): x is not definedSafari 4-12 : Can&#x27;t find variable: xEdge (16 – 18): &#x27;x&#x27; is not definedEdge 15: &#x27;x&#x27; is undefinedIE6-11 and Windows Phone IE: &#x27;x&#x27; is undefinedUC Browser (all versions): x is not definedSamsung browser (all versions): x is not definedOpera Mini and Pre-Chromium Opera: Undefined variable: x
 
 
 
 Even if you, dear reader, are wise enough to never try and detect this error, I thought you may find the variety (or lack thereof) above interesting.
 
 
 
 I also did a little bit of testing with a different UI language (I picked Greek), but it didn’t seem to localize the error messages. If you’re using a different UI language, please open the page above and if the message is not in English, let me know!
 
 
 
 In the end, I decided to go ahead with it, and time will tell if it was foolish to do so. For anyone wishing to also dabble in such dangerous waters, this was my checking code:
 
 
 
 if (e instanceof ReferenceError 
     &amp;&amp; /is (not |un)defined$|^(Can&#x27;t find|Undefined) variable/.test(e.message)) {
     // do stuff
 }
 
 
 
 Found any cases I missed? Or perhaps you found a different ReferenceError that would erroneously match the regex above? Let me know in the comments!
 
 
 
 One thing that’s important to note is that even if the code above is bulletproof for today’s browser landscape, the more developers that do things like this, the harder it is for browser makers to improve these error messages. However, until there’s a better way to do this, pointing fingers at developers for wanting to do perfectly reasonable things, is not the solution. This is why HTTP has status codes, so we don’t have to string match on the text. Imagine having to string match “Not Found” to figure out if a request was found or not! Similarly, many other technologies have error codes, so that different types of errors can be distinguished without resulting to flimsy string matching. I’m hoping that one day JS will also have a better way to distinguish errors more precisely than the general error categories of today, and we’ll look back to posts like this with a nostalgic smile, being so glad we don’t have to do crap like this ever again.</content>
     </entry>
     <entry>
       <title>Refresh CSS Bookmarklet v2</title>
         <link href="https://lea.verou.me/2018/09/refresh-css-bookmarklet-v2/"/>
       <updated>2018-09-18T19:07:40.000Z</updated>
       <content type="text">Almost 11 years ago, Paul Irish posted this brilliant bookmarklet to refresh all stylesheets on the current page. Despite the amount of tools, plugins, servers to live reload that have been released over the years, I’ve always kept coming back to it. It’s incredibly elegant in its simplicity. It works everywhere: locally or remotely, on any domain and protocol. No need to set up anything, no need to alter my process in any way, no need to use a specific local server or tool. It quietly just accepts your preferences and workflow instead of trying to change them. Sure, it doesn’t automatically detect changes and reload, but in most cases, I don’t want it to.
 I’ve been using this almost daily for a decade and there’s always been one thing that bothered me: It doesn’t work with iframes. If the stylesheet you’re editing is inside an iframe, tough luck. If you can open the frame in a new tab, that works, but often that’s nontrivial (e.g. the frame is dynamically generated). After dealing with this issue today once more, I thought “this is just a few lines of JS, why not fix it?”.
 The first step was to get Paul’s code in a readable format, since the bookmarklet is heavily minified:
 (function() {
 	var links &#x3D; document.getElementsByTagName(&#x27;link&#x27;);
 	for (var i &#x3D; 0; i &lt; links.length; i++) {
 		var link &#x3D; links[i];
 		if (link.rel.toLowerCase().match(/stylesheet/) &amp;&amp; link.href) {
 			var href &#x3D; link.href.replace(/(&amp;|%5C?)forceReload&#x3D;\d+/, &#x27;&#x27;);
 			link.href &#x3D; href + (href.match(/\?/) ? &#x27;&amp;&#x27; : &#x27;?&#x27;) + &#x27;forceReload&#x3D;&#x27; + (new Date().valueOf())
 		}
 	}
 })()
 Once I did that, it became obvious to me that this could be shortened a lot; the last 10 years have been wonderful for JS evolution!
 (()&#x3D;&gt;{
 	for (let link of Array.from(document.querySelectorAll(&quot;link[rel&#x3D;stylesheet][href]&quot;))) {
 		var href &#x3D; new URL(link.href, location);
 		href.searchParams.set(&quot;forceReload&quot;, Date.now());
 		link.href &#x3D; href;
 	}
 })()
 Sure, this reduces browser support a bit (most notably it excludes IE11), but since this is a local development tool, that’s not such a big problem.
 Now, let’s extend this to support iframes as well:
 {
 	let $$ &#x3D; (selector, root &#x3D; document) &#x3D;&gt; Array.from(root.querySelectorAll(selector));
 	
 	let refresh &#x3D; (document) &#x3D;&gt; {
 		for (let link of $$(&quot;link[rel&#x3D;stylesheet][href]&quot;, document)) {
 			let href &#x3D; new URL(link.href);
 			href.searchParams.set(&quot;forceReload&quot;, Date.now());
 			link.href &#x3D; href;
 		}
 
 		for (let iframe of $$(&quot;iframe&quot;, document)) {
 			iframe.contentDocument &amp;&amp; refresh(iframe.contentDocument);
 		}
 	}
 
 	refresh();
 }
 That’s it! Do keep in mind that this will not work with cross-origin iframes, but then again, you probably don’t expect it to in that case.
 Now all we need to do to turn it into a bookmarklet is to prepend it with javascript: and minify the code. Here you go:
 Refresh CSS
 Hope this is useful to someone else as well 
 Any improvements are always welcome!
 Credits
 
 Paul Irish, for the original bookmarklet
 Maurício Kishi, for making the iframe traversal recursive (comment)
 </content>
     </entry>
     <entry>
       <title>Easy Dynamic Regular Expressions with Tagged Template Literals and Proxies</title>
         <link href="https://lea.verou.me/2018/06/easy-dynamic-regular-expressions-with-tagged-template-literals-and-proxies/"/>
       <updated>2018-06-04T19:47:55.000Z</updated>
       <content type="text">If you use regular expressions a lot, you probably also create them from existing strings that you first need to escape in case they contain special characters that need to be matched literally, like $ or +. Usually, a helper function is defined (hopefully this will soon change as RegExp.escape() is coming!) that basically looks like this:
 var escapeRegExp &#x3D; s &#x3D;&gt; s.replace(/[-\/\\^$*+?.()|[\]{}]/g, &quot;\\$&amp;&quot;);
 and then regexps are created by escaping the static strings and concatenating them with the rest of the regex like this:
 var regex &#x3D; RegExp(escapeRegExp(start) + &#x27;([\\S\\s]+?)&#x27; + escapeRegExp(end), &quot;gi&quot;)
 or, with ES6 template literals, like this:
 var regex &#x3D; RegExp(&#x60;${escapeRegExp(start)}([\\S\\s]+?)${escapeRegExp(end)}&#x60;, &quot;gi&quot;)
 (In case you were wondering, this regex is taken directly from the Mavo source code)
 Isn’t this horribly verbose? What if we could define a regex with just a template literal (&#x60;${start}([\\S\\s]+?)${end}&#x60; for the regex above) and it just worked? Well, it turns out we can! If you haven’t seen tagged template literals before, I suggest you click that MDN link and read up. Basically, you can prepend an ES6 template literal with a reference to a function and the function accepts the static parts of the string and the dynamic parts separately, allowing you to operate on them!
 So, what if we defined such a function that returns a RegExp object and escapes the dynamic parts? Let’s try to do that:
 var regexp &#x3D; (strings, ...values) &#x3D;&gt; {
 	return RegExp(strings[0] + values.map((v, i) &#x3D;&gt; escapeRegExp(v) + strings[i+1]).join(&quot;&quot;))
 };
 And now we can try it in the console:
 &gt; regexp&#x60;^${&#x27;/*&#x27;}([\\S\\s]+?)${&#x27;*/&#x27;}&#x60;;
 &lt; /^\/\*([\S\s]+?)\*\//
 Won’t somebody, please, think of the flags?!
 This is all fine and dandy, but how do we specify flags? Note that the original regexp had flags (“gi”). The tagged template syntax doesn’t really allow us to pass in any additional parameters. However, thanks to functions being first-class objects in JS, we can have a function that takes the flags in as parameters and returns a function that generates regexps with the right flags:
 var regexp &#x3D; flags &#x3D;&gt; {
 	return (strings, ...values) &#x3D;&gt; {
 		var pattern &#x3D; strings[0] + values.map((v, i) &#x3D;&gt; escapeRegExp(v) + strings[i+1]).join(&quot;&quot;)
 		return RegExp(pattern, flags);
 	}
 };
 And now we can try it in the console:
 &gt; regexp(&quot;gi&quot;)&#x60;^${&#x27;/*&#x27;}([\\S\\s]+?)${&#x27;*/&#x27;}&#x60;;
 &lt; /^\/\*([\S\s]+?)\*\//gi
 This works nice, but now even if we don’t want any flags, we can’t use the nice simple syntax we had earlier, we need to include a pair of empty parens:
 &gt; regexp()&#x60;^${&#x27;/*&#x27;}([\\S\\s]+?)${&#x27;*/&#x27;}&#x60;;
 &lt; /^\/\*([\S\s]+?)\*\//
 Can we have our cake and eat it too? Can we have the short parenthesis-less syntax when we have no flags, and still be able to specify flags? Of course! We can check the arguments we have and either return a function, or call the function. If our function is used as a tag, the first argument will be an array (thanks Roman!). If we’re expecting it to return a function, the first argument would be a string: the flags. So, let’s try this approach!
 var regexp &#x3D; (...args) &#x3D;&gt; {
 	var ret &#x3D; (flags, strings, ...values) &#x3D;&gt; {
 		var pattern &#x3D; strings[0] + values.map((v, i) &#x3D;&gt; escapeRegExp(v) + strings[i+1]).join(&quot;&quot;);
 		return RegExp(pattern, flags);
 	};
 
 	if (Array.isArray(args[0])) {
 		// Used as a template tag
 		return ret(&quot;&quot;, ...args);
 	}
 
 	return ret.bind(undefined, args[0]);
 };
 And now we can try it in the console and verify that both syntaxes work:
 &gt; regexp(&quot;gi&quot;)&#x60;^${&#x27;/*&#x27;}([\\S\\s]+?)${&#x27;*/&#x27;}&#x60;;
 &lt; /^\/\*([\S\s]+?)\*\//gi
 regexp&#x60;^${&#x27;/*&#x27;}([\\S\\s]+?)${&#x27;*/&#x27;}&#x60;;
 &lt; /^\/\*([\S\s]+?)\*\//
 Even nicer syntax, with proxies!
 Is there a better way? If this is not super critical for performance, we could use proxies to return the right function with a template tag like regexp.gi, no parentheses or quotes needed and the code is actually shorter too:
 var _regexp &#x3D; (flags, strings, ...values) &#x3D;&gt; {
 	var pattern &#x3D; strings[0] + values.map((v, i) &#x3D;&gt; escapeRegExp(v) + strings[i+1]).join(&quot;&quot;);
 	return RegExp(pattern, flags);
 };
 var regexp &#x3D; new Proxy(_regexp.bind(undefined, &quot;&quot;), {
 	get: (t, property) &#x3D;&gt; _regexp.bind(undefined, property)
 });
 And now we can try it in the console, both with and without flags!
 &gt; regexp.gi&#x60;^${&#x27;/*&#x27;}([\\S\\s]+?)${&#x27;*/&#x27;}&#x60;;
 &lt; /^\/\*([\S\s]+?)\*\//gi
 &gt; regexp&#x60;^${&#x27;/*&#x27;}([\\S\\s]+?)${&#x27;*/&#x27;}&#x60;;
 &lt; /^\/\*([\S\s]+?)\*\//
 That’s some beauty right there! ?
 PS: If you liked this, take a look at this mini-library by Dr. Axel Rauschmayer that uses a similar idea and turns it into a library that does more than just escaping strings (different syntax for flags though, they become part of the template string, like in PHP)</content>
     </entry>
     <entry>
       <title>Never forget type&#x3D;”button” on generated buttons!</title>
         <link href="https://lea.verou.me/2018/05/never-forget-typebutton-on-generated-buttons/"/>
       <updated>2018-05-19T21:40:20.000Z</updated>
       <content type="text"> I just dealt with one of the weirdest bugs and thought you may find it amusing too.
 In one of my slides for my upcoming talk “Even More CSS Secrets”, I had a Mavo app on a &lt;form&gt;, and the app included a collection to quickly create a UI to manage pairs of values for something I wanted to calculate in one of my live demos. A Mavo collection is a repeatable HTML element with affordances to add items, delete items, move items etc. Many of these affordances are implemented via &lt;button&gt; elements generated by Mavo.
 Normally, hitting Enter inside a text field within a collection adds a new item, as one would expect. However, I noticed that when I hit Enter inside any item, not only no item was added, but an item was being deleted, with the usual “Item deleted [Undo]” UI and everything!
 At first I thought it was a bug with the part of Mavo code that adds items on Enter and deletes empty items on backspace, so I commented that out. Nope, still happening. I was already very puzzled, since I couldn’t remember any other part of the codebase that deletes items in response to keyboard events.
 So, I added breakpoints on the delete(item) method of Mavo.Collection to inspect the call stack and see how execution got there. Turned out, it got there via a normal …click event on the actual delete button! What fresh hell was this? I never clicked any delete button!
 And then it dawned on me: &lt;button&gt; elements with no type attribute set are submit buttons by default! Quote from spec: The missing value default and invalid value default are the Submit Button state.. This makes no difference in most cases, UNLESS you’re inside a form. The delete button of the first item had been turned into the de facto default submit button just because it was the first button in that form and it had no type!
 I also remembered that regardless of how you submit a form (e.g. by hitting Enter on a single-line text field) it also fires a click event on the default submit button, because people often listen to that instead of the form’s submit event. Ironically, I was cancelling the form’s submit event in my code, but it still generated that fake click event, making it even harder to track down as no form submission was actually happening.
 The solution was of course to go through every part of the Mavo code that generates buttons and add type&#x3D;”button” to them. I would recommend this to everyone who is writing libraries that will operate in unfamiliar HTML code. Most of the time a type-less &lt;button&gt; will work just fine, but when it doesn’t, things get really weird.</content>
     </entry>
 
 </feed>